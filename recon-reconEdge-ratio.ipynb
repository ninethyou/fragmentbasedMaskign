{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cbc95c-8c82-4b39-9acb-eac3d7dea845",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:0', 'task_name': 'freesolv', 'model_type': 'gin', 'model': {'num_layer': 3, 'emb_dim': 64, 'feat_dim': 64, 'drop_ratio': 0.5, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 777, 'task': 'regression', 'data_path': 'data/freesolv/freesolv.csv', 'target': ['expt']}}\n",
      "Running on: cuda:0\n",
      "641\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/641\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 22.7620906829834\n",
      "Validation loss: 67.77978515625 RMSE: 8.232848\n",
      "Validation loss: 61.74009895324707 RMSE: 7.857487\n",
      "Validation loss: 53.0878963470459 RMSE: 7.2861443\n",
      "3 2 12.752490997314453\n",
      "Validation loss: 39.333699226379395 RMSE: 6.271658\n",
      "Validation loss: 15.961605548858643 RMSE: 3.9951978\n",
      "Validation loss: 12.691813945770264 RMSE: 3.5625572\n",
      "6 4 5.116441249847412\n",
      "Validation loss: 13.046525478363037 RMSE: 3.6119971\n",
      "Validation loss: 13.552494049072266 RMSE: 3.6813715\n",
      "Validation loss: 12.62365436553955 RMSE: 3.552978\n",
      "9 6 6.017263889312744\n",
      "Validation loss: 10.99582576751709 RMSE: 3.3159952\n",
      "Validation loss: 13.938427925109863 RMSE: 3.7334204\n",
      "Validation loss: 11.831789493560791 RMSE: 3.4397368\n",
      "12 8 5.170060157775879\n",
      "Validation loss: 12.13125228881836 RMSE: 3.4829948\n",
      "Validation loss: 12.89212417602539 RMSE: 3.5905607\n",
      "Validation loss: 13.410550117492676 RMSE: 3.662042\n",
      "15 10 5.214682579040527\n",
      "Validation loss: 12.063694953918457 RMSE: 3.473283\n",
      "Validation loss: 11.459219455718994 RMSE: 3.385147\n",
      "Validation loss: 11.674052715301514 RMSE: 3.416731\n",
      "18 12 5.60288667678833\n",
      "Validation loss: 10.528656005859375 RMSE: 3.2447891\n",
      "Validation loss: 11.901696681976318 RMSE: 3.4498835\n",
      "Validation loss: 13.520530223846436 RMSE: 3.6770272\n",
      "21 14 4.306913375854492\n",
      "Validation loss: 10.499156475067139 RMSE: 3.2402399\n",
      "Validation loss: 11.420647621154785 RMSE: 3.3794448\n",
      "Validation loss: 11.433465003967285 RMSE: 3.3813405\n",
      "Validation loss: 14.86410140991211 RMSE: 3.855399\n",
      "25 0 5.1048197746276855\n",
      "Validation loss: 10.634771347045898 RMSE: 3.2610998\n",
      "Validation loss: 11.725927352905273 RMSE: 3.4243145\n",
      "Validation loss: 11.44421100616455 RMSE: 3.3829296\n",
      "28 2 3.9682118892669678\n",
      "Validation loss: 10.92754602432251 RMSE: 3.3056839\n",
      "Validation loss: 11.370304584503174 RMSE: 3.371988\n",
      "Validation loss: 10.38930368423462 RMSE: 3.2232444\n",
      "31 4 4.546786308288574\n",
      "Validation loss: 15.833233833312988 RMSE: 3.9790998\n",
      "Validation loss: 12.274673461914062 RMSE: 3.5035229\n",
      "Validation loss: 15.33194351196289 RMSE: 3.915603\n",
      "34 6 6.234127998352051\n",
      "Validation loss: 12.022583961486816 RMSE: 3.46736\n",
      "Validation loss: 14.637728691101074 RMSE: 3.8259287\n",
      "Validation loss: 13.12629747390747 RMSE: 3.6230235\n",
      "37 8 3.4500675201416016\n",
      "Validation loss: 11.37066650390625 RMSE: 3.372042\n",
      "Validation loss: 12.496332168579102 RMSE: 3.535015\n",
      "Validation loss: 11.672089576721191 RMSE: 3.4164438\n",
      "40 10 4.599148273468018\n",
      "Validation loss: 10.392420291900635 RMSE: 3.2237277\n",
      "Validation loss: 12.066607475280762 RMSE: 3.4737024\n",
      "Validation loss: 11.48840045928955 RMSE: 3.3894544\n",
      "43 12 2.614635944366455\n",
      "Validation loss: 12.522250652313232 RMSE: 3.5386786\n",
      "Validation loss: 12.283107280731201 RMSE: 3.5047264\n",
      "Validation loss: 10.652396202087402 RMSE: 3.2638009\n",
      "46 14 2.1850578784942627\n",
      "Validation loss: 11.139851570129395 RMSE: 3.3376417\n",
      "Validation loss: 11.556519508361816 RMSE: 3.3994882\n",
      "Validation loss: 9.670757293701172 RMSE: 3.1097841\n",
      "Validation loss: 10.499416828155518 RMSE: 3.2402804\n",
      "50 0 4.2523908615112305\n",
      "Validation loss: 9.379145860671997 RMSE: 3.062539\n",
      "Validation loss: 18.156872749328613 RMSE: 4.261088\n",
      "Validation loss: 11.36678409576416 RMSE: 3.3714664\n",
      "53 2 1.9749912023544312\n",
      "Validation loss: 11.963517665863037 RMSE: 3.4588318\n",
      "Validation loss: 14.97169017791748 RMSE: 3.8693268\n",
      "Validation loss: 11.713829040527344 RMSE: 3.4225473\n",
      "56 4 3.2730607986450195\n",
      "Validation loss: 11.846823692321777 RMSE: 3.4419215\n",
      "Validation loss: 11.709856510162354 RMSE: 3.4219668\n",
      "Validation loss: 13.819784164428711 RMSE: 3.717497\n",
      "59 6 3.9429898262023926\n",
      "Validation loss: 8.587730884552002 RMSE: 2.9304833\n",
      "Validation loss: 12.989001750946045 RMSE: 3.604026\n",
      "Validation loss: 13.414612770080566 RMSE: 3.662597\n",
      "62 8 2.3357012271881104\n",
      "Validation loss: 14.13440990447998 RMSE: 3.7595756\n",
      "Validation loss: 15.35030460357666 RMSE: 3.9179466\n",
      "Validation loss: 10.815638065338135 RMSE: 3.2887137\n",
      "65 10 2.198998212814331\n",
      "Validation loss: 11.208430528640747 RMSE: 3.3478992\n",
      "Validation loss: 14.906593561172485 RMSE: 3.8609056\n",
      "Validation loss: 12.573121786117554 RMSE: 3.5458598\n",
      "68 12 1.9632868766784668\n",
      "Validation loss: 12.591163635253906 RMSE: 3.548403\n",
      "Validation loss: 12.959352493286133 RMSE: 3.5999103\n",
      "Validation loss: 17.805946826934814 RMSE: 4.2197094\n",
      "71 14 1.7699679136276245\n",
      "Validation loss: 10.979219436645508 RMSE: 3.3134906\n",
      "Validation loss: 12.86627197265625 RMSE: 3.5869586\n",
      "Validation loss: 13.675334930419922 RMSE: 3.6980174\n",
      "Validation loss: 10.905096054077148 RMSE: 3.3022864\n",
      "75 0 1.8422564268112183\n",
      "Validation loss: 12.980527877807617 RMSE: 3.6028497\n",
      "Validation loss: 13.436774730682373 RMSE: 3.6656208\n",
      "Validation loss: 11.714603424072266 RMSE: 3.4226604\n",
      "78 2 2.0872135162353516\n",
      "Validation loss: 12.552421569824219 RMSE: 3.54294\n",
      "Validation loss: 11.56762170791626 RMSE: 3.4011207\n",
      "Validation loss: 9.817071437835693 RMSE: 3.1332207\n",
      "81 4 2.3236806392669678\n",
      "Validation loss: 13.850953102111816 RMSE: 3.7216868\n",
      "Validation loss: 9.80313491821289 RMSE: 3.1309958\n",
      "Validation loss: 15.03345775604248 RMSE: 3.8773007\n",
      "84 6 2.5654144287109375\n",
      "Validation loss: 16.394572257995605 RMSE: 4.0490212\n",
      "Validation loss: 12.055886268615723 RMSE: 3.4721587\n",
      "Validation loss: 11.64288330078125 RMSE: 3.4121668\n",
      "87 8 2.3360917568206787\n",
      "Validation loss: 10.3240647315979 RMSE: 3.2131085\n",
      "Validation loss: 10.01294994354248 RMSE: 3.1643245\n",
      "Validation loss: 12.172083377838135 RMSE: 3.488851\n",
      "90 10 2.6725542545318604\n",
      "Validation loss: 12.17090892791748 RMSE: 3.4886832\n",
      "Validation loss: 11.294510841369629 RMSE: 3.3607302\n",
      "Validation loss: 11.849560737609863 RMSE: 3.4423192\n",
      "93 12 2.2414050102233887\n",
      "Validation loss: 16.582485675811768 RMSE: 4.07216\n",
      "Validation loss: 12.02762746810913 RMSE: 3.4680874\n",
      "Validation loss: 13.790257453918457 RMSE: 3.7135236\n",
      "96 14 2.3805391788482666\n",
      "Validation loss: 13.54710054397583 RMSE: 3.6806386\n",
      "Validation loss: 15.078312873840332 RMSE: 3.8830805\n",
      "Validation loss: 13.667517900466919 RMSE: 3.6969607\n",
      "Validation loss: 12.23952341079712 RMSE: 3.498503\n",
      "100 0 2.490630865097046\n",
      "Validation loss: 16.001293182373047 RMSE: 4.0001616\n",
      "Validation loss: 14.09700345993042 RMSE: 3.7545974\n",
      "Validation loss: 13.73324728012085 RMSE: 3.7058394\n",
      "103 2 1.994907021522522\n",
      "Validation loss: 14.140492916107178 RMSE: 3.7603846\n",
      "Validation loss: 11.78598690032959 RMSE: 3.4330726\n",
      "Validation loss: 12.6247718334198 RMSE: 3.5531354\n",
      "106 4 6.473537921905518\n",
      "Validation loss: 18.33113670349121 RMSE: 4.2814875\n",
      "Validation loss: 16.227932453155518 RMSE: 4.0283914\n",
      "Validation loss: 12.161200523376465 RMSE: 3.4872913\n",
      "109 6 1.7609251737594604\n",
      "Validation loss: 14.992954730987549 RMSE: 3.8720737\n",
      "Validation loss: 16.821558952331543 RMSE: 4.1014094\n",
      "Validation loss: 19.023449897766113 RMSE: 4.3615885\n",
      "112 8 1.7784035205841064\n",
      "Validation loss: 14.95146369934082 RMSE: 3.8667123\n",
      "Validation loss: 13.778519630432129 RMSE: 3.711943\n",
      "Validation loss: 11.358141899108887 RMSE: 3.3701844\n",
      "115 10 1.8607890605926514\n",
      "Validation loss: 16.344292640686035 RMSE: 4.0428076\n",
      "Validation loss: 10.190916061401367 RMSE: 3.1923215\n",
      "Validation loss: 14.994202136993408 RMSE: 3.8722348\n",
      "118 12 2.5600547790527344\n",
      "Validation loss: 14.802143096923828 RMSE: 3.8473551\n",
      "Validation loss: 16.23478078842163 RMSE: 4.0292406\n",
      "Validation loss: 14.421255588531494 RMSE: 3.7975328\n",
      "121 14 5.506410121917725\n",
      "Validation loss: 13.138757705688477 RMSE: 3.6247425\n",
      "Validation loss: 18.69122052192688 RMSE: 4.323334\n",
      "Validation loss: 14.116366863250732 RMSE: 3.7571754\n",
      "Validation loss: 14.153526306152344 RMSE: 3.7621171\n",
      "125 0 3.0452160835266113\n",
      "Validation loss: 12.64544153213501 RMSE: 3.556043\n",
      "Validation loss: 17.05158758163452 RMSE: 4.129357\n",
      "Validation loss: 12.89033842086792 RMSE: 3.5903118\n",
      "128 2 2.541022539138794\n",
      "Validation loss: 14.711482048034668 RMSE: 3.835555\n",
      "Validation loss: 16.15192222595215 RMSE: 4.018945\n",
      "Validation loss: 13.849626541137695 RMSE: 3.7215087\n",
      "131 4 1.9818333387374878\n",
      "Validation loss: 14.625895023345947 RMSE: 3.824381\n",
      "Validation loss: 13.572351932525635 RMSE: 3.6840672\n",
      "Validation loss: 13.290133476257324 RMSE: 3.6455636\n",
      "134 6 1.230107307434082\n",
      "Validation loss: 12.427486419677734 RMSE: 3.525264\n",
      "Validation loss: 11.933825016021729 RMSE: 3.4545372\n",
      "Validation loss: 19.211312294006348 RMSE: 4.3830714\n",
      "137 8 1.5066238641738892\n",
      "Validation loss: 15.081794738769531 RMSE: 3.8835285\n",
      "Validation loss: 13.148287773132324 RMSE: 3.6260567\n",
      "Validation loss: 14.376741170883179 RMSE: 3.7916672\n",
      "140 10 1.6684025526046753\n",
      "Validation loss: 13.152245998382568 RMSE: 3.6266026\n",
      "Validation loss: 14.632811546325684 RMSE: 3.825286\n",
      "Validation loss: 14.404287815093994 RMSE: 3.795298\n",
      "143 12 2.688558340072632\n",
      "Validation loss: 11.5122709274292 RMSE: 3.3929737\n",
      "Validation loss: 12.75668716430664 RMSE: 3.5716505\n",
      "Validation loss: 18.93916130065918 RMSE: 4.351915\n",
      "146 14 1.3368067741394043\n",
      "Validation loss: 14.730276107788086 RMSE: 3.838004\n",
      "Validation loss: 15.184812545776367 RMSE: 3.8967698\n",
      "Validation loss: 15.275524139404297 RMSE: 3.9083917\n",
      "Validation loss: 22.21304225921631 RMSE: 4.7130713\n",
      "150 0 2.251371383666992\n",
      "Validation loss: 13.2948317527771 RMSE: 3.646208\n",
      "Validation loss: 10.998554229736328 RMSE: 3.3164067\n",
      "Validation loss: 14.160961627960205 RMSE: 3.763105\n",
      "153 2 2.2646853923797607\n",
      "Validation loss: 14.446357727050781 RMSE: 3.8008358\n",
      "Validation loss: 15.722196578979492 RMSE: 3.9651227\n",
      "Validation loss: 15.546647071838379 RMSE: 3.9429238\n",
      "156 4 1.8408126831054688\n",
      "Validation loss: 21.237707376480103 RMSE: 4.6084385\n",
      "Validation loss: 12.330279350280762 RMSE: 3.5114496\n",
      "Validation loss: 14.193130493164062 RMSE: 3.767377\n",
      "159 6 1.889697790145874\n",
      "Validation loss: 17.204773902893066 RMSE: 4.1478634\n",
      "Validation loss: 24.575336456298828 RMSE: 4.9573517\n",
      "Validation loss: 18.820456504821777 RMSE: 4.338255\n",
      "162 8 1.9648548364639282\n",
      "Validation loss: 14.914165496826172 RMSE: 3.861886\n",
      "Validation loss: 16.843876361846924 RMSE: 4.104129\n",
      "Validation loss: 17.331642150878906 RMSE: 4.163129\n",
      "165 10 2.3302154541015625\n",
      "Validation loss: 14.75871753692627 RMSE: 3.8417077\n",
      "Validation loss: 11.906049728393555 RMSE: 3.4505143\n",
      "Validation loss: 15.567097663879395 RMSE: 3.9455156\n",
      "168 12 1.9297735691070557\n",
      "Validation loss: 15.7382230758667 RMSE: 3.967143\n",
      "Validation loss: 16.99962329864502 RMSE: 4.1230597\n",
      "Validation loss: 15.084425926208496 RMSE: 3.8838673\n",
      "171 14 2.335016965866089\n",
      "Validation loss: 16.53508472442627 RMSE: 4.0663357\n",
      "Validation loss: 11.071542263031006 RMSE: 3.3273926\n",
      "Validation loss: 11.740175724029541 RMSE: 3.4263937\n",
      "Validation loss: 24.482563018798828 RMSE: 4.9479856\n",
      "175 0 0.9807628989219666\n",
      "Validation loss: 16.16364288330078 RMSE: 4.0204034\n",
      "Validation loss: 19.18300771713257 RMSE: 4.3798413\n",
      "Validation loss: 18.150629997253418 RMSE: 4.260356\n",
      "178 2 0.8610345125198364\n",
      "Validation loss: 17.733598232269287 RMSE: 4.211128\n",
      "Validation loss: 23.761988639831543 RMSE: 4.874627\n",
      "Validation loss: 19.48664140701294 RMSE: 4.4143677\n",
      "181 4 1.9410744905471802\n",
      "Validation loss: 22.914649963378906 RMSE: 4.7869253\n",
      "Validation loss: 24.63191318511963 RMSE: 4.963055\n",
      "Validation loss: 15.925907611846924 RMSE: 3.9907274\n",
      "184 6 2.0248796939849854\n",
      "Validation loss: 16.95447039604187 RMSE: 4.117581\n",
      "Validation loss: 15.780698776245117 RMSE: 3.9724932\n",
      "Validation loss: 19.47586750984192 RMSE: 4.413147\n",
      "187 8 1.6930278539657593\n",
      "Validation loss: 21.660690307617188 RMSE: 4.654104\n",
      "Validation loss: 16.690922021865845 RMSE: 4.085453\n",
      "Validation loss: 25.678180694580078 RMSE: 5.0673637\n",
      "190 10 2.625648260116577\n",
      "Validation loss: 18.995716094970703 RMSE: 4.3584075\n",
      "Validation loss: 14.13472843170166 RMSE: 3.7596183\n",
      "Validation loss: 18.480700492858887 RMSE: 4.2989187\n",
      "193 12 2.1290714740753174\n",
      "Validation loss: 15.470898151397705 RMSE: 3.9333062\n",
      "Validation loss: 15.048437118530273 RMSE: 3.8792312\n",
      "Validation loss: 16.390055179595947 RMSE: 4.048463\n",
      "196 14 1.8006788492202759\n",
      "Validation loss: 17.292792797088623 RMSE: 4.1584606\n",
      "Validation loss: 15.26099443435669 RMSE: 3.9065323\n",
      "Validation loss: 12.090061664581299 RMSE: 3.4770763\n",
      "Validation loss: 13.457224369049072 RMSE: 3.6684089\n",
      "Loaded trained model with success.\n",
      "Test loss: 8.742069757901705 Test RMSE: 2.9566991\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:0', 'task_name': 'freesolv', 'model_type': 'gin', 'model': {'num_layer': 3, 'emb_dim': 64, 'feat_dim': 64, 'drop_ratio': 0.5, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 778, 'task': 'regression', 'data_path': 'data/freesolv/freesolv.csv', 'target': ['expt']}}\n",
      "Running on: cuda:0\n",
      "641\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/641\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 18.915061950683594\n",
      "Validation loss: 63.34727478027344 RMSE: 7.9591\n",
      "Validation loss: 58.22322654724121 RMSE: 7.6304145\n",
      "Validation loss: 50.9572811126709 RMSE: 7.138437\n",
      "3 2 13.356307983398438\n",
      "Validation loss: 40.318867683410645 RMSE: 6.349714\n",
      "Validation loss: 29.90566062927246 RMSE: 5.468607\n",
      "Validation loss: 17.70562171936035 RMSE: 4.207805\n",
      "6 4 11.417560577392578\n",
      "Validation loss: 17.418155670166016 RMSE: 4.1735063\n",
      "Validation loss: 15.504340171813965 RMSE: 3.937555\n",
      "Validation loss: 13.594223022460938 RMSE: 3.6870346\n",
      "9 6 4.935690879821777\n",
      "Validation loss: 14.609055519104004 RMSE: 3.8221793\n",
      "Validation loss: 12.635900497436523 RMSE: 3.554701\n",
      "Validation loss: 13.517581939697266 RMSE: 3.6766264\n",
      "12 8 8.827157974243164\n",
      "Validation loss: 15.140941619873047 RMSE: 3.8911364\n",
      "Validation loss: 10.991780757904053 RMSE: 3.3153856\n",
      "Validation loss: 11.214363098144531 RMSE: 3.3487854\n",
      "15 10 3.7711668014526367\n",
      "Validation loss: 12.58751106262207 RMSE: 3.5478883\n",
      "Validation loss: 10.970455169677734 RMSE: 3.3121676\n",
      "Validation loss: 11.938435554504395 RMSE: 3.4552042\n",
      "18 12 6.349896430969238\n",
      "Validation loss: 13.075665950775146 RMSE: 3.616029\n",
      "Validation loss: 12.205650329589844 RMSE: 3.4936585\n",
      "Validation loss: 13.140569686889648 RMSE: 3.6249926\n",
      "21 14 3.713411569595337\n",
      "Validation loss: 12.334171295166016 RMSE: 3.5120037\n",
      "Validation loss: 13.349408149719238 RMSE: 3.6536841\n",
      "Validation loss: 13.896118640899658 RMSE: 3.72775\n",
      "Validation loss: 12.43751049041748 RMSE: 3.5266855\n",
      "25 0 3.4259212017059326\n",
      "Validation loss: 11.906734466552734 RMSE: 3.4506137\n",
      "Validation loss: 13.609045505523682 RMSE: 3.689044\n",
      "Validation loss: 11.664883136749268 RMSE: 3.415389\n",
      "28 2 2.786439895629883\n",
      "Validation loss: 12.178876876831055 RMSE: 3.4898248\n",
      "Validation loss: 15.411680221557617 RMSE: 3.9257715\n",
      "Validation loss: 15.417940139770508 RMSE: 3.9265685\n",
      "31 4 5.006708145141602\n",
      "Validation loss: 12.250007629394531 RMSE: 3.500001\n",
      "Validation loss: 12.96683406829834 RMSE: 3.6009493\n",
      "Validation loss: 14.043745040893555 RMSE: 3.7474988\n",
      "34 6 3.943491220474243\n",
      "Validation loss: 15.400489330291748 RMSE: 3.9243457\n",
      "Validation loss: 14.553573608398438 RMSE: 3.8149145\n",
      "Validation loss: 12.530197620391846 RMSE: 3.5398023\n",
      "37 8 2.8800065517425537\n",
      "Validation loss: 11.88393497467041 RMSE: 3.4473083\n",
      "Validation loss: 12.998562335968018 RMSE: 3.605352\n",
      "Validation loss: 12.929393768310547 RMSE: 3.5957465\n",
      "40 10 2.133551597595215\n",
      "Validation loss: 14.289376258850098 RMSE: 3.7801287\n",
      "Validation loss: 15.25481128692627 RMSE: 3.905741\n",
      "Validation loss: 14.966070175170898 RMSE: 3.8686006\n",
      "43 12 3.0771660804748535\n",
      "Validation loss: 13.526172637939453 RMSE: 3.6777947\n",
      "Validation loss: 16.056554794311523 RMSE: 4.0070634\n",
      "Validation loss: 12.85068941116333 RMSE: 3.584786\n",
      "46 14 2.5193212032318115\n",
      "Validation loss: 14.724546432495117 RMSE: 3.8372576\n",
      "Validation loss: 13.633211851119995 RMSE: 3.6923182\n",
      "Validation loss: 14.230896949768066 RMSE: 3.772386\n",
      "Validation loss: 14.315424919128418 RMSE: 3.7835732\n",
      "50 0 2.644078254699707\n",
      "Validation loss: 16.75617504119873 RMSE: 4.093431\n",
      "Validation loss: 15.69135856628418 RMSE: 3.9612324\n",
      "Validation loss: 19.506366729736328 RMSE: 4.416601\n",
      "53 2 2.750760078430176\n",
      "Validation loss: 13.702919006347656 RMSE: 3.7017453\n",
      "Validation loss: 15.097970485687256 RMSE: 3.8856106\n",
      "Validation loss: 16.787801265716553 RMSE: 4.097292\n",
      "56 4 3.3637335300445557\n",
      "Validation loss: 21.143771648406982 RMSE: 4.598235\n",
      "Validation loss: 11.80262041091919 RMSE: 3.4354942\n",
      "Validation loss: 9.197535037994385 RMSE: 3.032744\n",
      "59 6 1.4828861951828003\n",
      "Validation loss: 13.223451614379883 RMSE: 3.6364064\n",
      "Validation loss: 16.3305983543396 RMSE: 4.0411134\n",
      "Validation loss: 13.86143445968628 RMSE: 3.7230947\n",
      "62 8 2.5415725708007812\n",
      "Validation loss: 12.650104522705078 RMSE: 3.5566983\n",
      "Validation loss: 16.967132091522217 RMSE: 4.1191177\n",
      "Validation loss: 14.086055755615234 RMSE: 3.7531397\n",
      "65 10 4.421396732330322\n",
      "Validation loss: 15.190083503723145 RMSE: 3.8974457\n",
      "Validation loss: 15.344130992889404 RMSE: 3.9171588\n",
      "Validation loss: 13.734325408935547 RMSE: 3.705985\n",
      "68 12 2.134082794189453\n",
      "Validation loss: 14.76758861541748 RMSE: 3.842862\n",
      "Validation loss: 17.220633506774902 RMSE: 4.1497746\n",
      "Validation loss: 16.1050021648407 RMSE: 4.0131035\n",
      "71 14 2.8093886375427246\n",
      "Validation loss: 18.176289558410645 RMSE: 4.2633657\n",
      "Validation loss: 14.758722305297852 RMSE: 3.8417082\n",
      "Validation loss: 17.999423503875732 RMSE: 4.2425723\n",
      "Validation loss: 16.934669494628906 RMSE: 4.1151757\n",
      "75 0 1.8571423292160034\n",
      "Validation loss: 19.717973709106445 RMSE: 4.440492\n",
      "Validation loss: 12.725783824920654 RMSE: 3.5673218\n",
      "Validation loss: 13.728996276855469 RMSE: 3.7052662\n",
      "78 2 2.2374563217163086\n",
      "Validation loss: 13.36947774887085 RMSE: 3.6564295\n",
      "Validation loss: 20.84651756286621 RMSE: 4.565799\n",
      "Validation loss: 15.785956382751465 RMSE: 3.9731545\n",
      "81 4 5.104433536529541\n",
      "Validation loss: 22.64330768585205 RMSE: 4.7584977\n",
      "Validation loss: 18.39531946182251 RMSE: 4.2889767\n",
      "Validation loss: 18.281809329986572 RMSE: 4.2757235\n",
      "84 6 2.2684028148651123\n",
      "Validation loss: 18.9181547164917 RMSE: 4.3495\n",
      "Validation loss: 12.811015605926514 RMSE: 3.579248\n",
      "Validation loss: 15.416373252868652 RMSE: 3.926369\n",
      "87 8 2.9589033126831055\n",
      "Validation loss: 20.294047355651855 RMSE: 4.504892\n",
      "Validation loss: 20.204334259033203 RMSE: 4.494923\n",
      "Validation loss: 23.46463394165039 RMSE: 4.844031\n",
      "90 10 3.1879940032958984\n",
      "Validation loss: 15.127837181091309 RMSE: 3.889452\n",
      "Validation loss: 17.47020435333252 RMSE: 4.179737\n",
      "Validation loss: 13.438591957092285 RMSE: 3.6658678\n",
      "93 12 2.2648420333862305\n",
      "Validation loss: 14.034160137176514 RMSE: 3.7462199\n",
      "Validation loss: 11.890256404876709 RMSE: 3.4482253\n",
      "Validation loss: 14.336550235748291 RMSE: 3.7863636\n",
      "96 14 2.360609769821167\n",
      "Validation loss: 13.008913040161133 RMSE: 3.606787\n",
      "Validation loss: 15.338867664337158 RMSE: 3.9164865\n",
      "Validation loss: 12.668776035308838 RMSE: 3.5593224\n",
      "Validation loss: 20.89459228515625 RMSE: 4.57106\n",
      "100 0 2.076493978500366\n",
      "Validation loss: 17.248404502868652 RMSE: 4.15312\n",
      "Validation loss: 16.273757934570312 RMSE: 4.034075\n",
      "Validation loss: 15.602000713348389 RMSE: 3.949937\n",
      "103 2 1.5312069654464722\n",
      "Validation loss: 16.481767654418945 RMSE: 4.059774\n",
      "Validation loss: 17.975828170776367 RMSE: 4.2397914\n",
      "Validation loss: 18.594816207885742 RMSE: 4.312171\n",
      "106 4 4.588254928588867\n",
      "Validation loss: 24.44743251800537 RMSE: 4.944434\n",
      "Validation loss: 15.538977146148682 RMSE: 3.9419513\n",
      "Validation loss: 18.449917793273926 RMSE: 4.2953367\n",
      "109 6 1.496752381324768\n",
      "Validation loss: 16.53484010696411 RMSE: 4.066305\n",
      "Validation loss: 20.30781841278076 RMSE: 4.5064197\n",
      "Validation loss: 14.977255821228027 RMSE: 3.8700464\n",
      "112 8 2.499241352081299\n",
      "Validation loss: 16.66433310508728 RMSE: 4.082197\n",
      "Validation loss: 16.845619201660156 RMSE: 4.1043415\n",
      "Validation loss: 18.15838623046875 RMSE: 4.2612658\n",
      "115 10 1.2202160358428955\n",
      "Validation loss: 19.026530265808105 RMSE: 4.3619413\n",
      "Validation loss: 19.39661979675293 RMSE: 4.4041595\n",
      "Validation loss: 18.206411361694336 RMSE: 4.2668967\n",
      "118 12 2.272327423095703\n",
      "Validation loss: 21.813730239868164 RMSE: 4.670517\n",
      "Validation loss: 18.651988983154297 RMSE: 4.3187947\n",
      "Validation loss: 16.546249389648438 RMSE: 4.067708\n",
      "121 14 2.210887908935547\n",
      "Validation loss: 13.950172901153564 RMSE: 3.7349927\n",
      "Validation loss: 20.231364250183105 RMSE: 4.4979296\n",
      "Validation loss: 19.415347576141357 RMSE: 4.406285\n",
      "Validation loss: 20.011545658111572 RMSE: 4.4734263\n",
      "125 0 1.4553989171981812\n",
      "Validation loss: 23.696704864501953 RMSE: 4.867926\n",
      "Validation loss: 18.49400043487549 RMSE: 4.300465\n",
      "Validation loss: 22.262377738952637 RMSE: 4.7183027\n",
      "128 2 2.595395803451538\n",
      "Validation loss: 23.694717407226562 RMSE: 4.867722\n",
      "Validation loss: 21.113386631011963 RMSE: 4.5949306\n",
      "Validation loss: 21.011420249938965 RMSE: 4.583822\n",
      "131 4 1.7209141254425049\n",
      "Validation loss: 22.892932891845703 RMSE: 4.7846556\n",
      "Validation loss: 25.934535026550293 RMSE: 5.092596\n",
      "Validation loss: 22.236677646636963 RMSE: 4.715578\n",
      "134 6 2.9154365062713623\n",
      "Validation loss: 20.804984092712402 RMSE: 4.5612483\n",
      "Validation loss: 27.01370096206665 RMSE: 5.197471\n",
      "Validation loss: 18.22144603729248 RMSE: 4.2686586\n",
      "137 8 3.0292108058929443\n",
      "Validation loss: 19.796534061431885 RMSE: 4.4493294\n",
      "Validation loss: 21.00881862640381 RMSE: 4.583538\n",
      "Validation loss: 18.04414653778076 RMSE: 4.24784\n",
      "140 10 1.791347861289978\n",
      "Validation loss: 16.278748512268066 RMSE: 4.0346932\n",
      "Validation loss: 18.3753604888916 RMSE: 4.2866488\n",
      "Validation loss: 24.389615058898926 RMSE: 4.9385843\n",
      "143 12 2.510227680206299\n",
      "Validation loss: 21.363937377929688 RMSE: 4.622114\n",
      "Validation loss: 23.190712928771973 RMSE: 4.8156734\n",
      "Validation loss: 22.79995059967041 RMSE: 4.774929\n",
      "146 14 1.7695906162261963\n",
      "Validation loss: 20.548362731933594 RMSE: 4.53303\n",
      "Validation loss: 15.280501365661621 RMSE: 3.9090283\n",
      "Validation loss: 18.20802879333496 RMSE: 4.2670865\n",
      "Validation loss: 21.29344081878662 RMSE: 4.614482\n",
      "150 0 1.215286135673523\n",
      "Validation loss: 17.459080696105957 RMSE: 4.1784067\n",
      "Validation loss: 28.744043350219727 RMSE: 5.361347\n",
      "Validation loss: 22.063138008117676 RMSE: 4.697142\n",
      "153 2 3.0733165740966797\n",
      "Validation loss: 22.36246681213379 RMSE: 4.7288966\n",
      "Validation loss: 19.451878547668457 RMSE: 4.410428\n",
      "Validation loss: 22.039188385009766 RMSE: 4.6945915\n",
      "156 4 1.9658327102661133\n",
      "Validation loss: 23.264516830444336 RMSE: 4.8233304\n",
      "Validation loss: 22.52887725830078 RMSE: 4.7464595\n",
      "Validation loss: 19.508026123046875 RMSE: 4.416789\n",
      "159 6 2.06069278717041\n",
      "Validation loss: 17.795259475708008 RMSE: 4.218443\n",
      "Validation loss: 13.574779987335205 RMSE: 3.6843967\n",
      "Validation loss: 18.08334732055664 RMSE: 4.252452\n",
      "162 8 2.0904054641723633\n",
      "Validation loss: 18.609739303588867 RMSE: 4.3139005\n",
      "Validation loss: 16.34539270401001 RMSE: 4.042944\n",
      "Validation loss: 20.278977394104004 RMSE: 4.5032187\n",
      "165 10 1.6236385107040405\n",
      "Validation loss: 20.529369354248047 RMSE: 4.530935\n",
      "Validation loss: 18.044328689575195 RMSE: 4.247862\n",
      "Validation loss: 17.36388874053955 RMSE: 4.167\n",
      "168 12 1.5590620040893555\n",
      "Validation loss: 16.667680263519287 RMSE: 4.082607\n",
      "Validation loss: 19.735837936401367 RMSE: 4.4425035\n",
      "Validation loss: 21.302852630615234 RMSE: 4.615501\n",
      "171 14 2.7299487590789795\n",
      "Validation loss: 20.479565620422363 RMSE: 4.5254354\n",
      "Validation loss: 26.80974006652832 RMSE: 5.177812\n",
      "Validation loss: 21.814289093017578 RMSE: 4.670577\n",
      "Validation loss: 18.814785957336426 RMSE: 4.337601\n",
      "175 0 2.415292501449585\n",
      "Validation loss: 17.97794246673584 RMSE: 4.2400403\n",
      "Validation loss: 20.556673049926758 RMSE: 4.533947\n",
      "Validation loss: 18.983051300048828 RMSE: 4.3569546\n",
      "178 2 2.8572497367858887\n",
      "Validation loss: 18.433011054992676 RMSE: 4.293368\n",
      "Validation loss: 23.089664459228516 RMSE: 4.8051705\n",
      "Validation loss: 21.54134464263916 RMSE: 4.6412654\n",
      "181 4 1.3932453393936157\n",
      "Validation loss: 22.650461196899414 RMSE: 4.7592497\n",
      "Validation loss: 24.052171230316162 RMSE: 4.904301\n",
      "Validation loss: 18.139936447143555 RMSE: 4.2591004\n",
      "184 6 2.5286548137664795\n",
      "Validation loss: 22.644402503967285 RMSE: 4.7586136\n",
      "Validation loss: 18.413652420043945 RMSE: 4.2911134\n",
      "Validation loss: 24.772192001342773 RMSE: 4.977167\n",
      "187 8 1.679440975189209\n",
      "Validation loss: 17.021286964416504 RMSE: 4.125686\n",
      "Validation loss: 17.379669666290283 RMSE: 4.168893\n",
      "Validation loss: 23.333725929260254 RMSE: 4.830499\n",
      "190 10 1.3587037324905396\n",
      "Validation loss: 20.299938201904297 RMSE: 4.5055456\n",
      "Validation loss: 31.309112310409546 RMSE: 5.5954547\n",
      "Validation loss: 25.11471176147461 RMSE: 5.0114574\n",
      "193 12 1.4829514026641846\n",
      "Validation loss: 25.790637969970703 RMSE: 5.0784483\n",
      "Validation loss: 21.012102127075195 RMSE: 4.5838957\n",
      "Validation loss: 19.028484344482422 RMSE: 4.362165\n",
      "196 14 1.9102917909622192\n",
      "Validation loss: 16.61966609954834 RMSE: 4.076722\n",
      "Validation loss: 17.091365814208984 RMSE: 4.1341705\n",
      "Validation loss: 19.337227821350098 RMSE: 4.3974113\n",
      "Validation loss: 19.273573875427246 RMSE: 4.3901677\n",
      "Loaded trained model with success.\n",
      "Test loss: 7.214388115589435 Test RMSE: 2.6859617\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:0', 'task_name': 'freesolv', 'model_type': 'gin', 'model': {'num_layer': 3, 'emb_dim': 64, 'feat_dim': 64, 'drop_ratio': 0.5, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 779, 'task': 'regression', 'data_path': 'data/freesolv/freesolv.csv', 'target': ['expt']}}\n",
      "Running on: cuda:0\n",
      "641\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/641\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 25.801593780517578\n",
      "Validation loss: 72.05347061157227 RMSE: 8.488431\n",
      "Validation loss: 66.29366302490234 RMSE: 8.142092\n",
      "Validation loss: 58.47208213806152 RMSE: 7.646704\n",
      "3 2 14.206962585449219\n",
      "Validation loss: 46.65827178955078 RMSE: 6.830686\n",
      "Validation loss: 28.645421981811523 RMSE: 5.352142\n",
      "Validation loss: 14.101882457733154 RMSE: 3.7552474\n",
      "6 4 7.795716285705566\n",
      "Validation loss: 12.052377700805664 RMSE: 3.4716535\n",
      "Validation loss: 14.758909225463867 RMSE: 3.8417325\n",
      "Validation loss: 13.793461799621582 RMSE: 3.713955\n",
      "9 6 5.444432735443115\n",
      "Validation loss: 12.63540506362915 RMSE: 3.5546315\n",
      "Validation loss: 14.310839653015137 RMSE: 3.782967\n",
      "Validation loss: 12.482111930847168 RMSE: 3.533003\n",
      "12 8 5.891191482543945\n",
      "Validation loss: 13.796107292175293 RMSE: 3.7143111\n",
      "Validation loss: 13.584239959716797 RMSE: 3.6856804\n",
      "Validation loss: 13.37588882446289 RMSE: 3.6573062\n",
      "15 10 3.7985305786132812\n",
      "Validation loss: 12.432538986206055 RMSE: 3.525981\n",
      "Validation loss: 13.183708667755127 RMSE: 3.6309376\n",
      "Validation loss: 13.198135375976562 RMSE: 3.6329238\n",
      "18 12 4.6603169441223145\n",
      "Validation loss: 15.013979434967041 RMSE: 3.8747878\n",
      "Validation loss: 16.347634315490723 RMSE: 4.043221\n",
      "Validation loss: 16.840413570404053 RMSE: 4.1037073\n",
      "21 14 4.242784023284912\n",
      "Validation loss: 19.757956504821777 RMSE: 4.4449925\n",
      "Validation loss: 18.567065715789795 RMSE: 4.308952\n",
      "Validation loss: 18.71925163269043 RMSE: 4.3265753\n",
      "Validation loss: 16.26661729812622 RMSE: 4.03319\n",
      "25 0 3.9977915287017822\n",
      "Validation loss: 18.65866184234619 RMSE: 4.3195677\n",
      "Validation loss: 16.532304763793945 RMSE: 4.065994\n",
      "Validation loss: 16.617713451385498 RMSE: 4.076483\n",
      "28 2 3.0695278644561768\n",
      "Validation loss: 21.20654296875 RMSE: 4.6050563\n",
      "Validation loss: 19.056272506713867 RMSE: 4.3653493\n",
      "Validation loss: 22.35107421875 RMSE: 4.727692\n",
      "31 4 2.312539577484131\n",
      "Validation loss: 20.834052085876465 RMSE: 4.564433\n",
      "Validation loss: 28.90005397796631 RMSE: 5.375877\n",
      "Validation loss: 22.31293773651123 RMSE: 4.723657\n",
      "34 6 3.1521027088165283\n",
      "Validation loss: 20.664340019226074 RMSE: 4.5458045\n",
      "Validation loss: 18.841063499450684 RMSE: 4.3406296\n",
      "Validation loss: 21.47390651702881 RMSE: 4.633995\n",
      "37 8 2.7892942428588867\n",
      "Validation loss: 19.824952125549316 RMSE: 4.4525223\n",
      "Validation loss: 18.1763277053833 RMSE: 4.26337\n",
      "Validation loss: 22.574251890182495 RMSE: 4.7512364\n",
      "40 10 2.92246675491333\n",
      "Validation loss: 17.60763931274414 RMSE: 4.1961455\n",
      "Validation loss: 17.46938133239746 RMSE: 4.179639\n",
      "Validation loss: 21.123887062072754 RMSE: 4.596073\n",
      "43 12 6.126704692840576\n",
      "Validation loss: 21.032294273376465 RMSE: 4.586098\n",
      "Validation loss: 16.88612174987793 RMSE: 4.109273\n",
      "Validation loss: 16.853783130645752 RMSE: 4.105335\n",
      "46 14 3.183799982070923\n",
      "Validation loss: 28.17535400390625 RMSE: 5.308046\n",
      "Validation loss: 23.272074699401855 RMSE: 4.8241143\n",
      "Validation loss: 22.427988052368164 RMSE: 4.7358193\n",
      "Validation loss: 28.515832901000977 RMSE: 5.3400216\n",
      "50 0 4.038825035095215\n",
      "Validation loss: 23.882482528686523 RMSE: 4.8869705\n",
      "Validation loss: 19.86318016052246 RMSE: 4.456813\n",
      "Validation loss: 21.001684188842773 RMSE: 4.58276\n",
      "53 2 2.240166187286377\n",
      "Validation loss: 26.6766619682312 RMSE: 5.1649456\n",
      "Validation loss: 26.672579765319824 RMSE: 5.1645503\n",
      "Validation loss: 15.260327816009521 RMSE: 3.9064465\n",
      "56 4 2.8026623725891113\n",
      "Validation loss: 18.667373657226562 RMSE: 4.3205757\n",
      "Validation loss: 19.17048168182373 RMSE: 4.378411\n",
      "Validation loss: 20.34976577758789 RMSE: 4.5110717\n",
      "59 6 2.408618211746216\n",
      "Validation loss: 19.53963041305542 RMSE: 4.4203653\n",
      "Validation loss: 21.031494140625 RMSE: 4.586011\n",
      "Validation loss: 18.812255859375 RMSE: 4.3373094\n",
      "62 8 4.057985305786133\n",
      "Validation loss: 20.629344940185547 RMSE: 4.541953\n",
      "Validation loss: 22.39867353439331 RMSE: 4.7327237\n",
      "Validation loss: 22.437644004821777 RMSE: 4.7368393\n",
      "65 10 1.5561119318008423\n",
      "Validation loss: 17.01850175857544 RMSE: 4.1253486\n",
      "Validation loss: 32.14203643798828 RMSE: 5.6693945\n",
      "Validation loss: 16.70552349090576 RMSE: 4.0872393\n",
      "68 12 2.9911704063415527\n",
      "Validation loss: 12.971519947052002 RMSE: 3.6015995\n",
      "Validation loss: 16.923158168792725 RMSE: 4.1137767\n",
      "Validation loss: 17.15297222137451 RMSE: 4.141615\n",
      "71 14 3.601440191268921\n",
      "Validation loss: 22.010815143585205 RMSE: 4.6915684\n",
      "Validation loss: 16.957108974456787 RMSE: 4.1179013\n",
      "Validation loss: 15.326536655426025 RMSE: 3.9149122\n",
      "Validation loss: 22.888105392456055 RMSE: 4.784151\n",
      "75 0 2.466237783432007\n",
      "Validation loss: 16.479416847229004 RMSE: 4.059485\n",
      "Validation loss: 16.40998363494873 RMSE: 4.0509243\n",
      "Validation loss: 18.67520809173584 RMSE: 4.321482\n",
      "78 2 4.099891662597656\n",
      "Validation loss: 20.035561561584473 RMSE: 4.47611\n",
      "Validation loss: 15.592953205108643 RMSE: 3.9487913\n",
      "Validation loss: 14.908020973205566 RMSE: 3.8610907\n",
      "81 4 2.9654452800750732\n",
      "Validation loss: 22.087682723999023 RMSE: 4.699753\n",
      "Validation loss: 18.46973752975464 RMSE: 4.2976427\n",
      "Validation loss: 22.839351654052734 RMSE: 4.779053\n",
      "84 6 3.5310206413269043\n",
      "Validation loss: 16.09918212890625 RMSE: 4.0123787\n",
      "Validation loss: 14.227588176727295 RMSE: 3.7719474\n",
      "Validation loss: 18.591246604919434 RMSE: 4.311757\n",
      "87 8 2.578392267227173\n",
      "Validation loss: 17.470885753631592 RMSE: 4.179819\n",
      "Validation loss: 14.342954635620117 RMSE: 3.7872095\n",
      "Validation loss: 13.297601222991943 RMSE: 3.6465874\n",
      "90 10 1.8175909519195557\n",
      "Validation loss: 18.93399429321289 RMSE: 4.351321\n",
      "Validation loss: 16.054297924041748 RMSE: 4.006781\n",
      "Validation loss: 25.366259574890137 RMSE: 5.036493\n",
      "93 12 3.8277459144592285\n",
      "Validation loss: 18.693814277648926 RMSE: 4.3236346\n",
      "Validation loss: 17.395923614501953 RMSE: 4.170841\n",
      "Validation loss: 21.013997077941895 RMSE: 4.584103\n",
      "96 14 2.5245718955993652\n",
      "Validation loss: 17.534342765808105 RMSE: 4.1874027\n",
      "Validation loss: 21.37799835205078 RMSE: 4.6236353\n",
      "Validation loss: 19.15215539932251 RMSE: 4.376318\n",
      "Validation loss: 15.896324634552002 RMSE: 3.9870195\n",
      "100 0 6.2339558601379395\n",
      "Validation loss: 13.929387092590332 RMSE: 3.7322094\n",
      "Validation loss: 15.265369415283203 RMSE: 3.9070923\n",
      "Validation loss: 17.481477737426758 RMSE: 4.1810856\n",
      "103 2 4.523460865020752\n",
      "Validation loss: 15.885303020477295 RMSE: 3.9856372\n",
      "Validation loss: 16.787471771240234 RMSE: 4.097252\n",
      "Validation loss: 12.335472106933594 RMSE: 3.5121891\n",
      "106 4 5.456976890563965\n",
      "Validation loss: 19.073315620422363 RMSE: 4.3673005\n",
      "Validation loss: 20.351251125335693 RMSE: 4.5112357\n",
      "Validation loss: 22.64816188812256 RMSE: 4.759008\n",
      "109 6 2.433722496032715\n",
      "Validation loss: 20.687893867492676 RMSE: 4.5483947\n",
      "Validation loss: 28.54341697692871 RMSE: 5.342604\n",
      "Validation loss: 17.550645351409912 RMSE: 4.189349\n",
      "112 8 2.650486946105957\n",
      "Validation loss: 14.86429500579834 RMSE: 3.8554242\n",
      "Validation loss: 24.976911544799805 RMSE: 4.99769\n",
      "Validation loss: 18.59217357635498 RMSE: 4.311864\n",
      "115 10 1.8728350400924683\n",
      "Validation loss: 16.52226209640503 RMSE: 4.064759\n",
      "Validation loss: 18.767386436462402 RMSE: 4.3321342\n",
      "Validation loss: 16.77830457687378 RMSE: 4.0961328\n",
      "118 12 2.4913461208343506\n",
      "Validation loss: 20.22921085357666 RMSE: 4.4976892\n",
      "Validation loss: 18.581472396850586 RMSE: 4.310623\n",
      "Validation loss: 21.43267822265625 RMSE: 4.6295443\n",
      "121 14 2.0372188091278076\n",
      "Validation loss: 19.3567533493042 RMSE: 4.3996305\n",
      "Validation loss: 13.666524410247803 RMSE: 3.6968267\n",
      "Validation loss: 17.51710319519043 RMSE: 4.1853437\n",
      "Validation loss: 12.539260387420654 RMSE: 3.5410817\n",
      "125 0 2.6409311294555664\n",
      "Validation loss: 18.648003101348877 RMSE: 4.3183336\n",
      "Validation loss: 19.80076789855957 RMSE: 4.4498057\n",
      "Validation loss: 13.637909412384033 RMSE: 3.692954\n",
      "128 2 1.9051614999771118\n",
      "Validation loss: 32.06601142883301 RMSE: 5.662686\n",
      "Validation loss: 24.231746196746826 RMSE: 4.9225755\n",
      "Validation loss: 20.236721992492676 RMSE: 4.4985247\n",
      "131 4 3.0518033504486084\n",
      "Validation loss: 20.765917778015137 RMSE: 4.5569634\n",
      "Validation loss: 17.61549186706543 RMSE: 4.1970816\n",
      "Validation loss: 25.992066383361816 RMSE: 5.098242\n",
      "134 6 1.2875992059707642\n",
      "Validation loss: 29.842975616455078 RMSE: 5.4628725\n",
      "Validation loss: 22.420628547668457 RMSE: 4.7350426\n",
      "Validation loss: 18.25091791152954 RMSE: 4.272109\n",
      "137 8 5.626416206359863\n",
      "Validation loss: 23.30607032775879 RMSE: 4.8276353\n",
      "Validation loss: 16.458208084106445 RMSE: 4.0568714\n",
      "Validation loss: 27.36127471923828 RMSE: 5.2308006\n",
      "140 10 1.1809381246566772\n",
      "Validation loss: 20.759380340576172 RMSE: 4.5562463\n",
      "Validation loss: 19.147756576538086 RMSE: 4.3758154\n",
      "Validation loss: 33.61010456085205 RMSE: 5.7974224\n",
      "143 12 3.4050135612487793\n",
      "Validation loss: 19.233644723892212 RMSE: 4.3856177\n",
      "Validation loss: 25.082786083221436 RMSE: 5.008272\n",
      "Validation loss: 26.781569480895996 RMSE: 5.1750913\n",
      "146 14 3.2687034606933594\n",
      "Validation loss: 29.57167625427246 RMSE: 5.437985\n",
      "Validation loss: 21.760123252868652 RMSE: 4.6647744\n",
      "Validation loss: 24.174749851226807 RMSE: 4.9167833\n",
      "Validation loss: 20.500311851501465 RMSE: 4.527727\n",
      "150 0 1.5329583883285522\n",
      "Validation loss: 20.532451152801514 RMSE: 4.5312753\n",
      "Validation loss: 20.898151397705078 RMSE: 4.5714493\n",
      "Validation loss: 23.766305923461914 RMSE: 4.87507\n",
      "153 2 2.057269811630249\n",
      "Validation loss: 17.216444492340088 RMSE: 4.1492705\n",
      "Validation loss: 16.675515174865723 RMSE: 4.0835667\n",
      "Validation loss: 22.38770294189453 RMSE: 4.7315645\n",
      "156 4 1.2179189920425415\n",
      "Validation loss: 22.607498168945312 RMSE: 4.7547345\n",
      "Validation loss: 19.962965965270996 RMSE: 4.4679937\n",
      "Validation loss: 22.790802001953125 RMSE: 4.7739706\n",
      "159 6 1.7488402128219604\n",
      "Validation loss: 29.628836631774902 RMSE: 5.4432373\n",
      "Validation loss: 19.828951835632324 RMSE: 4.4529715\n",
      "Validation loss: 22.94714069366455 RMSE: 4.790318\n",
      "162 8 1.7627602815628052\n",
      "Validation loss: 18.418830394744873 RMSE: 4.291716\n",
      "Validation loss: 18.973857879638672 RMSE: 4.3558993\n",
      "Validation loss: 14.789496421813965 RMSE: 3.845711\n",
      "165 10 1.8583838939666748\n",
      "Validation loss: 19.229186058044434 RMSE: 4.3851094\n",
      "Validation loss: 16.04345464706421 RMSE: 4.005428\n",
      "Validation loss: 18.94414758682251 RMSE: 4.352488\n",
      "168 12 2.667306900024414\n",
      "Validation loss: 23.606682300567627 RMSE: 4.8586707\n",
      "Validation loss: 22.874984741210938 RMSE: 4.78278\n",
      "Validation loss: 16.54599618911743 RMSE: 4.067677\n",
      "171 14 1.949830174446106\n",
      "Validation loss: 22.852201461791992 RMSE: 4.780398\n",
      "Validation loss: 21.454264640808105 RMSE: 4.631875\n",
      "Validation loss: 16.719077587127686 RMSE: 4.0888968\n",
      "Validation loss: 18.328286170959473 RMSE: 4.2811546\n",
      "175 0 0.9533287882804871\n",
      "Validation loss: 13.396728038787842 RMSE: 3.660154\n",
      "Validation loss: 22.406493186950684 RMSE: 4.73355\n",
      "Validation loss: 18.705105781555176 RMSE: 4.3249397\n",
      "178 2 2.0264477729797363\n",
      "Validation loss: 19.551307678222656 RMSE: 4.421686\n",
      "Validation loss: 18.471418857574463 RMSE: 4.2978387\n",
      "Validation loss: 22.37254047393799 RMSE: 4.729962\n",
      "181 4 2.947721242904663\n",
      "Validation loss: 20.68129253387451 RMSE: 4.547669\n",
      "Validation loss: 20.86536741256714 RMSE: 4.5678625\n",
      "Validation loss: 25.07920551300049 RMSE: 5.007915\n",
      "184 6 1.6522979736328125\n",
      "Validation loss: 16.923497200012207 RMSE: 4.1138177\n",
      "Validation loss: 24.68067979812622 RMSE: 4.967965\n",
      "Validation loss: 25.143290519714355 RMSE: 5.0143085\n",
      "187 8 0.9593099355697632\n",
      "Validation loss: 19.264617919921875 RMSE: 4.3891478\n",
      "Validation loss: 20.061753273010254 RMSE: 4.479035\n",
      "Validation loss: 21.31580114364624 RMSE: 4.6169043\n",
      "190 10 1.735616683959961\n",
      "Validation loss: 22.343924522399902 RMSE: 4.7269363\n",
      "Validation loss: 24.88663387298584 RMSE: 4.988651\n",
      "Validation loss: 22.53815269470215 RMSE: 4.747436\n",
      "193 12 1.2019301652908325\n",
      "Validation loss: 15.945789337158203 RMSE: 3.993218\n",
      "Validation loss: 22.200079917907715 RMSE: 4.711696\n",
      "Validation loss: 26.41366672515869 RMSE: 5.139423\n",
      "196 14 2.9698901176452637\n",
      "Validation loss: 30.090506553649902 RMSE: 5.485481\n",
      "Validation loss: 22.594916343688965 RMSE: 4.7534113\n",
      "Validation loss: 24.916792392730713 RMSE: 4.9916725\n",
      "Validation loss: 19.874450206756592 RMSE: 4.458077\n",
      "Loaded trained model with success.\n",
      "Test loss: 12.444729379507212 Test RMSE: 3.527709\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:0', 'task_name': 'freesolv', 'model_type': 'gin', 'model': {'num_layer': 3, 'emb_dim': 64, 'feat_dim': 64, 'drop_ratio': 0.5, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 780, 'task': 'regression', 'data_path': 'data/freesolv/freesolv.csv', 'target': ['expt']}}\n",
      "Running on: cuda:0\n",
      "641\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/641\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 14.561737060546875\n",
      "Validation loss: 68.11935424804688 RMSE: 8.253445\n",
      "Validation loss: 61.8367805480957 RMSE: 7.8636365\n",
      "Validation loss: 53.98637771606445 RMSE: 7.347543\n",
      "3 2 15.921963691711426\n",
      "Validation loss: 40.58570098876953 RMSE: 6.370691\n",
      "Validation loss: 20.553959846496582 RMSE: 4.5336475\n",
      "Validation loss: 10.581151247024536 RMSE: 3.2528682\n",
      "6 4 7.195033073425293\n",
      "Validation loss: 11.083868503570557 RMSE: 3.3292444\n",
      "Validation loss: 14.210172653198242 RMSE: 3.7696383\n",
      "Validation loss: 14.915950298309326 RMSE: 3.8621173\n",
      "9 6 12.743512153625488\n",
      "Validation loss: 12.654884815216064 RMSE: 3.5573707\n",
      "Validation loss: 13.134998321533203 RMSE: 3.6242237\n",
      "Validation loss: 12.29306411743164 RMSE: 3.5061467\n",
      "12 8 6.013669967651367\n",
      "Validation loss: 11.423032283782959 RMSE: 3.3797977\n",
      "Validation loss: 15.02906608581543 RMSE: 3.876734\n",
      "Validation loss: 11.8157958984375 RMSE: 3.4374113\n",
      "15 10 6.592226982116699\n",
      "Validation loss: 11.739994525909424 RMSE: 3.4263675\n",
      "Validation loss: 11.755481719970703 RMSE: 3.4286265\n",
      "Validation loss: 12.726580619812012 RMSE: 3.5674334\n",
      "18 12 3.699270725250244\n",
      "Validation loss: 11.438647747039795 RMSE: 3.382107\n",
      "Validation loss: 13.968567848205566 RMSE: 3.7374547\n",
      "Validation loss: 12.966687202453613 RMSE: 3.6009283\n",
      "21 14 5.2256760597229\n",
      "Validation loss: 16.97640323638916 RMSE: 4.1202435\n",
      "Validation loss: 13.335944175720215 RMSE: 3.6518412\n",
      "Validation loss: 24.167665481567383 RMSE: 4.916062\n",
      "Validation loss: 15.874138355255127 RMSE: 3.9842362\n",
      "25 0 2.7849361896514893\n",
      "Validation loss: 15.954713821411133 RMSE: 3.9943352\n",
      "Validation loss: 16.143632411956787 RMSE: 4.017914\n",
      "Validation loss: 18.413031578063965 RMSE: 4.2910414\n",
      "28 2 4.557835102081299\n",
      "Validation loss: 13.535080909729004 RMSE: 3.6790054\n",
      "Validation loss: 17.45668601989746 RMSE: 4.1781197\n",
      "Validation loss: 16.20684242248535 RMSE: 4.025772\n",
      "31 4 3.2683017253875732\n",
      "Validation loss: 15.944867134094238 RMSE: 3.9931023\n",
      "Validation loss: 18.750548362731934 RMSE: 4.3301907\n",
      "Validation loss: 17.795326232910156 RMSE: 4.2184505\n",
      "34 6 2.1618547439575195\n",
      "Validation loss: 12.852828979492188 RMSE: 3.585084\n",
      "Validation loss: 16.027870655059814 RMSE: 4.0034823\n",
      "Validation loss: 17.54020118713379 RMSE: 4.1881022\n",
      "37 8 4.345487594604492\n",
      "Validation loss: 19.109126091003418 RMSE: 4.371399\n",
      "Validation loss: 16.010372161865234 RMSE: 4.0012965\n",
      "Validation loss: 17.567349433898926 RMSE: 4.191342\n",
      "40 10 3.6661102771759033\n",
      "Validation loss: 16.71285343170166 RMSE: 4.0881352\n",
      "Validation loss: 20.65872859954834 RMSE: 4.545188\n",
      "Validation loss: 11.174302577972412 RMSE: 3.3427987\n",
      "43 12 5.855464935302734\n",
      "Validation loss: 17.161031007766724 RMSE: 4.1425877\n",
      "Validation loss: 20.09999942779541 RMSE: 4.483302\n",
      "Validation loss: 16.931390285491943 RMSE: 4.114777\n",
      "46 14 2.5257718563079834\n",
      "Validation loss: 15.893532752990723 RMSE: 3.986669\n",
      "Validation loss: 17.723354816436768 RMSE: 4.2099113\n",
      "Validation loss: 17.11994504928589 RMSE: 4.1376257\n",
      "Validation loss: 15.735641479492188 RMSE: 3.9668176\n",
      "50 0 5.669369220733643\n",
      "Validation loss: 13.541701793670654 RMSE: 3.6799052\n",
      "Validation loss: 16.866230010986328 RMSE: 4.106852\n",
      "Validation loss: 16.387475967407227 RMSE: 4.0481443\n",
      "53 2 5.74747896194458\n",
      "Validation loss: 17.56991481781006 RMSE: 4.1916485\n",
      "Validation loss: 18.223875522613525 RMSE: 4.268943\n",
      "Validation loss: 17.261622428894043 RMSE: 4.154711\n",
      "56 4 2.433269500732422\n",
      "Validation loss: 13.896608352661133 RMSE: 3.7278154\n",
      "Validation loss: 16.95655059814453 RMSE: 4.117833\n",
      "Validation loss: 16.9290714263916 RMSE: 4.1144953\n",
      "59 6 2.5624125003814697\n",
      "Validation loss: 19.718079566955566 RMSE: 4.440504\n",
      "Validation loss: 14.851014137268066 RMSE: 3.8537018\n",
      "Validation loss: 12.666522026062012 RMSE: 3.5590057\n",
      "62 8 2.459990978240967\n",
      "Validation loss: 15.248613357543945 RMSE: 3.9049475\n",
      "Validation loss: 18.39467763900757 RMSE: 4.2889013\n",
      "Validation loss: 15.583399772644043 RMSE: 3.9475815\n",
      "65 10 3.708559989929199\n",
      "Validation loss: 16.058614253997803 RMSE: 4.00732\n",
      "Validation loss: 13.738256454467773 RMSE: 3.7065153\n",
      "Validation loss: 12.391613483428955 RMSE: 3.5201724\n",
      "68 12 1.8665093183517456\n",
      "Validation loss: 12.112534523010254 RMSE: 3.4803066\n",
      "Validation loss: 13.829706192016602 RMSE: 3.7188313\n",
      "Validation loss: 15.995981216430664 RMSE: 3.9994977\n",
      "71 14 3.323262929916382\n",
      "Validation loss: 15.907571792602539 RMSE: 3.9884293\n",
      "Validation loss: 10.967240810394287 RMSE: 3.3116825\n",
      "Validation loss: 11.733562469482422 RMSE: 3.4254289\n",
      "Validation loss: 16.173206329345703 RMSE: 4.0215926\n",
      "75 0 1.9951289892196655\n",
      "Validation loss: 11.038589477539062 RMSE: 3.3224373\n",
      "Validation loss: 14.48664903640747 RMSE: 3.806133\n",
      "Validation loss: 16.059914588928223 RMSE: 4.0074825\n",
      "78 2 2.175081491470337\n",
      "Validation loss: 15.192506790161133 RMSE: 3.8977568\n",
      "Validation loss: 14.57007122039795 RMSE: 3.8170762\n",
      "Validation loss: 18.18528699874878 RMSE: 4.264421\n",
      "81 4 2.3126912117004395\n",
      "Validation loss: 15.16754961013794 RMSE: 3.8945537\n",
      "Validation loss: 13.149205207824707 RMSE: 3.6261833\n",
      "Validation loss: 14.452478408813477 RMSE: 3.8016412\n",
      "84 6 2.007336378097534\n",
      "Validation loss: 16.700937747955322 RMSE: 4.086678\n",
      "Validation loss: 17.662017822265625 RMSE: 4.2026205\n",
      "Validation loss: 16.13263177871704 RMSE: 4.016545\n",
      "87 8 3.9528706073760986\n",
      "Validation loss: 17.0158371925354 RMSE: 4.125026\n",
      "Validation loss: 15.287677764892578 RMSE: 3.909946\n",
      "Validation loss: 16.41804265975952 RMSE: 4.051918\n",
      "90 10 1.923440933227539\n",
      "Validation loss: 19.81720495223999 RMSE: 4.4516516\n",
      "Validation loss: 19.827306747436523 RMSE: 4.4527864\n",
      "Validation loss: 16.620991706848145 RMSE: 4.076885\n",
      "93 12 3.145420551300049\n",
      "Validation loss: 14.982578754425049 RMSE: 3.8707337\n",
      "Validation loss: 17.741111755371094 RMSE: 4.21202\n",
      "Validation loss: 19.3242826461792 RMSE: 4.3959394\n",
      "96 14 1.0697792768478394\n",
      "Validation loss: 17.804616451263428 RMSE: 4.219551\n",
      "Validation loss: 15.133538961410522 RMSE: 3.8901849\n",
      "Validation loss: 14.724344968795776 RMSE: 3.8372312\n",
      "Validation loss: 18.986242294311523 RMSE: 4.357321\n",
      "100 0 1.1611344814300537\n",
      "Validation loss: 25.069405555725098 RMSE: 5.006936\n",
      "Validation loss: 23.665823936462402 RMSE: 4.8647532\n",
      "Validation loss: 18.88836669921875 RMSE: 4.346075\n",
      "103 2 1.8344762325286865\n",
      "Validation loss: 13.969436645507812 RMSE: 3.7375708\n",
      "Validation loss: 16.204793453216553 RMSE: 4.0255175\n",
      "Validation loss: 22.823397636413574 RMSE: 4.777384\n",
      "106 4 1.4717116355895996\n",
      "Validation loss: 15.265458106994629 RMSE: 3.9071035\n",
      "Validation loss: 15.642374038696289 RMSE: 3.9550445\n",
      "Validation loss: 19.23738670349121 RMSE: 4.3860445\n",
      "109 6 1.931166172027588\n",
      "Validation loss: 16.49720335006714 RMSE: 4.0616746\n",
      "Validation loss: 17.774806022644043 RMSE: 4.2160177\n",
      "Validation loss: 21.345765113830566 RMSE: 4.620147\n",
      "112 8 1.4861539602279663\n",
      "Validation loss: 15.242623805999756 RMSE: 3.9041803\n",
      "Validation loss: 15.849370002746582 RMSE: 3.9811265\n",
      "Validation loss: 17.71400547027588 RMSE: 4.2088013\n",
      "115 10 1.8549498319625854\n",
      "Validation loss: 19.45327377319336 RMSE: 4.410587\n",
      "Validation loss: 15.80932092666626 RMSE: 3.976094\n",
      "Validation loss: 17.722103118896484 RMSE: 4.2097626\n",
      "118 12 1.4108445644378662\n",
      "Validation loss: 14.443773746490479 RMSE: 3.800496\n",
      "Validation loss: 14.626283168792725 RMSE: 3.8244324\n",
      "Validation loss: 13.072661399841309 RMSE: 3.6156135\n",
      "121 14 1.8805432319641113\n",
      "Validation loss: 18.5842924118042 RMSE: 4.3109503\n",
      "Validation loss: 14.833457469940186 RMSE: 3.8514228\n",
      "Validation loss: 18.310799598693848 RMSE: 4.2791123\n",
      "Validation loss: 17.932514190673828 RMSE: 4.23468\n",
      "125 0 1.9889698028564453\n",
      "Validation loss: 15.457711219787598 RMSE: 3.9316292\n",
      "Validation loss: 16.222073554992676 RMSE: 4.027663\n",
      "Validation loss: 15.229807376861572 RMSE: 3.902538\n",
      "128 2 2.0621135234832764\n",
      "Validation loss: 13.223481178283691 RMSE: 3.6364107\n",
      "Validation loss: 16.37030792236328 RMSE: 4.046024\n",
      "Validation loss: 14.76057243347168 RMSE: 3.841949\n",
      "131 4 1.4598358869552612\n",
      "Validation loss: 16.72121024131775 RMSE: 4.0891576\n",
      "Validation loss: 17.631422519683838 RMSE: 4.1989784\n",
      "Validation loss: 17.4996337890625 RMSE: 4.183256\n",
      "134 6 2.2179853916168213\n",
      "Validation loss: 16.307964324951172 RMSE: 4.038312\n",
      "Validation loss: 18.637967109680176 RMSE: 4.317171\n",
      "Validation loss: 16.784378051757812 RMSE: 4.096874\n",
      "137 8 3.7897095680236816\n",
      "Validation loss: 16.096406936645508 RMSE: 4.0120325\n",
      "Validation loss: 20.01956081390381 RMSE: 4.4743223\n",
      "Validation loss: 24.151130199432373 RMSE: 4.9143796\n",
      "140 10 3.4864137172698975\n",
      "Validation loss: 21.299549102783203 RMSE: 4.6151433\n",
      "Validation loss: 20.06082534790039 RMSE: 4.4789314\n",
      "Validation loss: 14.836670637130737 RMSE: 3.8518398\n",
      "143 12 1.2561347484588623\n",
      "Validation loss: 16.94350242614746 RMSE: 4.1162486\n",
      "Validation loss: 16.84256362915039 RMSE: 4.103969\n",
      "Validation loss: 14.12322187423706 RMSE: 3.7580876\n",
      "146 14 1.0217008590698242\n",
      "Validation loss: 13.144696950912476 RMSE: 3.6255617\n",
      "Validation loss: 16.441434383392334 RMSE: 4.0548043\n",
      "Validation loss: 16.174455642700195 RMSE: 4.021748\n",
      "Validation loss: 15.834470748901367 RMSE: 3.979255\n",
      "150 0 1.0251836776733398\n",
      "Validation loss: 17.731080532073975 RMSE: 4.2108293\n",
      "Validation loss: 21.12210750579834 RMSE: 4.595879\n",
      "Validation loss: 19.130828857421875 RMSE: 4.3738804\n",
      "153 2 1.5886706113815308\n",
      "Validation loss: 13.9785795211792 RMSE: 3.7387938\n",
      "Validation loss: 19.452009677886963 RMSE: 4.410443\n",
      "Validation loss: 24.616437911987305 RMSE: 4.961496\n",
      "156 4 1.1909769773483276\n",
      "Validation loss: 16.332321166992188 RMSE: 4.041327\n",
      "Validation loss: 19.437186241149902 RMSE: 4.408763\n",
      "Validation loss: 16.65552282333374 RMSE: 4.0811176\n",
      "159 6 3.8525748252868652\n",
      "Validation loss: 20.605831146240234 RMSE: 4.539365\n",
      "Validation loss: 20.60057544708252 RMSE: 4.538786\n",
      "Validation loss: 21.48293972015381 RMSE: 4.634969\n",
      "162 8 2.1110079288482666\n",
      "Validation loss: 20.978124141693115 RMSE: 4.5801883\n",
      "Validation loss: 18.53464984893799 RMSE: 4.3051887\n",
      "Validation loss: 25.675430297851562 RMSE: 5.067093\n",
      "165 10 1.1974250078201294\n",
      "Validation loss: 21.986408233642578 RMSE: 4.6889668\n",
      "Validation loss: 18.632540702819824 RMSE: 4.316542\n",
      "Validation loss: 15.876608848571777 RMSE: 3.9845462\n",
      "168 12 1.4397037029266357\n",
      "Validation loss: 22.32636594772339 RMSE: 4.725078\n",
      "Validation loss: 15.78222942352295 RMSE: 3.9726853\n",
      "Validation loss: 19.658649444580078 RMSE: 4.4338074\n",
      "171 14 3.5170187950134277\n",
      "Validation loss: 15.866720199584961 RMSE: 3.9833052\n",
      "Validation loss: 23.097383499145508 RMSE: 4.8059735\n",
      "Validation loss: 18.11544132232666 RMSE: 4.256224\n",
      "Validation loss: 20.6008243560791 RMSE: 4.538813\n",
      "175 0 2.858555555343628\n",
      "Validation loss: 18.669559001922607 RMSE: 4.320828\n",
      "Validation loss: 20.64325475692749 RMSE: 4.5434847\n",
      "Validation loss: 23.1025972366333 RMSE: 4.806516\n",
      "178 2 1.794189214706421\n",
      "Validation loss: 14.998616218566895 RMSE: 3.8728046\n",
      "Validation loss: 17.441996097564697 RMSE: 4.1763616\n",
      "Validation loss: 21.211896896362305 RMSE: 4.6056376\n",
      "181 4 1.5622379779815674\n",
      "Validation loss: 22.981913566589355 RMSE: 4.793945\n",
      "Validation loss: 19.807384490966797 RMSE: 4.4505486\n",
      "Validation loss: 26.380516052246094 RMSE: 5.1361966\n",
      "184 6 0.9525099992752075\n",
      "Validation loss: 20.57501983642578 RMSE: 4.53597\n",
      "Validation loss: 24.993281841278076 RMSE: 4.9993286\n",
      "Validation loss: 18.970659255981445 RMSE: 4.355532\n",
      "187 8 0.6559220552444458\n",
      "Validation loss: 16.00129985809326 RMSE: 4.0001626\n",
      "Validation loss: 17.76885986328125 RMSE: 4.2153125\n",
      "Validation loss: 17.84870719909668 RMSE: 4.224773\n",
      "190 10 1.760936975479126\n",
      "Validation loss: 19.546135902404785 RMSE: 4.4211016\n",
      "Validation loss: 25.261391639709473 RMSE: 5.026071\n",
      "Validation loss: 21.442293167114258 RMSE: 4.6305823\n",
      "193 12 1.0253361463546753\n",
      "Validation loss: 18.864439964294434 RMSE: 4.3433213\n",
      "Validation loss: 25.65391731262207 RMSE: 5.06497\n",
      "Validation loss: 18.238245010375977 RMSE: 4.270626\n",
      "196 14 1.9015209674835205\n",
      "Validation loss: 16.66969347000122 RMSE: 4.0828533\n",
      "Validation loss: 25.249433517456055 RMSE: 5.0248814\n",
      "Validation loss: 20.515846729278564 RMSE: 4.5294423\n",
      "Validation loss: 16.48401165008545 RMSE: 4.0600505\n",
      "Loaded trained model with success.\n",
      "Test loss: 11.25438707791842 Test RMSE: 3.354756\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:0', 'task_name': 'freesolv', 'model_type': 'gin', 'model': {'num_layer': 3, 'emb_dim': 64, 'feat_dim': 64, 'drop_ratio': 0.5, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 781, 'task': 'regression', 'data_path': 'data/freesolv/freesolv.csv', 'target': ['expt']}}\n",
      "Running on: cuda:0\n",
      "641\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/641\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 16.74556541442871\n",
      "Validation loss: 63.38660430908203 RMSE: 7.9615707\n",
      "Validation loss: 57.684173583984375 RMSE: 7.59501\n",
      "Validation loss: 51.00223350524902 RMSE: 7.141585\n",
      "3 2 6.402101039886475\n",
      "Validation loss: 41.13797187805176 RMSE: 6.413889\n",
      "Validation loss: 30.807451248168945 RMSE: 5.550446\n",
      "Validation loss: 23.026949882507324 RMSE: 4.7986407\n",
      "6 4 4.326451301574707\n",
      "Validation loss: 19.19808864593506 RMSE: 4.381562\n",
      "Validation loss: 18.323620796203613 RMSE: 4.28061\n",
      "Validation loss: 17.632620811462402 RMSE: 4.1991215\n",
      "9 6 4.613740921020508\n",
      "Validation loss: 18.8660945892334 RMSE: 4.3435116\n",
      "Validation loss: 17.379679679870605 RMSE: 4.1688943\n",
      "Validation loss: 16.571602821350098 RMSE: 4.0708237\n",
      "12 8 6.492757320404053\n",
      "Validation loss: 16.61894702911377 RMSE: 4.076634\n",
      "Validation loss: 15.198864459991455 RMSE: 3.8985722\n",
      "Validation loss: 15.441926002502441 RMSE: 3.9296217\n",
      "15 10 6.838229656219482\n",
      "Validation loss: 15.954621315002441 RMSE: 3.9943237\n",
      "Validation loss: 14.677209854125977 RMSE: 3.8310847\n",
      "Validation loss: 15.088149547576904 RMSE: 3.8843467\n",
      "18 12 3.542149305343628\n",
      "Validation loss: 14.174734592437744 RMSE: 3.7649348\n",
      "Validation loss: 14.747479677200317 RMSE: 3.8402448\n",
      "Validation loss: 12.244252681732178 RMSE: 3.499179\n",
      "21 14 5.340814590454102\n",
      "Validation loss: 11.53340482711792 RMSE: 3.3960865\n",
      "Validation loss: 13.29342794418335 RMSE: 3.6460154\n",
      "Validation loss: 14.131017684936523 RMSE: 3.7591248\n",
      "Validation loss: 12.758755207061768 RMSE: 3.5719402\n",
      "25 0 4.086006164550781\n",
      "Validation loss: 12.918633937835693 RMSE: 3.5942502\n",
      "Validation loss: 14.188650608062744 RMSE: 3.7667825\n",
      "Validation loss: 12.993949890136719 RMSE: 3.6047122\n",
      "28 2 4.551969051361084\n",
      "Validation loss: 13.889570236206055 RMSE: 3.7268715\n",
      "Validation loss: 13.502369403839111 RMSE: 3.674557\n",
      "Validation loss: 9.97744607925415 RMSE: 3.1587098\n",
      "31 4 7.849936008453369\n",
      "Validation loss: 11.811340570449829 RMSE: 3.436763\n",
      "Validation loss: 11.601058006286621 RMSE: 3.4060326\n",
      "Validation loss: 10.781723976135254 RMSE: 3.2835534\n",
      "34 6 3.4022443294525146\n",
      "Validation loss: 11.001989841461182 RMSE: 3.3169246\n",
      "Validation loss: 13.52143383026123 RMSE: 3.6771502\n",
      "Validation loss: 14.091110229492188 RMSE: 3.7538128\n",
      "37 8 4.887110233306885\n",
      "Validation loss: 11.06200885772705 RMSE: 3.32596\n",
      "Validation loss: 10.793646335601807 RMSE: 3.2853682\n",
      "Validation loss: 11.900909900665283 RMSE: 3.4497693\n",
      "40 10 2.9925477504730225\n",
      "Validation loss: 11.059616088867188 RMSE: 3.3256004\n",
      "Validation loss: 12.780544757843018 RMSE: 3.5749886\n",
      "Validation loss: 13.062439918518066 RMSE: 3.6141999\n",
      "43 12 5.120192527770996\n",
      "Validation loss: 13.697444915771484 RMSE: 3.701006\n",
      "Validation loss: 13.697967529296875 RMSE: 3.7010767\n",
      "Validation loss: 11.247716426849365 RMSE: 3.3537617\n",
      "46 14 3.47725248336792\n",
      "Validation loss: 11.248709201812744 RMSE: 3.3539095\n",
      "Validation loss: 14.829768180847168 RMSE: 3.850944\n",
      "Validation loss: 15.380952835083008 RMSE: 3.9218557\n",
      "Validation loss: 11.429895401000977 RMSE: 3.380813\n",
      "50 0 3.7881338596343994\n",
      "Validation loss: 12.536645889282227 RMSE: 3.5407126\n",
      "Validation loss: 13.847443103790283 RMSE: 3.7212152\n",
      "Validation loss: 15.746686935424805 RMSE: 3.9682095\n",
      "53 2 3.791456937789917\n",
      "Validation loss: 13.715331077575684 RMSE: 3.7034216\n",
      "Validation loss: 10.250007390975952 RMSE: 3.2015631\n",
      "Validation loss: 10.666882514953613 RMSE: 3.266019\n",
      "56 4 5.367001056671143\n",
      "Validation loss: 14.021526336669922 RMSE: 3.744533\n",
      "Validation loss: 14.005155563354492 RMSE: 3.7423463\n",
      "Validation loss: 15.575643539428711 RMSE: 3.9465988\n",
      "59 6 3.633005142211914\n",
      "Validation loss: 16.31309700012207 RMSE: 4.0389476\n",
      "Validation loss: 14.434450626373291 RMSE: 3.7992694\n",
      "Validation loss: 15.316649436950684 RMSE: 3.9136493\n",
      "62 8 2.8955564498901367\n",
      "Validation loss: 12.6702561378479 RMSE: 3.5595303\n",
      "Validation loss: 15.136563777923584 RMSE: 3.8905735\n",
      "Validation loss: 13.38088083267212 RMSE: 3.6579888\n",
      "65 10 3.4928476810455322\n",
      "Validation loss: 14.535232067108154 RMSE: 3.8125098\n",
      "Validation loss: 13.237459182739258 RMSE: 3.638332\n",
      "Validation loss: 13.376732349395752 RMSE: 3.6574214\n",
      "68 12 1.8745694160461426\n",
      "Validation loss: 12.939635753631592 RMSE: 3.5971706\n",
      "Validation loss: 14.416783332824707 RMSE: 3.796944\n",
      "Validation loss: 11.94535505771637 RMSE: 3.4562051\n",
      "71 14 1.800370454788208\n",
      "Validation loss: 13.196065902709961 RMSE: 3.6326392\n",
      "Validation loss: 16.441332817077637 RMSE: 4.0547915\n",
      "Validation loss: 9.338644981384277 RMSE: 3.05592\n",
      "Validation loss: 13.917295455932617 RMSE: 3.730589\n",
      "75 0 2.025597333908081\n",
      "Validation loss: 16.018157958984375 RMSE: 4.0022693\n",
      "Validation loss: 12.110769271850586 RMSE: 3.4800527\n",
      "Validation loss: 12.925306797027588 RMSE: 3.5951781\n",
      "78 2 1.7988204956054688\n",
      "Validation loss: 10.509582042694092 RMSE: 3.2418482\n",
      "Validation loss: 10.637791633605957 RMSE: 3.2615626\n",
      "Validation loss: 12.273801803588867 RMSE: 3.5033987\n",
      "81 4 3.774691581726074\n",
      "Validation loss: 14.912134885787964 RMSE: 3.8616238\n",
      "Validation loss: 12.51008415222168 RMSE: 3.5369596\n",
      "Validation loss: 15.073023319244385 RMSE: 3.8823988\n",
      "84 6 3.2688827514648438\n",
      "Validation loss: 12.124000072479248 RMSE: 3.4819536\n",
      "Validation loss: 14.815370082855225 RMSE: 3.849074\n",
      "Validation loss: 13.434607028961182 RMSE: 3.665325\n",
      "87 8 1.669991374015808\n",
      "Validation loss: 14.54570484161377 RMSE: 3.813883\n",
      "Validation loss: 13.205615997314453 RMSE: 3.6339536\n",
      "Validation loss: 15.685457229614258 RMSE: 3.960487\n",
      "90 10 4.763577938079834\n",
      "Validation loss: 13.097210884094238 RMSE: 3.6190069\n",
      "Validation loss: 16.0765438079834 RMSE: 4.0095563\n",
      "Validation loss: 14.707859992980957 RMSE: 3.835083\n",
      "93 12 2.5867977142333984\n",
      "Validation loss: 18.386558532714844 RMSE: 4.287955\n",
      "Validation loss: 21.234917402267456 RMSE: 4.6081357\n",
      "Validation loss: 13.204264163970947 RMSE: 3.6337671\n",
      "96 14 2.279416084289551\n",
      "Validation loss: 12.092931747436523 RMSE: 3.4774892\n",
      "Validation loss: 11.860168695449829 RMSE: 3.4438596\n",
      "Validation loss: 13.369680404663086 RMSE: 3.6564574\n",
      "Validation loss: 16.704278469085693 RMSE: 4.0870867\n",
      "100 0 2.417187213897705\n",
      "Validation loss: 14.621443748474121 RMSE: 3.8237996\n",
      "Validation loss: 18.27851104736328 RMSE: 4.2753377\n",
      "Validation loss: 16.682238578796387 RMSE: 4.084389\n",
      "103 2 2.0693633556365967\n",
      "Validation loss: 17.459505319595337 RMSE: 4.178457\n",
      "Validation loss: 15.556756019592285 RMSE: 3.944205\n",
      "Validation loss: 17.369789600372314 RMSE: 4.1677074\n",
      "106 4 1.893152117729187\n",
      "Validation loss: 14.220834255218506 RMSE: 3.7710521\n",
      "Validation loss: 18.990248918533325 RMSE: 4.35778\n",
      "Validation loss: 16.69533061981201 RMSE: 4.085992\n",
      "109 6 2.1967051029205322\n",
      "Validation loss: 16.98377799987793 RMSE: 4.121138\n",
      "Validation loss: 13.070507526397705 RMSE: 3.6153157\n",
      "Validation loss: 13.635213851928711 RMSE: 3.6925895\n",
      "112 8 2.200380802154541\n",
      "Validation loss: 18.390940189361572 RMSE: 4.288466\n",
      "Validation loss: 19.20833969116211 RMSE: 4.382732\n",
      "Validation loss: 17.97623300552368 RMSE: 4.2398386\n",
      "115 10 3.1463704109191895\n",
      "Validation loss: 17.773866653442383 RMSE: 4.2159066\n",
      "Validation loss: 16.00180959701538 RMSE: 4.000226\n",
      "Validation loss: 15.978066444396973 RMSE: 3.9972575\n",
      "118 12 2.5509722232818604\n",
      "Validation loss: 15.545241355895996 RMSE: 3.9427454\n",
      "Validation loss: 17.15029287338257 RMSE: 4.141291\n",
      "Validation loss: 16.84872579574585 RMSE: 4.1047196\n",
      "121 14 1.371463656425476\n",
      "Validation loss: 14.799016952514648 RMSE: 3.8469493\n",
      "Validation loss: 16.81536340713501 RMSE: 4.100654\n",
      "Validation loss: 14.969862937927246 RMSE: 3.8690908\n",
      "Validation loss: 12.951550483703613 RMSE: 3.5988262\n",
      "125 0 2.093282699584961\n",
      "Validation loss: 19.981207847595215 RMSE: 4.4700346\n",
      "Validation loss: 16.15994882583618 RMSE: 4.019944\n",
      "Validation loss: 20.25848960876465 RMSE: 4.5009427\n",
      "128 2 1.1541897058486938\n",
      "Validation loss: 16.768974781036377 RMSE: 4.0949936\n",
      "Validation loss: 20.13719892501831 RMSE: 4.4874487\n",
      "Validation loss: 11.673525333404541 RMSE: 3.416654\n",
      "131 4 2.2492058277130127\n",
      "Validation loss: 18.08255672454834 RMSE: 4.252359\n",
      "Validation loss: 14.543004512786865 RMSE: 3.8135293\n",
      "Validation loss: 21.74094295501709 RMSE: 4.662718\n",
      "134 6 2.131573438644409\n",
      "Validation loss: 17.305312156677246 RMSE: 4.159965\n",
      "Validation loss: 13.208703517913818 RMSE: 3.6343777\n",
      "Validation loss: 20.259512901306152 RMSE: 4.501057\n",
      "137 8 2.0610289573669434\n",
      "Validation loss: 17.652621269226074 RMSE: 4.201502\n",
      "Validation loss: 15.931838035583496 RMSE: 3.9914706\n",
      "Validation loss: 21.596474647521973 RMSE: 4.647201\n",
      "140 10 1.9610216617584229\n",
      "Validation loss: 13.879083156585693 RMSE: 3.725464\n",
      "Validation loss: 12.25015115737915 RMSE: 3.5000217\n",
      "Validation loss: 19.435582637786865 RMSE: 4.4085803\n",
      "143 12 1.285661220550537\n",
      "Validation loss: 14.95486068725586 RMSE: 3.8671517\n",
      "Validation loss: 15.76915693283081 RMSE: 3.9710395\n",
      "Validation loss: 16.871129035949707 RMSE: 4.1074486\n",
      "146 14 1.1094681024551392\n",
      "Validation loss: 15.529818534851074 RMSE: 3.940789\n",
      "Validation loss: 19.080629348754883 RMSE: 4.368138\n",
      "Validation loss: 17.49357032775879 RMSE: 4.1825314\n",
      "Validation loss: 18.878417015075684 RMSE: 4.3449297\n",
      "150 0 1.4086049795150757\n",
      "Validation loss: 19.910926818847656 RMSE: 4.4621663\n",
      "Validation loss: 16.691426753997803 RMSE: 4.085514\n",
      "Validation loss: 22.58553695678711 RMSE: 4.7524242\n",
      "153 2 2.244403839111328\n",
      "Validation loss: 15.141536235809326 RMSE: 3.891213\n",
      "Validation loss: 13.170625686645508 RMSE: 3.6291358\n",
      "Validation loss: 16.678268432617188 RMSE: 4.083904\n",
      "156 4 4.744884490966797\n",
      "Validation loss: 28.30927085876465 RMSE: 5.320646\n",
      "Validation loss: 19.39470100402832 RMSE: 4.4039416\n",
      "Validation loss: 19.0298433303833 RMSE: 4.3623214\n",
      "159 6 2.493027925491333\n",
      "Validation loss: 17.402413368225098 RMSE: 4.17162\n",
      "Validation loss: 23.459585666656494 RMSE: 4.843509\n",
      "Validation loss: 19.539624214172363 RMSE: 4.4203644\n",
      "162 8 3.1074347496032715\n",
      "Validation loss: 16.19172763824463 RMSE: 4.023895\n",
      "Validation loss: 20.79789924621582 RMSE: 4.560471\n",
      "Validation loss: 21.252107620239258 RMSE: 4.610001\n",
      "165 10 1.7717574834823608\n",
      "Validation loss: 17.766573429107666 RMSE: 4.215041\n",
      "Validation loss: 22.70857000350952 RMSE: 4.765351\n",
      "Validation loss: 23.831751823425293 RMSE: 4.8817773\n",
      "168 12 3.0557022094726562\n",
      "Validation loss: 19.061219215393066 RMSE: 4.3659153\n",
      "Validation loss: 30.019468307495117 RMSE: 5.4790025\n",
      "Validation loss: 23.91339683532715 RMSE: 4.890133\n",
      "171 14 1.5903412103652954\n",
      "Validation loss: 17.676351070404053 RMSE: 4.204325\n",
      "Validation loss: 14.388567447662354 RMSE: 3.7932265\n",
      "Validation loss: 20.051398277282715 RMSE: 4.477879\n",
      "Validation loss: 14.948643684387207 RMSE: 3.8663476\n",
      "175 0 0.9657242894172668\n",
      "Validation loss: 17.705203533172607 RMSE: 4.2077556\n",
      "Validation loss: 19.989730834960938 RMSE: 4.470988\n",
      "Validation loss: 21.80582904815674 RMSE: 4.669671\n",
      "178 2 2.6533520221710205\n",
      "Validation loss: 15.48774242401123 RMSE: 3.935447\n",
      "Validation loss: 15.178406715393066 RMSE: 3.8959475\n",
      "Validation loss: 17.016157627105713 RMSE: 4.125065\n",
      "181 4 2.0592315196990967\n",
      "Validation loss: 18.76251792907715 RMSE: 4.331572\n",
      "Validation loss: 18.714917182922363 RMSE: 4.3260746\n",
      "Validation loss: 16.240910530090332 RMSE: 4.030001\n",
      "184 6 1.966886043548584\n",
      "Validation loss: 15.406102180480957 RMSE: 3.9250607\n",
      "Validation loss: 21.27266836166382 RMSE: 4.6122303\n",
      "Validation loss: 22.976909160614014 RMSE: 4.7934237\n",
      "187 8 2.0571372509002686\n",
      "Validation loss: 20.770456314086914 RMSE: 4.5574617\n",
      "Validation loss: 24.446197986602783 RMSE: 4.94431\n",
      "Validation loss: 21.671290397644043 RMSE: 4.6552434\n",
      "190 10 1.7624297142028809\n",
      "Validation loss: 27.394164562225342 RMSE: 5.233944\n",
      "Validation loss: 25.715182304382324 RMSE: 5.0710144\n",
      "Validation loss: 17.331271171569824 RMSE: 4.1630845\n",
      "193 12 1.4886925220489502\n",
      "Validation loss: 20.397324562072754 RMSE: 4.51634\n",
      "Validation loss: 23.284489631652832 RMSE: 4.825401\n",
      "Validation loss: 15.687317848205566 RMSE: 3.960722\n",
      "196 14 2.0438435077667236\n",
      "Validation loss: 14.101327419281006 RMSE: 3.7551734\n",
      "Validation loss: 17.349361419677734 RMSE: 4.165257\n",
      "Validation loss: 21.493144512176514 RMSE: 4.6360703\n",
      "Validation loss: 14.441487789154053 RMSE: 3.8001955\n",
      "Loaded trained model with success.\n",
      "Test loss: 8.226399447367742 Test RMSE: 2.86817\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:0', 'task_name': 'esol', 'model_type': 'gin', 'model': {'num_layer': 5, 'emb_dim': 300, 'feat_dim': 300, 'drop_ratio': 0.3, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 777, 'task': 'regression', 'data_path': 'data/esol/esol.csv', 'target': ['measured log solubility in mols per litre']}}\n",
      "Running on: cuda:0\n",
      "1127\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/1127\n",
      "Generating scaffold 1000/1127\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 7.9666008949279785\n",
      "Validation loss: 3.0504301796972224 RMSE: 1.7465479\n",
      "1 21 1.7198612689971924\n",
      "Validation loss: 3.3096286849638004 RMSE: 1.8192384\n",
      "Validation loss: 4.067866021552972 RMSE: 2.0168953\n",
      "3 13 1.5450520515441895\n",
      "Validation loss: 2.6926144751827272 RMSE: 1.6409189\n",
      "Validation loss: 2.8900236703653253 RMSE: 1.7000068\n",
      "5 5 1.2559664249420166\n",
      "Validation loss: 3.248302601080025 RMSE: 1.8023049\n",
      "6 26 1.6953125\n",
      "Validation loss: 2.2355273761580476 RMSE: 1.4951681\n",
      "Validation loss: 2.753463787315166 RMSE: 1.6593564\n",
      "8 18 1.7832000255584717\n",
      "Validation loss: 1.8962851288044347 RMSE: 1.3770567\n",
      "Validation loss: 1.8963220604753073 RMSE: 1.3770701\n",
      "10 10 1.5475863218307495\n",
      "Validation loss: 2.744300669273444 RMSE: 1.6565931\n",
      "Validation loss: 2.6098877202093074 RMSE: 1.6155146\n",
      "12 2 0.7584631443023682\n",
      "Validation loss: 3.093509836534483 RMSE: 1.7588376\n",
      "13 23 0.8566159605979919\n",
      "Validation loss: 2.3412234593281704 RMSE: 1.5301057\n",
      "Validation loss: 2.587583735980819 RMSE: 1.6085968\n",
      "15 15 0.9661257863044739\n",
      "Validation loss: 1.9163980378513843 RMSE: 1.3843403\n",
      "Validation loss: 2.472827738365241 RMSE: 1.5725228\n",
      "17 7 1.3946844339370728\n",
      "Validation loss: 1.7563866777757629 RMSE: 1.3252873\n",
      "18 28 1.0727770328521729\n",
      "Validation loss: 1.9624850728870493 RMSE: 1.4008873\n",
      "Validation loss: 1.6424256067360397 RMSE: 1.2815715\n",
      "20 20 1.6592650413513184\n",
      "Validation loss: 2.976296353129159 RMSE: 1.7251946\n",
      "Validation loss: 1.4660028662301798 RMSE: 1.210786\n",
      "22 12 0.6798304915428162\n",
      "Validation loss: 1.668460135438801 RMSE: 1.2916889\n",
      "Validation loss: 2.6152168522893855 RMSE: 1.6171631\n",
      "24 4 0.9427969455718994\n",
      "Validation loss: 1.9275664238803154 RMSE: 1.3883682\n",
      "25 25 0.5691336393356323\n",
      "Validation loss: 1.8085112972597106 RMSE: 1.344809\n",
      "Validation loss: 2.231128331834236 RMSE: 1.4936962\n",
      "27 17 0.8072579503059387\n",
      "Validation loss: 2.6926579158917994 RMSE: 1.6409318\n",
      "Validation loss: 1.9736842455062191 RMSE: 1.4048787\n",
      "29 9 0.8251346349716187\n",
      "Validation loss: 1.4326045196668236 RMSE: 1.1969147\n",
      "Validation loss: 2.435956640581114 RMSE: 1.5607551\n",
      "31 1 1.1130117177963257\n",
      "Validation loss: 1.5880655757093851 RMSE: 1.2601848\n",
      "32 22 1.0424764156341553\n",
      "Validation loss: 1.7370848064928983 RMSE: 1.317985\n",
      "Validation loss: 1.989101844551289 RMSE: 1.4103552\n",
      "34 14 0.9572731852531433\n",
      "Validation loss: 1.7788082540562722 RMSE: 1.3337197\n",
      "Validation loss: 1.6671244169758483 RMSE: 1.2911717\n",
      "36 6 0.6756823658943176\n",
      "Validation loss: 1.5978419622488782 RMSE: 1.2640578\n",
      "37 27 1.3207426071166992\n",
      "Validation loss: 2.080507120199963 RMSE: 1.4423963\n",
      "Validation loss: 2.172790531563548 RMSE: 1.4740388\n",
      "39 19 0.940467357635498\n",
      "Validation loss: 2.4894793202391767 RMSE: 1.5778084\n",
      "Validation loss: 1.5212425204504907 RMSE: 1.2333866\n",
      "41 11 1.133658766746521\n",
      "Validation loss: 1.9974360466003418 RMSE: 1.4133067\n",
      "Validation loss: 2.3889496263149566 RMSE: 1.5456227\n",
      "43 3 0.7632214426994324\n",
      "Validation loss: 1.9329138482566428 RMSE: 1.3902926\n",
      "44 24 0.9569575190544128\n",
      "Validation loss: 1.6984184547863175 RMSE: 1.3032339\n",
      "Validation loss: 2.5786459319359434 RMSE: 1.6058162\n",
      "46 16 0.49717092514038086\n",
      "Validation loss: 1.7982908107538138 RMSE: 1.3410037\n",
      "Validation loss: 1.7468777378048517 RMSE: 1.321695\n",
      "48 8 0.6059127449989319\n",
      "Validation loss: 1.9660558436824158 RMSE: 1.4021611\n",
      "Validation loss: 1.9880755295795678 RMSE: 1.4099914\n",
      "50 0 0.6185934543609619\n",
      "Validation loss: 2.0720691132334483 RMSE: 1.4394683\n",
      "51 21 0.7373723387718201\n",
      "Validation loss: 2.0491229053092215 RMSE: 1.4314758\n",
      "Validation loss: 1.7870508843818598 RMSE: 1.3368063\n",
      "53 13 0.5237336754798889\n",
      "Validation loss: 1.9395974684605557 RMSE: 1.3926942\n",
      "Validation loss: 2.0599756283042705 RMSE: 1.4352616\n",
      "55 5 1.0547447204589844\n",
      "Validation loss: 2.0351403012739873 RMSE: 1.4265834\n",
      "56 26 1.4604227542877197\n",
      "Validation loss: 1.4845329660229978 RMSE: 1.2184141\n",
      "Validation loss: 1.8086334213746333 RMSE: 1.3448545\n",
      "58 18 0.7616488933563232\n",
      "Validation loss: 1.762007871560291 RMSE: 1.3274064\n",
      "Validation loss: 1.968165543227069 RMSE: 1.4029131\n",
      "60 10 0.755372166633606\n",
      "Validation loss: 2.116535796528369 RMSE: 1.4548318\n",
      "Validation loss: 1.775606360055704 RMSE: 1.3325188\n",
      "62 2 0.552680253982544\n",
      "Validation loss: 1.7035229733559938 RMSE: 1.3051908\n",
      "63 23 0.366057425737381\n",
      "Validation loss: 1.5957201350051744 RMSE: 1.2632182\n",
      "Validation loss: 1.9111864482407022 RMSE: 1.3824567\n",
      "65 15 0.4819067716598511\n",
      "Validation loss: 1.3596493596524264 RMSE: 1.1660401\n",
      "Validation loss: 1.4477780276695185 RMSE: 1.2032365\n",
      "67 7 0.5340243577957153\n",
      "Validation loss: 1.62644220453448 RMSE: 1.2753204\n",
      "68 28 3.481602191925049\n",
      "Validation loss: 1.9757512227623863 RMSE: 1.4056143\n",
      "Validation loss: 2.228052931549275 RMSE: 1.4926662\n",
      "70 20 0.7734472751617432\n",
      "Validation loss: 1.7568204613913476 RMSE: 1.325451\n",
      "Validation loss: 2.2952743156821325 RMSE: 1.5150163\n",
      "72 12 1.1642683744430542\n",
      "Validation loss: 1.957836355783243 RMSE: 1.399227\n",
      "Validation loss: 2.502282195386633 RMSE: 1.5818603\n",
      "74 4 0.982238233089447\n",
      "Validation loss: 1.7870413223199084 RMSE: 1.3368027\n",
      "75 25 0.6358850002288818\n",
      "Validation loss: 1.6564075651422012 RMSE: 1.287015\n",
      "Validation loss: 1.3361651190614279 RMSE: 1.1559261\n",
      "77 17 0.6947178244590759\n",
      "Validation loss: 1.8098054755050523 RMSE: 1.3452901\n",
      "Validation loss: 1.4154176880828047 RMSE: 1.1897131\n",
      "79 9 0.6666361093521118\n",
      "Validation loss: 1.874567060343987 RMSE: 1.3691483\n",
      "Validation loss: 1.5380572449844496 RMSE: 1.2401844\n",
      "81 1 0.8692800998687744\n",
      "Validation loss: 2.236733404935989 RMSE: 1.4955713\n",
      "82 22 0.7074838876724243\n",
      "Validation loss: 1.781725614471773 RMSE: 1.3348129\n",
      "Validation loss: 1.6739980288311445 RMSE: 1.2938308\n",
      "84 14 0.6992225646972656\n",
      "Validation loss: 2.0515898687649616 RMSE: 1.4323372\n",
      "Validation loss: 1.6745912965420073 RMSE: 1.29406\n",
      "86 6 0.7896856665611267\n",
      "Validation loss: 1.5543181526977403 RMSE: 1.2467229\n",
      "87 27 1.007430076599121\n",
      "Validation loss: 1.6736696895244902 RMSE: 1.2937039\n",
      "Validation loss: 1.7660537377922936 RMSE: 1.3289294\n",
      "89 19 0.4818243682384491\n",
      "Validation loss: 1.4822703863667175 RMSE: 1.2174853\n",
      "Validation loss: 2.2372703161914793 RMSE: 1.4957508\n",
      "91 11 0.44572749733924866\n",
      "Validation loss: 1.868631712103312 RMSE: 1.366979\n",
      "Validation loss: 1.95944191185774 RMSE: 1.3998007\n",
      "93 3 0.2747959792613983\n",
      "Validation loss: 1.7119692690604555 RMSE: 1.3084224\n",
      "94 24 0.3580451011657715\n",
      "Validation loss: 1.8520033602165964 RMSE: 1.3608832\n",
      "Validation loss: 1.7572178851186702 RMSE: 1.3256011\n",
      "96 16 0.42230626940727234\n",
      "Validation loss: 1.897075372459614 RMSE: 1.3773437\n",
      "Validation loss: 2.234764215165535 RMSE: 1.4949129\n",
      "98 8 0.6704267859458923\n",
      "Validation loss: 1.9522016354366742 RMSE: 1.3972121\n",
      "Validation loss: 1.9342464282449368 RMSE: 1.3907719\n",
      "100 0 0.5328727960586548\n",
      "Validation loss: 1.729464638022195 RMSE: 1.3150911\n",
      "101 21 0.288041889667511\n",
      "Validation loss: 1.731651816747885 RMSE: 1.3159224\n",
      "Validation loss: 1.516073767062837 RMSE: 1.2312895\n",
      "103 13 0.341680109500885\n",
      "Validation loss: 2.1077736689981106 RMSE: 1.4518174\n",
      "Validation loss: 1.7871177945516805 RMSE: 1.3368312\n",
      "105 5 0.9084823131561279\n",
      "Validation loss: 2.043629503355617 RMSE: 1.4295557\n",
      "106 26 0.452514111995697\n",
      "Validation loss: 1.6522212387186237 RMSE: 1.2853876\n",
      "Validation loss: 1.9251144850148565 RMSE: 1.3874849\n",
      "108 18 0.49869677424430847\n",
      "Validation loss: 1.533318454712893 RMSE: 1.2382724\n",
      "Validation loss: 1.5559487226790032 RMSE: 1.2473768\n",
      "110 10 0.9463213086128235\n",
      "Validation loss: 1.709328607120345 RMSE: 1.3074129\n",
      "Validation loss: 1.6821753504001988 RMSE: 1.296987\n",
      "112 2 0.74485182762146\n",
      "Validation loss: 1.6920539235646745 RMSE: 1.3007897\n",
      "113 23 0.48795679211616516\n",
      "Validation loss: 1.54220814725994 RMSE: 1.2418567\n",
      "Validation loss: 2.0224236228824717 RMSE: 1.4221194\n",
      "115 15 0.7831339240074158\n",
      "Validation loss: 1.5485499284963693 RMSE: 1.2444075\n",
      "Validation loss: 1.8966671365552243 RMSE: 1.3771954\n",
      "117 7 0.5135574340820312\n",
      "Validation loss: 1.745369603148604 RMSE: 1.3211243\n",
      "118 28 0.8105055689811707\n",
      "Validation loss: 1.4815964867583418 RMSE: 1.2172085\n",
      "Validation loss: 1.5565359708482185 RMSE: 1.247612\n",
      "120 20 0.47943490743637085\n",
      "Validation loss: 1.4487552263040457 RMSE: 1.2036424\n",
      "Validation loss: 1.4382244111162372 RMSE: 1.19926\n",
      "122 12 0.5183612704277039\n",
      "Validation loss: 1.7793339142757179 RMSE: 1.3339167\n",
      "Validation loss: 1.7465475829301682 RMSE: 1.3215702\n",
      "124 4 0.5217676162719727\n",
      "Validation loss: 1.5580315948587604 RMSE: 1.2482114\n",
      "125 25 0.38201653957366943\n",
      "Validation loss: 1.6950203391303003 RMSE: 1.3019295\n",
      "Validation loss: 1.6898105608678498 RMSE: 1.2999271\n",
      "127 17 0.3663908839225769\n",
      "Validation loss: 1.7087303262896243 RMSE: 1.3071841\n",
      "Validation loss: 1.8812561008782513 RMSE: 1.371589\n",
      "129 9 0.7218230962753296\n",
      "Validation loss: 2.303972386680873 RMSE: 1.517884\n",
      "Validation loss: 1.946685808422291 RMSE: 1.3952367\n",
      "131 1 0.8773831725120544\n",
      "Validation loss: 1.512286867715616 RMSE: 1.2297508\n",
      "132 22 0.4282042980194092\n",
      "Validation loss: 1.6371628341421616 RMSE: 1.2795167\n",
      "Validation loss: 1.5117127541947153 RMSE: 1.2295172\n",
      "134 14 0.48590484261512756\n",
      "Validation loss: 1.5801448283997257 RMSE: 1.2570381\n",
      "Validation loss: 1.852681845690297 RMSE: 1.3611326\n",
      "136 6 0.8895686864852905\n",
      "Validation loss: 1.8627204694579134 RMSE: 1.3648152\n",
      "137 27 0.28686630725860596\n",
      "Validation loss: 1.6986992485755312 RMSE: 1.3033415\n",
      "Validation loss: 1.9753233806221886 RMSE: 1.405462\n",
      "139 19 0.44716620445251465\n",
      "Validation loss: 2.228190890455668 RMSE: 1.4927125\n",
      "Validation loss: 1.6218203538287002 RMSE: 1.2735071\n",
      "141 11 0.36759331822395325\n",
      "Validation loss: 1.4387083412271686 RMSE: 1.1994617\n",
      "Validation loss: 1.9093521327044056 RMSE: 1.3817931\n",
      "143 3 0.6588448286056519\n",
      "Validation loss: 1.7085631663820384 RMSE: 1.3071202\n",
      "144 24 0.5396010875701904\n",
      "Validation loss: 1.5510853898208754 RMSE: 1.2454258\n",
      "Validation loss: 1.6922212796928608 RMSE: 1.3008541\n",
      "146 16 0.7267388105392456\n",
      "Validation loss: 1.4370144869373962 RMSE: 1.1987555\n",
      "Validation loss: 1.9712050689005218 RMSE: 1.4039961\n",
      "148 8 0.8736611604690552\n",
      "Validation loss: 1.617469480607362 RMSE: 1.2717977\n",
      "Validation loss: 1.9406480493798721 RMSE: 1.3930714\n",
      "150 0 0.5086185336112976\n",
      "Validation loss: 1.5633931908987264 RMSE: 1.2503572\n",
      "151 21 0.5439521074295044\n",
      "Validation loss: 1.813499308265416 RMSE: 1.3466623\n",
      "Validation loss: 1.751062203297573 RMSE: 1.323277\n",
      "153 13 0.3042253851890564\n",
      "Validation loss: 2.022592474928999 RMSE: 1.4221787\n",
      "Validation loss: 1.759889984552839 RMSE: 1.3266084\n",
      "155 5 0.3408796787261963\n",
      "Validation loss: 1.8446315706303689 RMSE: 1.3581722\n",
      "156 26 0.8586868047714233\n",
      "Validation loss: 1.780899017258028 RMSE: 1.3345033\n",
      "Validation loss: 1.54274713043618 RMSE: 1.2420737\n",
      "158 18 0.604171633720398\n",
      "Validation loss: 1.8789084727785228 RMSE: 1.3707329\n",
      "Validation loss: 1.5986815089673068 RMSE: 1.2643898\n",
      "160 10 0.37349948287010193\n",
      "Validation loss: 1.6476886768256669 RMSE: 1.2836232\n",
      "Validation loss: 1.7268213234116545 RMSE: 1.3140857\n",
      "162 2 0.3251417875289917\n",
      "Validation loss: 1.9025340987517771 RMSE: 1.3793238\n",
      "163 23 0.35794565081596375\n",
      "Validation loss: 1.6453163011939125 RMSE: 1.2826989\n",
      "Validation loss: 1.6470464325584142 RMSE: 1.2833731\n",
      "165 15 0.32118698954582214\n",
      "Validation loss: 1.632385734963206 RMSE: 1.2776486\n",
      "Validation loss: 1.786049287931054 RMSE: 1.3364316\n",
      "167 7 0.44168007373809814\n",
      "Validation loss: 1.71086997690454 RMSE: 1.3080024\n",
      "168 28 0.7851014733314514\n",
      "Validation loss: 1.5477313742173457 RMSE: 1.2440785\n",
      "Validation loss: 1.6221268577913268 RMSE: 1.2736275\n",
      "170 20 0.35743576288223267\n",
      "Validation loss: 1.5546883829927023 RMSE: 1.2468715\n",
      "Validation loss: 1.6743600094212896 RMSE: 1.2939706\n",
      "172 12 0.3447642922401428\n",
      "Validation loss: 1.5492524362243383 RMSE: 1.2446897\n",
      "Validation loss: 1.8430874210543338 RMSE: 1.3576035\n",
      "174 4 0.36047741770744324\n",
      "Validation loss: 2.301239709938522 RMSE: 1.5169836\n",
      "175 25 0.4533170759677887\n",
      "Validation loss: 1.6895084423301494 RMSE: 1.2998109\n",
      "Validation loss: 1.5385266183751873 RMSE: 1.2403736\n",
      "177 17 0.41404542326927185\n",
      "Validation loss: 1.455043211447454 RMSE: 1.2062517\n",
      "Validation loss: 1.6551929554053113 RMSE: 1.286543\n",
      "179 9 0.29702678322792053\n",
      "Validation loss: 1.6909439996280502 RMSE: 1.3003631\n",
      "Validation loss: 1.444516731574472 RMSE: 1.2018806\n",
      "181 1 0.1922929286956787\n",
      "Validation loss: 1.584705266277347 RMSE: 1.2588508\n",
      "182 22 0.6199151277542114\n",
      "Validation loss: 1.4591666512784705 RMSE: 1.2079597\n",
      "Validation loss: 1.6757719326863247 RMSE: 1.2945162\n",
      "184 14 0.6965204477310181\n",
      "Validation loss: 1.502900891599402 RMSE: 1.2259287\n",
      "Validation loss: 1.5529084775300153 RMSE: 1.2461574\n",
      "186 6 0.18613484501838684\n",
      "Validation loss: 1.587885824979934 RMSE: 1.2601134\n",
      "187 27 0.3940448462963104\n",
      "Validation loss: 1.3072999515364656 RMSE: 1.1433722\n",
      "Validation loss: 1.5174843136188203 RMSE: 1.2318622\n",
      "189 19 0.19515123963356018\n",
      "Validation loss: 1.4216305186263227 RMSE: 1.1923215\n",
      "Validation loss: 1.5447055270186567 RMSE: 1.2428619\n",
      "191 11 0.181460440158844\n",
      "Validation loss: 1.2570832345337994 RMSE: 1.1211972\n",
      "Validation loss: 1.439751483697807 RMSE: 1.1998965\n",
      "193 3 0.28177177906036377\n",
      "Validation loss: 1.617283697149395 RMSE: 1.2717247\n",
      "194 24 0.21015679836273193\n",
      "Validation loss: 1.7599046799988873 RMSE: 1.326614\n",
      "Validation loss: 1.344617819364092 RMSE: 1.1595765\n",
      "196 16 0.3114341199398041\n",
      "Validation loss: 1.4801499658981256 RMSE: 1.2166142\n",
      "Validation loss: 1.7344350783170852 RMSE: 1.3169795\n",
      "198 8 0.9358501434326172\n",
      "Validation loss: 1.8231541741210802 RMSE: 1.3502423\n",
      "Validation loss: 1.547226420546 RMSE: 1.2438755\n",
      "Loaded trained model with success.\n",
      "Test loss: 1.4805989887862079 Test RMSE: 1.2167987\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:0', 'task_name': 'esol', 'model_type': 'gin', 'model': {'num_layer': 5, 'emb_dim': 300, 'feat_dim': 300, 'drop_ratio': 0.3, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 778, 'task': 'regression', 'data_path': 'data/esol/esol.csv', 'target': ['measured log solubility in mols per litre']}}\n",
      "Running on: cuda:0\n",
      "1127\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/1127\n",
      "Generating scaffold 1000/1127\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 17.196157455444336\n",
      "Validation loss: 3.3473682023782647 RMSE: 1.8295814\n",
      "1 21 2.7868282794952393\n",
      "Validation loss: 7.166057816648905 RMSE: 2.6769493\n",
      "Validation loss: 3.0773634045524934 RMSE: 1.7542416\n",
      "3 13 2.2097795009613037\n",
      "Validation loss: 4.047114739375832 RMSE: 2.0117443\n",
      "Validation loss: 4.083155733294191 RMSE: 2.0206819\n",
      "5 5 1.5247828960418701\n",
      "Validation loss: 4.5310784820961745 RMSE: 2.128633\n",
      "6 26 1.4130481481552124\n",
      "Validation loss: 2.9800094524316028 RMSE: 1.7262703\n",
      "Validation loss: 3.220059082571384 RMSE: 1.7944523\n",
      "8 18 0.8847011923789978\n",
      "Validation loss: 2.7357481623117903 RMSE: 1.6540097\n",
      "Validation loss: 1.7863201852393362 RMSE: 1.336533\n",
      "10 10 1.1498464345932007\n",
      "Validation loss: 2.30368315118604 RMSE: 1.5177889\n",
      "Validation loss: 4.5891696415116305 RMSE: 2.1422348\n",
      "12 2 0.7713059186935425\n",
      "Validation loss: 2.0097280582495496 RMSE: 1.4176488\n",
      "13 23 0.874595582485199\n",
      "Validation loss: 4.115165482580134 RMSE: 2.028587\n",
      "Validation loss: 3.917543717190228 RMSE: 1.9792787\n",
      "15 15 1.2409611940383911\n",
      "Validation loss: 2.5721723896212283 RMSE: 1.6037993\n",
      "Validation loss: 3.1350248303033608 RMSE: 1.7706\n",
      "17 7 1.8380218744277954\n",
      "Validation loss: 4.101161285839249 RMSE: 2.0251324\n",
      "18 28 1.37633216381073\n",
      "Validation loss: 1.9275882982574732 RMSE: 1.3883762\n",
      "Validation loss: 2.570120551944834 RMSE: 1.6031595\n",
      "20 20 0.5648087859153748\n",
      "Validation loss: 2.257183361897426 RMSE: 1.5023925\n",
      "Validation loss: 2.333331043741344 RMSE: 1.5275245\n",
      "22 12 2.6059913635253906\n",
      "Validation loss: 2.6679456761453 RMSE: 1.6333848\n",
      "Validation loss: 2.7701145399988225 RMSE: 1.6643661\n",
      "24 4 0.701665997505188\n",
      "Validation loss: 2.3921260242968536 RMSE: 1.5466499\n",
      "25 25 1.4062438011169434\n",
      "Validation loss: 2.23413971356586 RMSE: 1.4947039\n",
      "Validation loss: 2.070257643682767 RMSE: 1.4388391\n",
      "27 17 0.8687304258346558\n",
      "Validation loss: 3.1062182071989617 RMSE: 1.7624466\n",
      "Validation loss: 1.8670280517730038 RMSE: 1.3663924\n",
      "29 9 0.6865922808647156\n",
      "Validation loss: 2.930530643041155 RMSE: 1.7118793\n",
      "Validation loss: 1.9251570089728431 RMSE: 1.3875003\n",
      "31 1 0.7836407423019409\n",
      "Validation loss: 2.3749942209868307 RMSE: 1.5411016\n",
      "32 22 1.0204851627349854\n",
      "Validation loss: 2.1864165668993927 RMSE: 1.4786537\n",
      "Validation loss: 2.6421588374450145 RMSE: 1.6254718\n",
      "34 14 0.5059347748756409\n",
      "Validation loss: 2.563510941193167 RMSE: 1.6010969\n",
      "Validation loss: 2.0170910653814804 RMSE: 1.4202433\n",
      "36 6 0.9173812866210938\n",
      "Validation loss: 2.4339810535971043 RMSE: 1.5601221\n",
      "37 27 0.8416534662246704\n",
      "Validation loss: 3.2225503436232037 RMSE: 1.7951462\n",
      "Validation loss: 2.2415254960017923 RMSE: 1.4971726\n",
      "39 19 0.598079264163971\n",
      "Validation loss: 2.7035103730395833 RMSE: 1.6442355\n",
      "Validation loss: 3.2803916635766495 RMSE: 1.8111851\n",
      "41 11 1.4253957271575928\n",
      "Validation loss: 2.4114562325772986 RMSE: 1.5528864\n",
      "Validation loss: 2.149695086268197 RMSE: 1.4661839\n",
      "43 3 1.038643479347229\n",
      "Validation loss: 2.0651445114507085 RMSE: 1.437061\n",
      "44 24 0.518010139465332\n",
      "Validation loss: 1.7955758561075261 RMSE: 1.339991\n",
      "Validation loss: 2.1411883029262575 RMSE: 1.46328\n",
      "46 16 0.42463502287864685\n",
      "Validation loss: 2.2353383904009796 RMSE: 1.4951048\n",
      "Validation loss: 2.1393016777207365 RMSE: 1.4626352\n",
      "48 8 0.6525341272354126\n",
      "Validation loss: 2.113059533380829 RMSE: 1.4536365\n",
      "Validation loss: 1.8909344588760781 RMSE: 1.3751125\n",
      "50 0 1.728333592414856\n",
      "Validation loss: 2.1877529093649537 RMSE: 1.4791055\n",
      "51 21 0.38303065299987793\n",
      "Validation loss: 1.857491208388742 RMSE: 1.3628981\n",
      "Validation loss: 1.8391034096743153 RMSE: 1.3561355\n",
      "53 13 0.43321290612220764\n",
      "Validation loss: 2.0899486900430864 RMSE: 1.4456655\n",
      "Validation loss: 2.288591300491738 RMSE: 1.5128089\n",
      "55 5 1.0187044143676758\n",
      "Validation loss: 2.6116347819302987 RMSE: 1.6160554\n",
      "56 26 1.348785400390625\n",
      "Validation loss: 1.959908563478858 RMSE: 1.3999674\n",
      "Validation loss: 2.4044717128297926 RMSE: 1.5506359\n",
      "58 18 0.4604383707046509\n",
      "Validation loss: 2.118990056282651 RMSE: 1.4556751\n",
      "Validation loss: 2.0753994631556285 RMSE: 1.4406247\n",
      "60 10 0.7763516902923584\n",
      "Validation loss: 3.090577219439819 RMSE: 1.7580038\n",
      "Validation loss: 2.0556970402202777 RMSE: 1.4337703\n",
      "62 2 0.6198155283927917\n",
      "Validation loss: 2.337786634411432 RMSE: 1.5289823\n",
      "63 23 0.5642013549804688\n",
      "Validation loss: 1.9955483938740417 RMSE: 1.4126388\n",
      "Validation loss: 1.8996410791852834 RMSE: 1.3782747\n",
      "65 15 0.9978333711624146\n",
      "Validation loss: 1.840871280273505 RMSE: 1.3567871\n",
      "Validation loss: 2.026945435895329 RMSE: 1.4237083\n",
      "67 7 0.9130088686943054\n",
      "Validation loss: 2.2554296666541984 RMSE: 1.5018089\n",
      "68 28 2.421046018600464\n",
      "Validation loss: 1.5727834785934043 RMSE: 1.2541066\n",
      "Validation loss: 1.5924501155330018 RMSE: 1.2619232\n",
      "70 20 0.6763587594032288\n",
      "Validation loss: 1.851437430466171 RMSE: 1.3606753\n",
      "Validation loss: 1.6040950137957009 RMSE: 1.2665287\n",
      "72 12 0.714590311050415\n",
      "Validation loss: 1.9045092278877191 RMSE: 1.3800396\n",
      "Validation loss: 1.8008726360523595 RMSE: 1.341966\n",
      "74 4 0.808659553527832\n",
      "Validation loss: 1.6867594402448265 RMSE: 1.298753\n",
      "75 25 0.963932991027832\n",
      "Validation loss: 1.6218913899058789 RMSE: 1.2735349\n",
      "Validation loss: 2.039744900391165 RMSE: 1.4281963\n",
      "77 17 0.5797668695449829\n",
      "Validation loss: 1.8447761852129372 RMSE: 1.3582255\n",
      "Validation loss: 1.6440695798502558 RMSE: 1.2822129\n",
      "79 9 0.26852792501449585\n",
      "Validation loss: 1.5968759123202974 RMSE: 1.2636756\n",
      "Validation loss: 2.232384581481461 RMSE: 1.4941167\n",
      "81 1 0.6705307960510254\n",
      "Validation loss: 1.770642107566901 RMSE: 1.3306547\n",
      "82 22 0.6668891906738281\n",
      "Validation loss: 1.7331372277926556 RMSE: 1.3164867\n",
      "Validation loss: 1.7651772773371333 RMSE: 1.3285998\n",
      "84 14 0.6240214705467224\n",
      "Validation loss: 1.8594059585470013 RMSE: 1.3636003\n",
      "Validation loss: 2.1840884812110293 RMSE: 1.4778662\n",
      "86 6 0.704711377620697\n",
      "Validation loss: 1.9259184804637874 RMSE: 1.3877746\n",
      "87 27 0.6925358772277832\n",
      "Validation loss: 1.974340812294884 RMSE: 1.4051124\n",
      "Validation loss: 1.756339115379131 RMSE: 1.3252695\n",
      "89 19 0.2862856686115265\n",
      "Validation loss: 1.8793397445594315 RMSE: 1.37089\n",
      "Validation loss: 1.676308745831515 RMSE: 1.2947234\n",
      "91 11 1.2048420906066895\n",
      "Validation loss: 1.7364069771977653 RMSE: 1.317728\n",
      "Validation loss: 2.339934239345314 RMSE: 1.5296843\n",
      "93 3 0.9630855321884155\n",
      "Validation loss: 1.8301851369638358 RMSE: 1.3528434\n",
      "94 24 0.6166505813598633\n",
      "Validation loss: 1.73166991018616 RMSE: 1.3159293\n",
      "Validation loss: 1.6952310695057422 RMSE: 1.3020104\n",
      "96 16 0.40255650877952576\n",
      "Validation loss: 1.913973365209799 RMSE: 1.3834643\n",
      "Validation loss: 1.8263454880334635 RMSE: 1.3514234\n",
      "98 8 0.6378259062767029\n",
      "Validation loss: 2.2503453556415254 RMSE: 1.5001152\n",
      "Validation loss: 1.8919714625957793 RMSE: 1.3754896\n",
      "100 0 0.6228129863739014\n",
      "Validation loss: 1.981605717566161 RMSE: 1.4076953\n",
      "101 21 0.49911606311798096\n",
      "Validation loss: 2.0232808663781765 RMSE: 1.4224209\n",
      "Validation loss: 2.0946084081599143 RMSE: 1.4472761\n",
      "103 13 0.5143104791641235\n",
      "Validation loss: 1.8548202999925192 RMSE: 1.3619179\n",
      "Validation loss: 1.7520029523731333 RMSE: 1.3236325\n",
      "105 5 0.786547064781189\n",
      "Validation loss: 1.7252640281103353 RMSE: 1.3134931\n",
      "106 26 1.4493848085403442\n",
      "Validation loss: 2.1712888489782283 RMSE: 1.4735295\n",
      "Validation loss: 1.4765796418738577 RMSE: 1.215146\n",
      "108 18 0.3134634792804718\n",
      "Validation loss: 1.7557989184835316 RMSE: 1.3250657\n",
      "Validation loss: 1.8689907599339444 RMSE: 1.3671104\n",
      "110 10 0.3792089521884918\n",
      "Validation loss: 2.387209045148529 RMSE: 1.5450597\n",
      "Validation loss: 1.8780073592093138 RMSE: 1.370404\n",
      "112 2 0.49905309081077576\n",
      "Validation loss: 1.84523915928022 RMSE: 1.3583958\n",
      "113 23 0.7141839861869812\n",
      "Validation loss: 1.9477311478251904 RMSE: 1.3956114\n",
      "Validation loss: 1.4753429383303212 RMSE: 1.2146369\n",
      "115 15 0.690636932849884\n",
      "Validation loss: 2.3767161263828784 RMSE: 1.5416601\n",
      "Validation loss: 2.0740549849197927 RMSE: 1.440158\n",
      "117 7 0.9480571746826172\n",
      "Validation loss: 1.9363559094150509 RMSE: 1.39153\n",
      "118 28 0.5812881588935852\n",
      "Validation loss: 1.9034009296282204 RMSE: 1.379638\n",
      "Validation loss: 2.030766639034305 RMSE: 1.4250497\n",
      "120 20 0.5023465156555176\n",
      "Validation loss: 1.893341279662816 RMSE: 1.3759874\n",
      "Validation loss: 1.9214323242153741 RMSE: 1.3861574\n",
      "122 12 0.37721559405326843\n",
      "Validation loss: 1.9538031774284566 RMSE: 1.3977852\n",
      "Validation loss: 1.830861441856992 RMSE: 1.3530933\n",
      "124 4 0.6194949746131897\n",
      "Validation loss: 1.674128194825839 RMSE: 1.293881\n",
      "125 25 0.6934823989868164\n",
      "Validation loss: 1.6209788470141655 RMSE: 1.2731767\n",
      "Validation loss: 1.6922050851636228 RMSE: 1.3008479\n",
      "127 17 0.39988866448402405\n",
      "Validation loss: 1.747709807041472 RMSE: 1.3220097\n",
      "Validation loss: 1.7376017032471378 RMSE: 1.3181812\n",
      "129 9 0.7848876118659973\n",
      "Validation loss: 1.6546254664395763 RMSE: 1.2863225\n",
      "Validation loss: 1.5298340974655826 RMSE: 1.2368646\n",
      "131 1 1.2377970218658447\n",
      "Validation loss: 1.7860665479592517 RMSE: 1.3364381\n",
      "132 22 0.8737255930900574\n",
      "Validation loss: 1.617552590581168 RMSE: 1.2718304\n",
      "Validation loss: 1.939093068637679 RMSE: 1.3925132\n",
      "134 14 0.8583856225013733\n",
      "Validation loss: 1.9317265780626145 RMSE: 1.3898656\n",
      "Validation loss: 1.7458436531303203 RMSE: 1.3213038\n",
      "136 6 1.0513145923614502\n",
      "Validation loss: 1.6424861945937166 RMSE: 1.2815952\n",
      "137 27 0.6801009178161621\n",
      "Validation loss: 1.7248068594299586 RMSE: 1.313319\n",
      "Validation loss: 1.53912064617714 RMSE: 1.240613\n",
      "139 19 0.35694798827171326\n",
      "Validation loss: 1.843597135712615 RMSE: 1.3577913\n",
      "Validation loss: 1.7344945553129754 RMSE: 1.3170022\n",
      "141 11 0.2581636607646942\n",
      "Validation loss: 1.7571497849658526 RMSE: 1.3255752\n",
      "Validation loss: 1.6672278558258462 RMSE: 1.2912117\n",
      "143 3 0.39136379957199097\n",
      "Validation loss: 1.5868327554348296 RMSE: 1.2596954\n",
      "144 24 0.5777312517166138\n",
      "Validation loss: 2.2901501201950345 RMSE: 1.5133243\n",
      "Validation loss: 1.8154673555255991 RMSE: 1.3473928\n",
      "146 16 0.2291254699230194\n",
      "Validation loss: 1.4568746385321152 RMSE: 1.2070106\n",
      "Validation loss: 1.844620355462606 RMSE: 1.3581681\n",
      "148 8 0.3800528943538666\n",
      "Validation loss: 1.7415142175370613 RMSE: 1.3196644\n",
      "Validation loss: 1.9268069731450714 RMSE: 1.3880947\n",
      "150 0 0.27478480339050293\n",
      "Validation loss: 1.7231049959638478 RMSE: 1.312671\n",
      "151 21 0.4713709354400635\n",
      "Validation loss: 1.831373206282084 RMSE: 1.3532823\n",
      "Validation loss: 1.3659321365103256 RMSE: 1.168731\n",
      "153 13 0.2361910045146942\n",
      "Validation loss: 1.853045527913929 RMSE: 1.3612661\n",
      "Validation loss: 1.8657819623440768 RMSE: 1.3659363\n",
      "155 5 0.36556699872016907\n",
      "Validation loss: 1.7644130086476824 RMSE: 1.328312\n",
      "156 26 0.4606444835662842\n",
      "Validation loss: 1.8301743548528282 RMSE: 1.3528394\n",
      "Validation loss: 1.564709100048099 RMSE: 1.2508835\n",
      "158 18 0.4777653217315674\n",
      "Validation loss: 1.71761161036196 RMSE: 1.3105768\n",
      "Validation loss: 1.717744797731923 RMSE: 1.3106277\n",
      "160 10 0.1732553392648697\n",
      "Validation loss: 1.5447639891531615 RMSE: 1.2428855\n",
      "Validation loss: 1.6381922327311693 RMSE: 1.2799189\n",
      "162 2 0.831273078918457\n",
      "Validation loss: 2.3546493855197874 RMSE: 1.5344867\n",
      "163 23 0.9831624627113342\n",
      "Validation loss: 1.6743304339130367 RMSE: 1.2939591\n",
      "Validation loss: 1.5441913710231274 RMSE: 1.2426549\n",
      "165 15 0.5859783887863159\n",
      "Validation loss: 1.9496013675115804 RMSE: 1.3962812\n",
      "Validation loss: 1.640617330517389 RMSE: 1.2808659\n",
      "167 7 0.7010650634765625\n",
      "Validation loss: 1.8406653615225732 RMSE: 1.3567113\n",
      "168 28 0.5445243120193481\n",
      "Validation loss: 1.4698843249177511 RMSE: 1.2123878\n",
      "Validation loss: 1.816900853562144 RMSE: 1.3479246\n",
      "170 20 0.5344921350479126\n",
      "Validation loss: 2.1067423778297627 RMSE: 1.4514621\n",
      "Validation loss: 1.824296299335176 RMSE: 1.3506652\n",
      "172 12 0.3622056245803833\n",
      "Validation loss: 1.6524173780880143 RMSE: 1.2854639\n",
      "Validation loss: 1.466863534091848 RMSE: 1.2111415\n",
      "174 4 0.20613692700862885\n",
      "Validation loss: 1.8025476573842816 RMSE: 1.3425899\n",
      "175 25 0.34063786268234253\n",
      "Validation loss: 1.7252363377967768 RMSE: 1.3134825\n",
      "Validation loss: 2.3670085193836585 RMSE: 1.5385085\n",
      "177 17 0.5577282905578613\n",
      "Validation loss: 1.7614445011172675 RMSE: 1.3271942\n",
      "Validation loss: 1.7645552770226403 RMSE: 1.3283656\n",
      "179 9 0.3795349895954132\n",
      "Validation loss: 1.7814434640175474 RMSE: 1.3347073\n",
      "Validation loss: 1.6398242268942098 RMSE: 1.2805563\n",
      "181 1 0.3899177312850952\n",
      "Validation loss: 1.3853266597849079 RMSE: 1.176999\n",
      "182 22 0.8255971074104309\n",
      "Validation loss: 1.8861598398833148 RMSE: 1.3733754\n",
      "Validation loss: 2.1869263205908043 RMSE: 1.478826\n",
      "184 14 0.44663944840431213\n",
      "Validation loss: 1.657792485920729 RMSE: 1.2875528\n",
      "Validation loss: 1.5739505185490161 RMSE: 1.2545718\n",
      "186 6 0.25265371799468994\n",
      "Validation loss: 2.1297801469279602 RMSE: 1.4593766\n",
      "187 27 0.2990751266479492\n",
      "Validation loss: 1.4701322317123413 RMSE: 1.2124902\n",
      "Validation loss: 1.6661944347145283 RMSE: 1.2908115\n",
      "189 19 0.4562707543373108\n",
      "Validation loss: 1.723298448376951 RMSE: 1.3127446\n",
      "Validation loss: 1.4921310549288724 RMSE: 1.2215283\n",
      "191 11 0.3565599024295807\n",
      "Validation loss: 1.7850611568552204 RMSE: 1.3360618\n",
      "Validation loss: 1.8760523680037102 RMSE: 1.3696907\n",
      "193 3 0.19453109800815582\n",
      "Validation loss: 1.9950094782145678 RMSE: 1.412448\n",
      "194 24 0.517255425453186\n",
      "Validation loss: 1.536000602013242 RMSE: 1.239355\n",
      "Validation loss: 1.533872154961645 RMSE: 1.238496\n",
      "196 16 0.35273757576942444\n",
      "Validation loss: 1.6246310187652049 RMSE: 1.2746102\n",
      "Validation loss: 1.5445485030655313 RMSE: 1.2427987\n",
      "198 8 0.4048007130622864\n",
      "Validation loss: 1.9906382434136045 RMSE: 1.4108999\n",
      "Validation loss: 1.5878263110608126 RMSE: 1.2600899\n",
      "Loaded trained model with success.\n",
      "Test loss: 1.4765113940281152 Test RMSE: 1.2151178\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:0', 'task_name': 'esol', 'model_type': 'gin', 'model': {'num_layer': 5, 'emb_dim': 300, 'feat_dim': 300, 'drop_ratio': 0.3, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 779, 'task': 'regression', 'data_path': 'data/esol/esol.csv', 'target': ['measured log solubility in mols per litre']}}\n",
      "Running on: cuda:0\n",
      "1127\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/1127\n",
      "Generating scaffold 1000/1127\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 14.994614601135254\n",
      "Validation loss: 2.502039820747038 RMSE: 1.5817837\n",
      "1 21 2.080718994140625\n",
      "Validation loss: 2.837293063644814 RMSE: 1.6844265\n",
      "Validation loss: 5.924118548367931 RMSE: 2.4339511\n",
      "3 13 1.8500230312347412\n",
      "Validation loss: 10.49944875092633 RMSE: 3.2402852\n",
      "Validation loss: 3.884089923537938 RMSE: 1.9708095\n",
      "5 5 1.7317360639572144\n",
      "Validation loss: 3.0762110431637386 RMSE: 1.753913\n",
      "6 26 1.5450552701950073\n",
      "Validation loss: 2.1598176491998995 RMSE: 1.4696319\n",
      "Validation loss: 2.860349686799851 RMSE: 1.6912568\n",
      "8 18 1.042786717414856\n",
      "Validation loss: 2.4993776251784467 RMSE: 1.580942\n",
      "Validation loss: 3.136812661601379 RMSE: 1.7711048\n",
      "10 10 0.913998007774353\n",
      "Validation loss: 2.0591773522638643 RMSE: 1.4349834\n",
      "Validation loss: 2.0719248826524854 RMSE: 1.4394183\n",
      "12 2 1.419729471206665\n",
      "Validation loss: 2.1796690109556756 RMSE: 1.4763702\n",
      "13 23 1.0256425142288208\n",
      "Validation loss: 1.8714373164472327 RMSE: 1.3680049\n",
      "Validation loss: 3.684518457513995 RMSE: 1.9195099\n",
      "15 15 2.027099609375\n",
      "Validation loss: 2.1622461655498606 RMSE: 1.4704579\n",
      "Validation loss: 2.1955949289608845 RMSE: 1.481754\n",
      "17 7 1.1739999055862427\n",
      "Validation loss: 2.682487207176411 RMSE: 1.63783\n",
      "18 28 1.7278114557266235\n",
      "Validation loss: 3.732808760837116 RMSE: 1.9320477\n",
      "Validation loss: 3.7195882079875573 RMSE: 1.9286233\n",
      "20 20 0.6291018128395081\n",
      "Validation loss: 3.3612850682925335 RMSE: 1.8333807\n",
      "Validation loss: 3.4226103926126936 RMSE: 1.8500297\n",
      "22 12 1.711755394935608\n",
      "Validation loss: 1.960024706030314 RMSE: 1.4000089\n",
      "Validation loss: 4.002292000086961 RMSE: 2.000573\n",
      "24 4 1.8962595462799072\n",
      "Validation loss: 2.85991094597673 RMSE: 1.6911273\n",
      "25 25 0.6834882497787476\n",
      "Validation loss: 2.1219937083995446 RMSE: 1.4567065\n",
      "Validation loss: 2.206157677996475 RMSE: 1.4853141\n",
      "27 17 0.5033459663391113\n",
      "Validation loss: 2.065130096621218 RMSE: 1.4370561\n",
      "Validation loss: 2.54641253336341 RMSE: 1.5957483\n",
      "29 9 0.9573835730552673\n",
      "Validation loss: 1.7489531947448191 RMSE: 1.32248\n",
      "Validation loss: 2.211036790788701 RMSE: 1.4869555\n",
      "31 1 1.2309035062789917\n",
      "Validation loss: 1.8280551370266265 RMSE: 1.3520559\n",
      "32 22 0.6495876908302307\n",
      "Validation loss: 1.8488179265925315 RMSE: 1.3597124\n",
      "Validation loss: 1.9680721358915345 RMSE: 1.40288\n",
      "34 14 1.294424295425415\n",
      "Validation loss: 2.058683059911812 RMSE: 1.4348111\n",
      "Validation loss: 2.076447073864726 RMSE: 1.4409883\n",
      "36 6 0.9694681167602539\n",
      "Validation loss: 2.34297959572446 RMSE: 1.5306793\n",
      "37 27 0.6211581230163574\n",
      "Validation loss: 1.8398270649192607 RMSE: 1.3564022\n",
      "Validation loss: 1.7632967993221451 RMSE: 1.3278918\n",
      "39 19 1.6526834964752197\n",
      "Validation loss: 2.5368674518787757 RMSE: 1.5927546\n",
      "Validation loss: 1.9827504284613955 RMSE: 1.4081017\n",
      "41 11 0.6745365262031555\n",
      "Validation loss: 3.266362553149198 RMSE: 1.8073081\n",
      "Validation loss: 2.3833389155632627 RMSE: 1.5438067\n",
      "43 3 0.49603351950645447\n",
      "Validation loss: 2.176854595673823 RMSE: 1.4754168\n",
      "44 24 0.48775818943977356\n",
      "Validation loss: 1.818893504353751 RMSE: 1.3486636\n",
      "Validation loss: 4.189497985671052 RMSE: 2.0468261\n",
      "46 16 0.9411431550979614\n",
      "Validation loss: 1.9427380572378108 RMSE: 1.3938214\n",
      "Validation loss: 1.8526598099058709 RMSE: 1.3611245\n",
      "48 8 0.6642326712608337\n",
      "Validation loss: 2.3603226368406176 RMSE: 1.5363342\n",
      "Validation loss: 1.999729430253527 RMSE: 1.4141179\n",
      "50 0 0.5434332489967346\n",
      "Validation loss: 1.5746121269411746 RMSE: 1.2548356\n",
      "51 21 0.910393476486206\n",
      "Validation loss: 1.9833055196610172 RMSE: 1.4082989\n",
      "Validation loss: 1.8225133366289392 RMSE: 1.350005\n",
      "53 13 0.7534596920013428\n",
      "Validation loss: 2.2667690673760608 RMSE: 1.5055792\n",
      "Validation loss: 2.265014705404771 RMSE: 1.5049965\n",
      "55 5 0.5484720468521118\n",
      "Validation loss: 2.205304426429546 RMSE: 1.4850267\n",
      "56 26 1.490604043006897\n",
      "Validation loss: 1.991989773986614 RMSE: 1.4113787\n",
      "Validation loss: 2.0009188419949693 RMSE: 1.4145384\n",
      "58 18 0.6631841063499451\n",
      "Validation loss: 1.8139458493848817 RMSE: 1.346828\n",
      "Validation loss: 2.1101292498343813 RMSE: 1.4526284\n",
      "60 10 0.9401751160621643\n",
      "Validation loss: 1.8127387181847496 RMSE: 1.3463798\n",
      "Validation loss: 3.318387120170931 RMSE: 1.8216442\n",
      "62 2 0.5199903249740601\n",
      "Validation loss: 2.6367624080286616 RMSE: 1.623811\n",
      "63 23 0.2697056233882904\n",
      "Validation loss: 1.480608006494235 RMSE: 1.2168024\n",
      "Validation loss: 1.7982148037547558 RMSE: 1.3409753\n",
      "65 15 1.7161362171173096\n",
      "Validation loss: 2.1368755176004055 RMSE: 1.4618056\n",
      "Validation loss: 2.12990596442096 RMSE: 1.4594197\n",
      "67 7 0.5008734464645386\n",
      "Validation loss: 1.9960190237095925 RMSE: 1.4128053\n",
      "68 28 1.7693909406661987\n",
      "Validation loss: 1.8504751002894038 RMSE: 1.3603216\n",
      "Validation loss: 1.860919260345729 RMSE: 1.364155\n",
      "70 20 0.5208225250244141\n",
      "Validation loss: 1.8586346512347196 RMSE: 1.3633175\n",
      "Validation loss: 1.7844986430311625 RMSE: 1.3358513\n",
      "72 12 0.62409508228302\n",
      "Validation loss: 2.8662118099432075 RMSE: 1.6929891\n",
      "Validation loss: 1.8867654673821104 RMSE: 1.3735958\n",
      "74 4 0.6311085224151611\n",
      "Validation loss: 1.9700504910629408 RMSE: 1.4035848\n",
      "75 25 0.6536821722984314\n",
      "Validation loss: 1.7363119178113684 RMSE: 1.3176919\n",
      "Validation loss: 1.9371492366875167 RMSE: 1.3918151\n",
      "77 17 0.8859512209892273\n",
      "Validation loss: 1.8693006481744547 RMSE: 1.3672237\n",
      "Validation loss: 1.8506976060107745 RMSE: 1.3604034\n",
      "79 9 0.5706547498703003\n",
      "Validation loss: 1.622310728098439 RMSE: 1.2736996\n",
      "Validation loss: 2.067890000554313 RMSE: 1.4380159\n",
      "81 1 0.5809484720230103\n",
      "Validation loss: 2.22165032403659 RMSE: 1.4905202\n",
      "82 22 0.7336065173149109\n",
      "Validation loss: 2.1138841821029124 RMSE: 1.4539202\n",
      "Validation loss: 1.7453945759123406 RMSE: 1.3211337\n",
      "84 14 0.603740394115448\n",
      "Validation loss: 1.6674667939675594 RMSE: 1.2913043\n",
      "Validation loss: 1.6661370929363555 RMSE: 1.2907892\n",
      "86 6 0.8208582997322083\n",
      "Validation loss: 1.8213759629072341 RMSE: 1.3495836\n",
      "87 27 0.7046065926551819\n",
      "Validation loss: 2.2028717003037444 RMSE: 1.4842074\n",
      "Validation loss: 1.8934427025043858 RMSE: 1.3760244\n",
      "89 19 0.6156482696533203\n",
      "Validation loss: 1.611475727199453 RMSE: 1.2694391\n",
      "Validation loss: 1.8203567171518782 RMSE: 1.3492059\n",
      "91 11 0.8138587474822998\n",
      "Validation loss: 1.5241871365403707 RMSE: 1.2345798\n",
      "Validation loss: 1.675823447978602 RMSE: 1.294536\n",
      "93 3 0.5613783001899719\n",
      "Validation loss: 1.8038686513900757 RMSE: 1.3430818\n",
      "94 24 0.5280270576477051\n",
      "Validation loss: 1.948130620264374 RMSE: 1.3957545\n",
      "Validation loss: 1.5104572393197928 RMSE: 1.2290065\n",
      "96 16 0.6369403004646301\n",
      "Validation loss: 1.7062370513392762 RMSE: 1.3062301\n",
      "Validation loss: 1.8462920051760379 RMSE: 1.3587832\n",
      "98 8 0.3637433648109436\n",
      "Validation loss: 1.6143315049399316 RMSE: 1.2705635\n",
      "Validation loss: 1.5489363374963272 RMSE: 1.2445626\n",
      "100 0 0.5297572612762451\n",
      "Validation loss: 1.711972940284594 RMSE: 1.3084239\n",
      "101 21 0.9303287267684937\n",
      "Validation loss: 1.664514499427998 RMSE: 1.2901608\n",
      "Validation loss: 1.599371039761906 RMSE: 1.2646625\n",
      "103 13 0.4438942074775696\n",
      "Validation loss: 2.1039403771932146 RMSE: 1.4504966\n",
      "Validation loss: 1.3970364891322313 RMSE: 1.181963\n",
      "105 5 0.5528267025947571\n",
      "Validation loss: 2.0185485192104777 RMSE: 1.4207563\n",
      "106 26 0.6381783485412598\n",
      "Validation loss: 1.5444530385785398 RMSE: 1.2427603\n",
      "Validation loss: 1.6098611544718784 RMSE: 1.268803\n",
      "108 18 0.31608936190605164\n",
      "Validation loss: 1.5388791371235806 RMSE: 1.2405157\n",
      "Validation loss: 1.7968587242396532 RMSE: 1.3404695\n",
      "110 10 0.40465691685676575\n",
      "Validation loss: 1.8628969656682648 RMSE: 1.3648798\n",
      "Validation loss: 1.44072709357844 RMSE: 1.2003028\n",
      "112 2 0.3869984745979309\n",
      "Validation loss: 1.5683804676596043 RMSE: 1.25235\n",
      "113 23 0.5491582751274109\n",
      "Validation loss: 1.7094697488092743 RMSE: 1.307467\n",
      "Validation loss: 1.9386804346489694 RMSE: 1.392365\n",
      "115 15 0.9516940116882324\n",
      "Validation loss: 1.8671766319106111 RMSE: 1.3664467\n",
      "Validation loss: 1.715672145902583 RMSE: 1.3098367\n",
      "117 7 0.9525347948074341\n",
      "Validation loss: 1.4432658178616413 RMSE: 1.20136\n",
      "118 28 1.0779541730880737\n",
      "Validation loss: 1.8826701852072656 RMSE: 1.3721043\n",
      "Validation loss: 1.671048386962013 RMSE: 1.2926903\n",
      "120 20 0.4851326048374176\n",
      "Validation loss: 1.7925919790183549 RMSE: 1.3388772\n",
      "Validation loss: 1.6540494671965067 RMSE: 1.2860986\n",
      "122 12 0.4558422863483429\n",
      "Validation loss: 1.8220878665426137 RMSE: 1.3498473\n",
      "Validation loss: 1.7096510946223167 RMSE: 1.3075362\n",
      "124 4 0.6193752884864807\n",
      "Validation loss: 1.8574497351604224 RMSE: 1.3628829\n",
      "125 25 0.48725277185440063\n",
      "Validation loss: 1.7171992664843534 RMSE: 1.3104196\n",
      "Validation loss: 1.9565015999616775 RMSE: 1.3987501\n",
      "127 17 1.2128108739852905\n",
      "Validation loss: 1.8734322733583704 RMSE: 1.3687338\n",
      "Validation loss: 1.7137069459510061 RMSE: 1.3090863\n",
      "129 9 0.3879506587982178\n",
      "Validation loss: 1.6779841574947392 RMSE: 1.2953703\n",
      "Validation loss: 1.7357120418970564 RMSE: 1.3174642\n",
      "131 1 0.7401235103607178\n",
      "Validation loss: 1.8386797683428875 RMSE: 1.3559793\n",
      "132 22 0.3340381681919098\n",
      "Validation loss: 2.312565710692279 RMSE: 1.5207121\n",
      "Validation loss: 2.2643322554309813 RMSE: 1.5047699\n",
      "134 14 0.5729729533195496\n",
      "Validation loss: 2.055145086440365 RMSE: 1.4335777\n",
      "Validation loss: 1.7785360728744912 RMSE: 1.3336177\n",
      "136 6 0.3425174951553345\n",
      "Validation loss: 2.2542204350496817 RMSE: 1.5014061\n",
      "137 27 0.19793179631233215\n",
      "Validation loss: 1.9269683804132243 RMSE: 1.3881528\n",
      "Validation loss: 1.6500306066158599 RMSE: 1.2845352\n",
      "139 19 1.296264886856079\n",
      "Validation loss: 1.7504062905775761 RMSE: 1.3230292\n",
      "Validation loss: 1.9343780353006008 RMSE: 1.3908192\n",
      "141 11 0.712582528591156\n",
      "Validation loss: 1.9383907824490978 RMSE: 1.3922611\n",
      "Validation loss: 1.3427534810209696 RMSE: 1.1587723\n",
      "143 3 0.2805776298046112\n",
      "Validation loss: 1.7984799336543125 RMSE: 1.3410742\n",
      "144 24 0.8548393249511719\n",
      "Validation loss: 1.846926125804935 RMSE: 1.3590167\n",
      "Validation loss: 1.561217764837552 RMSE: 1.249487\n",
      "146 16 1.2431259155273438\n",
      "Validation loss: 1.7027727745275583 RMSE: 1.3049034\n",
      "Validation loss: 1.6513362901400677 RMSE: 1.2850434\n",
      "148 8 0.40056514739990234\n",
      "Validation loss: 1.6402303634491642 RMSE: 1.2807148\n",
      "Validation loss: 1.6645767720399705 RMSE: 1.2901849\n",
      "150 0 0.6984583735466003\n",
      "Validation loss: 1.38033115969295 RMSE: 1.1748749\n",
      "151 21 0.5365453362464905\n",
      "Validation loss: 1.4928750907425332 RMSE: 1.2218326\n",
      "Validation loss: 1.9239346580167787 RMSE: 1.3870598\n",
      "153 13 0.4011986255645752\n",
      "Validation loss: 1.8686055461917304 RMSE: 1.3669695\n",
      "Validation loss: 1.6422228180201708 RMSE: 1.2814924\n",
      "155 5 0.327027291059494\n",
      "Validation loss: 1.6744795541847701 RMSE: 1.2940168\n",
      "156 26 0.6250690221786499\n",
      "Validation loss: 1.410569188869105 RMSE: 1.1876739\n",
      "Validation loss: 1.5009209181355163 RMSE: 1.2251208\n",
      "158 18 0.32736730575561523\n",
      "Validation loss: 1.807171832143733 RMSE: 1.3443109\n",
      "Validation loss: 1.6282499564432464 RMSE: 1.276029\n",
      "160 10 0.5625\n",
      "Validation loss: 1.7258217925519015 RMSE: 1.3137054\n",
      "Validation loss: 1.8356781554433097 RMSE: 1.354872\n",
      "162 2 0.4394752085208893\n",
      "Validation loss: 1.3638677597045898 RMSE: 1.1678475\n",
      "163 23 0.4644857347011566\n",
      "Validation loss: 1.4608515785858693 RMSE: 1.2086569\n",
      "Validation loss: 1.6976634791467042 RMSE: 1.3029442\n",
      "165 15 0.2867666184902191\n",
      "Validation loss: 1.5299116510205564 RMSE: 1.2368959\n",
      "Validation loss: 1.442648929832256 RMSE: 1.2011033\n",
      "167 7 0.24422019720077515\n",
      "Validation loss: 1.4141992910773353 RMSE: 1.189201\n",
      "168 28 1.74910306930542\n",
      "Validation loss: 2.315229707059607 RMSE: 1.521588\n",
      "Validation loss: 1.7403178215026855 RMSE: 1.319211\n",
      "170 20 0.20992513000965118\n",
      "Validation loss: 1.4865686735220716 RMSE: 1.2192491\n",
      "Validation loss: 1.3415179896143685 RMSE: 1.1582391\n",
      "172 12 0.5558921098709106\n",
      "Validation loss: 1.530643436761029 RMSE: 1.2371918\n",
      "Validation loss: 1.7701877666785655 RMSE: 1.3304842\n",
      "174 4 0.4101067781448364\n",
      "Validation loss: 1.3612154321332948 RMSE: 1.1667113\n",
      "175 25 0.42068734765052795\n",
      "Validation loss: 1.567463309891456 RMSE: 1.2519838\n",
      "Validation loss: 1.502393922974578 RMSE: 1.2257218\n",
      "177 17 0.34971925616264343\n",
      "Validation loss: 1.563318991028102 RMSE: 1.2503276\n",
      "Validation loss: 1.5489104342671622 RMSE: 1.2445524\n",
      "179 9 0.7561771273612976\n",
      "Validation loss: 1.3431522487539105 RMSE: 1.1589444\n",
      "Validation loss: 1.4801002698662007 RMSE: 1.2165937\n",
      "181 1 0.3821731209754944\n",
      "Validation loss: 1.345434923087601 RMSE: 1.1599288\n",
      "182 22 0.5870550870895386\n",
      "Validation loss: 1.8093709502599935 RMSE: 1.3451285\n",
      "Validation loss: 1.4699841030931051 RMSE: 1.212429\n",
      "184 14 0.3170923888683319\n",
      "Validation loss: 1.6617919850138436 RMSE: 1.289105\n",
      "Validation loss: 1.4546243817405362 RMSE: 1.2060782\n",
      "186 6 0.5613834261894226\n",
      "Validation loss: 1.5842631833743206 RMSE: 1.2586752\n",
      "187 27 0.3488844335079193\n",
      "Validation loss: 1.5155433047134264 RMSE: 1.2310741\n",
      "Validation loss: 1.9423004314962742 RMSE: 1.3936644\n",
      "189 19 0.20282600820064545\n",
      "Validation loss: 1.4232909911501723 RMSE: 1.1930176\n",
      "Validation loss: 1.5011120433300997 RMSE: 1.2251987\n",
      "191 11 0.3300817310810089\n",
      "Validation loss: 1.3523681580492881 RMSE: 1.1629137\n",
      "Validation loss: 1.405552282270077 RMSE: 1.1855599\n",
      "193 3 0.6373791694641113\n",
      "Validation loss: 1.3908148708596695 RMSE: 1.1793282\n",
      "194 24 0.5380967259407043\n",
      "Validation loss: 1.5311356086646561 RMSE: 1.2373906\n",
      "Validation loss: 1.289909730970332 RMSE: 1.135742\n",
      "196 16 0.2444872260093689\n",
      "Validation loss: 1.6481603173028052 RMSE: 1.2838069\n",
      "Validation loss: 1.5763752312786812 RMSE: 1.2555379\n",
      "198 8 0.7103949785232544\n",
      "Validation loss: 1.4867390868938075 RMSE: 1.2193191\n",
      "Validation loss: 1.6358592394178948 RMSE: 1.2790071\n",
      "Loaded trained model with success.\n",
      "Test loss: 1.570826648610883 Test RMSE: 1.2533262\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:0', 'task_name': 'esol', 'model_type': 'gin', 'model': {'num_layer': 5, 'emb_dim': 300, 'feat_dim': 300, 'drop_ratio': 0.3, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 780, 'task': 'regression', 'data_path': 'data/esol/esol.csv', 'target': ['measured log solubility in mols per litre']}}\n",
      "Running on: cuda:0\n",
      "1127\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/1127\n",
      "Generating scaffold 1000/1127\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 17.33469009399414\n",
      "Validation loss: 3.021825054050547 RMSE: 1.7383397\n",
      "1 21 4.491988182067871\n",
      "Validation loss: 2.885430825495087 RMSE: 1.6986556\n",
      "Validation loss: 4.555973500277089 RMSE: 2.1344728\n",
      "3 13 1.9383652210235596\n",
      "Validation loss: 4.394784311277676 RMSE: 2.096374\n",
      "Validation loss: 2.917291070507691 RMSE: 1.7080079\n",
      "5 5 1.4699578285217285\n",
      "Validation loss: 3.0688906401659537 RMSE: 1.751825\n",
      "6 26 1.4732365608215332\n",
      "Validation loss: 3.15992910883068 RMSE: 1.7776188\n",
      "Validation loss: 3.0534892989470896 RMSE: 1.7474236\n",
      "8 18 1.516183853149414\n",
      "Validation loss: 3.097281291421536 RMSE: 1.7599095\n",
      "Validation loss: 2.400212918762612 RMSE: 1.549262\n",
      "10 10 1.2160075902938843\n",
      "Validation loss: 3.3078665965426284 RMSE: 1.8187542\n",
      "Validation loss: 2.641614783126696 RMSE: 1.6253046\n",
      "12 2 0.8948867917060852\n",
      "Validation loss: 2.851413769004619 RMSE: 1.6886129\n",
      "13 23 1.889132022857666\n",
      "Validation loss: 3.8423362833208743 RMSE: 1.9601878\n",
      "Validation loss: 2.3550775705185614 RMSE: 1.5346262\n",
      "15 15 1.6193211078643799\n",
      "Validation loss: 2.253614567022408 RMSE: 1.5012044\n",
      "Validation loss: 2.5590312101144708 RMSE: 1.5996971\n",
      "17 7 0.8096559643745422\n",
      "Validation loss: 2.9668667907208466 RMSE: 1.7224597\n",
      "18 28 4.66093111038208\n",
      "Validation loss: 2.6859153350897595 RMSE: 1.6388761\n",
      "Validation loss: 3.0777945412998706 RMSE: 1.7543644\n",
      "20 20 0.8536627888679504\n",
      "Validation loss: 2.09688008781028 RMSE: 1.4480608\n",
      "Validation loss: 2.0558977612351947 RMSE: 1.4338402\n",
      "22 12 0.7031452655792236\n",
      "Validation loss: 2.1286946039284227 RMSE: 1.4590046\n",
      "Validation loss: 3.2517175463448584 RMSE: 1.803252\n",
      "24 4 1.199483871459961\n",
      "Validation loss: 2.4107365956348654 RMSE: 1.5526546\n",
      "25 25 0.8359476327896118\n",
      "Validation loss: 2.5195237746280905 RMSE: 1.5873008\n",
      "Validation loss: 2.3470773201073167 RMSE: 1.5320175\n",
      "27 17 1.0296666622161865\n",
      "Validation loss: 2.4684273352665183 RMSE: 1.5711229\n",
      "Validation loss: 2.72560256139367 RMSE: 1.65094\n",
      "29 9 0.5319602489471436\n",
      "Validation loss: 1.7421399207241768 RMSE: 1.3199015\n",
      "Validation loss: 2.531658016474901 RMSE: 1.5911183\n",
      "31 1 0.8871469497680664\n",
      "Validation loss: 2.026965752112127 RMSE: 1.4237155\n",
      "32 22 0.6063215732574463\n",
      "Validation loss: 1.9170575458391579 RMSE: 1.3845785\n",
      "Validation loss: 1.9382513867015332 RMSE: 1.3922111\n",
      "34 14 0.7618597149848938\n",
      "Validation loss: 2.1247678741944576 RMSE: 1.4576584\n",
      "Validation loss: 2.150373040047367 RMSE: 1.466415\n",
      "36 6 1.0262501239776611\n",
      "Validation loss: 2.4310141073918974 RMSE: 1.559171\n",
      "37 27 1.0198945999145508\n",
      "Validation loss: 1.9479933006573567 RMSE: 1.3957053\n",
      "Validation loss: 2.0479892291853914 RMSE: 1.4310797\n",
      "39 19 1.6954728364944458\n",
      "Validation loss: 2.517750829721974 RMSE: 1.5867423\n",
      "Validation loss: 1.7188332165237021 RMSE: 1.3110428\n",
      "41 11 0.5412241816520691\n",
      "Validation loss: 2.119434306051879 RMSE: 1.4558277\n",
      "Validation loss: 2.7025335953298923 RMSE: 1.6439384\n",
      "43 3 0.4799591302871704\n",
      "Validation loss: 3.5019084626594474 RMSE: 1.8713386\n",
      "44 24 0.5410565733909607\n",
      "Validation loss: 1.8244351412342712 RMSE: 1.3507165\n",
      "Validation loss: 1.8220546393267876 RMSE: 1.3498349\n",
      "46 16 1.1440768241882324\n",
      "Validation loss: 2.0397330807373586 RMSE: 1.4281921\n",
      "Validation loss: 2.4929540136219126 RMSE: 1.5789092\n",
      "48 8 0.7015304565429688\n",
      "Validation loss: 1.990162943316772 RMSE: 1.4107313\n",
      "Validation loss: 2.877230618907287 RMSE: 1.6962402\n",
      "50 0 0.48067641258239746\n",
      "Validation loss: 1.4532165147561942 RMSE: 1.2054943\n",
      "51 21 1.0559369325637817\n",
      "Validation loss: 2.8337601940188786 RMSE: 1.6833776\n",
      "Validation loss: 2.4082160882190267 RMSE: 1.5518427\n",
      "53 13 0.7080095410346985\n",
      "Validation loss: 2.22753322546461 RMSE: 1.4924922\n",
      "Validation loss: 1.821902585240592 RMSE: 1.3497787\n",
      "55 5 0.808444619178772\n",
      "Validation loss: 1.9522677143063165 RMSE: 1.3972358\n",
      "56 26 0.26336824893951416\n",
      "Validation loss: 1.3948474658274017 RMSE: 1.1810367\n",
      "Validation loss: 1.874399910985896 RMSE: 1.3690872\n",
      "58 18 0.6437007188796997\n",
      "Validation loss: 1.7891382295473488 RMSE: 1.3375868\n",
      "Validation loss: 1.9952671591159516 RMSE: 1.4125392\n",
      "60 10 0.7098419070243835\n",
      "Validation loss: 2.53978819762711 RMSE: 1.5936714\n",
      "Validation loss: 1.8143393624145372 RMSE: 1.3469741\n",
      "62 2 0.4688442647457123\n",
      "Validation loss: 1.8443161017071885 RMSE: 1.358056\n",
      "63 23 0.9668935537338257\n",
      "Validation loss: 1.9902877923661628 RMSE: 1.4107755\n",
      "Validation loss: 1.764391527766675 RMSE: 1.3283039\n",
      "65 15 0.37028998136520386\n",
      "Validation loss: 2.1921209740427745 RMSE: 1.4805813\n",
      "Validation loss: 1.652325421307994 RMSE: 1.2854282\n",
      "67 7 0.46240320801734924\n",
      "Validation loss: 2.136485373024392 RMSE: 1.4616722\n",
      "68 28 2.650183916091919\n",
      "Validation loss: 1.8942599528658706 RMSE: 1.3763212\n",
      "Validation loss: 1.8374552557953692 RMSE: 1.3555276\n",
      "70 20 1.2725859880447388\n",
      "Validation loss: 1.8617106206649172 RMSE: 1.3644452\n",
      "Validation loss: 2.019787826369294 RMSE: 1.4211924\n",
      "72 12 0.6008849143981934\n",
      "Validation loss: 1.6017187812687022 RMSE: 1.2655903\n",
      "Validation loss: 1.6909887284304188 RMSE: 1.3003802\n",
      "74 4 1.1816389560699463\n",
      "Validation loss: 2.353056185013425 RMSE: 1.5339675\n",
      "75 25 0.45158901810646057\n",
      "Validation loss: 1.9613461578841758 RMSE: 1.4004806\n",
      "Validation loss: 1.6602008300544941 RMSE: 1.2884878\n",
      "77 17 0.8861883878707886\n",
      "Validation loss: 1.9591132518464485 RMSE: 1.3996834\n",
      "Validation loss: 1.887010821198995 RMSE: 1.3736851\n",
      "79 9 1.018369197845459\n",
      "Validation loss: 1.9289972106967352 RMSE: 1.3888835\n",
      "Validation loss: 1.7724088588647082 RMSE: 1.3313185\n",
      "81 1 0.6071116924285889\n",
      "Validation loss: 1.9535581158325734 RMSE: 1.3976974\n",
      "82 22 0.38968005776405334\n",
      "Validation loss: 1.8993086751583403 RMSE: 1.378154\n",
      "Validation loss: 2.081673260283681 RMSE: 1.4428004\n",
      "84 14 0.3302498161792755\n",
      "Validation loss: 2.042371692910659 RMSE: 1.4291158\n",
      "Validation loss: 1.5672870726711983 RMSE: 1.2519133\n",
      "86 6 0.42482224106788635\n",
      "Validation loss: 1.7471686464495364 RMSE: 1.3218051\n",
      "87 27 0.536687433719635\n",
      "Validation loss: 1.821211730484414 RMSE: 1.3495228\n",
      "Validation loss: 1.9441321360326447 RMSE: 1.3943213\n",
      "89 19 0.882394552230835\n",
      "Validation loss: 1.689337487769338 RMSE: 1.2997452\n",
      "Validation loss: 2.1263416362019765 RMSE: 1.4581981\n",
      "91 11 0.7755019664764404\n",
      "Validation loss: 3.0491617211198383 RMSE: 1.746185\n",
      "Validation loss: 2.890882359141797 RMSE: 1.7002596\n",
      "93 3 1.1599787473678589\n",
      "Validation loss: 2.350947432813391 RMSE: 1.5332799\n",
      "94 24 0.6497853398323059\n",
      "Validation loss: 1.9549119926131933 RMSE: 1.3981817\n",
      "Validation loss: 1.5016846909987187 RMSE: 1.2254325\n",
      "96 16 0.8419758677482605\n",
      "Validation loss: 2.0116022514030996 RMSE: 1.4183097\n",
      "Validation loss: 1.7674592279754908 RMSE: 1.3294584\n",
      "98 8 0.8233320713043213\n",
      "Validation loss: 1.5684938283093208 RMSE: 1.2523952\n",
      "Validation loss: 2.4342449597552815 RMSE: 1.5602068\n",
      "100 0 0.30187880992889404\n",
      "Validation loss: 2.188365866652632 RMSE: 1.4793127\n",
      "101 21 1.052512526512146\n",
      "Validation loss: 1.4729808210271649 RMSE: 1.2136642\n",
      "Validation loss: 1.5593356136727121 RMSE: 1.2487336\n",
      "103 13 0.34933510422706604\n",
      "Validation loss: 1.7616196786407876 RMSE: 1.3272603\n",
      "Validation loss: 1.6208891045730727 RMSE: 1.2731414\n",
      "105 5 0.5459710359573364\n",
      "Validation loss: 1.6885760083662724 RMSE: 1.2994522\n",
      "106 26 0.4839668273925781\n",
      "Validation loss: 1.8056781713941457 RMSE: 1.3437552\n",
      "Validation loss: 2.014165890955292 RMSE: 1.4192132\n",
      "108 18 0.579062819480896\n",
      "Validation loss: 1.9597931636118255 RMSE: 1.3999261\n",
      "Validation loss: 1.9397382810052517 RMSE: 1.3927448\n",
      "110 10 1.0262905359268188\n",
      "Validation loss: 1.5705354698991354 RMSE: 1.2532101\n",
      "Validation loss: 1.8630631086045661 RMSE: 1.3649406\n",
      "112 2 0.6902291178703308\n",
      "Validation loss: 1.5404847069124206 RMSE: 1.2411627\n",
      "113 23 0.44720134139060974\n",
      "Validation loss: 1.5821498210451244 RMSE: 1.2578354\n",
      "Validation loss: 1.8520981062830022 RMSE: 1.360918\n",
      "115 15 1.0420081615447998\n",
      "Validation loss: 1.637375395909875 RMSE: 1.2795998\n",
      "Validation loss: 1.8353350373495996 RMSE: 1.3547454\n",
      "117 7 0.5224569439888\n",
      "Validation loss: 2.2087789974381438 RMSE: 1.4861962\n",
      "118 28 0.7384222149848938\n",
      "Validation loss: 1.9352161525625042 RMSE: 1.3911204\n",
      "Validation loss: 1.6271623362482122 RMSE: 1.2756028\n",
      "120 20 0.27593210339546204\n",
      "Validation loss: 2.4065860341080523 RMSE: 1.5513175\n",
      "Validation loss: 1.3766584238119886 RMSE: 1.1733109\n",
      "122 12 0.6452924013137817\n",
      "Validation loss: 1.7284475765397063 RMSE: 1.3147043\n",
      "Validation loss: 1.415850934729112 RMSE: 1.1898953\n",
      "124 4 1.466239094734192\n",
      "Validation loss: 1.6623832662548639 RMSE: 1.2893344\n",
      "125 25 0.4509938955307007\n",
      "Validation loss: 1.9407974006855382 RMSE: 1.393125\n",
      "Validation loss: 1.714607580573158 RMSE: 1.3094304\n",
      "127 17 1.2081414461135864\n",
      "Validation loss: 2.0208366529076502 RMSE: 1.4215614\n",
      "Validation loss: 1.6616081157616809 RMSE: 1.2890338\n",
      "129 9 0.8431288599967957\n",
      "Validation loss: 1.8964717272108635 RMSE: 1.3771245\n",
      "Validation loss: 1.8216189589120646 RMSE: 1.3496736\n",
      "131 1 0.6293414831161499\n",
      "Validation loss: 1.9415313311382734 RMSE: 1.3933885\n",
      "132 22 0.5761202573776245\n",
      "Validation loss: 1.5493327337028706 RMSE: 1.2447219\n",
      "Validation loss: 1.6432533359105608 RMSE: 1.2818944\n",
      "134 14 0.6466915011405945\n",
      "Validation loss: 1.636918544769287 RMSE: 1.2794212\n",
      "Validation loss: 1.7066974777036008 RMSE: 1.3064064\n",
      "136 6 0.45730406045913696\n",
      "Validation loss: 1.4180968461838444 RMSE: 1.1908387\n",
      "137 27 0.5189948678016663\n",
      "Validation loss: 1.5990994788904105 RMSE: 1.2645551\n",
      "Validation loss: 1.969353693776426 RMSE: 1.4033366\n",
      "139 19 0.754096508026123\n",
      "Validation loss: 1.8395098205161307 RMSE: 1.3562853\n",
      "Validation loss: 1.8507493934800139 RMSE: 1.3604225\n",
      "141 11 0.4893434941768646\n",
      "Validation loss: 1.6665063052050835 RMSE: 1.2909323\n",
      "Validation loss: 1.6298811889327733 RMSE: 1.276668\n",
      "143 3 0.7370563745498657\n",
      "Validation loss: 1.620238318907476 RMSE: 1.2728858\n",
      "144 24 0.2173043042421341\n",
      "Validation loss: 1.5525655150413513 RMSE: 1.24602\n",
      "Validation loss: 1.6863510017901395 RMSE: 1.2985957\n",
      "146 16 0.35783839225769043\n",
      "Validation loss: 1.542189541116225 RMSE: 1.2418493\n",
      "Validation loss: 1.5297923119722214 RMSE: 1.2368478\n",
      "148 8 0.5062679052352905\n",
      "Validation loss: 1.8118758876766778 RMSE: 1.3460593\n",
      "Validation loss: 1.4269221177143334 RMSE: 1.1945385\n",
      "150 0 0.35656091570854187\n",
      "Validation loss: 1.7204252333767647 RMSE: 1.3116498\n",
      "151 21 1.970963716506958\n",
      "Validation loss: 2.262548885514251 RMSE: 1.5041772\n",
      "Validation loss: 1.7360874625433862 RMSE: 1.3176067\n",
      "153 13 0.4001119136810303\n",
      "Validation loss: 1.4900557762753648 RMSE: 1.2206784\n",
      "Validation loss: 1.5546096658284685 RMSE: 1.2468398\n",
      "155 5 0.3775131404399872\n",
      "Validation loss: 1.5705710396302486 RMSE: 1.2532243\n",
      "156 26 0.4185241460800171\n",
      "Validation loss: 1.6511913717320534 RMSE: 1.284987\n",
      "Validation loss: 2.375105648969127 RMSE: 1.5411378\n",
      "158 18 0.23112986981868744\n",
      "Validation loss: 1.5043541836527596 RMSE: 1.2265214\n",
      "Validation loss: 2.1460165840334597 RMSE: 1.464929\n",
      "160 10 1.163413643836975\n",
      "Validation loss: 1.8029772939935196 RMSE: 1.3427498\n",
      "Validation loss: 1.6716280479346757 RMSE: 1.2929146\n",
      "162 2 1.0727709531784058\n",
      "Validation loss: 1.6270943331507455 RMSE: 1.2755761\n",
      "163 23 0.7013553977012634\n",
      "Validation loss: 1.5243332597006738 RMSE: 1.2346388\n",
      "Validation loss: 1.7448470887884628 RMSE: 1.3209267\n",
      "165 15 0.20050974190235138\n",
      "Validation loss: 1.4807510829604833 RMSE: 1.2168611\n",
      "Validation loss: 1.3659557093561223 RMSE: 1.1687411\n",
      "167 7 0.45556119084358215\n",
      "Validation loss: 1.4807488021597397 RMSE: 1.2168602\n",
      "168 28 0.2820799648761749\n",
      "Validation loss: 1.7638961319374826 RMSE: 1.3281175\n",
      "Validation loss: 1.504540538365862 RMSE: 1.2265972\n",
      "170 20 0.4168822169303894\n",
      "Validation loss: 1.6355660679066075 RMSE: 1.2788925\n",
      "Validation loss: 1.582439333991667 RMSE: 1.2579504\n",
      "172 12 0.6370567083358765\n",
      "Validation loss: 1.4880080750558229 RMSE: 1.2198393\n",
      "Validation loss: 1.6280825370180922 RMSE: 1.2759634\n",
      "174 4 0.34872034192085266\n",
      "Validation loss: 1.299105682204255 RMSE: 1.1397831\n",
      "175 25 0.7638216614723206\n",
      "Validation loss: 1.395274328974496 RMSE: 1.1812173\n",
      "Validation loss: 1.536219429125828 RMSE: 1.2394432\n",
      "177 17 0.4679436683654785\n",
      "Validation loss: 1.3185300273177898 RMSE: 1.1482725\n",
      "Validation loss: 1.4633151788627152 RMSE: 1.2096756\n",
      "179 9 0.2806534767150879\n",
      "Validation loss: 1.3845479108590995 RMSE: 1.1766682\n",
      "Validation loss: 1.3625021371166264 RMSE: 1.1672626\n",
      "181 1 0.3641771674156189\n",
      "Validation loss: 1.5914298485865634 RMSE: 1.2615188\n",
      "182 22 0.20828472077846527\n",
      "Validation loss: 1.4137922219470538 RMSE: 1.1890299\n",
      "Validation loss: 1.6284168336243756 RMSE: 1.2760943\n",
      "184 14 0.47776588797569275\n",
      "Validation loss: 1.4077283513229506 RMSE: 1.1864773\n",
      "Validation loss: 1.3248408958975193 RMSE: 1.1510173\n",
      "186 6 0.2963763177394867\n",
      "Validation loss: 1.3681392258247442 RMSE: 1.1696749\n",
      "187 27 0.4071963429450989\n",
      "Validation loss: 1.5522537115400872 RMSE: 1.2458947\n",
      "Validation loss: 1.8139428797021377 RMSE: 1.3468269\n",
      "189 19 0.33494728803634644\n",
      "Validation loss: 1.5685391742571266 RMSE: 1.2524134\n",
      "Validation loss: 1.3632647168319838 RMSE: 1.1675893\n",
      "191 11 0.41390785574913025\n",
      "Validation loss: 1.4231714252877024 RMSE: 1.1929674\n",
      "Validation loss: 1.2626672909323093 RMSE: 1.1236848\n",
      "193 3 0.3400896489620209\n",
      "Validation loss: 1.5880543848054598 RMSE: 1.2601804\n",
      "194 24 0.4847220182418823\n",
      "Validation loss: 1.6932024343878822 RMSE: 1.3012311\n",
      "Validation loss: 1.3542084250829916 RMSE: 1.1637046\n",
      "196 16 0.6053069233894348\n",
      "Validation loss: 1.604472218361576 RMSE: 1.2666776\n",
      "Validation loss: 1.6880023574407121 RMSE: 1.2992314\n",
      "198 8 0.5991409420967102\n",
      "Validation loss: 1.4847567049802932 RMSE: 1.218506\n",
      "Validation loss: 1.9454019259562534 RMSE: 1.3947767\n",
      "Loaded trained model with success.\n",
      "Test loss: 1.5831452914043866 Test RMSE: 1.258231\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:0', 'task_name': 'esol', 'model_type': 'gin', 'model': {'num_layer': 5, 'emb_dim': 300, 'feat_dim': 300, 'drop_ratio': 0.3, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 781, 'task': 'regression', 'data_path': 'data/esol/esol.csv', 'target': ['measured log solubility in mols per litre']}}\n",
      "Running on: cuda:0\n",
      "1127\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/1127\n",
      "Generating scaffold 1000/1127\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 15.098115921020508\n",
      "Validation loss: 5.905708123097377 RMSE: 2.4301665\n",
      "1 21 2.83724045753479\n",
      "Validation loss: 2.9422046779531295 RMSE: 1.7152854\n",
      "Validation loss: 4.263652987184778 RMSE: 2.0648615\n",
      "3 13 1.6171112060546875\n",
      "Validation loss: 3.794878259169317 RMSE: 1.9480447\n",
      "Validation loss: 3.070695627052172 RMSE: 1.75234\n",
      "5 5 2.059664249420166\n",
      "Validation loss: 5.061815373665463 RMSE: 2.2498481\n",
      "6 26 1.0069841146469116\n",
      "Validation loss: 2.5787342164368754 RMSE: 1.6058438\n",
      "Validation loss: 2.5260188779999724 RMSE: 1.5893455\n",
      "8 18 1.371416687965393\n",
      "Validation loss: 3.339714039743474 RMSE: 1.8274884\n",
      "Validation loss: 2.6158670260842922 RMSE: 1.6173642\n",
      "10 10 2.3149688243865967\n",
      "Validation loss: 2.886735975214865 RMSE: 1.6990398\n",
      "Validation loss: 2.3192954221657947 RMSE: 1.5229234\n",
      "12 2 2.152400016784668\n",
      "Validation loss: 3.9509459765611497 RMSE: 1.9876986\n",
      "13 23 1.7149622440338135\n",
      "Validation loss: 3.1967416611392943 RMSE: 1.7879434\n",
      "Validation loss: 2.4899934266520813 RMSE: 1.5779713\n",
      "15 15 0.8819084167480469\n",
      "Validation loss: 3.483596181447527 RMSE: 1.8664395\n",
      "Validation loss: 2.319027963992769 RMSE: 1.5228356\n",
      "17 7 1.23980712890625\n",
      "Validation loss: 2.2418289648748075 RMSE: 1.4972738\n",
      "18 28 1.1705410480499268\n",
      "Validation loss: 3.749543202661835 RMSE: 1.9363737\n",
      "Validation loss: 2.2648167420277554 RMSE: 1.5049307\n",
      "20 20 0.7992375493049622\n",
      "Validation loss: 2.434747971264662 RMSE: 1.5603678\n",
      "Validation loss: 2.0176164070061877 RMSE: 1.4204282\n",
      "22 12 1.342078685760498\n",
      "Validation loss: 2.1813129133882776 RMSE: 1.4769269\n",
      "Validation loss: 2.0760571386961812 RMSE: 1.4408528\n",
      "24 4 0.6808210611343384\n",
      "Validation loss: 1.9305719443127118 RMSE: 1.3894502\n",
      "25 25 0.6181447505950928\n",
      "Validation loss: 2.0841284431187455 RMSE: 1.4436511\n",
      "Validation loss: 2.724869301888795 RMSE: 1.6507179\n",
      "27 17 0.4350331425666809\n",
      "Validation loss: 3.194766897015867 RMSE: 1.7873912\n",
      "Validation loss: 1.862191641225224 RMSE: 1.3646214\n",
      "29 9 0.7355709671974182\n",
      "Validation loss: 2.1071246851862004 RMSE: 1.451594\n",
      "Validation loss: 1.6671804274077964 RMSE: 1.2911934\n",
      "31 1 0.9356372952461243\n",
      "Validation loss: 2.547782220671662 RMSE: 1.5961773\n",
      "32 22 1.094915747642517\n",
      "Validation loss: 2.93275741982249 RMSE: 1.7125295\n",
      "Validation loss: 2.5205459869013422 RMSE: 1.5876228\n",
      "34 14 1.3249398469924927\n",
      "Validation loss: 2.748110215220831 RMSE: 1.6577425\n",
      "Validation loss: 2.114596731894839 RMSE: 1.4541653\n",
      "36 6 0.7831525802612305\n",
      "Validation loss: 2.2591480322643718 RMSE: 1.5030462\n",
      "37 27 0.6295649409294128\n",
      "Validation loss: 2.6925316046824497 RMSE: 1.6408936\n",
      "Validation loss: 2.2044922756937755 RMSE: 1.4847533\n",
      "39 19 1.2535086870193481\n",
      "Validation loss: 3.13497079790166 RMSE: 1.7705848\n",
      "Validation loss: 1.9774622157611679 RMSE: 1.4062227\n",
      "41 11 0.5879191160202026\n",
      "Validation loss: 2.0707046384305023 RMSE: 1.4389943\n",
      "Validation loss: 2.073722449024167 RMSE: 1.4400425\n",
      "43 3 0.6961839199066162\n",
      "Validation loss: 2.176807196794358 RMSE: 1.4754007\n",
      "44 24 0.7665790915489197\n",
      "Validation loss: 1.3573962101894141 RMSE: 1.1650735\n",
      "Validation loss: 2.1679784734692196 RMSE: 1.4724056\n",
      "46 16 1.035477638244629\n",
      "Validation loss: 1.7629467502104497 RMSE: 1.32776\n",
      "Validation loss: 2.0864862889315177 RMSE: 1.4444675\n",
      "48 8 0.8672928214073181\n",
      "Validation loss: 2.0832753867174674 RMSE: 1.4433557\n",
      "Validation loss: 1.9626166504041282 RMSE: 1.4009342\n",
      "50 0 0.5915452837944031\n",
      "Validation loss: 1.8248506098721935 RMSE: 1.3508704\n",
      "51 21 0.5269460678100586\n",
      "Validation loss: 2.5616685837770987 RMSE: 1.6005213\n",
      "Validation loss: 1.762699467945943 RMSE: 1.3276669\n",
      "53 13 0.5073595643043518\n",
      "Validation loss: 1.6908093089550997 RMSE: 1.3003112\n",
      "Validation loss: 1.6765468885413313 RMSE: 1.2948154\n",
      "55 5 0.6684894561767578\n",
      "Validation loss: 1.7887354203030073 RMSE: 1.3374361\n",
      "56 26 0.8450686931610107\n",
      "Validation loss: 2.4381773957109028 RMSE: 1.5614665\n",
      "Validation loss: 1.8008002295958256 RMSE: 1.341939\n",
      "58 18 0.3206213712692261\n",
      "Validation loss: 2.809612445071735 RMSE: 1.6761899\n",
      "Validation loss: 2.1418169335981387 RMSE: 1.4634948\n",
      "60 10 1.8675322532653809\n",
      "Validation loss: 2.084864642240305 RMSE: 1.4439061\n",
      "Validation loss: 1.8293165295524936 RMSE: 1.3525223\n",
      "62 2 0.756144642829895\n",
      "Validation loss: 1.6296080034391014 RMSE: 1.276561\n",
      "63 23 0.4942631721496582\n",
      "Validation loss: 1.5772717041251934 RMSE: 1.2558948\n",
      "Validation loss: 1.9263961610540878 RMSE: 1.3879467\n",
      "65 15 0.7442065477371216\n",
      "Validation loss: 1.8898162134980734 RMSE: 1.3747059\n",
      "Validation loss: 1.9955501282109624 RMSE: 1.4126395\n",
      "67 7 0.8115408420562744\n",
      "Validation loss: 2.0368856771857335 RMSE: 1.427195\n",
      "68 28 1.7866817712783813\n",
      "Validation loss: 1.6456651539929146 RMSE: 1.2828348\n",
      "Validation loss: 1.9410968649703844 RMSE: 1.3932326\n",
      "70 20 1.0593249797821045\n",
      "Validation loss: 1.9529990094952878 RMSE: 1.3974974\n",
      "Validation loss: 1.7616534876612435 RMSE: 1.3272729\n",
      "72 12 1.149819016456604\n",
      "Validation loss: 1.7962202266254257 RMSE: 1.3402314\n",
      "Validation loss: 1.9723383156599197 RMSE: 1.4043996\n",
      "74 4 0.8421599864959717\n",
      "Validation loss: 1.8169245946723802 RMSE: 1.3479335\n",
      "75 25 0.7164809107780457\n",
      "Validation loss: 1.721802785333279 RMSE: 1.3121748\n",
      "Validation loss: 2.071389523227658 RMSE: 1.4392322\n",
      "77 17 0.838691771030426\n",
      "Validation loss: 1.7183668360245967 RMSE: 1.3108649\n",
      "Validation loss: 2.2275655343469265 RMSE: 1.4925032\n",
      "79 9 0.43738842010498047\n",
      "Validation loss: 1.8444922118060356 RMSE: 1.3581209\n",
      "Validation loss: 2.066529670647815 RMSE: 1.4375429\n",
      "81 1 0.4966244697570801\n",
      "Validation loss: 1.720314909926558 RMSE: 1.3116077\n",
      "82 22 0.34871432185173035\n",
      "Validation loss: 1.5259678553690952 RMSE: 1.2353007\n",
      "Validation loss: 1.857262105013417 RMSE: 1.3628141\n",
      "84 14 0.34846484661102295\n",
      "Validation loss: 2.350239357062146 RMSE: 1.5330492\n",
      "Validation loss: 1.680602074724383 RMSE: 1.2963804\n",
      "86 6 0.8973244428634644\n",
      "Validation loss: 1.4077082951511957 RMSE: 1.1864688\n",
      "87 27 0.45966243743896484\n",
      "Validation loss: 1.9894531363934542 RMSE: 1.4104798\n",
      "Validation loss: 2.330972135594461 RMSE: 1.5267522\n",
      "89 19 0.4687173664569855\n",
      "Validation loss: 1.5990186739811856 RMSE: 1.2645231\n",
      "Validation loss: 1.976752492178858 RMSE: 1.4059703\n",
      "91 11 1.0190626382827759\n",
      "Validation loss: 2.0781609336886784 RMSE: 1.4415827\n",
      "Validation loss: 2.0618112340437627 RMSE: 1.4359008\n",
      "93 3 0.5348926186561584\n",
      "Validation loss: 1.8693750875186077 RMSE: 1.3672509\n",
      "94 24 0.44378650188446045\n",
      "Validation loss: 1.9324200438187185 RMSE: 1.3901151\n",
      "Validation loss: 1.3770107589991747 RMSE: 1.1734611\n",
      "96 16 0.8155655264854431\n",
      "Validation loss: 1.9392266273498535 RMSE: 1.3925611\n",
      "Validation loss: 1.601855092344031 RMSE: 1.2656442\n",
      "98 8 0.5223084688186646\n",
      "Validation loss: 1.5984460948842816 RMSE: 1.2642967\n",
      "Validation loss: 1.9457958957790273 RMSE: 1.3949178\n",
      "100 0 0.6458256244659424\n",
      "Validation loss: 1.8000279981478127 RMSE: 1.3416512\n",
      "101 21 0.6647293567657471\n",
      "Validation loss: 1.6594071045386052 RMSE: 1.2881798\n",
      "Validation loss: 1.877488648996944 RMSE: 1.3702148\n",
      "103 13 0.4526364505290985\n",
      "Validation loss: 1.9702920681607408 RMSE: 1.4036709\n",
      "Validation loss: 1.7092638079044038 RMSE: 1.3073882\n",
      "105 5 0.6373777389526367\n",
      "Validation loss: 1.9923776810148122 RMSE: 1.4115161\n",
      "106 26 0.7415151596069336\n",
      "Validation loss: 1.5674583300025062 RMSE: 1.2519817\n",
      "Validation loss: 1.8304696642192064 RMSE: 1.3529485\n",
      "108 18 0.5644829869270325\n",
      "Validation loss: 1.829216754541988 RMSE: 1.3524854\n",
      "Validation loss: 1.6464772277173743 RMSE: 1.2831513\n",
      "110 10 0.4504769444465637\n",
      "Validation loss: 2.05836428161216 RMSE: 1.4347\n",
      "Validation loss: 1.9692359904272367 RMSE: 1.4032947\n",
      "112 2 0.5645304918289185\n",
      "Validation loss: 1.6349659831123013 RMSE: 1.2786578\n",
      "113 23 0.3633861839771271\n",
      "Validation loss: 1.9178528026141952 RMSE: 1.3848656\n",
      "Validation loss: 1.5224672577022451 RMSE: 1.233883\n",
      "115 15 0.3543151319026947\n",
      "Validation loss: 1.8639320694239794 RMSE: 1.365259\n",
      "Validation loss: 1.5737096050144297 RMSE: 1.2544758\n",
      "117 7 0.6520357131958008\n",
      "Validation loss: 1.710293374757851 RMSE: 1.3077818\n",
      "118 28 0.20725564658641815\n",
      "Validation loss: 1.6415951505171513 RMSE: 1.2812475\n",
      "Validation loss: 1.6162864982554344 RMSE: 1.2713326\n",
      "120 20 0.36855217814445496\n",
      "Validation loss: 2.6292548728200185 RMSE: 1.6214978\n",
      "Validation loss: 2.2470723738712546 RMSE: 1.4990238\n",
      "122 12 0.7056143879890442\n",
      "Validation loss: 2.2785539352788335 RMSE: 1.509488\n",
      "Validation loss: 1.3568315780268305 RMSE: 1.1648312\n",
      "124 4 0.9127125144004822\n",
      "Validation loss: 1.4455495186611615 RMSE: 1.2023101\n",
      "125 25 0.6142485737800598\n",
      "Validation loss: 1.9305641271371756 RMSE: 1.3894475\n",
      "Validation loss: 2.0695175903033367 RMSE: 1.4385818\n",
      "127 17 1.0982613563537598\n",
      "Validation loss: 1.7904883169494898 RMSE: 1.3380914\n",
      "Validation loss: 1.777310702653058 RMSE: 1.3331583\n",
      "129 9 0.4394797086715698\n",
      "Validation loss: 1.9480714565884751 RMSE: 1.3957334\n",
      "Validation loss: 1.549649484389651 RMSE: 1.2448491\n",
      "131 1 0.22527514398097992\n",
      "Validation loss: 1.7044950913538974 RMSE: 1.3055631\n",
      "132 22 0.4396190047264099\n",
      "Validation loss: 1.8908312489501142 RMSE: 1.375075\n",
      "Validation loss: 1.565071833872162 RMSE: 1.2510283\n",
      "134 14 0.37642359733581543\n",
      "Validation loss: 2.1990693986943337 RMSE: 1.4829259\n",
      "Validation loss: 1.5768906004660952 RMSE: 1.255743\n",
      "136 6 0.5836727619171143\n",
      "Validation loss: 1.4174417668739252 RMSE: 1.1905636\n",
      "137 27 0.3794441819190979\n",
      "Validation loss: 1.5335296962113507 RMSE: 1.2383575\n",
      "Validation loss: 1.7333139951250194 RMSE: 1.3165538\n",
      "139 19 0.5313913822174072\n",
      "Validation loss: 1.7880371946149167 RMSE: 1.3371751\n",
      "Validation loss: 1.581799168502335 RMSE: 1.257696\n",
      "141 11 0.5432969927787781\n",
      "Validation loss: 1.7476718763334562 RMSE: 1.3219955\n",
      "Validation loss: 1.6769669836601324 RMSE: 1.2949777\n",
      "143 3 0.3535349369049072\n",
      "Validation loss: 1.6315547154013035 RMSE: 1.2773234\n",
      "144 24 0.20674194395542145\n",
      "Validation loss: 1.8851523146165157 RMSE: 1.3730085\n",
      "Validation loss: 1.663937838731614 RMSE: 1.2899371\n",
      "146 16 0.4539242684841156\n",
      "Validation loss: 1.513557063794769 RMSE: 1.2302672\n",
      "Validation loss: 1.596867048634892 RMSE: 1.263672\n",
      "148 8 0.34717288613319397\n",
      "Validation loss: 1.5816796912556201 RMSE: 1.2576485\n",
      "Validation loss: 1.7411640238972892 RMSE: 1.3195318\n",
      "150 0 0.7788588404655457\n",
      "Validation loss: 1.4770144509003225 RMSE: 1.2153249\n",
      "151 21 0.41797053813934326\n",
      "Validation loss: 1.4318924688659938 RMSE: 1.1966171\n",
      "Validation loss: 1.5790127178209017 RMSE: 1.2565877\n",
      "153 13 0.5773561000823975\n",
      "Validation loss: 1.6645034192937664 RMSE: 1.2901564\n",
      "Validation loss: 1.227704764467425 RMSE: 1.1080184\n",
      "155 5 0.637141227722168\n",
      "Validation loss: 1.6756258227128897 RMSE: 1.2944597\n",
      "156 26 0.5314712524414062\n",
      "Validation loss: 1.32836148907653 RMSE: 1.1525457\n",
      "Validation loss: 1.7297962323754235 RMSE: 1.3152173\n",
      "158 18 0.4403279721736908\n",
      "Validation loss: 1.59293905085167 RMSE: 1.2621169\n",
      "Validation loss: 1.4123794388982047 RMSE: 1.1884357\n",
      "160 10 0.5989166498184204\n",
      "Validation loss: 1.601020097732544 RMSE: 1.2653142\n",
      "Validation loss: 1.796578022231043 RMSE: 1.3403649\n",
      "162 2 0.6370909214019775\n",
      "Validation loss: 1.7766658447485055 RMSE: 1.3329164\n",
      "163 23 0.7712544798851013\n",
      "Validation loss: 1.4891888588930653 RMSE: 1.2203232\n",
      "Validation loss: 1.4859399869378689 RMSE: 1.2189914\n",
      "165 15 0.38992032408714294\n",
      "Validation loss: 1.5485492670430547 RMSE: 1.2444072\n",
      "Validation loss: 1.421853023292744 RMSE: 1.1924148\n",
      "167 7 0.5584187507629395\n",
      "Validation loss: 1.4267056521061248 RMSE: 1.1944479\n",
      "168 28 0.8418708443641663\n",
      "Validation loss: 1.7482474957947183 RMSE: 1.322213\n",
      "Validation loss: 1.845623280094788 RMSE: 1.3585372\n",
      "170 20 0.7051073312759399\n",
      "Validation loss: 1.511641275566236 RMSE: 1.2294883\n",
      "Validation loss: 1.7299843435793851 RMSE: 1.3152887\n",
      "172 12 0.41861987113952637\n",
      "Validation loss: 1.396712534195554 RMSE: 1.1818259\n",
      "Validation loss: 1.4183989763259888 RMSE: 1.1909657\n",
      "174 4 0.528336763381958\n",
      "Validation loss: 1.6071681469942616 RMSE: 1.2677413\n",
      "175 25 0.3094722628593445\n",
      "Validation loss: 1.2131421006886305 RMSE: 1.1014273\n",
      "Validation loss: 1.295163883572131 RMSE: 1.1380527\n",
      "177 17 0.7326358556747437\n",
      "Validation loss: 1.2822219745247765 RMSE: 1.1323525\n",
      "Validation loss: 1.564904854360935 RMSE: 1.2509615\n",
      "179 9 0.2820999324321747\n",
      "Validation loss: 1.6365803089817013 RMSE: 1.279289\n",
      "Validation loss: 1.7150631773788316 RMSE: 1.3096042\n",
      "181 1 0.33492833375930786\n",
      "Validation loss: 1.6549933616038972 RMSE: 1.2864655\n",
      "182 22 0.3574269413948059\n",
      "Validation loss: 1.7357584111458433 RMSE: 1.3174819\n",
      "Validation loss: 1.5474616384084245 RMSE: 1.2439702\n",
      "184 14 0.6405072808265686\n",
      "Validation loss: 1.8447067821975303 RMSE: 1.3581998\n",
      "Validation loss: 1.6278616807102102 RMSE: 1.2758769\n",
      "186 6 0.45464879274368286\n",
      "Validation loss: 1.6896236122181985 RMSE: 1.2998554\n",
      "187 27 0.4611162841320038\n",
      "Validation loss: 1.6152238592637325 RMSE: 1.2709146\n",
      "Validation loss: 1.7192834932192238 RMSE: 1.3112144\n",
      "189 19 0.5833089351654053\n",
      "Validation loss: 1.3446705868813844 RMSE: 1.1595993\n",
      "Validation loss: 1.519184874222342 RMSE: 1.2325522\n",
      "191 11 0.48943382501602173\n",
      "Validation loss: 1.6426964196483647 RMSE: 1.2816771\n",
      "Validation loss: 1.4832744113111918 RMSE: 1.2178975\n",
      "193 3 0.35299694538116455\n",
      "Validation loss: 1.3778228248115134 RMSE: 1.1738069\n",
      "194 24 0.46904000639915466\n",
      "Validation loss: 1.3736423623245375 RMSE: 1.1720248\n",
      "Validation loss: 1.4873032970766051 RMSE: 1.2195505\n",
      "196 16 0.6559354066848755\n",
      "Validation loss: 1.6221813691400848 RMSE: 1.2736489\n",
      "Validation loss: 1.4100833582667123 RMSE: 1.1874692\n",
      "198 8 0.22800861299037933\n",
      "Validation loss: 1.5520156970066308 RMSE: 1.2457992\n",
      "Validation loss: 1.6618819764230104 RMSE: 1.28914\n",
      "Loaded trained model with success.\n",
      "Test loss: 1.543778117779082 Test RMSE: 1.2424886\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:0', 'task_name': 'lipo', 'model_type': 'gin', 'model': {'num_layer': 5, 'emb_dim': 300, 'feat_dim': 300, 'drop_ratio': 0.3, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 777, 'task': 'regression', 'data_path': 'data/lipophilicity/Lipophilicity.csv', 'target': ['exp']}}\n",
      "Running on: cuda:0\n",
      "4199\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/4199\n",
      "Generating scaffold 1000/4199\n",
      "Generating scaffold 2000/4199\n",
      "Generating scaffold 3000/4199\n",
      "Generating scaffold 4000/4199\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 7.633002281188965\n",
      "0 50 1.1554462909698486\n",
      "0 100 1.333011269569397\n",
      "Validation loss: 1.3992178564979916 RMSE: 1.1828854\n",
      "1 45 1.5086807012557983\n",
      "1 95 1.167222023010254\n",
      "Validation loss: 1.261478278750465 RMSE: 1.1231555\n",
      "2 40 0.7484045624732971\n",
      "2 90 0.7063473463058472\n",
      "Validation loss: 0.9417142152786255 RMSE: 0.97041965\n",
      "3 35 0.807632327079773\n",
      "3 85 0.7272883057594299\n",
      "Validation loss: 0.949323254539853 RMSE: 0.9743322\n",
      "4 30 0.5310506820678711\n",
      "4 80 0.9035890102386475\n",
      "Validation loss: 0.8734870059149605 RMSE: 0.93460524\n",
      "5 25 0.7311309576034546\n",
      "5 75 0.6840580701828003\n",
      "Validation loss: 1.0461822731154307 RMSE: 1.0228305\n",
      "6 20 0.6650470495223999\n",
      "6 70 0.7431104183197021\n",
      "Validation loss: 0.855641824290866 RMSE: 0.92500913\n",
      "7 15 0.598370373249054\n",
      "7 65 0.46096163988113403\n",
      "Validation loss: 0.8168248823710851 RMSE: 0.9037837\n",
      "8 10 0.9239538908004761\n",
      "8 60 0.8991228342056274\n",
      "Validation loss: 0.8222627855482556 RMSE: 0.90678704\n",
      "9 5 1.0230120420455933\n",
      "9 55 0.5381972193717957\n",
      "Validation loss: 0.8139809256508237 RMSE: 0.90220886\n",
      "10 0 0.7787613868713379\n",
      "10 50 0.6398967504501343\n",
      "10 100 0.7218530774116516\n",
      "Validation loss: 0.8460844777879261 RMSE: 0.91982853\n",
      "11 45 0.6465767621994019\n",
      "11 95 0.8548393249511719\n",
      "Validation loss: 0.8573387251013801 RMSE: 0.92592585\n",
      "12 40 0.5001441836357117\n",
      "12 90 0.7529293298721313\n",
      "Validation loss: 0.874684583005451 RMSE: 0.93524575\n",
      "13 35 0.5416080951690674\n",
      "13 85 0.6535607576370239\n",
      "Validation loss: 0.780743576231457 RMSE: 0.88359696\n",
      "14 30 0.47970178723335266\n",
      "14 80 0.5742921233177185\n",
      "Validation loss: 0.6950268348058065 RMSE: 0.83368266\n",
      "15 25 0.5432900786399841\n",
      "15 75 0.45557311177253723\n",
      "Validation loss: 0.6908657777877081 RMSE: 0.8311834\n",
      "16 20 0.6633200645446777\n",
      "16 70 0.6847485303878784\n",
      "Validation loss: 0.7539982273465111 RMSE: 0.8683307\n",
      "17 15 1.230830192565918\n",
      "17 65 0.4795931875705719\n",
      "Validation loss: 0.713106278862272 RMSE: 0.8444562\n",
      "18 10 0.8825395107269287\n",
      "18 60 0.4658992886543274\n",
      "Validation loss: 0.7035672307014466 RMSE: 0.8387891\n",
      "19 5 0.8225103616714478\n",
      "19 55 0.31080999970436096\n",
      "Validation loss: 0.648309151899247 RMSE: 0.8051765\n",
      "20 0 0.4405784010887146\n",
      "20 50 0.6841912865638733\n",
      "20 100 0.6689560413360596\n",
      "Validation loss: 0.6658774466741653 RMSE: 0.8160131\n",
      "21 45 1.014204502105713\n",
      "21 95 0.33868128061294556\n",
      "Validation loss: 0.8004237195920376 RMSE: 0.89466405\n",
      "22 40 0.6374439001083374\n",
      "22 90 0.4884979724884033\n",
      "Validation loss: 0.7918088674545288 RMSE: 0.88983643\n",
      "23 35 0.8853408098220825\n",
      "23 85 0.32943540811538696\n",
      "Validation loss: 0.7169930162883941 RMSE: 0.84675443\n",
      "24 30 0.582324206829071\n",
      "24 80 0.41536128520965576\n",
      "Validation loss: 0.6942669996193477 RMSE: 0.83322686\n",
      "25 25 0.4956740438938141\n",
      "25 75 0.8342201709747314\n",
      "Validation loss: 0.6841689749842599 RMSE: 0.82714504\n",
      "26 20 0.3734629452228546\n",
      "26 70 0.550149142742157\n",
      "Validation loss: 0.6601953821522849 RMSE: 0.8125241\n",
      "27 15 0.6650505065917969\n",
      "27 65 0.8702266216278076\n",
      "Validation loss: 0.6231023039136614 RMSE: 0.7893683\n",
      "28 10 0.3851296305656433\n",
      "28 60 0.5087104439735413\n",
      "Validation loss: 0.6189359691880998 RMSE: 0.78672487\n",
      "29 5 0.5999890565872192\n",
      "29 55 0.7552593350410461\n",
      "Validation loss: 0.7021465074448359 RMSE: 0.8379418\n",
      "30 0 0.5153709053993225\n",
      "30 50 0.5959376096725464\n",
      "30 100 0.4151657521724701\n",
      "Validation loss: 0.6474953759284247 RMSE: 0.804671\n",
      "31 45 0.7905983328819275\n",
      "31 95 0.5237239003181458\n",
      "Validation loss: 0.6369087480363391 RMSE: 0.79806566\n",
      "32 40 0.5155397057533264\n",
      "32 90 0.4791744649410248\n",
      "Validation loss: 0.6503212826592581 RMSE: 0.80642503\n",
      "33 35 0.5303823351860046\n",
      "33 85 0.6545541286468506\n",
      "Validation loss: 0.790502421061198 RMSE: 0.88910204\n",
      "34 30 0.30535662174224854\n",
      "34 80 0.361177533864975\n",
      "Validation loss: 0.6517755420435043 RMSE: 0.80732614\n",
      "35 25 0.4032719135284424\n",
      "35 75 0.535071849822998\n",
      "Validation loss: 0.6332283645868302 RMSE: 0.7957565\n",
      "36 20 0.23515217006206512\n",
      "36 70 0.5944796204566956\n",
      "Validation loss: 0.6516425981408074 RMSE: 0.8072438\n",
      "37 15 0.7055033445358276\n",
      "37 65 0.4679350256919861\n",
      "Validation loss: 0.7740124577567691 RMSE: 0.87977976\n",
      "38 10 0.877685546875\n",
      "38 60 0.5996140837669373\n",
      "Validation loss: 0.6676804644720895 RMSE: 0.81711715\n",
      "39 5 0.24874535202980042\n",
      "39 55 0.8179342746734619\n",
      "Validation loss: 0.6507233903521583 RMSE: 0.80667424\n",
      "40 0 0.48663923144340515\n",
      "40 50 0.3995354473590851\n",
      "40 100 0.38025638461112976\n",
      "Validation loss: 0.639228664125715 RMSE: 0.7995177\n",
      "41 45 0.3872799873352051\n",
      "41 95 0.5667489767074585\n",
      "Validation loss: 0.6711259174914587 RMSE: 0.81922275\n",
      "42 40 0.2753785252571106\n",
      "42 90 0.3262171149253845\n",
      "Validation loss: 0.607779389052164 RMSE: 0.77960205\n",
      "43 35 0.6158395409584045\n",
      "43 85 0.513158917427063\n",
      "Validation loss: 0.6553561744235811 RMSE: 0.8095408\n",
      "44 30 0.4672642946243286\n",
      "44 80 0.5652347207069397\n",
      "Validation loss: 0.6459073520842052 RMSE: 0.80368364\n",
      "45 25 0.2163296639919281\n",
      "45 75 0.4009557068347931\n",
      "Validation loss: 0.618389181863694 RMSE: 0.78637725\n",
      "46 20 0.2766726016998291\n",
      "46 70 0.3208627998828888\n",
      "Validation loss: 0.5993621780758812 RMSE: 0.7741849\n",
      "47 15 0.5231685042381287\n",
      "47 65 0.29174166917800903\n",
      "Validation loss: 0.7387793132237026 RMSE: 0.85952276\n",
      "48 10 0.47614729404449463\n",
      "48 60 0.3227495849132538\n",
      "Validation loss: 0.6282225479682286 RMSE: 0.7926049\n",
      "49 5 0.5715394020080566\n",
      "49 55 0.3653908669948578\n",
      "Validation loss: 0.6233760266076951 RMSE: 0.78954166\n",
      "50 0 0.6509851813316345\n",
      "50 50 0.44962453842163086\n",
      "50 100 0.2608415186405182\n",
      "Validation loss: 0.5470189934685117 RMSE: 0.73960733\n",
      "51 45 0.5698465704917908\n",
      "51 95 0.6501439213752747\n",
      "Validation loss: 0.6024173980667478 RMSE: 0.77615553\n",
      "52 40 0.4244663715362549\n",
      "52 90 0.27976036071777344\n",
      "Validation loss: 0.6280338548478626 RMSE: 0.79248583\n",
      "53 35 0.5552853941917419\n",
      "53 85 0.45025765895843506\n",
      "Validation loss: 0.5952732983089629 RMSE: 0.77153957\n",
      "54 30 0.3119426667690277\n",
      "54 80 0.20923566818237305\n",
      "Validation loss: 0.5579520427754947 RMSE: 0.7469619\n",
      "55 25 0.158391535282135\n",
      "55 75 0.46877798438072205\n",
      "Validation loss: 0.5827777800105867 RMSE: 0.7633988\n",
      "56 20 0.3157674968242645\n",
      "56 70 0.4900263845920563\n",
      "Validation loss: 0.5966082286267054 RMSE: 0.7724042\n",
      "57 15 0.32938703894615173\n",
      "57 65 0.4632742702960968\n",
      "Validation loss: 0.5604911821229117 RMSE: 0.74865955\n",
      "58 10 0.5302415490150452\n",
      "58 60 0.32674774527549744\n",
      "Validation loss: 0.6206961609068371 RMSE: 0.7878427\n",
      "59 5 0.5931583642959595\n",
      "59 55 0.393389493227005\n",
      "Validation loss: 0.6129471324739002 RMSE: 0.78290945\n",
      "60 0 0.3240824043750763\n",
      "60 50 0.5592354536056519\n",
      "60 100 0.42454788088798523\n",
      "Validation loss: 0.5854896738415672 RMSE: 0.76517296\n",
      "61 45 0.2752748727798462\n",
      "61 95 0.583112895488739\n",
      "Validation loss: 0.5637685934702555 RMSE: 0.75084525\n",
      "62 40 0.6356945633888245\n",
      "62 90 0.4629483222961426\n",
      "Validation loss: 0.5627915371031988 RMSE: 0.7501943\n",
      "63 35 0.42564502358436584\n",
      "63 85 0.4054025113582611\n",
      "Validation loss: 0.6095186403819493 RMSE: 0.7807168\n",
      "64 30 0.5726562142372131\n",
      "64 80 0.22547417879104614\n",
      "Validation loss: 0.6110450946149372 RMSE: 0.78169376\n",
      "65 25 0.27867159247398376\n",
      "65 75 0.15969893336296082\n",
      "Validation loss: 0.594247643720536 RMSE: 0.7708746\n",
      "66 20 0.78272944688797\n",
      "66 70 0.6354818344116211\n",
      "Validation loss: 0.5464271295638311 RMSE: 0.7392071\n",
      "67 15 0.41707974672317505\n",
      "67 65 0.2872694134712219\n",
      "Validation loss: 0.6698838617120471 RMSE: 0.81846434\n",
      "68 10 0.6223251819610596\n",
      "68 60 0.4255428910255432\n",
      "Validation loss: 0.5583323143777393 RMSE: 0.7472164\n",
      "69 5 0.3569509983062744\n",
      "69 55 0.34326988458633423\n",
      "Validation loss: 0.5201635440190633 RMSE: 0.72122365\n",
      "70 0 0.35763072967529297\n",
      "70 50 0.2772996723651886\n",
      "70 100 0.6582816243171692\n",
      "Validation loss: 0.6477795717262086 RMSE: 0.8048476\n",
      "71 45 0.20114365220069885\n",
      "71 95 0.29445698857307434\n",
      "Validation loss: 0.6727820228962671 RMSE: 0.82023287\n",
      "72 40 0.39280274510383606\n",
      "72 90 0.44910064339637756\n",
      "Validation loss: 0.641273908388047 RMSE: 0.8007958\n",
      "73 35 0.37969622015953064\n",
      "73 85 0.3714662194252014\n",
      "Validation loss: 0.5104390886567888 RMSE: 0.7144502\n",
      "74 30 0.2481396645307541\n",
      "74 80 0.20144185423851013\n",
      "Validation loss: 0.5762231747309366 RMSE: 0.75909364\n",
      "75 25 0.22651077806949615\n",
      "75 75 0.6683822274208069\n",
      "Validation loss: 0.5587097244603293 RMSE: 0.74746895\n",
      "76 20 0.2211136817932129\n",
      "76 70 0.42604538798332214\n",
      "Validation loss: 0.5692792324792771 RMSE: 0.754506\n",
      "77 15 0.31975212693214417\n",
      "77 65 0.45564568042755127\n",
      "Validation loss: 0.5873812255405244 RMSE: 0.76640797\n",
      "78 10 0.5370280146598816\n",
      "78 60 0.2523285448551178\n",
      "Validation loss: 0.5379779463722593 RMSE: 0.7334698\n",
      "79 5 0.4979209899902344\n",
      "79 55 0.3075670003890991\n",
      "Validation loss: 0.5472578417687189 RMSE: 0.73976874\n",
      "80 0 0.2928335666656494\n",
      "80 50 0.17802956700325012\n",
      "80 100 0.2772255539894104\n",
      "Validation loss: 0.511524488244738 RMSE: 0.7152094\n",
      "81 45 0.26630690693855286\n",
      "81 95 0.21468500792980194\n",
      "Validation loss: 0.5471710840861003 RMSE: 0.73971015\n",
      "82 40 0.2897264063358307\n",
      "82 90 0.23976904153823853\n",
      "Validation loss: 0.556714803690002 RMSE: 0.74613327\n",
      "83 35 0.27911704778671265\n",
      "83 85 0.15200205147266388\n",
      "Validation loss: 0.522621990953173 RMSE: 0.72292596\n",
      "84 30 0.5296978950500488\n",
      "84 80 0.24839431047439575\n",
      "Validation loss: 0.5314583352633885 RMSE: 0.7290119\n",
      "85 25 0.3624262511730194\n",
      "85 75 0.3922446668148041\n",
      "Validation loss: 0.5347823100430625 RMSE: 0.7312881\n",
      "86 20 0.25354379415512085\n",
      "86 70 0.17098799347877502\n",
      "Validation loss: 0.5195747307368688 RMSE: 0.72081536\n",
      "87 15 0.25946536660194397\n",
      "87 65 0.3181731402873993\n",
      "Validation loss: 0.5188643140452248 RMSE: 0.7203223\n",
      "88 10 0.20544041693210602\n",
      "88 60 0.3007977306842804\n",
      "Validation loss: 0.5527188877264658 RMSE: 0.74345064\n",
      "89 5 0.24974367022514343\n",
      "89 55 0.2327314168214798\n",
      "Validation loss: 0.5030227950641087 RMSE: 0.709241\n",
      "90 0 0.40945422649383545\n",
      "90 50 0.44460979104042053\n",
      "90 100 0.3782816529273987\n",
      "Validation loss: 0.5860113336926415 RMSE: 0.7655138\n",
      "91 45 0.44455018639564514\n",
      "91 95 0.31796473264694214\n",
      "Validation loss: 0.5755926086789086 RMSE: 0.7586782\n",
      "92 40 0.2500784993171692\n",
      "92 90 0.2900446057319641\n",
      "Validation loss: 0.5890984251385644 RMSE: 0.7675275\n",
      "93 35 0.22427046298980713\n",
      "93 85 0.2583199143409729\n",
      "Validation loss: 0.5118226542359307 RMSE: 0.7154178\n",
      "94 30 0.30091965198516846\n",
      "94 80 0.3948889970779419\n",
      "Validation loss: 0.532338729926518 RMSE: 0.72961545\n",
      "95 25 0.27923113107681274\n",
      "95 75 0.2724195718765259\n",
      "Validation loss: 0.5284781978243873 RMSE: 0.726965\n",
      "96 20 0.4209456443786621\n",
      "96 70 0.16822418570518494\n",
      "Validation loss: 0.5230533193974268 RMSE: 0.7232242\n",
      "97 15 0.27336299419403076\n",
      "97 65 0.19734832644462585\n",
      "Validation loss: 0.5460549883899235 RMSE: 0.7389553\n",
      "98 10 0.4028486907482147\n",
      "98 60 0.21204084157943726\n",
      "Validation loss: 0.5315275050344921 RMSE: 0.72905934\n",
      "99 5 0.3499574363231659\n",
      "99 55 0.46080482006073\n",
      "Validation loss: 0.5125634772436959 RMSE: 0.7159354\n",
      "100 0 0.2948282063007355\n",
      "100 50 0.3229580819606781\n",
      "100 100 0.24437423050403595\n",
      "Validation loss: 0.5224438179106939 RMSE: 0.72280276\n",
      "101 45 0.3299205005168915\n",
      "101 95 0.30368906259536743\n",
      "Validation loss: 0.5227675159772237 RMSE: 0.72302663\n",
      "102 40 0.2542228698730469\n",
      "102 90 0.25198203325271606\n",
      "Validation loss: 0.5064352035522461 RMSE: 0.7116426\n",
      "103 35 0.2862897515296936\n",
      "103 85 0.29875698685646057\n",
      "Validation loss: 0.5230814757801238 RMSE: 0.7232437\n",
      "104 30 0.19122718274593353\n",
      "104 80 0.3992817997932434\n",
      "Validation loss: 0.5133249050094968 RMSE: 0.7164669\n",
      "105 25 0.21961982548236847\n",
      "105 75 0.3207908570766449\n",
      "Validation loss: 0.5316370685895284 RMSE: 0.72913444\n",
      "106 20 0.262591689825058\n",
      "106 70 0.19971385598182678\n",
      "Validation loss: 0.5307920870326814 RMSE: 0.7285548\n",
      "107 15 0.3692181408405304\n",
      "107 65 0.3311794698238373\n",
      "Validation loss: 0.5086800541196551 RMSE: 0.7132181\n",
      "108 10 0.16308380663394928\n",
      "108 60 0.2848857045173645\n",
      "Validation loss: 0.5427253175349462 RMSE: 0.7366989\n",
      "109 5 0.337552011013031\n",
      "109 55 0.19949164986610413\n",
      "Validation loss: 0.508248454900015 RMSE: 0.7129155\n",
      "110 0 0.21373805403709412\n",
      "110 50 0.2968190014362335\n",
      "110 100 0.38002440333366394\n",
      "Validation loss: 0.49853602534248714 RMSE: 0.70607084\n",
      "111 45 0.22128383815288544\n",
      "111 95 0.2810187339782715\n",
      "Validation loss: 0.5367339906238374 RMSE: 0.7326213\n",
      "112 40 0.2279105931520462\n",
      "112 90 0.1970590353012085\n",
      "Validation loss: 0.5288916065579369 RMSE: 0.7272493\n",
      "113 35 0.2473682463169098\n",
      "113 85 0.36800235509872437\n",
      "Validation loss: 0.5033224382570811 RMSE: 0.7094522\n",
      "114 30 0.2568647861480713\n",
      "114 80 0.26824453473091125\n",
      "Validation loss: 0.5090335505349296 RMSE: 0.71346587\n",
      "115 25 0.23792408406734467\n",
      "115 75 0.24407337605953217\n",
      "Validation loss: 0.5028691938945226 RMSE: 0.70913273\n",
      "116 20 0.20566077530384064\n",
      "116 70 0.5546673536300659\n",
      "Validation loss: 0.5259203626995995 RMSE: 0.72520363\n",
      "117 15 0.2206781953573227\n",
      "117 65 0.33661991357803345\n",
      "Validation loss: 0.5093204143501463 RMSE: 0.71366686\n",
      "118 10 0.1880258321762085\n",
      "118 60 0.17259888350963593\n",
      "Validation loss: 0.5230669952574231 RMSE: 0.7232337\n",
      "119 5 0.11075633019208908\n",
      "119 55 0.2162061333656311\n",
      "Validation loss: 0.5362478449231103 RMSE: 0.73228943\n",
      "120 0 0.20238785445690155\n",
      "120 50 0.25037384033203125\n",
      "120 100 0.4097977578639984\n",
      "Validation loss: 0.5215977657408941 RMSE: 0.72221726\n",
      "121 45 0.18520627915859222\n",
      "121 95 0.2948036789894104\n",
      "Validation loss: 0.5386595941725232 RMSE: 0.73393434\n",
      "122 40 0.23397724330425262\n",
      "122 90 0.15999260544776917\n",
      "Validation loss: 0.5293800950050354 RMSE: 0.7275851\n",
      "123 35 0.1501694768667221\n",
      "123 85 0.16530390083789825\n",
      "Validation loss: 0.5242704483015197 RMSE: 0.7240652\n",
      "124 30 0.22373205423355103\n",
      "124 80 0.42883265018463135\n",
      "Validation loss: 0.5312667381195795 RMSE: 0.72888047\n",
      "125 25 0.1274106651544571\n",
      "125 75 0.21346427500247955\n",
      "Validation loss: 0.5159396160216558 RMSE: 0.7182894\n",
      "126 20 0.23963724076747894\n",
      "126 70 0.21059896051883698\n",
      "Validation loss: 0.4934575569062006 RMSE: 0.70246536\n",
      "127 15 0.18248076736927032\n",
      "127 65 0.34752389788627625\n",
      "Validation loss: 0.5208147707439604 RMSE: 0.721675\n",
      "128 10 0.3816492557525635\n",
      "128 60 0.18110333383083344\n",
      "Validation loss: 0.4951137772628239 RMSE: 0.7036432\n",
      "129 5 0.16411443054676056\n",
      "129 55 0.33454182744026184\n",
      "Validation loss: 0.5268544016849427 RMSE: 0.72584736\n",
      "130 0 0.2561691105365753\n",
      "130 50 0.40243756771087646\n",
      "130 100 0.24503694474697113\n",
      "Validation loss: 0.508591237522307 RMSE: 0.7131558\n",
      "131 45 0.2653769254684448\n",
      "131 95 0.21463927626609802\n",
      "Validation loss: 0.5231898075058347 RMSE: 0.7233186\n",
      "132 40 0.22885753214359283\n",
      "132 90 0.20451201498508453\n",
      "Validation loss: 0.4964095626558576 RMSE: 0.7045634\n",
      "133 35 0.2771264910697937\n",
      "133 85 0.24060973525047302\n",
      "Validation loss: 0.5320632400966826 RMSE: 0.7294267\n",
      "134 30 0.20532384514808655\n",
      "134 80 0.15030953288078308\n",
      "Validation loss: 0.4977191959108625 RMSE: 0.70549214\n",
      "135 25 0.23852209746837616\n",
      "135 75 0.3056795299053192\n",
      "Validation loss: 0.5649208664894104 RMSE: 0.7516122\n",
      "136 20 0.18834242224693298\n",
      "136 70 0.24292311072349548\n",
      "Validation loss: 0.5125409066677094 RMSE: 0.7159197\n",
      "137 15 0.21793174743652344\n",
      "137 65 0.30716702342033386\n",
      "Validation loss: 0.5019774797416868 RMSE: 0.70850366\n",
      "138 10 0.2054983526468277\n",
      "138 60 0.20484137535095215\n",
      "Validation loss: 0.4781468438250678 RMSE: 0.69148165\n",
      "139 5 0.13352900743484497\n",
      "139 55 0.2920480966567993\n",
      "Validation loss: 0.5043757552192325 RMSE: 0.7101942\n",
      "140 0 0.2008945792913437\n",
      "140 50 0.28195223212242126\n",
      "140 100 0.1817617118358612\n",
      "Validation loss: 0.525419967515128 RMSE: 0.7248586\n",
      "141 45 0.26631879806518555\n",
      "141 95 0.21743358671665192\n",
      "Validation loss: 0.49657439901715233 RMSE: 0.7046804\n",
      "142 40 0.21913573145866394\n",
      "142 90 0.1033514142036438\n",
      "Validation loss: 0.49199137744449434 RMSE: 0.70142096\n",
      "146 70 0.26114922761917114\n",
      "Validation loss: 0.5495099516141982 RMSE: 0.74128944\n",
      "147 15 0.1523289829492569\n",
      "147 65 0.17839647829532623\n",
      "Validation loss: 0.5171983415172213 RMSE: 0.719165\n",
      "148 10 0.22800156474113464\n",
      "148 60 0.2023009955883026\n",
      "Validation loss: 0.501905776205517 RMSE: 0.7084531\n",
      "149 5 0.1455686241388321\n",
      "149 55 0.28073108196258545\n",
      "Validation loss: 0.5164064078103928 RMSE: 0.7186142\n",
      "150 0 0.24399954080581665\n",
      "150 50 0.2837758958339691\n",
      "150 100 0.2825281322002411\n",
      "Validation loss: 0.5079974219912574 RMSE: 0.71273935\n",
      "151 45 0.12279370427131653\n",
      "151 95 0.17258203029632568\n",
      "Validation loss: 0.544715918813433 RMSE: 0.7380487\n",
      "152 40 0.25448936223983765\n",
      "152 90 0.3535299599170685\n",
      "Validation loss: 0.46870789840107874 RMSE: 0.68462247\n",
      "153 35 0.15551598370075226\n",
      "153 85 0.27029842138290405\n",
      "Validation loss: 0.4881545242809114 RMSE: 0.6986806\n",
      "154 30 0.298644483089447\n",
      "154 80 0.1628378927707672\n",
      "Validation loss: 0.48272681065968104 RMSE: 0.6947854\n",
      "155 25 0.15472133457660675\n",
      "155 75 0.32395848631858826\n",
      "Validation loss: 0.5216392240353993 RMSE: 0.72224593\n",
      "156 20 0.187554270029068\n",
      "156 70 0.28006231784820557\n",
      "Validation loss: 0.5352218679728962 RMSE: 0.7315886\n",
      "157 15 0.21553924679756165\n",
      "157 65 0.15661847591400146\n",
      "Validation loss: 0.5012558517001924 RMSE: 0.7079943\n",
      "158 10 0.1908682882785797\n",
      "158 60 0.2873389720916748\n",
      "Validation loss: 0.5445455755506243 RMSE: 0.7379333\n",
      "159 5 0.2040843367576599\n",
      "159 55 0.13104835152626038\n",
      "Validation loss: 0.5049832170917874 RMSE: 0.7106217\n",
      "160 0 0.15273617208003998\n",
      "160 50 0.26907554268836975\n",
      "160 100 0.19832377135753632\n",
      "Validation loss: 0.5218260188897451 RMSE: 0.72237533\n",
      "161 45 0.09366892278194427\n",
      "161 95 0.21490025520324707\n",
      "Validation loss: 0.5220693491754078 RMSE: 0.72254366\n",
      "162 40 0.23582929372787476\n",
      "162 90 0.21306313574314117\n",
      "Validation loss: 0.49859984886078607 RMSE: 0.706116\n",
      "163 35 0.23067431151866913\n",
      "163 85 0.16250552237033844\n",
      "Validation loss: 0.5152776950881595 RMSE: 0.71782845\n",
      "164 30 0.16212815046310425\n",
      "164 80 0.23936103284358978\n",
      "Validation loss: 0.49678101106768563 RMSE: 0.70482695\n",
      "165 25 0.17901884019374847\n",
      "165 75 0.26426225900650024\n",
      "Validation loss: 0.515609762214479 RMSE: 0.7180597\n",
      "166 20 0.13810765743255615\n",
      "166 70 0.15838594734668732\n",
      "Validation loss: 0.5094116696289608 RMSE: 0.7137308\n",
      "167 15 0.15613391995429993\n",
      "167 65 0.18201449513435364\n",
      "Validation loss: 0.5130183708100092 RMSE: 0.716253\n",
      "168 10 0.2957976162433624\n",
      "168 60 0.12803103029727936\n",
      "Validation loss: 0.5114122453190032 RMSE: 0.7151309\n",
      "169 5 0.17124146223068237\n",
      "169 55 0.22466059029102325\n",
      "Validation loss: 0.5257823173488889 RMSE: 0.7251085\n",
      "170 0 0.09396414458751678\n",
      "170 50 0.26390084624290466\n",
      "170 100 0.19307854771614075\n",
      "Validation loss: 0.521300642830985 RMSE: 0.7220115\n",
      "171 45 0.15154829621315002\n",
      "171 95 0.2291497141122818\n",
      "Validation loss: 0.5275977791065262 RMSE: 0.72635925\n",
      "172 40 0.2516551613807678\n",
      "172 90 0.1777593344449997\n",
      "Validation loss: 0.5316760494595483 RMSE: 0.72916114\n",
      "173 35 0.16702400147914886\n",
      "173 85 0.184832364320755\n",
      "Validation loss: 0.48266618762697494 RMSE: 0.6947418\n",
      "174 30 0.1645963042974472\n",
      "174 80 0.3229917585849762\n",
      "Validation loss: 0.491544394833701 RMSE: 0.7011023\n",
      "175 25 0.15867343544960022\n",
      "175 75 0.5594348907470703\n",
      "Validation loss: 0.5267015930442583 RMSE: 0.7257421\n",
      "176 20 0.14824804663658142\n",
      "176 70 0.13012786209583282\n",
      "Validation loss: 0.5125329217740467 RMSE: 0.715914\n",
      "177 15 0.138594850897789\n",
      "177 65 0.18405871093273163\n",
      "Validation loss: 0.5114321782475426 RMSE: 0.7151449\n",
      "178 10 0.11757027357816696\n",
      "178 60 0.1490773856639862\n",
      "Validation loss: 0.48653023881571633 RMSE: 0.6975172\n",
      "179 5 0.24956224858760834\n",
      "179 55 0.1389126181602478\n",
      "Validation loss: 0.5215114783673059 RMSE: 0.72215754\n",
      "180 0 0.11040171980857849\n",
      "180 50 0.23698951303958893\n",
      "180 100 0.20087026059627533\n",
      "Validation loss: 0.5125383899325416 RMSE: 0.7159178\n",
      "181 45 0.2874821424484253\n",
      "181 95 0.17189697921276093\n",
      "Validation loss: 0.5009343981742859 RMSE: 0.7077672\n",
      "182 40 0.17335614562034607\n",
      "182 90 0.18225495517253876\n",
      "Validation loss: 0.5587886197226388 RMSE: 0.74752164\n",
      "183 35 0.1348610669374466\n",
      "183 85 0.1864537000656128\n",
      "Validation loss: 0.5082618157068889 RMSE: 0.71292484\n",
      "184 30 0.20129555463790894\n",
      "184 80 0.1274557262659073\n",
      "Validation loss: 0.5439078353700184 RMSE: 0.7375011\n",
      "185 25 0.2156199812889099\n",
      "185 75 0.15518434345722198\n",
      "Validation loss: 0.48393938882010323 RMSE: 0.69565755\n",
      "186 20 0.10660029947757721\n",
      "186 70 0.20158648490905762\n",
      "Validation loss: 0.5116849132946559 RMSE: 0.71532154\n",
      "187 15 0.2242281436920166\n",
      "187 65 0.19985415041446686\n",
      "Validation loss: 0.519817700059641 RMSE: 0.7209838\n",
      "188 10 0.13015924394130707\n",
      "188 60 0.12676215171813965\n",
      "Validation loss: 0.5003098485015688 RMSE: 0.70732576\n",
      "189 5 0.11940615624189377\n",
      "189 55 0.12765488028526306\n",
      "Validation loss: 0.4881333218443961 RMSE: 0.6986654\n",
      "190 0 0.18932172656059265\n",
      "190 50 0.22869005799293518\n",
      "190 100 0.2601572871208191\n",
      "Validation loss: 0.5113435092426482 RMSE: 0.7150829\n",
      "191 45 0.21719355881214142\n",
      "191 95 0.3168650269508362\n",
      "Validation loss: 0.5047622808388301 RMSE: 0.71046627\n",
      "192 40 0.13731150329113007\n",
      "192 90 0.17443227767944336\n",
      "Validation loss: 0.4913678085520154 RMSE: 0.7009763\n",
      "193 35 0.19778667390346527\n",
      "193 85 0.10677018761634827\n",
      "Validation loss: 0.4937602993987855 RMSE: 0.7026808\n",
      "194 30 0.11949677765369415\n",
      "194 80 0.06869432330131531\n",
      "Validation loss: 0.5134563052938098 RMSE: 0.71655864\n",
      "195 25 0.1479727178812027\n",
      "195 75 0.1272333264350891\n",
      "Validation loss: 0.48670787470681326 RMSE: 0.6976445\n",
      "196 20 0.21531005203723907\n",
      "196 70 0.39684155583381653\n",
      "Validation loss: 0.5045603215694427 RMSE: 0.71032405\n",
      "197 15 0.20465274155139923\n",
      "197 65 0.15067467093467712\n",
      "Validation loss: 0.5087736742837089 RMSE: 0.7132837\n",
      "198 10 0.18201909959316254\n",
      "198 60 0.0884980782866478\n",
      "Validation loss: 0.4912913015910557 RMSE: 0.7009218\n",
      "199 5 0.1317349374294281\n",
      "199 55 0.16982150077819824\n",
      "Validation loss: 0.5127296101479303 RMSE: 0.7160514\n",
      "Loaded trained model with success.\n",
      "Test loss: 0.5330396606808617 Test RMSE: 0.7300957\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:0', 'task_name': 'lipo', 'model_type': 'gin', 'model': {'num_layer': 5, 'emb_dim': 300, 'feat_dim': 300, 'drop_ratio': 0.3, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 778, 'task': 'regression', 'data_path': 'data/lipophilicity/Lipophilicity.csv', 'target': ['exp']}}\n",
      "Running on: cuda:0\n",
      "4199\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/4199\n",
      "Generating scaffold 1000/4199\n",
      "Generating scaffold 2000/4199\n",
      "Generating scaffold 3000/4199\n",
      "Generating scaffold 4000/4199\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 7.9152679443359375\n",
      "0 50 2.2723236083984375\n",
      "0 100 1.4422682523727417\n",
      "Validation loss: 1.1466998054867699 RMSE: 1.0708407\n",
      "1 45 1.0924973487854004\n",
      "1 95 1.1122629642486572\n",
      "Validation loss: 1.1473133473169237 RMSE: 1.071127\n",
      "2 40 1.0057395696640015\n",
      "2 90 0.9081629514694214\n",
      "Validation loss: 1.2444666680835543 RMSE: 1.1155567\n",
      "3 35 1.2845958471298218\n",
      "3 85 1.242967128753662\n",
      "Validation loss: 1.1535477794352031 RMSE: 1.0740334\n",
      "4 30 0.8724686503410339\n",
      "4 80 1.2812777757644653\n",
      "Validation loss: 0.9418693293418203 RMSE: 0.9704995\n",
      "5 25 0.7453016042709351\n",
      "5 75 0.5322459936141968\n",
      "Validation loss: 0.9503135289464678 RMSE: 0.9748403\n",
      "6 20 0.9731546640396118\n",
      "6 70 0.5838228464126587\n",
      "Validation loss: 0.8964068151655651 RMSE: 0.9467876\n",
      "7 15 0.798366367816925\n",
      "7 65 0.8100699782371521\n",
      "Validation loss: 0.8252570646149772 RMSE: 0.9084366\n",
      "8 10 0.8820827603340149\n",
      "8 60 0.5740338563919067\n",
      "Validation loss: 1.10318314461481 RMSE: 1.0503253\n",
      "9 5 1.2383378744125366\n",
      "9 55 0.6594939231872559\n",
      "Validation loss: 0.7409652221770514 RMSE: 0.86079335\n",
      "10 0 0.9868757724761963\n",
      "10 50 0.6058390140533447\n",
      "10 100 0.44454818964004517\n",
      "Validation loss: 1.3295514941215516 RMSE: 1.1530617\n",
      "11 45 0.6922215819358826\n",
      "11 95 0.4423239231109619\n",
      "Validation loss: 0.7922337253888448 RMSE: 0.8900751\n",
      "12 40 0.7090150713920593\n",
      "12 90 1.0202423334121704\n",
      "Validation loss: 0.8050032263710385 RMSE: 0.8972197\n",
      "13 35 0.7318308353424072\n",
      "13 85 0.8667713403701782\n",
      "Validation loss: 0.9432978349072593 RMSE: 0.9712353\n",
      "14 30 0.6742483377456665\n",
      "14 80 0.33456042408943176\n",
      "Validation loss: 0.7806179046630859 RMSE: 0.8835258\n",
      "15 25 0.6348673701286316\n",
      "15 75 1.2490990161895752\n",
      "Validation loss: 0.859094710577102 RMSE: 0.9268736\n",
      "16 20 0.6556574702262878\n",
      "16 70 0.7549375295639038\n",
      "Validation loss: 0.8562106952780769 RMSE: 0.9253166\n",
      "17 15 0.5042005777359009\n",
      "17 65 0.41930198669433594\n",
      "Validation loss: 0.7080144354275295 RMSE: 0.8414359\n",
      "18 10 0.4692300260066986\n",
      "18 60 0.6771325469017029\n",
      "Validation loss: 0.8571679228828066 RMSE: 0.9258336\n",
      "19 5 0.5752463936805725\n",
      "19 55 0.5868262648582458\n",
      "Validation loss: 0.6922508398691813 RMSE: 0.8320161\n",
      "20 0 0.7021397352218628\n",
      "20 50 0.5225744843482971\n",
      "20 100 0.344404935836792\n",
      "Validation loss: 0.7191211305913471 RMSE: 0.8480101\n",
      "21 45 0.5202020406723022\n",
      "21 95 1.0713189840316772\n",
      "Validation loss: 0.6806691442217145 RMSE: 0.82502675\n",
      "22 40 0.48537224531173706\n",
      "22 90 0.5600282549858093\n",
      "Validation loss: 0.6623479892100607 RMSE: 0.81384766\n",
      "23 35 0.5297305583953857\n",
      "23 85 0.5893254280090332\n",
      "Validation loss: 0.7670954695769718 RMSE: 0.8758398\n",
      "24 30 0.789191722869873\n",
      "24 80 0.5743518471717834\n",
      "Validation loss: 0.692351568312872 RMSE: 0.83207667\n",
      "25 25 0.5806532502174377\n",
      "25 75 0.4381111264228821\n",
      "Validation loss: 0.6537255925791604 RMSE: 0.80853295\n",
      "26 20 0.7177246809005737\n",
      "26 70 0.6044571399688721\n",
      "Validation loss: 0.7017646458886918 RMSE: 0.83771396\n",
      "27 15 0.5361534953117371\n",
      "27 65 0.7262723445892334\n",
      "Validation loss: 0.6489682231630598 RMSE: 0.8055856\n",
      "28 10 0.8015071153640747\n",
      "28 60 0.7356109619140625\n",
      "Validation loss: 0.6163117113567534 RMSE: 0.7850552\n",
      "29 5 0.5677942633628845\n",
      "29 55 0.9450560808181763\n",
      "Validation loss: 0.6481637795766194 RMSE: 0.8050862\n",
      "30 0 0.4260166585445404\n",
      "30 50 0.4622877836227417\n",
      "30 100 0.365054190158844\n",
      "Validation loss: 0.7178559538863954 RMSE: 0.8472638\n",
      "31 45 0.4836118221282959\n",
      "31 95 0.5441559553146362\n",
      "Validation loss: 0.7219345762616112 RMSE: 0.8496673\n",
      "32 40 0.44701769948005676\n",
      "32 90 0.47042006254196167\n",
      "Validation loss: 0.9141751712276822 RMSE: 0.9561251\n",
      "33 35 0.7764243483543396\n",
      "33 85 0.8021664023399353\n",
      "Validation loss: 0.6631704807281494 RMSE: 0.8143528\n",
      "34 30 0.4422728717327118\n",
      "34 80 0.7657345533370972\n",
      "Validation loss: 0.6158155611583165 RMSE: 0.7847392\n",
      "35 25 0.7736698389053345\n",
      "35 75 0.4748726487159729\n",
      "Validation loss: 0.635508766628447 RMSE: 0.79718804\n",
      "36 20 0.528765857219696\n",
      "36 70 0.8744723200798035\n",
      "Validation loss: 0.6137642797969637 RMSE: 0.7834311\n",
      "37 15 0.4715573191642761\n",
      "37 65 0.3401714563369751\n",
      "Validation loss: 0.6452243271328154 RMSE: 0.80325854\n",
      "38 10 0.3755204975605011\n",
      "38 60 0.43419891595840454\n",
      "Validation loss: 0.6561311108725412 RMSE: 0.81001925\n",
      "39 5 0.4837522804737091\n",
      "39 55 0.5130004286766052\n",
      "Validation loss: 0.5653080037661962 RMSE: 0.7518697\n",
      "40 0 0.3932473063468933\n",
      "40 50 0.3523198068141937\n",
      "40 100 0.15998335182666779\n",
      "Validation loss: 0.6473448696590606 RMSE: 0.8045774\n",
      "41 45 0.3625285029411316\n",
      "41 95 0.4916233718395233\n",
      "Validation loss: 0.6415688934780303 RMSE: 0.8009799\n",
      "42 40 0.4614782929420471\n",
      "42 90 0.33945053815841675\n",
      "Validation loss: 0.6957979982807523 RMSE: 0.83414507\n",
      "43 35 0.47071439027786255\n",
      "43 85 0.41134342551231384\n",
      "Validation loss: 0.7061176470347813 RMSE: 0.84030807\n",
      "44 30 0.4961387515068054\n",
      "44 80 0.2750239670276642\n",
      "Validation loss: 0.5666225456056141 RMSE: 0.7527433\n",
      "45 25 0.3107801675796509\n",
      "45 75 0.38630372285842896\n",
      "Validation loss: 0.5884900259120124 RMSE: 0.76713103\n",
      "46 20 0.41897425055503845\n",
      "46 70 0.7356904149055481\n",
      "Validation loss: 0.6015816235826129 RMSE: 0.77561694\n",
      "47 15 0.3999810218811035\n",
      "47 65 0.6151469945907593\n",
      "Validation loss: 0.6572980489049639 RMSE: 0.8107392\n",
      "48 10 0.3639693558216095\n",
      "48 60 0.3692810535430908\n",
      "Validation loss: 0.6018471785954067 RMSE: 0.77578807\n",
      "49 5 0.5096220374107361\n",
      "49 55 0.39270517230033875\n",
      "Validation loss: 0.6237357366652716 RMSE: 0.7897694\n",
      "50 0 0.3319968283176422\n",
      "50 50 0.30931538343429565\n",
      "50 100 0.563418447971344\n",
      "Validation loss: 0.6046149776095435 RMSE: 0.7775699\n",
      "51 45 0.3009600341320038\n",
      "51 95 0.33465734124183655\n",
      "Validation loss: 0.5907035294033233 RMSE: 0.7685724\n",
      "52 40 0.47843843698501587\n",
      "52 90 0.47280314564704895\n",
      "Validation loss: 0.5810223352341425 RMSE: 0.7622482\n",
      "53 35 0.25849097967147827\n",
      "53 85 0.4680141806602478\n",
      "Validation loss: 0.5909045605432419 RMSE: 0.76870316\n",
      "54 30 0.5534841418266296\n",
      "54 80 0.3057002127170563\n",
      "Validation loss: 0.5892850416047233 RMSE: 0.767649\n",
      "55 25 0.3838679790496826\n",
      "55 75 0.4413243532180786\n",
      "Validation loss: 0.7370537343479339 RMSE: 0.85851836\n",
      "56 20 0.23611412942409515\n",
      "56 70 0.3178868591785431\n",
      "Validation loss: 0.6214711427688598 RMSE: 0.7883344\n",
      "57 15 0.4390920102596283\n",
      "57 65 0.843319296836853\n",
      "Validation loss: 0.5731868477094741 RMSE: 0.75709105\n",
      "58 10 0.30058860778808594\n",
      "58 60 0.4290737807750702\n",
      "Validation loss: 0.6087302781286694 RMSE: 0.7802117\n",
      "59 5 0.3156377971172333\n",
      "59 55 0.35089874267578125\n",
      "Validation loss: 0.6134065502456256 RMSE: 0.78320277\n",
      "60 0 0.42926347255706787\n",
      "60 50 0.5284439921379089\n",
      "60 100 0.3503761887550354\n",
      "Validation loss: 0.6106278555733817 RMSE: 0.7814268\n",
      "61 45 0.5520891547203064\n",
      "61 95 0.3839373290538788\n",
      "Validation loss: 0.6000960313138508 RMSE: 0.7746587\n",
      "62 40 0.37370964884757996\n",
      "62 90 0.32381319999694824\n",
      "Validation loss: 0.639551872298831 RMSE: 0.79971987\n",
      "63 35 0.39933738112449646\n",
      "63 85 0.3291068375110626\n",
      "Validation loss: 0.5519094512576148 RMSE: 0.7429061\n",
      "64 30 0.4887550175189972\n",
      "64 80 0.514028787612915\n",
      "Validation loss: 0.6095029889118104 RMSE: 0.78070676\n",
      "65 25 0.312680184841156\n",
      "65 75 0.41379329562187195\n",
      "Validation loss: 0.5862505685715448 RMSE: 0.76567\n",
      "66 20 0.2306618094444275\n",
      "66 70 0.38239043951034546\n",
      "Validation loss: 0.5810230311893282 RMSE: 0.7622487\n",
      "67 15 0.44120869040489197\n",
      "67 65 0.6071683168411255\n",
      "Validation loss: 0.5896906313442049 RMSE: 0.76791316\n",
      "68 10 0.35559749603271484\n",
      "68 60 0.41052258014678955\n",
      "Validation loss: 0.5431028082257225 RMSE: 0.7369551\n",
      "69 5 0.6062812209129333\n",
      "69 55 0.3628561496734619\n",
      "Validation loss: 0.5950250300977912 RMSE: 0.77137864\n",
      "70 0 0.32764315605163574\n",
      "70 50 0.23946087062358856\n",
      "70 100 0.49607953429222107\n",
      "Validation loss: 0.585898369550705 RMSE: 0.76544\n",
      "71 45 0.569340705871582\n",
      "71 95 0.40135741233825684\n",
      "Validation loss: 0.5515410775230044 RMSE: 0.74265814\n",
      "72 40 0.19437529146671295\n",
      "72 90 0.19713570177555084\n",
      "Validation loss: 0.5196531460398719 RMSE: 0.7208698\n",
      "73 35 0.18272386491298676\n",
      "73 85 0.4463290274143219\n",
      "Validation loss: 0.5960293996901739 RMSE: 0.7720294\n",
      "74 30 0.3229764401912689\n",
      "74 80 0.5086845755577087\n",
      "Validation loss: 0.5392052003315517 RMSE: 0.73430586\n",
      "75 25 0.4865950345993042\n",
      "75 75 0.3814528286457062\n",
      "Validation loss: 0.5511038519087292 RMSE: 0.7423637\n",
      "76 20 0.24930599331855774\n",
      "76 70 0.42448800802230835\n",
      "Validation loss: 0.5833972147532872 RMSE: 0.76380444\n",
      "77 15 0.23461619019508362\n",
      "77 65 0.4862632751464844\n",
      "Validation loss: 0.5405456279005323 RMSE: 0.7352181\n",
      "78 10 0.21954841911792755\n",
      "78 60 0.32955285906791687\n",
      "Validation loss: 0.6144039362668992 RMSE: 0.7838392\n",
      "79 5 0.21722467243671417\n",
      "79 55 0.312705397605896\n",
      "Validation loss: 0.5620171076130299 RMSE: 0.749678\n",
      "80 0 0.2775985598564148\n",
      "80 50 0.27501434087753296\n",
      "80 100 0.3099496364593506\n",
      "Validation loss: 0.5875773549079895 RMSE: 0.766536\n",
      "81 45 0.1681308001279831\n",
      "81 95 0.2847042977809906\n",
      "Validation loss: 0.5301166940303076 RMSE: 0.7280912\n",
      "82 40 0.31129956245422363\n",
      "82 90 0.3626466691493988\n",
      "Validation loss: 0.568653107540948 RMSE: 0.75409085\n",
      "83 35 0.1569506824016571\n",
      "83 85 0.29958632588386536\n",
      "Validation loss: 0.5731008200418382 RMSE: 0.75703424\n",
      "84 30 0.3568240702152252\n",
      "84 80 0.16513140499591827\n",
      "Validation loss: 0.596694879304795 RMSE: 0.7724603\n",
      "85 25 0.2777926027774811\n",
      "85 75 0.34755250811576843\n",
      "Validation loss: 0.5525880915778024 RMSE: 0.7433627\n",
      "86 20 0.48364150524139404\n",
      "86 70 0.17211677134037018\n",
      "Validation loss: 0.5674491272086188 RMSE: 0.7532922\n",
      "87 15 0.41417109966278076\n",
      "87 65 0.32312390208244324\n",
      "Validation loss: 0.5631358458882286 RMSE: 0.7504238\n",
      "88 10 0.3135029673576355\n",
      "88 60 0.18333998322486877\n",
      "Validation loss: 0.5770359922023046 RMSE: 0.75962883\n",
      "89 5 0.7312200665473938\n",
      "89 55 0.38244542479515076\n",
      "Validation loss: 0.5702107054846627 RMSE: 0.75512296\n",
      "90 0 0.3490632176399231\n",
      "90 50 0.380607008934021\n",
      "90 100 0.21166236698627472\n",
      "Validation loss: 0.5396219639551072 RMSE: 0.7345897\n",
      "91 45 0.45063573122024536\n",
      "91 95 0.2802110016345978\n",
      "Validation loss: 0.5437861533392043 RMSE: 0.7374186\n",
      "92 40 0.2603112459182739\n",
      "92 90 0.5808863639831543\n",
      "Validation loss: 0.5254067091714768 RMSE: 0.72484946\n",
      "93 35 0.5007956624031067\n",
      "93 85 0.17526064813137054\n",
      "Validation loss: 0.5379975832643963 RMSE: 0.7334832\n",
      "94 30 0.14445647597312927\n",
      "94 80 0.3814806044101715\n",
      "Validation loss: 0.5191533693245479 RMSE: 0.720523\n",
      "95 25 0.1114867553114891\n",
      "95 75 0.3439170718193054\n",
      "Validation loss: 0.5496587560290382 RMSE: 0.74138975\n",
      "96 20 0.42405322194099426\n",
      "96 70 0.487932026386261\n",
      "Validation loss: 0.5216474131459281 RMSE: 0.7222516\n",
      "97 15 0.32341811060905457\n",
      "97 65 0.30843645334243774\n",
      "Validation loss: 0.5534885957127526 RMSE: 0.7439681\n",
      "98 10 0.4622929096221924\n",
      "98 60 0.2016705870628357\n",
      "Validation loss: 0.584586337066832 RMSE: 0.76458246\n",
      "99 5 0.21235263347625732\n",
      "99 55 0.21653202176094055\n",
      "Validation loss: 0.5521076151302883 RMSE: 0.7430394\n",
      "100 0 0.3515307307243347\n",
      "100 50 0.19058538973331451\n",
      "100 100 0.289985328912735\n",
      "Validation loss: 0.5208343714475632 RMSE: 0.72168857\n",
      "101 45 0.2500341832637787\n",
      "101 95 0.24825827777385712\n",
      "Validation loss: 0.5177926483608427 RMSE: 0.7195781\n",
      "102 40 0.26678702235221863\n",
      "102 90 0.24118481576442719\n",
      "Validation loss: 0.5204972925640288 RMSE: 0.721455\n",
      "103 35 0.17572897672653198\n",
      "103 85 0.2156701236963272\n",
      "Validation loss: 0.5198208402310099 RMSE: 0.72098607\n",
      "104 30 0.19318987429141998\n",
      "104 80 0.2768397331237793\n",
      "Validation loss: 0.5424431502819062 RMSE: 0.7365074\n",
      "105 25 0.26798588037490845\n",
      "105 75 0.28737255930900574\n",
      "Validation loss: 0.5082389820189703 RMSE: 0.7129088\n",
      "106 20 0.2447395771741867\n",
      "106 70 0.23147037625312805\n",
      "Validation loss: 0.5160593643074944 RMSE: 0.7183727\n",
      "107 15 0.20734308660030365\n",
      "107 65 0.3381054699420929\n",
      "Validation loss: 0.565140906402043 RMSE: 0.7517586\n",
      "108 10 0.21877746284008026\n",
      "108 60 0.2792567312717438\n",
      "Validation loss: 0.5345926719052451 RMSE: 0.73115844\n",
      "109 5 0.29908257722854614\n",
      "109 55 0.31486591696739197\n",
      "Validation loss: 0.5119655004569462 RMSE: 0.71551764\n",
      "110 0 0.2723274528980255\n",
      "110 50 0.2599831223487854\n",
      "110 100 0.35721850395202637\n",
      "Validation loss: 0.49535329270930517 RMSE: 0.70381343\n",
      "111 45 0.3032352924346924\n",
      "111 95 0.3748016655445099\n",
      "Validation loss: 0.5089448906126477 RMSE: 0.71340376\n",
      "112 40 0.25767579674720764\n",
      "112 90 0.42527538537979126\n",
      "Validation loss: 0.5286881181455794 RMSE: 0.72710943\n",
      "113 35 0.27179187536239624\n",
      "113 85 0.2758506238460541\n",
      "Validation loss: 0.5338803813571021 RMSE: 0.7306712\n",
      "114 30 0.26334092020988464\n",
      "114 80 0.3045003116130829\n",
      "Validation loss: 0.5374063287462507 RMSE: 0.73308\n",
      "115 25 0.39020979404449463\n",
      "115 75 0.21830229461193085\n",
      "Validation loss: 0.521060265033018 RMSE: 0.72184503\n",
      "116 20 0.17427492141723633\n",
      "116 70 0.340962678194046\n",
      "Validation loss: 0.49790110361008416 RMSE: 0.70562106\n",
      "117 15 0.3593176603317261\n",
      "117 65 0.1980118751525879\n",
      "Validation loss: 0.5038694154648554 RMSE: 0.7098376\n",
      "118 10 0.3145849406719208\n",
      "118 60 0.32814961671829224\n",
      "Validation loss: 0.522391905103411 RMSE: 0.7227668\n",
      "119 5 0.15783321857452393\n",
      "119 55 0.2079368233680725\n",
      "Validation loss: 0.5309840633755638 RMSE: 0.7286865\n",
      "120 0 0.2592650353908539\n",
      "120 50 0.08840999752283096\n",
      "120 100 0.23083224892616272\n",
      "Validation loss: 0.5303938184465681 RMSE: 0.72828144\n",
      "121 45 0.216536283493042\n",
      "121 95 0.24952882528305054\n",
      "Validation loss: 0.47968126088380814 RMSE: 0.6925903\n",
      "122 40 0.24922235310077667\n",
      "122 90 0.21946434676647186\n",
      "Validation loss: 0.5304760813713074 RMSE: 0.7283379\n",
      "123 35 0.14036938548088074\n",
      "123 85 0.31505855917930603\n",
      "Validation loss: 0.483955987294515 RMSE: 0.6956695\n",
      "124 30 0.21499338746070862\n",
      "124 80 0.43195676803588867\n",
      "Validation loss: 0.5094495143209185 RMSE: 0.71375734\n",
      "125 25 0.21739330887794495\n",
      "125 75 0.276482492685318\n",
      "Validation loss: 0.49301339728491644 RMSE: 0.7021491\n",
      "126 20 0.10832532495260239\n",
      "126 70 0.3501843810081482\n",
      "Validation loss: 0.5277624470846993 RMSE: 0.7264726\n",
      "127 15 0.18449510633945465\n",
      "127 65 0.17284005880355835\n",
      "Validation loss: 0.588032999492827 RMSE: 0.7668331\n",
      "128 10 0.19115880131721497\n",
      "128 60 0.21910089254379272\n",
      "Validation loss: 0.523719519944418 RMSE: 0.7236847\n",
      "129 5 0.29912886023521423\n",
      "129 55 0.1717958003282547\n",
      "Validation loss: 0.517512043317159 RMSE: 0.71938306\n",
      "130 0 0.21079684793949127\n",
      "130 50 0.14220982789993286\n",
      "130 100 0.20488391816616058\n",
      "Validation loss: 0.5103986706052508 RMSE: 0.71442187\n",
      "131 45 0.41683685779571533\n",
      "131 95 0.13487504422664642\n",
      "Validation loss: 0.5330053499766758 RMSE: 0.73007214\n",
      "132 40 0.17804862558841705\n",
      "132 90 0.23829029500484467\n",
      "Validation loss: 0.5133848702623731 RMSE: 0.7165088\n",
      "133 35 0.18724818527698517\n",
      "133 85 0.24600224196910858\n",
      "Validation loss: 0.4936489843186878 RMSE: 0.7026016\n",
      "134 30 0.23887723684310913\n",
      "134 80 0.08966096490621567\n",
      "Validation loss: 0.5510700915540968 RMSE: 0.742341\n",
      "135 25 0.1273181289434433\n",
      "135 75 0.16522522270679474\n",
      "Validation loss: 0.5010543618883405 RMSE: 0.70785195\n",
      "136 20 0.3031251132488251\n",
      "136 70 0.15393099188804626\n",
      "Validation loss: 0.5219349263679414 RMSE: 0.7224506\n",
      "137 15 0.20057691633701324\n",
      "137 65 0.16033609211444855\n",
      "Validation loss: 0.5222444046111334 RMSE: 0.7226648\n",
      "138 10 0.20744463801383972\n",
      "138 60 0.22155797481536865\n",
      "Validation loss: 0.5138474725541614 RMSE: 0.71683156\n",
      "139 5 0.18242806196212769\n",
      "139 55 0.14069916307926178\n",
      "Validation loss: 0.5273236639442898 RMSE: 0.7261706\n",
      "140 0 0.13164450228214264\n",
      "140 50 0.34738561511039734\n",
      "140 100 0.28897032141685486\n",
      "Validation loss: 0.5539550559861319 RMSE: 0.7442816\n",
      "141 45 0.17851991951465607\n",
      "141 95 0.251409113407135\n",
      "Validation loss: 0.48798331089672586 RMSE: 0.698558\n",
      "142 40 0.21899618208408356\n",
      "142 90 0.2597542405128479\n",
      "Validation loss: 0.521415342603411 RMSE: 0.72209096\n",
      "143 35 0.19067265093326569\n",
      "143 85 0.19449326395988464\n",
      "Validation loss: 0.5101492515632085 RMSE: 0.71424735\n",
      "144 30 0.23577891290187836\n",
      "144 80 0.18357615172863007\n",
      "Validation loss: 0.5141486190614246 RMSE: 0.71704155\n",
      "145 25 0.2356811761856079\n",
      "145 75 0.2807571589946747\n",
      "Validation loss: 0.513068458863667 RMSE: 0.716288\n",
      "146 20 0.4460811913013458\n",
      "146 70 0.274923175573349\n",
      "Validation loss: 0.5222688286077408 RMSE: 0.7226817\n",
      "147 15 0.20562106370925903\n",
      "147 65 0.16869674623012543\n",
      "Validation loss: 0.48644300486360276 RMSE: 0.6974547\n",
      "148 10 0.15767718851566315\n",
      "148 60 0.1199800968170166\n",
      "Validation loss: 0.5229743653819674 RMSE: 0.7231697\n",
      "149 5 0.2797395586967468\n",
      "149 55 0.19181600213050842\n",
      "Validation loss: 0.4996821026007334 RMSE: 0.70688194\n",
      "150 0 0.18989859521389008\n",
      "150 50 0.2159576565027237\n",
      "150 100 0.131180077791214\n",
      "Validation loss: 0.5283613857768831 RMSE: 0.7268847\n",
      "151 45 0.1613779067993164\n",
      "151 95 0.18750880658626556\n",
      "Validation loss: 0.5203691681226095 RMSE: 0.72136617\n",
      "152 40 0.29900380969047546\n",
      "152 90 0.2301619052886963\n",
      "Validation loss: 0.5165411790211996 RMSE: 0.718708\n",
      "153 35 0.27098801732063293\n",
      "153 85 0.3160651922225952\n",
      "Validation loss: 0.505719708828699 RMSE: 0.7111397\n",
      "154 30 0.2509320080280304\n",
      "154 80 0.3306708037853241\n",
      "Validation loss: 0.5475691585313707 RMSE: 0.73997915\n",
      "155 25 0.24579687416553497\n",
      "155 75 0.21151643991470337\n",
      "Validation loss: 0.5270838868050348 RMSE: 0.72600543\n",
      "156 20 0.24179929494857788\n",
      "156 70 0.2257741391658783\n",
      "Validation loss: 0.552742536295028 RMSE: 0.74346656\n",
      "157 15 0.2658522129058838\n"
     ]
    }
   ],
   "source": [
    "datasets = ['FreeSolv', 'ESOL', 'Lipo', 'qm7', \"bace\",  \"bbbp\",  'tox21', 'sider',]\n",
    "seeds = [777, 778, 779, 780, 781]\n",
    "\n",
    "for dataset in datasets:\n",
    "    for seed in seeds:\n",
    "            if dataset == 'FreeSolv':\n",
    "            # FreeSolv     \n",
    "                !python finetuneRecon.py \\\n",
    "                --task_name {dataset} \\\n",
    "                --seed {seed} \\\n",
    "                --dropout 0.5 \\\n",
    "                --num_layer 3 \\\n",
    "                --emb_dim 64 \\\n",
    "                --feat_dim 64 \\\n",
    "                --alpha 0.1 \\\n",
    "                --mask_edge 1 \\\n",
    "                --gpu cuda:0 \\\n",
    "                \n",
    "            else:\n",
    "                !python finetuneRecon.py \\\n",
    "                --task_name {dataset} \\\n",
    "                --seed {seed} \\\n",
    "                --alpha 0.1 \\\n",
    "                --mask_edge 1 \\\n",
    "                --gpu cuda:0                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2a46ae-a6b6-4a66-befc-c4e38c83fdb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:0', 'task_name': 'freesolv', 'model_type': 'gin', 'model': {'num_layer': 5, 'emb_dim': 300, 'feat_dim': 300, 'drop_ratio': 0.3, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'random', 'seed': 777, 'task': 'regression', 'data_path': 'data/freesolv/freesolv.csv', 'target': ['expt']}}\n",
      "Running on: cuda:0\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 14.321107864379883\n",
      "Validation loss: 20.91863441467285 RMSE: 4.5736895\n",
      "Validation loss: 17.256500959396362 RMSE: 4.154094\n",
      "2 16 1.6740686893463135\n",
      "Validation loss: 9.5014009475708 RMSE: 3.0824342\n",
      "Validation loss: 18.261054039001465 RMSE: 4.2732954\n",
      "Validation loss: 4.354099988937378 RMSE: 2.0866482\n",
      "5 15 2.0978336334228516\n",
      "Validation loss: 2.7761409282684326 RMSE: 1.6661755\n",
      "Validation loss: 6.134705543518066 RMSE: 2.4768338\n",
      "Validation loss: 6.321134567260742 RMSE: 2.5141869\n",
      "8 14 3.4172286987304688\n",
      "Validation loss: 5.5209267139434814 RMSE: 2.3496654\n",
      "Validation loss: 13.93534231185913 RMSE: 3.7330072\n",
      "Validation loss: 5.228390216827393 RMSE: 2.2865674\n",
      "11 13 3.121549129486084\n",
      "Validation loss: 7.373233079910278 RMSE: 2.7153695\n",
      "Validation loss: 4.980787515640259 RMSE: 2.231768\n",
      "Validation loss: 4.149728178977966 RMSE: 2.0370882\n",
      "14 12 3.0783309936523438\n",
      "Validation loss: 5.393917560577393 RMSE: 2.322481\n",
      "Validation loss: 9.424787402153015 RMSE: 3.069982\n",
      "Validation loss: 3.304729461669922 RMSE: 1.8178914\n",
      "17 11 2.1237094402313232\n",
      "Validation loss: 13.586473822593689 RMSE: 3.6859837\n",
      "Validation loss: 2.705667734146118 RMSE: 1.6448911\n",
      "Validation loss: 4.860611200332642 RMSE: 2.204679\n",
      "20 10 2.180887460708618\n",
      "Validation loss: 9.186617851257324 RMSE: 3.0309432\n",
      "Validation loss: 6.59820419549942 RMSE: 2.5686972\n",
      "Validation loss: 5.45841646194458 RMSE: 2.3363254\n",
      "23 9 2.2084107398986816\n",
      "Validation loss: 10.025069236755371 RMSE: 3.1662388\n",
      "Validation loss: 2.3286338448524475 RMSE: 1.5259862\n",
      "Validation loss: 8.440638542175293 RMSE: 2.9052775\n",
      "26 8 1.3372224569320679\n",
      "Validation loss: 6.463868141174316 RMSE: 2.5424137\n",
      "Validation loss: 12.901103496551514 RMSE: 3.5918105\n",
      "Validation loss: 12.712461471557617 RMSE: 3.565454\n",
      "29 7 3.387467861175537\n",
      "Validation loss: 1.6997678875923157 RMSE: 1.3037512\n",
      "Validation loss: 8.209753513336182 RMSE: 2.865267\n",
      "Validation loss: 3.7261006832122803 RMSE: 1.9303108\n",
      "32 6 1.3978638648986816\n",
      "Validation loss: 3.634792923927307 RMSE: 1.9065133\n",
      "Validation loss: 23.70071315765381 RMSE: 4.8683376\n",
      "Validation loss: 2.1626139879226685 RMSE: 1.4705827\n",
      "35 5 2.1506173610687256\n",
      "Validation loss: 7.431595802307129 RMSE: 2.7260954\n",
      "Validation loss: 6.668334126472473 RMSE: 2.5823119\n",
      "Validation loss: 2.303868055343628 RMSE: 1.5178498\n",
      "38 4 5.720208168029785\n",
      "Validation loss: 10.526123046875 RMSE: 3.2443988\n",
      "Validation loss: 4.498493194580078 RMSE: 2.1209652\n",
      "Validation loss: 4.741388440132141 RMSE: 2.1774728\n",
      "41 3 1.1128249168395996\n",
      "Validation loss: 15.646967649459839 RMSE: 3.9556246\n",
      "Validation loss: 1.95924711227417 RMSE: 1.3997312\n",
      "Validation loss: 3.023181438446045 RMSE: 1.7387301\n",
      "44 2 1.488884449005127\n",
      "Validation loss: 8.31072986125946 RMSE: 2.8828337\n",
      "Validation loss: 3.8074792623519897 RMSE: 1.9512762\n",
      "Validation loss: 15.148390293121338 RMSE: 3.8920937\n",
      "47 1 1.3609793186187744\n",
      "Validation loss: 11.078186750411987 RMSE: 3.3283908\n",
      "Validation loss: 2.7154356241226196 RMSE: 1.6478577\n",
      "Validation loss: 9.06560730934143 RMSE: 3.0109143\n",
      "50 0 1.7610065937042236\n",
      "Validation loss: 5.494647741317749 RMSE: 2.3440666\n",
      "Validation loss: 4.9150495529174805 RMSE: 2.216991\n",
      "52 16 4.530332565307617\n",
      "Validation loss: 2.4536731243133545 RMSE: 1.5664204\n",
      "Validation loss: 7.778294801712036 RMSE: 2.7889593\n",
      "Validation loss: 11.89563512802124 RMSE: 3.4490044\n",
      "55 15 1.5943224430084229\n",
      "Validation loss: 8.33542251586914 RMSE: 2.887113\n",
      "Validation loss: 4.603169918060303 RMSE: 2.1454997\n",
      "Validation loss: 8.7169189453125 RMSE: 2.9524424\n",
      "58 14 6.113883972167969\n",
      "Validation loss: 3.524573802947998 RMSE: 1.8773848\n",
      "Validation loss: 3.1536667346954346 RMSE: 1.7758563\n",
      "Validation loss: 2.669476866722107 RMSE: 1.6338534\n",
      "61 13 3.593710422515869\n",
      "Validation loss: 2.729556918144226 RMSE: 1.6521369\n",
      "Validation loss: 1.4966734647750854 RMSE: 1.223386\n",
      "Validation loss: 10.484611511230469 RMSE: 3.2379944\n",
      "64 12 1.4421100616455078\n",
      "Validation loss: 15.941698551177979 RMSE: 3.9927056\n",
      "Validation loss: 2.03757643699646 RMSE: 1.4274371\n",
      "Validation loss: 4.891714930534363 RMSE: 2.2117226\n",
      "67 11 0.8361790180206299\n",
      "Validation loss: 1.9478195905685425 RMSE: 1.3956432\n",
      "Validation loss: 4.604289650917053 RMSE: 2.1457608\n",
      "Validation loss: 6.783350348472595 RMSE: 2.6044862\n",
      "70 10 2.0118415355682373\n",
      "Validation loss: 5.839918375015259 RMSE: 2.4165924\n",
      "Validation loss: 8.688260555267334 RMSE: 2.9475856\n",
      "Validation loss: 2.0918793082237244 RMSE: 1.446333\n",
      "73 9 0.996292233467102\n",
      "Validation loss: 2.770226240158081 RMSE: 1.6643996\n",
      "Validation loss: 3.364711880683899 RMSE: 1.8343152\n",
      "Validation loss: 2.3308213353157043 RMSE: 1.5267028\n",
      "76 8 1.6383395195007324\n",
      "Validation loss: 2.394361436367035 RMSE: 1.5473725\n",
      "Validation loss: 1.2451589703559875 RMSE: 1.1158668\n",
      "Validation loss: 9.45081090927124 RMSE: 3.0742173\n",
      "79 7 2.138673782348633\n",
      "Validation loss: 6.314155101776123 RMSE: 2.5127983\n",
      "Validation loss: 11.514304161071777 RMSE: 3.3932729\n",
      "Validation loss: 1.5443845093250275 RMSE: 1.2427326\n",
      "82 6 1.8259228467941284\n",
      "Validation loss: 2.806889295578003 RMSE: 1.6753772\n",
      "Validation loss: 2.180079460144043 RMSE: 1.4765092\n",
      "Validation loss: 13.70202922821045 RMSE: 3.7016258\n",
      "85 5 1.7427281141281128\n",
      "Validation loss: 2.6307966709136963 RMSE: 1.6219732\n",
      "Validation loss: 5.62000185251236 RMSE: 2.370654\n",
      "Validation loss: 2.6636844873428345 RMSE: 1.6320797\n",
      "88 4 0.8470762968063354\n",
      "Validation loss: 1.5560477375984192 RMSE: 1.2474165\n",
      "Validation loss: 6.000298738479614 RMSE: 2.4495509\n",
      "Validation loss: 3.4865431785583496 RMSE: 1.867229\n",
      "91 3 0.5117723345756531\n",
      "Validation loss: 1.3801007866859436 RMSE: 1.1747768\n",
      "Validation loss: 3.1592814922332764 RMSE: 1.7774367\n",
      "Validation loss: 2.77260684967041 RMSE: 1.6651146\n",
      "94 2 2.832580327987671\n",
      "Validation loss: 2.2825942039489746 RMSE: 1.5108255\n",
      "Validation loss: 1.2492355108261108 RMSE: 1.117692\n",
      "Validation loss: 4.164891719818115 RMSE: 2.0408063\n",
      "97 1 2.0177931785583496\n",
      "Validation loss: 3.6904574632644653 RMSE: 1.9210564\n",
      "Validation loss: 5.913467675447464 RMSE: 2.4317622\n",
      "Validation loss: 1.072115421295166 RMSE: 1.0354302\n",
      "100 0 1.43095862865448\n",
      "Validation loss: 2.359165281057358 RMSE: 1.5359572\n",
      "Validation loss: 3.742803692817688 RMSE: 1.9346328\n",
      "102 16 0.6668446660041809\n",
      "Validation loss: 2.897221565246582 RMSE: 1.7021226\n",
      "Validation loss: 1.9340905249118805 RMSE: 1.3907158\n",
      "Validation loss: 3.806449294090271 RMSE: 1.9510124\n",
      "105 15 2.747955322265625\n",
      "Validation loss: 2.2267810106277466 RMSE: 1.4922402\n",
      "Validation loss: 2.8945993185043335 RMSE: 1.7013521\n",
      "Validation loss: 2.2886324524879456 RMSE: 1.5128231\n",
      "108 14 1.856547236442566\n",
      "Validation loss: 5.9955174922943115 RMSE: 2.4485748\n",
      "Validation loss: 1.249184787273407 RMSE: 1.1176693\n",
      "Validation loss: 3.0336804389953613 RMSE: 1.7417461\n",
      "111 13 0.8801333904266357\n",
      "Validation loss: 1.4764221906661987 RMSE: 1.2150811\n",
      "Validation loss: 6.9158570766448975 RMSE: 2.6298018\n",
      "Validation loss: 1.338023066520691 RMSE: 1.1567296\n",
      "114 12 1.0879697799682617\n",
      "Validation loss: 2.7781455516815186 RMSE: 1.6667773\n",
      "Validation loss: 4.137959718704224 RMSE: 2.0341976\n",
      "Validation loss: 2.805605858564377 RMSE: 1.6749942\n",
      "117 11 1.7006199359893799\n",
      "Validation loss: 2.56577730178833 RMSE: 1.6018044\n",
      "Validation loss: 1.2324445247650146 RMSE: 1.1101552\n",
      "Validation loss: 1.9795788526535034 RMSE: 1.4069748\n",
      "120 10 1.2716553211212158\n",
      "Validation loss: 1.340436190366745 RMSE: 1.157772\n",
      "Validation loss: 1.4255233108997345 RMSE: 1.1939527\n",
      "Validation loss: 6.794361591339111 RMSE: 2.6065993\n",
      "123 9 1.2550132274627686\n",
      "Validation loss: 1.4356001019477844 RMSE: 1.1981653\n",
      "Validation loss: 5.750680923461914 RMSE: 2.3980577\n",
      "Validation loss: 1.3114582300186157 RMSE: 1.145189\n",
      "126 8 1.7500109672546387\n",
      "Validation loss: 4.8761069774627686 RMSE: 2.208191\n",
      "Validation loss: 1.45756334066391 RMSE: 1.2072957\n",
      "Validation loss: 1.1161895394325256 RMSE: 1.0564988\n",
      "129 7 0.6116553544998169\n",
      "Validation loss: 1.739623486995697 RMSE: 1.3189479\n",
      "Validation loss: 3.356816291809082 RMSE: 1.8321614\n",
      "Validation loss: 3.7187633514404297 RMSE: 1.9284096\n",
      "132 6 1.253115177154541\n",
      "Validation loss: 1.6520788371562958 RMSE: 1.2853323\n",
      "Validation loss: 1.0393221378326416 RMSE: 1.0194714\n",
      "Validation loss: 2.311925768852234 RMSE: 1.5205017\n",
      "135 5 0.5788388848304749\n",
      "Validation loss: 2.79392671585083 RMSE: 1.6715044\n",
      "Validation loss: 2.091659724712372 RMSE: 1.4462571\n",
      "Validation loss: 1.4883313179016113 RMSE: 1.2199719\n",
      "138 4 1.0223125219345093\n",
      "Validation loss: 2.0054184198379517 RMSE: 1.4161279\n",
      "Validation loss: 1.3556776642799377 RMSE: 1.1643357\n",
      "Validation loss: 1.5639651715755463 RMSE: 1.2505859\n",
      "141 3 1.521355152130127\n",
      "Validation loss: 0.9317711591720581 RMSE: 0.9652828\n",
      "Validation loss: 1.3833388686180115 RMSE: 1.1761543\n",
      "Validation loss: 1.6674079895019531 RMSE: 1.2912816\n",
      "144 2 1.359778642654419\n",
      "Validation loss: 6.275326251983643 RMSE: 2.5050602\n",
      "Validation loss: 2.056834638118744 RMSE: 1.4341669\n",
      "Validation loss: 1.3124879002571106 RMSE: 1.1456387\n",
      "147 1 1.4902451038360596\n",
      "Validation loss: 1.7479283809661865 RMSE: 1.3220924\n",
      "Validation loss: 3.0501595735549927 RMSE: 1.7464706\n",
      "Validation loss: 2.12426096200943 RMSE: 1.4574845\n",
      "150 0 0.7807320356369019\n",
      "Validation loss: 1.9108963012695312 RMSE: 1.3823519\n",
      "Validation loss: 1.4102342128753662 RMSE: 1.1875328\n",
      "152 16 0.3569975197315216\n",
      "Validation loss: 1.5386036038398743 RMSE: 1.2404047\n",
      "Validation loss: 0.9323343336582184 RMSE: 0.9655746\n",
      "Validation loss: 1.196922391653061 RMSE: 1.0940394\n",
      "155 15 1.0107722282409668\n",
      "Validation loss: 3.7611218690872192 RMSE: 1.9393616\n",
      "Validation loss: 3.757067382335663 RMSE: 1.9383152\n",
      "Validation loss: 5.740548729896545 RMSE: 2.395944\n",
      "158 14 2.9125609397888184\n",
      "Validation loss: 1.1788890063762665 RMSE: 1.0857666\n",
      "Validation loss: 1.6328120231628418 RMSE: 1.2778153\n",
      "Validation loss: 1.2574752569198608 RMSE: 1.1213721\n",
      "161 13 1.1761541366577148\n",
      "Validation loss: 4.028734087944031 RMSE: 2.0071704\n",
      "Validation loss: 0.9544443190097809 RMSE: 0.97695667\n",
      "Validation loss: 1.6894562244415283 RMSE: 1.2997906\n",
      "164 12 0.6288465261459351\n",
      "Validation loss: 1.0672178864479065 RMSE: 1.0330625\n",
      "Validation loss: 0.966910719871521 RMSE: 0.98331624\n",
      "Validation loss: 1.1631883382797241 RMSE: 1.0785121\n",
      "167 11 0.9410708546638489\n",
      "Validation loss: 1.181836485862732 RMSE: 1.087123\n",
      "Validation loss: 1.3985340595245361 RMSE: 1.1825962\n",
      "Validation loss: 3.3499534130096436 RMSE: 1.8302876\n",
      "170 10 1.0085904598236084\n",
      "Validation loss: 2.5982906818389893 RMSE: 1.6119213\n",
      "Validation loss: 1.89385986328125 RMSE: 1.3761758\n",
      "Validation loss: 1.6625536680221558 RMSE: 1.2894006\n",
      "173 9 0.9965384006500244\n",
      "Validation loss: 4.122945308685303 RMSE: 2.0305035\n",
      "Validation loss: 1.4736483097076416 RMSE: 1.2139392\n",
      "Validation loss: 0.7938440442085266 RMSE: 0.8909793\n",
      "176 8 1.9352898597717285\n",
      "Validation loss: 1.3663355112075806 RMSE: 1.1689036\n",
      "Validation loss: 1.0736965537071228 RMSE: 1.0361933\n",
      "Validation loss: 5.307312250137329 RMSE: 2.3037603\n",
      "179 7 1.3857712745666504\n",
      "Validation loss: 1.7182368636131287 RMSE: 1.3108155\n",
      "Validation loss: 1.4312275648117065 RMSE: 1.1963391\n",
      "Validation loss: 0.9997207522392273 RMSE: 0.9998604\n",
      "182 6 0.9685672521591187\n",
      "Validation loss: 1.9417122602462769 RMSE: 1.3934534\n",
      "Validation loss: 1.6379902362823486 RMSE: 1.27984\n",
      "Validation loss: 1.0898262858390808 RMSE: 1.0439473\n",
      "185 5 1.3943133354187012\n",
      "Validation loss: 1.1425244808197021 RMSE: 1.0688894\n",
      "Validation loss: 1.9647348523139954 RMSE: 1.4016898\n",
      "Validation loss: 1.6245195865631104 RMSE: 1.2745663\n",
      "188 4 0.5735785961151123\n",
      "Validation loss: 2.1779500246047974 RMSE: 1.4757879\n",
      "Validation loss: 1.380947470664978 RMSE: 1.1751372\n",
      "Validation loss: 1.5370616912841797 RMSE: 1.239783\n",
      "191 3 0.5352512001991272\n",
      "Validation loss: 2.5072742700576782 RMSE: 1.5834376\n",
      "Validation loss: 1.66990065574646 RMSE: 1.2922465\n",
      "Validation loss: 3.1005266904830933 RMSE: 1.7608316\n",
      "194 2 1.3924925327301025\n",
      "Validation loss: 4.185925126075745 RMSE: 2.0459528\n",
      "Validation loss: 0.7411659359931946 RMSE: 0.86090994\n",
      "Validation loss: 4.66570520401001 RMSE: 2.1600242\n",
      "197 1 1.9714056253433228\n",
      "Validation loss: 2.1266363859176636 RMSE: 1.4582992\n",
      "Validation loss: 0.8483651876449585 RMSE: 0.9210674\n",
      "Validation loss: 3.0864381194114685 RMSE: 1.7568266\n",
      "Loaded trained model with success.\n",
      "Test loss: 1.9685813188552856 Test RMSE: 1.4030615\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:0', 'task_name': 'freesolv', 'model_type': 'gin', 'model': {'num_layer': 5, 'emb_dim': 300, 'feat_dim': 300, 'drop_ratio': 0.3, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'random', 'seed': 778, 'task': 'regression', 'data_path': 'data/freesolv/freesolv.csv', 'target': ['expt']}}\n",
      "Running on: cuda:0\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 16.177167892456055\n",
      "Validation loss: 24.261737823486328 RMSE: 4.9256206\n",
      "Validation loss: 19.728859424591064 RMSE: 4.441718\n",
      "2 16 17.045284271240234\n",
      "Validation loss: 19.012720108032227 RMSE: 4.360358\n",
      "Validation loss: 6.400212526321411 RMSE: 2.5298643\n",
      "Validation loss: 17.22266435623169 RMSE: 4.1500196\n",
      "5 15 3.8309154510498047\n",
      "Validation loss: 5.235531568527222 RMSE: 2.2881284\n",
      "Validation loss: 27.575143814086914 RMSE: 5.251204\n",
      "Validation loss: 5.97870135307312 RMSE: 2.4451385\n",
      "8 14 3.077336311340332\n",
      "Validation loss: 6.119457244873047 RMSE: 2.4737537\n",
      "Validation loss: 5.8354332447052 RMSE: 2.4156642\n",
      "Validation loss: 7.723174333572388 RMSE: 2.7790601\n",
      "11 13 3.958364963531494\n",
      "Validation loss: 6.013454914093018 RMSE: 2.4522345\n",
      "Validation loss: 5.204346179962158 RMSE: 2.2813036\n",
      "Validation loss: 6.283418893814087 RMSE: 2.506675\n",
      "14 12 2.3135015964508057\n",
      "Validation loss: 4.100240707397461 RMSE: 2.0249052\n",
      "Validation loss: 9.616214275360107 RMSE: 3.1010022\n",
      "Validation loss: 7.279036045074463 RMSE: 2.6979687\n",
      "17 11 3.8547940254211426\n",
      "Validation loss: 9.974107682704926 RMSE: 3.1581812\n",
      "Validation loss: 3.30516254901886 RMSE: 1.8180106\n",
      "Validation loss: 2.5077884197235107 RMSE: 1.5835998\n",
      "20 10 2.145199775695801\n",
      "Validation loss: 3.2945258617401123 RMSE: 1.815083\n",
      "Validation loss: 7.0554587841033936 RMSE: 2.6562114\n",
      "Validation loss: 3.3896178603172302 RMSE: 1.8410914\n",
      "23 9 1.5921759605407715\n",
      "Validation loss: 5.975407600402832 RMSE: 2.4444647\n",
      "Validation loss: 3.8915568590164185 RMSE: 1.9727031\n",
      "Validation loss: 3.271517276763916 RMSE: 1.8087337\n",
      "26 8 1.3833270072937012\n",
      "Validation loss: 3.5469539165496826 RMSE: 1.8833357\n",
      "Validation loss: 3.483122944831848 RMSE: 1.8663127\n",
      "Validation loss: 6.065205097198486 RMSE: 2.4627638\n",
      "29 7 1.1739394664764404\n",
      "Validation loss: 6.60438871383667 RMSE: 2.5699005\n",
      "Validation loss: 3.480749785900116 RMSE: 1.8656768\n",
      "Validation loss: 2.2743804454803467 RMSE: 1.508105\n",
      "32 6 1.3863418102264404\n",
      "Validation loss: 8.4001704454422 RMSE: 2.8983047\n",
      "Validation loss: 3.368551254272461 RMSE: 1.8353615\n",
      "Validation loss: 7.511385917663574 RMSE: 2.7406907\n",
      "35 5 2.5965898036956787\n",
      "Validation loss: 3.83659827709198 RMSE: 1.9587238\n",
      "Validation loss: 3.257595717906952 RMSE: 1.804881\n",
      "Validation loss: 2.791750907897949 RMSE: 1.6708534\n",
      "38 4 1.881471037864685\n",
      "Validation loss: 9.365322828292847 RMSE: 3.0602813\n",
      "Validation loss: 11.13070011138916 RMSE: 3.3362706\n",
      "Validation loss: 6.849674940109253 RMSE: 2.6171882\n",
      "41 3 3.5577640533447266\n",
      "Validation loss: 4.5565197467803955 RMSE: 2.1346006\n",
      "Validation loss: 5.489298582077026 RMSE: 2.3429246\n",
      "Validation loss: 2.858259618282318 RMSE: 1.6906388\n",
      "44 2 2.3805320262908936\n",
      "Validation loss: 4.33515477180481 RMSE: 2.0821033\n",
      "Validation loss: 3.2206575870513916 RMSE: 1.7946192\n",
      "Validation loss: 5.4128406047821045 RMSE: 2.326551\n",
      "47 1 1.824760913848877\n",
      "Validation loss: 3.548068881034851 RMSE: 1.883632\n",
      "Validation loss: 4.326249122619629 RMSE: 2.0799637\n",
      "Validation loss: 3.7500126361846924 RMSE: 1.9364951\n",
      "50 0 2.920224189758301\n",
      "Validation loss: 2.8610774278640747 RMSE: 1.6914718\n",
      "Validation loss: 4.7691570520401 RMSE: 2.18384\n",
      "52 16 8.06419563293457\n",
      "Validation loss: 5.873327732086182 RMSE: 2.423495\n",
      "Validation loss: 4.766313314437866 RMSE: 2.1831892\n",
      "Validation loss: 5.336477994918823 RMSE: 2.310082\n",
      "55 15 0.8392414450645447\n",
      "Validation loss: 4.3221787214279175 RMSE: 2.0789852\n",
      "Validation loss: 4.687639951705933 RMSE: 2.1650958\n",
      "Validation loss: 5.042495012283325 RMSE: 2.2455502\n",
      "58 14 0.9096124768257141\n",
      "Validation loss: 3.4262540340423584 RMSE: 1.8510143\n",
      "Validation loss: 3.1890887022018433 RMSE: 1.7858019\n",
      "Validation loss: 8.866388320922852 RMSE: 2.9776487\n",
      "61 13 1.245629072189331\n",
      "Validation loss: 4.22921359539032 RMSE: 2.0565054\n",
      "Validation loss: 5.495846509933472 RMSE: 2.344322\n",
      "Validation loss: 3.5939204692840576 RMSE: 1.8957638\n",
      "64 12 0.957255482673645\n",
      "Validation loss: 3.9504001140594482 RMSE: 1.9875616\n",
      "Validation loss: 3.2795557975769043 RMSE: 1.8109546\n",
      "Validation loss: 6.611280679702759 RMSE: 2.5712411\n",
      "67 11 0.6362749338150024\n",
      "Validation loss: 3.502771496772766 RMSE: 1.8715692\n",
      "Validation loss: 4.1181793212890625 RMSE: 2.0293298\n",
      "Validation loss: 7.210947036743164 RMSE: 2.6853206\n",
      "70 10 2.4402623176574707\n",
      "Validation loss: 2.392658770084381 RMSE: 1.5468222\n",
      "Validation loss: 4.481552720069885 RMSE: 2.1169677\n",
      "Validation loss: 4.823304891586304 RMSE: 2.1962025\n",
      "73 9 0.842302680015564\n",
      "Validation loss: 7.994356632232666 RMSE: 2.8274293\n",
      "Validation loss: 2.0276458263397217 RMSE: 1.4239542\n",
      "Validation loss: 2.9081162214279175 RMSE: 1.70532\n",
      "76 8 1.0372307300567627\n",
      "Validation loss: 2.5272045135498047 RMSE: 1.5897183\n",
      "Validation loss: 3.901069402694702 RMSE: 1.9751126\n",
      "Validation loss: 1.6934426426887512 RMSE: 1.3013235\n",
      "79 7 5.582481861114502\n",
      "Validation loss: 4.278210818767548 RMSE: 2.0683837\n",
      "Validation loss: 2.150050461292267 RMSE: 1.4663051\n",
      "Validation loss: 4.639265656471252 RMSE: 2.1538954\n",
      "82 6 1.0610411167144775\n",
      "Validation loss: 3.7479405403137207 RMSE: 1.9359597\n",
      "Validation loss: 2.750980257987976 RMSE: 1.658608\n",
      "Validation loss: 7.1140453815460205 RMSE: 2.6672168\n",
      "85 5 2.820385456085205\n",
      "Validation loss: 2.742839813232422 RMSE: 1.6561521\n",
      "Validation loss: 6.140491247177124 RMSE: 2.4780014\n",
      "Validation loss: 4.836590051651001 RMSE: 2.199225\n",
      "88 4 1.0982080698013306\n",
      "Validation loss: 3.1317538022994995 RMSE: 1.7696763\n",
      "Validation loss: 6.137798070907593 RMSE: 2.477458\n",
      "Validation loss: 2.7881836891174316 RMSE: 1.6697855\n",
      "91 3 0.6676109433174133\n",
      "Validation loss: 3.0917364358901978 RMSE: 1.7583334\n",
      "Validation loss: 3.5074254274368286 RMSE: 1.8728125\n",
      "Validation loss: 3.1494717597961426 RMSE: 1.7746753\n",
      "94 2 1.818185567855835\n",
      "Validation loss: 3.318026840686798 RMSE: 1.8215451\n",
      "Validation loss: 3.2647088766098022 RMSE: 1.8068506\n",
      "Validation loss: 3.2623045444488525 RMSE: 1.806185\n",
      "97 1 3.451174736022949\n",
      "Validation loss: 4.204235553741455 RMSE: 2.0504236\n",
      "Validation loss: 2.0708935260772705 RMSE: 1.4390599\n",
      "Validation loss: 2.3481072187423706 RMSE: 1.5323536\n",
      "100 0 0.7608904838562012\n",
      "Validation loss: 1.9471333026885986 RMSE: 1.3953973\n",
      "Validation loss: 1.894763708114624 RMSE: 1.3765043\n",
      "102 16 0.40318411588668823\n",
      "Validation loss: 3.6346575021743774 RMSE: 1.906478\n",
      "Validation loss: 2.1790401935577393 RMSE: 1.4761574\n",
      "Validation loss: 2.420938491821289 RMSE: 1.5559366\n",
      "105 15 6.433834075927734\n",
      "Validation loss: 1.8929924964904785 RMSE: 1.3758606\n",
      "Validation loss: 2.8596739768981934 RMSE: 1.6910571\n",
      "Validation loss: 8.327637195587158 RMSE: 2.8857648\n",
      "108 14 0.6014808416366577\n",
      "Validation loss: 14.256386280059814 RMSE: 3.775763\n",
      "Validation loss: 15.315126657485962 RMSE: 3.9134548\n",
      "Validation loss: 2.776508629322052 RMSE: 1.666286\n",
      "111 13 1.560638666152954\n",
      "Validation loss: 4.6485559940338135 RMSE: 2.156051\n",
      "Validation loss: 2.097421407699585 RMSE: 1.4482478\n",
      "Validation loss: 2.6311044096946716 RMSE: 1.6220679\n",
      "114 12 1.1249645948410034\n",
      "Validation loss: 2.067474067211151 RMSE: 1.4378713\n",
      "Validation loss: 2.8440637588500977 RMSE: 1.6864352\n",
      "Validation loss: 10.749679565429688 RMSE: 3.2786703\n",
      "117 11 0.7704058885574341\n",
      "Validation loss: 2.4998421669006348 RMSE: 1.5810889\n",
      "Validation loss: 2.1385265588760376 RMSE: 1.4623704\n",
      "Validation loss: 2.2349127531051636 RMSE: 1.4949625\n",
      "120 10 2.030350923538208\n",
      "Validation loss: 2.8448140621185303 RMSE: 1.6866575\n",
      "Validation loss: 6.282335042953491 RMSE: 2.506459\n",
      "Validation loss: 3.8403666019439697 RMSE: 1.9596854\n",
      "123 9 0.34163355827331543\n",
      "Validation loss: 4.635653853416443 RMSE: 2.153057\n",
      "Validation loss: 3.7509816884994507 RMSE: 1.936745\n",
      "Validation loss: 4.0451420545578 RMSE: 2.011254\n",
      "126 8 0.39654847979545593\n",
      "Validation loss: 2.977458953857422 RMSE: 1.7255315\n",
      "Validation loss: 1.9943581819534302 RMSE: 1.4122175\n",
      "Validation loss: 9.022719502449036 RMSE: 3.0037842\n",
      "129 7 0.6537832021713257\n",
      "Validation loss: 2.475521445274353 RMSE: 1.573379\n",
      "Validation loss: 3.081158995628357 RMSE: 1.7553228\n",
      "Validation loss: 6.174708366394043 RMSE: 2.484896\n",
      "132 6 2.7577638626098633\n",
      "Validation loss: 6.466942071914673 RMSE: 2.5430183\n",
      "Validation loss: 2.8816038370132446 RMSE: 1.697529\n",
      "Validation loss: 2.4262691736221313 RMSE: 1.5576488\n",
      "135 5 1.3890268802642822\n",
      "Validation loss: 3.71411395072937 RMSE: 1.9272038\n",
      "Validation loss: 5.127269744873047 RMSE: 2.2643476\n",
      "Validation loss: 6.524946093559265 RMSE: 2.554398\n",
      "138 4 0.7051113247871399\n",
      "Validation loss: 2.26407253742218 RMSE: 1.5046835\n",
      "Validation loss: 5.4928752183914185 RMSE: 2.3436882\n",
      "Validation loss: 5.883825302124023 RMSE: 2.4256597\n",
      "141 3 3.058467149734497\n",
      "Validation loss: 2.166216492652893 RMSE: 1.4718072\n",
      "Validation loss: 2.9101377725601196 RMSE: 1.7059125\n",
      "Validation loss: 6.473652362823486 RMSE: 2.5443375\n",
      "144 2 1.248665452003479\n",
      "Validation loss: 3.6994166374206543 RMSE: 1.9233868\n",
      "Validation loss: 3.130277633666992 RMSE: 1.7692591\n",
      "Validation loss: 4.930642366409302 RMSE: 2.2205055\n",
      "147 1 1.0608155727386475\n",
      "Validation loss: 5.398394346237183 RMSE: 2.3234441\n",
      "Validation loss: 1.9192267656326294 RMSE: 1.3853614\n",
      "Validation loss: 2.368110775947571 RMSE: 1.5388666\n",
      "150 0 0.34353500604629517\n",
      "Validation loss: 3.953379511833191 RMSE: 1.9883107\n",
      "Validation loss: 2.4039742946624756 RMSE: 1.5504756\n",
      "152 16 8.636552810668945\n",
      "Validation loss: 2.7699525356292725 RMSE: 1.6643175\n",
      "Validation loss: 2.876649796962738 RMSE: 1.696069\n",
      "Validation loss: 6.884784698486328 RMSE: 2.6238875\n",
      "155 15 0.9985655546188354\n",
      "Validation loss: 1.9693673849105835 RMSE: 1.4033415\n",
      "Validation loss: 2.6073597073554993 RMSE: 1.6147323\n",
      "Validation loss: 2.4745023250579834 RMSE: 1.5730551\n",
      "158 14 1.2581570148468018\n",
      "Validation loss: 4.624174475669861 RMSE: 2.1503892\n",
      "Validation loss: 2.2568711042404175 RMSE: 1.5022886\n",
      "Validation loss: 3.3900887966156006 RMSE: 1.8412194\n",
      "161 13 0.8555150032043457\n",
      "Validation loss: 2.448672890663147 RMSE: 1.5648235\n",
      "Validation loss: 4.446964740753174 RMSE: 2.1087828\n",
      "Validation loss: 3.0658514499664307 RMSE: 1.7509575\n",
      "164 12 0.4280672073364258\n",
      "Validation loss: 2.7690131664276123 RMSE: 1.6640353\n",
      "Validation loss: 2.9017393589019775 RMSE: 1.7034489\n",
      "Validation loss: 2.7440223693847656 RMSE: 1.6565093\n",
      "167 11 4.516729354858398\n",
      "Validation loss: 16.870887279510498 RMSE: 4.1074195\n",
      "Validation loss: 4.072938561439514 RMSE: 2.0181525\n",
      "Validation loss: 3.5237984657287598 RMSE: 1.8771784\n",
      "170 10 3.036881923675537\n",
      "Validation loss: 3.036983370780945 RMSE: 1.7426944\n",
      "Validation loss: 2.44356369972229 RMSE: 1.5631903\n",
      "Validation loss: 3.3137908577919006 RMSE: 1.820382\n",
      "173 9 0.9306402206420898\n",
      "Validation loss: 3.2512301206588745 RMSE: 1.803117\n",
      "Validation loss: 2.7262980937957764 RMSE: 1.6511506\n",
      "Validation loss: 3.3111889362335205 RMSE: 1.819667\n",
      "176 8 1.9048376083374023\n",
      "Validation loss: 3.0413849353790283 RMSE: 1.7439568\n",
      "Validation loss: 2.903261661529541 RMSE: 1.703896\n",
      "Validation loss: 2.482684373855591 RMSE: 1.5756537\n",
      "179 7 1.1275672912597656\n",
      "Validation loss: 3.1045680046081543 RMSE: 1.7619786\n",
      "Validation loss: 2.8164854645729065 RMSE: 1.6782386\n",
      "Validation loss: 3.712785482406616 RMSE: 1.926859\n",
      "182 6 0.6586277484893799\n",
      "Validation loss: 10.226084232330322 RMSE: 3.197825\n",
      "Validation loss: 1.4486350417137146 RMSE: 1.2035925\n",
      "Validation loss: 2.521359086036682 RMSE: 1.5878788\n",
      "185 5 0.3939151465892792\n",
      "Validation loss: 2.095668315887451 RMSE: 1.4476424\n",
      "Validation loss: 2.631614923477173 RMSE: 1.6222253\n",
      "Validation loss: 2.4122458696365356 RMSE: 1.5531406\n",
      "188 4 0.7573904991149902\n",
      "Validation loss: 3.088388741016388 RMSE: 1.7573813\n",
      "Validation loss: 2.1823490858078003 RMSE: 1.4772775\n",
      "Validation loss: 2.7811315059661865 RMSE: 1.6676725\n",
      "191 3 0.6242269277572632\n",
      "Validation loss: 2.3720672130584717 RMSE: 1.5401515\n",
      "Validation loss: 1.6066965460777283 RMSE: 1.2675554\n",
      "Validation loss: 1.9161116480827332 RMSE: 1.3842369\n",
      "194 2 0.40893393754959106\n",
      "Validation loss: 2.829254627227783 RMSE: 1.682039\n",
      "Validation loss: 3.5460554361343384 RMSE: 1.883097\n",
      "Validation loss: 3.258698523044586 RMSE: 1.8051865\n",
      "197 1 0.43936824798583984\n",
      "Validation loss: 2.2613322734832764 RMSE: 1.5037729\n",
      "Validation loss: 3.0301384925842285 RMSE: 1.7407295\n",
      "Validation loss: 2.5140167474746704 RMSE: 1.5855651\n",
      "Loaded trained model with success.\n",
      "Test loss: 1.296982854604721 Test RMSE: 1.1388515\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:0', 'task_name': 'freesolv', 'model_type': 'gin', 'model': {'num_layer': 5, 'emb_dim': 300, 'feat_dim': 300, 'drop_ratio': 0.3, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'random', 'seed': 779, 'task': 'regression', 'data_path': 'data/freesolv/freesolv.csv', 'target': ['expt']}}\n",
      "Running on: cuda:0\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 25.34465789794922\n",
      "Validation loss: 12.407707452774048 RMSE: 3.5224576\n",
      "Validation loss: 10.950044393539429 RMSE: 3.3090851\n",
      "2 16 9.865896224975586\n",
      "Validation loss: 7.281047344207764 RMSE: 2.6983416\n",
      "Validation loss: 3.1205453872680664 RMSE: 1.7665067\n",
      "Validation loss: 8.189875364303589 RMSE: 2.8617957\n",
      "5 15 5.4235687255859375\n",
      "Validation loss: 11.925300598144531 RMSE: 3.4533033\n",
      "Validation loss: 11.280821323394775 RMSE: 3.3586934\n",
      "Validation loss: 9.048739194869995 RMSE: 3.0081117\n",
      "8 14 3.725132942199707\n",
      "Validation loss: 8.653762340545654 RMSE: 2.9417279\n",
      "Validation loss: 8.24027967453003 RMSE: 2.8705888\n",
      "Validation loss: 13.310264706611633 RMSE: 3.6483235\n",
      "11 13 3.842151165008545\n",
      "Validation loss: 13.993754386901855 RMSE: 3.7408228\n",
      "Validation loss: 40.17470169067383 RMSE: 6.3383517\n",
      "Validation loss: 5.9025468826293945 RMSE: 2.429516\n",
      "14 12 2.875326633453369\n",
      "Validation loss: 15.69568932056427 RMSE: 3.9617782\n",
      "Validation loss: 12.538475036621094 RMSE: 3.5409706\n",
      "Validation loss: 2.867928624153137 RMSE: 1.6934958\n",
      "17 11 1.5011425018310547\n",
      "Validation loss: 11.986506223678589 RMSE: 3.4621537\n",
      "Validation loss: 19.080206632614136 RMSE: 4.368089\n",
      "Validation loss: 9.991400957107544 RMSE: 3.1609185\n",
      "20 10 2.6338210105895996\n",
      "Validation loss: 7.560392141342163 RMSE: 2.7496164\n",
      "Validation loss: 6.972215056419373 RMSE: 2.640495\n",
      "Validation loss: 20.8851158618927 RMSE: 4.5700235\n",
      "23 9 2.8116395473480225\n",
      "Validation loss: 8.898826122283936 RMSE: 2.9830902\n",
      "Validation loss: 13.516381740570068 RMSE: 3.6764634\n",
      "Validation loss: 8.907856225967407 RMSE: 2.9846036\n",
      "26 8 3.271012306213379\n",
      "Validation loss: 4.919792026281357 RMSE: 2.21806\n",
      "Validation loss: 15.52579927444458 RMSE: 3.9402792\n",
      "Validation loss: 16.0887770652771 RMSE: 4.0110817\n",
      "29 7 2.6327834129333496\n",
      "Validation loss: 9.56667709350586 RMSE: 3.093005\n",
      "Validation loss: 18.736005783081055 RMSE: 4.328511\n",
      "Validation loss: 9.579350233078003 RMSE: 3.0950525\n",
      "32 6 7.236246109008789\n",
      "Validation loss: 13.506068885326385 RMSE: 3.6750605\n",
      "Validation loss: 7.609158158302307 RMSE: 2.7584705\n",
      "Validation loss: 9.251606613397598 RMSE: 3.0416455\n",
      "35 5 2.20535945892334\n",
      "Validation loss: 9.817100524902344 RMSE: 3.1332252\n",
      "Validation loss: 5.903929233551025 RMSE: 2.4298003\n",
      "Validation loss: 8.708036184310913 RMSE: 2.9509382\n",
      "38 4 2.9949190616607666\n",
      "Validation loss: 7.169894695281982 RMSE: 2.6776655\n",
      "Validation loss: 12.196779012680054 RMSE: 3.4923887\n",
      "Validation loss: 17.020306825637817 RMSE: 4.1255674\n",
      "41 3 0.8254646062850952\n",
      "Validation loss: 11.572498798370361 RMSE: 3.4018376\n",
      "Validation loss: 10.738515615463257 RMSE: 3.2769675\n",
      "Validation loss: 12.545060276985168 RMSE: 3.5419002\n",
      "44 2 1.2029037475585938\n",
      "Validation loss: 15.750840663909912 RMSE: 3.9687326\n",
      "Validation loss: 23.04930591583252 RMSE: 4.800969\n",
      "Validation loss: 5.263496160507202 RMSE: 2.2942312\n",
      "47 1 2.828524112701416\n",
      "Validation loss: 5.428640604019165 RMSE: 2.3299444\n",
      "Validation loss: 8.358381688594818 RMSE: 2.8910866\n",
      "Validation loss: 12.04343867301941 RMSE: 3.4703658\n",
      "50 0 3.5597305297851562\n",
      "Validation loss: 4.632461130619049 RMSE: 2.1523154\n",
      "Validation loss: 10.834916353225708 RMSE: 3.2916436\n",
      "52 16 0.014107247814536095\n",
      "Validation loss: 7.682161629199982 RMSE: 2.7716713\n",
      "Validation loss: 9.323116421699524 RMSE: 3.0533776\n",
      "Validation loss: 10.95346474647522 RMSE: 3.3096018\n",
      "55 15 1.3543438911437988\n",
      "Validation loss: 10.487406373023987 RMSE: 3.2384264\n",
      "Validation loss: 12.428297877311707 RMSE: 3.5253794\n",
      "Validation loss: 17.106172800064087 RMSE: 4.135961\n",
      "58 14 1.1939666271209717\n",
      "Validation loss: 3.819244623184204 RMSE: 1.9542888\n",
      "Validation loss: 8.40926730632782 RMSE: 2.8998737\n",
      "Validation loss: 4.636030197143555 RMSE: 2.1531446\n",
      "61 13 0.6500518321990967\n",
      "Validation loss: 11.045481145381927 RMSE: 3.3234744\n",
      "Validation loss: 12.095356941223145 RMSE: 3.477838\n",
      "Validation loss: 2.532334089279175 RMSE: 1.591331\n",
      "64 12 1.9962137937545776\n",
      "Validation loss: 12.296108186244965 RMSE: 3.5065806\n",
      "Validation loss: 9.195169925689697 RMSE: 3.0323539\n",
      "Validation loss: 7.6196160316467285 RMSE: 2.7603652\n",
      "67 11 0.8398916721343994\n",
      "Validation loss: 11.689944744110107 RMSE: 3.419056\n",
      "Validation loss: 11.393112003803253 RMSE: 3.3753684\n",
      "Validation loss: 8.976502358913422 RMSE: 2.996081\n",
      "70 10 0.8679982423782349\n",
      "Validation loss: 4.520162433385849 RMSE: 2.1260674\n",
      "Validation loss: 13.051826000213623 RMSE: 3.612731\n",
      "Validation loss: 5.0417174100875854 RMSE: 2.2453768\n",
      "73 9 3.008023262023926\n",
      "Validation loss: 13.149206697940826 RMSE: 3.6261835\n",
      "Validation loss: 6.895186543464661 RMSE: 2.6258693\n",
      "Validation loss: 10.904086887836456 RMSE: 3.3021333\n",
      "76 8 3.3582215309143066\n",
      "Validation loss: 6.922343492507935 RMSE: 2.6310346\n",
      "Validation loss: 8.562838077545166 RMSE: 2.9262328\n",
      "Validation loss: 9.650052189826965 RMSE: 3.1064532\n",
      "79 7 0.5592973232269287\n",
      "Validation loss: 5.833475112915039 RMSE: 2.415259\n",
      "Validation loss: 9.370941936969757 RMSE: 3.0611997\n",
      "Validation loss: 2.1663047075271606 RMSE: 1.471837\n",
      "82 6 2.181236505508423\n",
      "Validation loss: 8.572536408901215 RMSE: 2.9278898\n",
      "Validation loss: 14.920010566711426 RMSE: 3.862643\n",
      "Validation loss: 7.8420023918151855 RMSE: 2.800358\n",
      "85 5 1.2096376419067383\n",
      "Validation loss: 7.246490836143494 RMSE: 2.6919308\n",
      "Validation loss: 6.272541761398315 RMSE: 2.5045042\n",
      "Validation loss: 2.543134033679962 RMSE: 1.5947206\n",
      "88 4 1.7187529802322388\n",
      "Validation loss: 6.0916712284088135 RMSE: 2.4681315\n",
      "Validation loss: 9.425927877426147 RMSE: 3.0701678\n",
      "Validation loss: 11.839131355285645 RMSE: 3.4408035\n",
      "91 3 1.6918084621429443\n",
      "Validation loss: 4.942620515823364 RMSE: 2.2232006\n",
      "Validation loss: 8.239064931869507 RMSE: 2.870377\n",
      "Validation loss: 6.177694797515869 RMSE: 2.485497\n",
      "94 2 3.3657453060150146\n",
      "Validation loss: 5.081050276756287 RMSE: 2.254119\n",
      "Validation loss: 6.333524644374847 RMSE: 2.5166495\n",
      "Validation loss: 4.724359154701233 RMSE: 2.173559\n",
      "97 1 1.5374150276184082\n",
      "Validation loss: 10.146036028862 RMSE: 3.1852846\n",
      "Validation loss: 3.6952667236328125 RMSE: 1.9223077\n",
      "Validation loss: 4.303432643413544 RMSE: 2.0744717\n",
      "100 0 1.144290804862976\n",
      "Validation loss: 3.2733246088027954 RMSE: 1.8092335\n",
      "Validation loss: 6.557059973478317 RMSE: 2.5606756\n",
      "102 16 26.50275993347168\n",
      "Validation loss: 4.829246401786804 RMSE: 2.1975548\n",
      "Validation loss: 4.797077119350433 RMSE: 2.190223\n",
      "Validation loss: 1.6351289749145508 RMSE: 1.2787216\n",
      "105 15 1.4952147006988525\n",
      "Validation loss: 16.360883355140686 RMSE: 4.044859\n",
      "Validation loss: 13.22340178489685 RMSE: 3.6363995\n",
      "Validation loss: 2.176056385040283 RMSE: 1.4751463\n",
      "108 14 1.1397364139556885\n",
      "Validation loss: 2.25118088722229 RMSE: 1.5003936\n",
      "Validation loss: 2.4837217330932617 RMSE: 1.5759828\n",
      "Validation loss: 2.353823661804199 RMSE: 1.5342176\n",
      "111 13 1.7176434993743896\n",
      "Validation loss: 5.941526770591736 RMSE: 2.4375246\n",
      "Validation loss: 7.349184036254883 RMSE: 2.710938\n",
      "Validation loss: 5.214462041854858 RMSE: 2.2835197\n",
      "114 12 1.4763509035110474\n",
      "Validation loss: 3.8320605754852295 RMSE: 1.9575648\n",
      "Validation loss: 4.372048258781433 RMSE: 2.0909443\n",
      "Validation loss: 4.4103734493255615 RMSE: 2.100089\n",
      "117 11 1.1774027347564697\n",
      "Validation loss: 8.59807574748993 RMSE: 2.9322476\n",
      "Validation loss: 3.3164197206497192 RMSE: 1.8211039\n",
      "Validation loss: 7.782244324684143 RMSE: 2.7896674\n",
      "120 10 1.4214004278182983\n",
      "Validation loss: 2.061392664909363 RMSE: 1.4357553\n",
      "Validation loss: 3.192260265350342 RMSE: 1.7866898\n",
      "Validation loss: 2.5014848709106445 RMSE: 1.5816084\n",
      "123 9 1.549255132675171\n",
      "Validation loss: 7.7422440350055695 RMSE: 2.7824883\n",
      "Validation loss: 4.844755828380585 RMSE: 2.201081\n",
      "Validation loss: 2.65641325712204 RMSE: 1.6298505\n",
      "126 8 1.752204418182373\n",
      "Validation loss: 7.358728766441345 RMSE: 2.7126977\n",
      "Validation loss: 5.25514554977417 RMSE: 2.2924106\n",
      "Validation loss: 3.194562792778015 RMSE: 1.787334\n",
      "129 7 1.192499041557312\n",
      "Validation loss: 2.7923807501792908 RMSE: 1.6710418\n",
      "Validation loss: 13.555424094200134 RMSE: 3.6817691\n",
      "Validation loss: 3.234817624092102 RMSE: 1.7985598\n",
      "132 6 1.3933422565460205\n",
      "Validation loss: 3.969472259283066 RMSE: 1.9923533\n",
      "Validation loss: 4.381146192550659 RMSE: 2.093119\n",
      "Validation loss: 5.872615694999695 RMSE: 2.423348\n",
      "135 5 1.9760018587112427\n",
      "Validation loss: 3.626494348049164 RMSE: 1.9043357\n",
      "Validation loss: 3.935815393924713 RMSE: 1.9838895\n",
      "Validation loss: 8.364284634590149 RMSE: 2.8921072\n",
      "138 4 2.033379316329956\n",
      "Validation loss: 6.116712212562561 RMSE: 2.4731987\n",
      "Validation loss: 4.067933976650238 RMSE: 2.0169122\n",
      "Validation loss: 5.310504674911499 RMSE: 2.3044527\n",
      "141 3 1.7096061706542969\n",
      "Validation loss: 1.2207102179527283 RMSE: 1.1048576\n",
      "Validation loss: 4.561801373958588 RMSE: 2.135837\n",
      "Validation loss: 2.648259162902832 RMSE: 1.6273472\n",
      "144 2 0.7973589897155762\n",
      "Validation loss: 2.383908271789551 RMSE: 1.5439911\n",
      "Validation loss: 5.260348558425903 RMSE: 2.293545\n",
      "Validation loss: 4.5857884883880615 RMSE: 2.1414452\n",
      "147 1 0.9492155313491821\n",
      "Validation loss: 6.9498714208602905 RMSE: 2.6362607\n",
      "Validation loss: 3.611623227596283 RMSE: 1.9004271\n",
      "Validation loss: 2.5452182292938232 RMSE: 1.595374\n",
      "150 0 0.3967480957508087\n",
      "Validation loss: 3.5009982585906982 RMSE: 1.8710954\n",
      "Validation loss: 3.1697704792022705 RMSE: 1.7803854\n",
      "152 16 28.2015380859375\n",
      "Validation loss: 7.195996642112732 RMSE: 2.682536\n",
      "Validation loss: 2.9755430817604065 RMSE: 1.7249761\n",
      "Validation loss: 1.8819925785064697 RMSE: 1.3718574\n",
      "155 15 1.3871794939041138\n",
      "Validation loss: 9.145497441291809 RMSE: 3.0241523\n",
      "Validation loss: 2.7685552835464478 RMSE: 1.6638978\n",
      "Validation loss: 9.539920210838318 RMSE: 3.088676\n",
      "158 14 0.4960346817970276\n",
      "Validation loss: 1.184550404548645 RMSE: 1.0883704\n",
      "Validation loss: 4.768923878669739 RMSE: 2.1837866\n",
      "Validation loss: 4.139249265193939 RMSE: 2.0345144\n",
      "161 13 2.6640396118164062\n",
      "Validation loss: 3.756318509578705 RMSE: 1.9381224\n",
      "Validation loss: 2.733267068862915 RMSE: 1.6532598\n"
     ]
    }
   ],
   "source": [
    "seeds = list(range(777,782))\n",
    "# datasets = [\"bace\",  \"bbbp\", \"tox21\", \"toxcast\", \"sider\",  ]\n",
    "datasets = [\"freeSolv\",  \"lipo\", \"esol\", \"qm7\", \"bace\",  \"bbbp\", ]\n",
    "\n",
    "for dataset in datasets:\n",
    "    for seed in seeds: \n",
    "        !python finetuneRecon.py \\\n",
    "        --task_name {dataset} \\\n",
    "        --splitting scaffold \\\n",
    "        --seed {seed} \\\n",
    "        --alpha 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6764b07a-c44b-44d3-ab6b-161100c0d53b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:0', 'task_name': 'esol', 'model_type': 'gin', 'model': {'num_layer': 5, 'emb_dim': 300, 'feat_dim': 300, 'drop_ratio': 0.3, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 751, 'task': 'regression', 'data_path': 'data/esol/esol.csv', 'target': ['measured log solubility in mols per litre']}}\n",
      "Running on: cuda:0\n",
      "1127\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/1127\n",
      "Generating scaffold 1000/1127\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "tensor([[ 0.0806, -0.0410, -0.0872,  ..., -0.0789, -0.0887, -0.1107],\n",
      "        [ 0.0313,  0.0164,  0.1077,  ..., -0.0785,  0.1171,  0.1046],\n",
      "        [-0.0002, -0.0291, -0.0762,  ...,  0.0110,  0.0630,  0.0511],\n",
      "        ...,\n",
      "        [ 0.0313,  0.0164,  0.1077,  ..., -0.0785,  0.1171,  0.1046],\n",
      "        [-0.0175,  0.1190, -0.0096,  ...,  0.0821,  0.0621,  0.0488],\n",
      "        [ 0.0313,  0.0164,  0.1077,  ..., -0.0785,  0.1171,  0.1046]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "0 0 8.955981254577637\n",
      "tensor([[ 0.0312,  0.0165,  0.1078,  ..., -0.0786,  0.1172,  0.1047],\n",
      "        [ 0.0312,  0.0165,  0.1078,  ..., -0.0786,  0.1172,  0.1047],\n",
      "        [ 0.0312,  0.0165,  0.1078,  ..., -0.0786,  0.1172,  0.1047],\n",
      "        ...,\n",
      "        [ 0.0312,  0.0165,  0.1078,  ..., -0.0786,  0.1172,  0.1047],\n",
      "        [ 0.0312,  0.0165,  0.1078,  ..., -0.0786,  0.1172,  0.1047],\n",
      "        [ 0.0312,  0.0165,  0.1078,  ..., -0.0786,  0.1172,  0.1047]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0004, -0.0289, -0.0764,  ...,  0.0108,  0.0629,  0.0509],\n",
      "        [-0.0004, -0.0289, -0.0764,  ...,  0.0108,  0.0629,  0.0509],\n",
      "        [-0.0004, -0.0289, -0.0764,  ...,  0.0108,  0.0629,  0.0509],\n",
      "        ...,\n",
      "        [ 0.0311,  0.0166,  0.1079,  ..., -0.0787,  0.1173,  0.1048],\n",
      "        [ 0.0311,  0.0166,  0.1079,  ..., -0.0787,  0.1173,  0.1048],\n",
      "        [ 0.0311,  0.0166,  0.1079,  ..., -0.0787,  0.1173,  0.1048]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0004, -0.0288, -0.0764,  ...,  0.0108,  0.0629,  0.0509],\n",
      "        [-0.0004, -0.0288, -0.0764,  ...,  0.0108,  0.0629,  0.0509],\n",
      "        [ 0.0311,  0.0167,  0.1079,  ..., -0.0788,  0.1174,  0.1048],\n",
      "        ...,\n",
      "        [ 0.0187, -0.0968, -0.0305,  ...,  0.0381, -0.0655, -0.0279],\n",
      "        [ 0.0311,  0.0167,  0.1079,  ..., -0.0788,  0.1174,  0.1048],\n",
      "        [ 0.0311,  0.0167,  0.1079,  ..., -0.0788,  0.1174,  0.1048]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0310,  0.0168,  0.1079,  ..., -0.0789,  0.1174,  0.1049],\n",
      "        [-0.0003, -0.0287, -0.0764,  ...,  0.0107,  0.0629,  0.0509],\n",
      "        [-0.0003, -0.0287, -0.0764,  ...,  0.0107,  0.0629,  0.0509],\n",
      "        ...,\n",
      "        [ 0.0310,  0.0168,  0.1079,  ..., -0.0789,  0.1174,  0.1049],\n",
      "        [ 0.0310,  0.0168,  0.1079,  ..., -0.0789,  0.1174,  0.1049],\n",
      "        [ 0.0310,  0.0168,  0.1079,  ..., -0.0789,  0.1174,  0.1049]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0309,  0.0168,  0.1080,  ..., -0.0789,  0.1175,  0.1050],\n",
      "        [ 0.0309,  0.0168,  0.1080,  ..., -0.0789,  0.1175,  0.1050],\n",
      "        [-0.0961, -0.0527,  0.0569,  ...,  0.0218, -0.0656,  0.0982],\n",
      "        ...,\n",
      "        [ 0.0309,  0.0168,  0.1080,  ..., -0.0789,  0.1175,  0.1050],\n",
      "        [-0.0961, -0.0527,  0.0569,  ...,  0.0218, -0.0656,  0.0982],\n",
      "        [ 0.0065, -0.1107,  0.0619,  ...,  0.0173,  0.0921, -0.0094]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0065, -0.1107,  0.0619,  ...,  0.0174,  0.0921, -0.0095],\n",
      "        [ 0.0065, -0.1107,  0.0619,  ...,  0.0174,  0.0921, -0.0095],\n",
      "        [ 0.0309,  0.0169,  0.1080,  ..., -0.0790,  0.1176,  0.1050],\n",
      "        ...,\n",
      "        [-0.0961, -0.0528,  0.0569,  ...,  0.0219, -0.0655,  0.0981],\n",
      "        [ 0.0309,  0.0169,  0.1080,  ..., -0.0790,  0.1176,  0.1050],\n",
      "        [ 0.0065, -0.1107,  0.0619,  ...,  0.0174,  0.0921, -0.0095]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0002, -0.0285, -0.0763,  ...,  0.0108,  0.0630,  0.0507],\n",
      "        [-0.0002, -0.0285, -0.0763,  ...,  0.0108,  0.0630,  0.0507],\n",
      "        [ 0.0309,  0.0169,  0.1081,  ..., -0.0791,  0.1177,  0.1051],\n",
      "        ...,\n",
      "        [ 0.0309,  0.0169,  0.1081,  ..., -0.0791,  0.1177,  0.1051],\n",
      "        [ 0.0309,  0.0169,  0.1081,  ..., -0.0791,  0.1177,  0.1051],\n",
      "        [-0.0123,  0.1087, -0.0120,  ...,  0.0790,  0.0517, -0.0861]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0961, -0.0528,  0.0569,  ...,  0.0220, -0.0653,  0.0979],\n",
      "        [-0.0961, -0.0528,  0.0569,  ...,  0.0220, -0.0653,  0.0979],\n",
      "        [ 0.0804, -0.0412, -0.0873,  ..., -0.0789, -0.0891, -0.1105],\n",
      "        ...,\n",
      "        [ 0.0309,  0.0170,  0.1081,  ..., -0.0791,  0.1178,  0.1052],\n",
      "        [ 0.0309,  0.0170,  0.1081,  ..., -0.0791,  0.1178,  0.1052],\n",
      "        [-0.0961, -0.0528,  0.0569,  ...,  0.0220, -0.0653,  0.0979]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0002, -0.0284, -0.0764,  ...,  0.0108,  0.0631,  0.0506],\n",
      "        [-0.0002, -0.0284, -0.0764,  ...,  0.0108,  0.0631,  0.0506],\n",
      "        [ 0.0309,  0.0170,  0.1082,  ..., -0.0792,  0.1179,  0.1052],\n",
      "        ...,\n",
      "        [ 0.0309,  0.0170,  0.1082,  ..., -0.0792,  0.1179,  0.1052],\n",
      "        [ 0.0309,  0.0170,  0.1082,  ..., -0.0792,  0.1179,  0.1052],\n",
      "        [ 0.0309,  0.0170,  0.1082,  ..., -0.0792,  0.1179,  0.1052]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0310,  0.0170,  0.1083,  ..., -0.0792,  0.1179,  0.1053],\n",
      "        [-0.0002, -0.0284, -0.0765,  ...,  0.0108,  0.0631,  0.0505],\n",
      "        [-0.0002, -0.0284, -0.0765,  ...,  0.0108,  0.0631,  0.0505],\n",
      "        ...,\n",
      "        [ 0.0310,  0.0170,  0.1083,  ..., -0.0792,  0.1179,  0.1053],\n",
      "        [ 0.0310,  0.0170,  0.1083,  ..., -0.0792,  0.1179,  0.1053],\n",
      "        [ 0.0310,  0.0170,  0.1083,  ..., -0.0792,  0.1179,  0.1053]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0310,  0.0171,  0.1083,  ..., -0.0793,  0.1179,  0.1054],\n",
      "        [ 0.0070, -0.1110,  0.0617,  ...,  0.0178,  0.0925, -0.0099],\n",
      "        [ 0.0310,  0.0171,  0.1083,  ..., -0.0793,  0.1179,  0.1054],\n",
      "        ...,\n",
      "        [ 0.0310,  0.0171,  0.1083,  ..., -0.0793,  0.1179,  0.1054],\n",
      "        [ 0.0310,  0.0171,  0.1083,  ..., -0.0793,  0.1179,  0.1054],\n",
      "        [-0.0961, -0.0529,  0.0568,  ...,  0.0223, -0.0651,  0.0977]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0309,  0.0171,  0.1084,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [ 0.0309,  0.0171,  0.1084,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [-0.0961, -0.0529,  0.0568,  ...,  0.0223, -0.0650,  0.0976],\n",
      "        ...,\n",
      "        [ 0.0309,  0.0171,  0.1084,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [ 0.0309,  0.0171,  0.1084,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [ 0.0309,  0.0171,  0.1084,  ..., -0.0794,  0.1179,  0.1054]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0961, -0.0529,  0.0568,  ...,  0.0224, -0.0649,  0.0975],\n",
      "        [-0.0003, -0.0284, -0.0766,  ...,  0.0107,  0.0633,  0.0504],\n",
      "        [-0.0003, -0.0284, -0.0766,  ...,  0.0107,  0.0633,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0309,  0.0171,  0.1085,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [ 0.0071, -0.1112,  0.0617,  ...,  0.0179,  0.0926, -0.0100],\n",
      "        [ 0.0309,  0.0171,  0.1085,  ..., -0.0794,  0.1179,  0.1054]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0309,  0.0171,  0.1085,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [ 0.0309,  0.0171,  0.1085,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [ 0.0309,  0.0171,  0.1085,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        ...,\n",
      "        [ 0.0071, -0.1112,  0.0616,  ...,  0.0180,  0.0927, -0.0100],\n",
      "        [ 0.0309,  0.0171,  0.1085,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [ 0.0309,  0.0171,  0.1085,  ..., -0.0794,  0.1179,  0.1054]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0309,  0.0171,  0.1086,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [ 0.0309,  0.0171,  0.1086,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [ 0.0309,  0.0171,  0.1086,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        ...,\n",
      "        [ 0.0309,  0.0171,  0.1086,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [-0.0962, -0.0529,  0.0568,  ...,  0.0225, -0.0647,  0.0974],\n",
      "        [-0.0127,  0.1084, -0.0120,  ...,  0.0788,  0.0512, -0.0857]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0309,  0.0171,  0.1087,  ..., -0.0795,  0.1179,  0.1054],\n",
      "        [-0.0002, -0.0284, -0.0766,  ...,  0.0107,  0.0632,  0.0503],\n",
      "        [-0.0002, -0.0284, -0.0766,  ...,  0.0107,  0.0632,  0.0503],\n",
      "        ...,\n",
      "        [ 0.0309,  0.0171,  0.1087,  ..., -0.0795,  0.1179,  0.1054],\n",
      "        [ 0.0309,  0.0171,  0.1087,  ..., -0.0795,  0.1179,  0.1054],\n",
      "        [ 0.0309,  0.0171,  0.1087,  ..., -0.0795,  0.1179,  0.1054]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 3.0917e-02,  1.7134e-02,  1.0878e-01,  ..., -7.9471e-02,\n",
      "          1.1795e-01,  1.0532e-01],\n",
      "        [-1.1599e-04, -2.8463e-02, -7.6550e-02,  ...,  1.0742e-02,\n",
      "          6.3165e-02,  5.0226e-02],\n",
      "        [-1.1599e-04, -2.8463e-02, -7.6550e-02,  ...,  1.0742e-02,\n",
      "          6.3165e-02,  5.0226e-02],\n",
      "        ...,\n",
      "        [ 3.0917e-02,  1.7134e-02,  1.0878e-01,  ..., -7.9471e-02,\n",
      "          1.1795e-01,  1.0532e-01],\n",
      "        [ 3.0917e-02,  1.7134e-02,  1.0878e-01,  ..., -7.9471e-02,\n",
      "          1.1795e-01,  1.0532e-01],\n",
      "        [ 3.0917e-02,  1.7134e-02,  1.0878e-01,  ..., -7.9471e-02,\n",
      "          1.1795e-01,  1.0532e-01]], device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0309,  0.0172,  0.1088,  ..., -0.0795,  0.1180,  0.1053],\n",
      "        [ 0.0309,  0.0172,  0.1088,  ..., -0.0795,  0.1180,  0.1053],\n",
      "        [ 0.0309,  0.0172,  0.1088,  ..., -0.0795,  0.1180,  0.1053],\n",
      "        ...,\n",
      "        [ 0.0074, -0.1113,  0.0618,  ...,  0.0180,  0.0929, -0.0100],\n",
      "        [ 0.0309,  0.0172,  0.1088,  ..., -0.0795,  0.1180,  0.1053],\n",
      "        [-0.0964, -0.0527,  0.0568,  ...,  0.0224, -0.0647,  0.0975]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-9.6424e-02, -5.2667e-02,  5.6764e-02,  ...,  2.2343e-02,\n",
      "         -6.4659e-02,  9.7490e-02],\n",
      "        [ 3.0807e-02,  1.7243e-02,  1.0880e-01,  ..., -7.9564e-02,\n",
      "          1.1803e-01,  1.0522e-01],\n",
      "        [-1.7022e-05, -2.8596e-02, -7.6470e-02,  ...,  1.0681e-02,\n",
      "          6.3049e-02,  5.0292e-02],\n",
      "        ...,\n",
      "        [ 3.0807e-02,  1.7243e-02,  1.0880e-01,  ..., -7.9564e-02,\n",
      "          1.1803e-01,  1.0522e-01],\n",
      "        [-1.7267e-02,  1.1858e-01, -1.0125e-02,  ...,  8.2536e-02,\n",
      "          6.1876e-02,  4.8307e-02],\n",
      "        [ 3.0807e-02,  1.7243e-02,  1.0880e-01,  ..., -7.9564e-02,\n",
      "          1.1803e-01,  1.0522e-01]], device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 3.0753e-02,  1.7299e-02,  1.0883e-01,  ..., -7.9583e-02,\n",
      "          1.1807e-01,  1.0516e-01],\n",
      "        [ 3.0753e-02,  1.7299e-02,  1.0883e-01,  ..., -7.9583e-02,\n",
      "          1.1807e-01,  1.0516e-01],\n",
      "        [ 4.1295e-05, -2.8627e-02, -7.6411e-02,  ...,  1.0639e-02,\n",
      "          6.3006e-02,  5.0348e-02],\n",
      "        ...,\n",
      "        [ 3.0753e-02,  1.7299e-02,  1.0883e-01,  ..., -7.9583e-02,\n",
      "          1.1807e-01,  1.0516e-01],\n",
      "        [-1.2426e-02,  1.0806e-01, -1.2240e-02,  ...,  7.9107e-02,\n",
      "          5.1456e-02, -8.5975e-02],\n",
      "        [ 3.0753e-02,  1.7299e-02,  1.0883e-01,  ..., -7.9583e-02,\n",
      "          1.1807e-01,  1.0516e-01]], device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0307,  0.0173,  0.1089,  ..., -0.0796,  0.1181,  0.1051],\n",
      "        [ 0.0307,  0.0173,  0.1089,  ..., -0.0796,  0.1181,  0.1051],\n",
      "        [ 0.0307,  0.0173,  0.1089,  ..., -0.0796,  0.1181,  0.1051],\n",
      "        ...,\n",
      "        [ 0.0076, -0.1114,  0.0619,  ...,  0.0181,  0.0930, -0.0100],\n",
      "        [ 0.0307,  0.0173,  0.1089,  ..., -0.0796,  0.1181,  0.1051],\n",
      "        [ 0.0307,  0.0173,  0.1089,  ..., -0.0796,  0.1181,  0.1051]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0307,  0.0174,  0.1089,  ..., -0.0796,  0.1181,  0.1051],\n",
      "        [ 0.0808, -0.0411, -0.0881,  ..., -0.0790, -0.0893, -0.1108],\n",
      "        [ 0.0076, -0.1113,  0.0619,  ...,  0.0181,  0.0930, -0.0100],\n",
      "        ...,\n",
      "        [ 0.0307,  0.0174,  0.1089,  ..., -0.0796,  0.1181,  0.1051],\n",
      "        [-0.0966, -0.0525,  0.0567,  ...,  0.0222, -0.0646,  0.0975],\n",
      "        [-0.0966, -0.0525,  0.0567,  ...,  0.0222, -0.0646,  0.0975]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0001, -0.0288, -0.0764,  ...,  0.0106,  0.0630,  0.0504],\n",
      "        [ 0.0898,  0.0321,  0.0691,  ...,  0.0426, -0.0070,  0.0773],\n",
      "        [ 0.0001, -0.0288, -0.0764,  ...,  0.0106,  0.0630,  0.0504],\n",
      "        ...,\n",
      "        [-0.0967, -0.0524,  0.0567,  ...,  0.0222, -0.0646,  0.0975],\n",
      "        [ 0.0307,  0.0174,  0.1089,  ..., -0.0797,  0.1182,  0.1051],\n",
      "        [-0.0967, -0.0524,  0.0567,  ...,  0.0222, -0.0646,  0.0975]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0968, -0.0523,  0.0567,  ...,  0.0222, -0.0646,  0.0974],\n",
      "        [ 0.0001, -0.0288, -0.0764,  ...,  0.0106,  0.0630,  0.0504],\n",
      "        [ 0.0001, -0.0288, -0.0764,  ...,  0.0106,  0.0630,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0307,  0.0174,  0.1090,  ..., -0.0797,  0.1182,  0.1052],\n",
      "        [-0.0968, -0.0523,  0.0567,  ...,  0.0222, -0.0646,  0.0974],\n",
      "        [ 0.0307,  0.0174,  0.1090,  ..., -0.0797,  0.1182,  0.1052]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0306,  0.0174,  0.1090,  ..., -0.0798,  0.1182,  0.1052],\n",
      "        [ 0.0001, -0.0289, -0.0764,  ...,  0.0106,  0.0630,  0.0504],\n",
      "        [ 0.0001, -0.0289, -0.0764,  ...,  0.0106,  0.0630,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0306,  0.0174,  0.1090,  ..., -0.0798,  0.1182,  0.1052],\n",
      "        [-0.0968, -0.0523,  0.0566,  ...,  0.0222, -0.0646,  0.0974],\n",
      "        [-0.0122,  0.1078, -0.0123,  ...,  0.0793,  0.0515, -0.0860]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0122,  0.1078, -0.0123,  ...,  0.0793,  0.0516, -0.0860],\n",
      "        [ 0.0306,  0.0174,  0.1091,  ..., -0.0798,  0.1182,  0.1053],\n",
      "        [ 0.0306,  0.0174,  0.1091,  ..., -0.0798,  0.1182,  0.1053],\n",
      "        ...,\n",
      "        [ 0.0306,  0.0174,  0.1091,  ..., -0.0798,  0.1182,  0.1053],\n",
      "        [ 0.0306,  0.0174,  0.1091,  ..., -0.0798,  0.1182,  0.1053],\n",
      "        [ 0.0306,  0.0174,  0.1091,  ..., -0.0798,  0.1182,  0.1053]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0306,  0.0174,  0.1091,  ..., -0.0799,  0.1182,  0.1053],\n",
      "        [-0.0121,  0.1078, -0.0122,  ...,  0.0793,  0.0516, -0.0860],\n",
      "        [ 0.0306,  0.0174,  0.1091,  ..., -0.0799,  0.1182,  0.1053],\n",
      "        ...,\n",
      "        [ 0.0306,  0.0174,  0.1091,  ..., -0.0799,  0.1182,  0.1053],\n",
      "        [ 0.0306,  0.0174,  0.1091,  ..., -0.0799,  0.1182,  0.1053],\n",
      "        [-0.0170,  0.1189, -0.0102,  ...,  0.0827,  0.0622,  0.0487]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0121,  0.1077, -0.0122,  ...,  0.0793,  0.0516, -0.0860],\n",
      "        [ 0.0305,  0.0174,  0.1092,  ..., -0.0799,  0.1182,  0.1053],\n",
      "        [ 0.0305,  0.0174,  0.1092,  ..., -0.0799,  0.1182,  0.1053],\n",
      "        ...,\n",
      "        [ 0.0305,  0.0174,  0.1092,  ..., -0.0799,  0.1182,  0.1053],\n",
      "        [ 0.0001, -0.0291, -0.0765,  ...,  0.0107,  0.0630,  0.0504],\n",
      "        [ 0.0001, -0.0291, -0.0765,  ...,  0.0107,  0.0630,  0.0504]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Validation loss: 2.2496231530619935 RMSE: 1.4998744\n",
      "tensor([[ 0.0305,  0.0174,  0.1092,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [-0.0970, -0.0521,  0.0565,  ...,  0.0224, -0.0646,  0.0973],\n",
      "        [ 0.0001, -0.0292, -0.0766,  ...,  0.0107,  0.0630,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0305,  0.0174,  0.1092,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [ 0.0305,  0.0174,  0.1092,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [ 0.0305,  0.0174,  0.1092,  ..., -0.0800,  0.1182,  0.1054]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0002, -0.0292, -0.0766,  ...,  0.0108,  0.0630,  0.0504],\n",
      "        [ 0.0305,  0.0174,  0.1092,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [ 0.0305,  0.0174,  0.1092,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        ...,\n",
      "        [ 0.0305,  0.0174,  0.1092,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [-0.0971, -0.0521,  0.0565,  ...,  0.0224, -0.0646,  0.0972],\n",
      "        [ 0.0078, -0.1111,  0.0621,  ...,  0.0182,  0.0933, -0.0101]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0971, -0.0521,  0.0565,  ...,  0.0224, -0.0647,  0.0972],\n",
      "        [ 0.0002, -0.0292, -0.0766,  ...,  0.0108,  0.0630,  0.0504],\n",
      "        [ 0.0002, -0.0292, -0.0766,  ...,  0.0108,  0.0630,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0305,  0.0174,  0.1093,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [ 0.0305,  0.0174,  0.1093,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [ 0.0078, -0.1111,  0.0621,  ...,  0.0182,  0.0934, -0.0101]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0304,  0.0174,  0.1093,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [ 0.0304,  0.0174,  0.1093,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [ 0.0002, -0.0293, -0.0766,  ...,  0.0108,  0.0630,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0304,  0.0174,  0.1093,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [ 0.0304,  0.0174,  0.1093,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [ 0.0304,  0.0174,  0.1093,  ..., -0.0800,  0.1182,  0.1054]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [-0.0120,  0.1077, -0.0121,  ...,  0.0793,  0.0515, -0.0860],\n",
      "        ...,\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [ 0.0809, -0.0410, -0.0884,  ..., -0.0791, -0.0894, -0.1111],\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [ 0.0003, -0.0293, -0.0766,  ...,  0.0108,  0.0631,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [-0.0120,  0.1076, -0.0121,  ...,  0.0793,  0.0515, -0.0860]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        ...,\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0304,  0.0174,  0.1095,  ..., -0.0801,  0.1182,  0.1055],\n",
      "        [ 0.0304,  0.0174,  0.1095,  ..., -0.0801,  0.1182,  0.1055],\n",
      "        [ 0.0304,  0.0174,  0.1095,  ..., -0.0801,  0.1182,  0.1055],\n",
      "        ...,\n",
      "        [-0.0974, -0.0521,  0.0564,  ...,  0.0225, -0.0648,  0.0970],\n",
      "        [ 0.0233,  0.0582, -0.1101,  ...,  0.0704,  0.0737,  0.0701],\n",
      "        [-0.0974, -0.0521,  0.0564,  ...,  0.0225, -0.0648,  0.0970]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0303,  0.0174,  0.1095,  ..., -0.0801,  0.1182,  0.1055],\n",
      "        [ 0.0303,  0.0174,  0.1095,  ..., -0.0801,  0.1182,  0.1055],\n",
      "        [ 0.0003, -0.0294, -0.0767,  ...,  0.0108,  0.0631,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0303,  0.0174,  0.1095,  ..., -0.0801,  0.1182,  0.1055],\n",
      "        [ 0.0303,  0.0174,  0.1095,  ..., -0.0801,  0.1182,  0.1055],\n",
      "        [-0.0974, -0.0522,  0.0564,  ...,  0.0225, -0.0649,  0.0970]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0003, -0.0295, -0.0767,  ...,  0.0108,  0.0631,  0.0503],\n",
      "        [ 0.0003, -0.0295, -0.0767,  ...,  0.0108,  0.0631,  0.0503],\n",
      "        [ 0.0303,  0.0174,  0.1096,  ..., -0.0801,  0.1182,  0.1055],\n",
      "        ...,\n",
      "        [ 0.0303,  0.0174,  0.1096,  ..., -0.0801,  0.1182,  0.1055],\n",
      "        [ 0.0303,  0.0174,  0.1096,  ..., -0.0801,  0.1182,  0.1055],\n",
      "        [ 0.0303,  0.0174,  0.1096,  ..., -0.0801,  0.1182,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0303,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1055],\n",
      "        [ 0.0003, -0.0295, -0.0767,  ...,  0.0108,  0.0631,  0.0503],\n",
      "        [ 0.0003, -0.0295, -0.0767,  ...,  0.0108,  0.0631,  0.0503],\n",
      "        ...,\n",
      "        [-0.0167,  0.1191, -0.0102,  ...,  0.0827,  0.0622,  0.0491],\n",
      "        [-0.0167,  0.1191, -0.0102,  ...,  0.0827,  0.0622,  0.0491],\n",
      "        [ 0.0303,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0302,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1055],\n",
      "        [ 0.0004, -0.0296, -0.0768,  ...,  0.0108,  0.0631,  0.0503],\n",
      "        [ 0.0004, -0.0296, -0.0768,  ...,  0.0108,  0.0631,  0.0503],\n",
      "        ...,\n",
      "        [ 0.0302,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1055],\n",
      "        [ 0.0302,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1055],\n",
      "        [ 0.0302,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0079, -0.1110,  0.0623,  ...,  0.0182,  0.0935, -0.0101],\n",
      "        [-0.0976, -0.0523,  0.0563,  ...,  0.0225, -0.0650,  0.0970],\n",
      "        [ 0.0302,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0302,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0302,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0302,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0302,  0.0174,  0.1097,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0302,  0.0174,  0.1097,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0302,  0.0174,  0.1097,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        ...,\n",
      "        [-0.0977, -0.0523,  0.0563,  ...,  0.0225, -0.0649,  0.0970],\n",
      "        [ 0.0302,  0.0174,  0.1097,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0302,  0.0174,  0.1097,  ..., -0.0802,  0.1182,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [-0.0977, -0.0524,  0.0563,  ...,  0.0225, -0.0649,  0.0969],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0118,  0.1075, -0.0120,  ...,  0.0793,  0.0513, -0.0859],\n",
      "        [-0.0118,  0.1075, -0.0120,  ...,  0.0793,  0.0513, -0.0859],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0803,  0.1182,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0803,  0.1182,  0.1056],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0803,  0.1182,  0.1056],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0803,  0.1182,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0003, -0.0297, -0.0768,  ...,  0.0108,  0.0631,  0.0504],\n",
      "        [ 0.0003, -0.0297, -0.0768,  ...,  0.0108,  0.0631,  0.0504],\n",
      "        [ 0.0003, -0.0297, -0.0768,  ...,  0.0108,  0.0631,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0082, -0.1109,  0.0626,  ...,  0.0178,  0.0937, -0.0099],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0803,  0.1181,  0.1056],\n",
      "        [ 0.0813, -0.0407, -0.0882,  ..., -0.0792, -0.0893, -0.1111]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0300,  0.0174,  0.1096,  ..., -0.0804,  0.1181,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0804,  0.1181,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0804,  0.1181,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0804,  0.1181,  0.1056],\n",
      "        [ 0.0082, -0.1109,  0.0626,  ...,  0.0178,  0.0938, -0.0099],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0804,  0.1181,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0002, -0.0298, -0.0769,  ...,  0.0107,  0.0631,  0.0504],\n",
      "        [ 0.0002, -0.0298, -0.0769,  ...,  0.0107,  0.0631,  0.0504],\n",
      "        [ 0.0002, -0.0298, -0.0769,  ...,  0.0107,  0.0631,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0804,  0.1181,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0804,  0.1181,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0804,  0.1181,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0300,  0.0174,  0.1096,  ..., -0.0805,  0.1181,  0.1055],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0805,  0.1181,  0.1055],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0805,  0.1181,  0.1055],\n",
      "        ...,\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0805,  0.1181,  0.1055],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0805,  0.1181,  0.1055],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0805,  0.1181,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0174,  0.1097,  ..., -0.0805,  0.1181,  0.1055],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0805,  0.1181,  0.1055],\n",
      "        [ 0.0001, -0.0298, -0.0769,  ...,  0.0107,  0.0631,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0805,  0.1181,  0.1055],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0805,  0.1181,  0.1055],\n",
      "        [ 0.0900,  0.0317,  0.0689,  ...,  0.0427, -0.0067,  0.0771]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "1 21 1.6086289882659912\n",
      "tensor([[ 0.0299,  0.0174,  0.1097,  ..., -0.0806,  0.1181,  0.1055],\n",
      "        [ 0.0083, -0.1108,  0.0627,  ...,  0.0176,  0.0939, -0.0098],\n",
      "        [ 0.0083, -0.1108,  0.0627,  ...,  0.0176,  0.0939, -0.0098],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0806,  0.1181,  0.1055],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0806,  0.1181,  0.1055],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0806,  0.1181,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0174,  0.1097,  ..., -0.0806,  0.1181,  0.1056],\n",
      "        [-0.0983, -0.0523,  0.0559,  ...,  0.0224, -0.0645,  0.0968],\n",
      "        [-0.0983, -0.0523,  0.0559,  ...,  0.0224, -0.0645,  0.0968],\n",
      "        ...,\n",
      "        [ 0.0083, -0.1107,  0.0627,  ...,  0.0175,  0.0939, -0.0097],\n",
      "        [-0.0983, -0.0523,  0.0559,  ...,  0.0224, -0.0645,  0.0968],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0806,  0.1181,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0174,  0.1097,  ..., -0.0807,  0.1182,  0.1056],\n",
      "        [ 0.0002, -0.0300, -0.0769,  ...,  0.0107,  0.0631,  0.0505],\n",
      "        [ 0.0002, -0.0300, -0.0769,  ...,  0.0107,  0.0631,  0.0505],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0807,  0.1182,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0807,  0.1182,  0.1056],\n",
      "        [ 0.0901,  0.0316,  0.0689,  ...,  0.0427, -0.0068,  0.0770]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0174,  0.1097,  ..., -0.0807,  0.1182,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0807,  0.1182,  0.1056],\n",
      "        [ 0.0003, -0.0300, -0.0769,  ...,  0.0106,  0.0631,  0.0506],\n",
      "        ...,\n",
      "        [ 0.0901,  0.0316,  0.0689,  ...,  0.0427, -0.0069,  0.0771],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0807,  0.1182,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0807,  0.1182,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0174,  0.1097,  ..., -0.0808,  0.1182,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0808,  0.1182,  0.1056],\n",
      "        [-0.0116,  0.1073, -0.0120,  ...,  0.0794,  0.0513, -0.0860],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0808,  0.1182,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0808,  0.1182,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0808,  0.1182,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0986, -0.0523,  0.0560,  ...,  0.0223, -0.0644,  0.0966],\n",
      "        [ 0.0299,  0.0175,  0.1097,  ..., -0.0808,  0.1182,  0.1056],\n",
      "        [ 0.0299,  0.0175,  0.1097,  ..., -0.0808,  0.1182,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0175,  0.1097,  ..., -0.0808,  0.1182,  0.1056],\n",
      "        [ 0.0299,  0.0175,  0.1097,  ..., -0.0808,  0.1182,  0.1056],\n",
      "        [ 0.0299,  0.0175,  0.1097,  ..., -0.0808,  0.1182,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0082, -0.1106,  0.0628,  ...,  0.0174,  0.0939, -0.0098],\n",
      "        [-0.0987, -0.0523,  0.0560,  ...,  0.0223, -0.0644,  0.0966],\n",
      "        [ 0.0004, -0.0301, -0.0769,  ...,  0.0105,  0.0631,  0.0508],\n",
      "        ...,\n",
      "        [ 0.0004, -0.0301, -0.0769,  ...,  0.0105,  0.0631,  0.0508],\n",
      "        [ 0.0004, -0.0301, -0.0769,  ...,  0.0105,  0.0631,  0.0508],\n",
      "        [ 0.0004, -0.0301, -0.0769,  ...,  0.0105,  0.0631,  0.0508]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Validation loss: 17.010034324848547 RMSE: 4.1243224\n",
      "tensor([[ 0.0004, -0.0302, -0.0769,  ...,  0.0105,  0.0631,  0.0508],\n",
      "        [-0.0115,  0.1073, -0.0120,  ...,  0.0794,  0.0512, -0.0860],\n",
      "        [ 0.0299,  0.0175,  0.1096,  ..., -0.0809,  0.1182,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0082, -0.1106,  0.0628,  ...,  0.0174,  0.0939, -0.0097],\n",
      "        [ 0.0299,  0.0175,  0.1096,  ..., -0.0809,  0.1182,  0.1056],\n",
      "        [ 0.0299,  0.0175,  0.1096,  ..., -0.0809,  0.1182,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0175,  0.1096,  ..., -0.0809,  0.1181,  0.1056],\n",
      "        [ 0.0004, -0.0302, -0.0769,  ...,  0.0105,  0.0631,  0.0508],\n",
      "        [ 0.0004, -0.0302, -0.0769,  ...,  0.0105,  0.0631,  0.0508],\n",
      "        ...,\n",
      "        [-0.0989, -0.0523,  0.0560,  ...,  0.0224, -0.0644,  0.0966],\n",
      "        [ 0.0299,  0.0175,  0.1096,  ..., -0.0809,  0.1181,  0.1056],\n",
      "        [ 0.0299,  0.0175,  0.1096,  ..., -0.0809,  0.1181,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0175,  0.1096,  ..., -0.0810,  0.1181,  0.1056],\n",
      "        [ 0.0004, -0.0302, -0.0769,  ...,  0.0105,  0.0631,  0.0508],\n",
      "        [ 0.0004, -0.0302, -0.0769,  ...,  0.0105,  0.0631,  0.0508],\n",
      "        ...,\n",
      "        [-0.0989, -0.0523,  0.0560,  ...,  0.0225, -0.0644,  0.0966],\n",
      "        [ 0.0299,  0.0175,  0.1096,  ..., -0.0810,  0.1181,  0.1056],\n",
      "        [-0.0115,  0.1073, -0.0119,  ...,  0.0795,  0.0512, -0.0860]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0174,  0.1096,  ..., -0.0810,  0.1181,  0.1056],\n",
      "        [ 0.0004, -0.0303, -0.0770,  ...,  0.0104,  0.0630,  0.0508],\n",
      "        [ 0.0004, -0.0303, -0.0770,  ...,  0.0104,  0.0630,  0.0508],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0810,  0.1181,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0810,  0.1181,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0810,  0.1181,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056],\n",
      "        [-0.0115,  0.1073, -0.0118,  ...,  0.0795,  0.0512, -0.0859],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0300,  0.0174,  0.1096,  ..., -0.0811,  0.1179,  0.1056],\n",
      "        [-0.0993, -0.0524,  0.0558,  ...,  0.0228, -0.0645,  0.0966],\n",
      "        [ 0.0004, -0.0304, -0.0772,  ...,  0.0104,  0.0629,  0.0507],\n",
      "        ...,\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0811,  0.1179,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0811,  0.1179,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0811,  0.1179,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1179,  0.1056],\n",
      "        [-0.0115,  0.1073, -0.0117,  ...,  0.0795,  0.0512, -0.0859],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1179,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1179,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1179,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1179,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0994, -0.0524,  0.0558,  ...,  0.0229, -0.0646,  0.0966],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1179,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1179,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1179,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1179,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1179,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1178,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1178,  0.1056],\n",
      "        [ 0.0003, -0.0305, -0.0773,  ...,  0.0104,  0.0628,  0.0506],\n",
      "        ...,\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1178,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1178,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0995, -0.0524,  0.0558,  ...,  0.0229, -0.0646,  0.0966],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        ...,\n",
      "        [-0.0995, -0.0524,  0.0558,  ...,  0.0229, -0.0646,  0.0966],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0174,  0.1097,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0004, -0.0305, -0.0774,  ...,  0.0104,  0.0628,  0.0506],\n",
      "        [ 0.0004, -0.0305, -0.0774,  ...,  0.0104,  0.0628,  0.0506],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0165,  0.1185, -0.0105,  ...,  0.0828,  0.0629,  0.0487],\n",
      "        [ 0.0299,  0.0174,  0.1098,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0004, -0.0305, -0.0774,  ...,  0.0104,  0.0627,  0.0506],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0174,  0.1098,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1098,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1098,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0115,  0.1073, -0.0117,  ...,  0.0795,  0.0511, -0.0859],\n",
      "        [ 0.0299,  0.0174,  0.1098,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [-0.0997, -0.0524,  0.0558,  ...,  0.0230, -0.0647,  0.0966],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0174,  0.1098,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1098,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1098,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0298,  0.0175,  0.1099,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        ...,\n",
      "        [-0.0998, -0.0524,  0.0558,  ...,  0.0230, -0.0647,  0.0966],\n",
      "        [ 0.0234,  0.0589, -0.1091,  ...,  0.0714,  0.0753,  0.0714],\n",
      "        [ 0.0808, -0.0412, -0.0879,  ..., -0.0785, -0.0889, -0.1103]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0298,  0.0175,  0.1100,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1100,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0004, -0.0305, -0.0774,  ...,  0.0103,  0.0626,  0.0507],\n",
      "        ...,\n",
      "        [ 0.0298,  0.0175,  0.1100,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1100,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1100,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0298,  0.0175,  0.1100,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1100,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1100,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        ...,\n",
      "        [-0.0998, -0.0524,  0.0558,  ...,  0.0230, -0.0647,  0.0966],\n",
      "        [ 0.0298,  0.0175,  0.1100,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1100,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0005, -0.0305, -0.0774,  ...,  0.0103,  0.0626,  0.0507],\n",
      "        [ 0.0005, -0.0305, -0.0774,  ...,  0.0103,  0.0626,  0.0507],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0005, -0.0306, -0.0774,  ...,  0.0102,  0.0626,  0.0507],\n",
      "        [ 0.0005, -0.0306, -0.0774,  ...,  0.0102,  0.0626,  0.0507],\n",
      "        [ 0.0005, -0.0306, -0.0774,  ...,  0.0102,  0.0626,  0.0507],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.1000, -0.0523,  0.0557,  ...,  0.0230, -0.0646,  0.0964],\n",
      "        [ 0.0005, -0.0306, -0.0775,  ...,  0.0103,  0.0625,  0.0507],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0806, -0.0413, -0.0879,  ..., -0.0785, -0.0888, -0.1102],\n",
      "        [-0.1000, -0.0523,  0.0557,  ...,  0.0230, -0.0646,  0.0964],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0005, -0.0305, -0.0775,  ...,  0.0103,  0.0625,  0.0507],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0005, -0.0305, -0.0776,  ...,  0.0103,  0.0625,  0.0507],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [ 0.0005, -0.0305, -0.0776,  ...,  0.0103,  0.0625,  0.0507],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [-0.1001, -0.0524,  0.0557,  ...,  0.0231, -0.0646,  0.0963],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0112,  0.1071, -0.0116,  ...,  0.0795,  0.0511, -0.0860],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [ 0.0087, -0.1107,  0.0631,  ...,  0.0173,  0.0944, -0.0097],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [-0.1001, -0.0524,  0.0557,  ...,  0.0232, -0.0646,  0.0963],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.1001, -0.0524,  0.0556,  ...,  0.0232, -0.0645,  0.0963],\n",
      "        [-0.1001, -0.0524,  0.0556,  ...,  0.0232, -0.0645,  0.0963],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [ 0.0005, -0.0306, -0.0777,  ...,  0.0103,  0.0624,  0.0507],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [-0.1002, -0.0524,  0.0556,  ...,  0.0232, -0.0645,  0.0962],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1177,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.1002, -0.0524,  0.0556,  ...,  0.0233, -0.0645,  0.0962],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1177,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1176,  0.1056],\n",
      "        [ 0.0005, -0.0305, -0.0778,  ...,  0.0104,  0.0624,  0.0506],\n",
      "        [ 0.0089, -0.1107,  0.0631,  ...,  0.0172,  0.0945, -0.0097],\n",
      "        ...,\n",
      "        [-0.1002, -0.0524,  0.0556,  ...,  0.0233, -0.0645,  0.0962],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1176,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Validation loss: 3.044654910543324 RMSE: 1.744894\n",
      "tensor([[ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1176,  0.1056],\n",
      "        ...,\n",
      "        [-0.1002, -0.0523,  0.0556,  ...,  0.0233, -0.0645,  0.0962],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1176,  0.1056],\n",
      "        [ 0.0089, -0.1107,  0.0632,  ...,  0.0171,  0.0946, -0.0098]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0005, -0.0306, -0.0779,  ...,  0.0104,  0.0623,  0.0507],\n",
      "        [ 0.0005, -0.0306, -0.0779,  ...,  0.0104,  0.0623,  0.0507],\n",
      "        [ 0.0297,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0005, -0.0306, -0.0779,  ...,  0.0105,  0.0622,  0.0507],\n",
      "        [ 0.0005, -0.0306, -0.0779,  ...,  0.0105,  0.0622,  0.0507],\n",
      "        [ 0.0005, -0.0306, -0.0779,  ...,  0.0105,  0.0622,  0.0507],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0297,  0.0175,  0.1100,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0175,  0.1100,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0175,  0.1100,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0175,  0.1100,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        [-0.1002, -0.0523,  0.0556,  ...,  0.0233, -0.0645,  0.0962],\n",
      "        [ 0.0297,  0.0175,  0.1100,  ..., -0.0811,  0.1176,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0297,  0.0175,  0.1099,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        [-0.1002, -0.0523,  0.0556,  ...,  0.0233, -0.0645,  0.0962],\n",
      "        [ 0.0005, -0.0306, -0.0779,  ...,  0.0105,  0.0621,  0.0507],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0175,  0.1099,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0175,  0.1099,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0175,  0.1099,  ..., -0.0811,  0.1176,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0113,  0.1071, -0.0114,  ...,  0.0795,  0.0511, -0.0859],\n",
      "        [ 0.0297,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [ 0.0005, -0.0306, -0.0779,  ...,  0.0105,  0.0621,  0.0507],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0297,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [ 0.0005, -0.0306, -0.0779,  ...,  0.0105,  0.0621,  0.0507],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [-0.1003, -0.0524,  0.0556,  ...,  0.0234, -0.0646,  0.0962],\n",
      "        [ 0.0297,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0909,  0.0324,  0.0702,  ...,  0.0420, -0.0075,  0.0781],\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0298,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [-0.1003, -0.0525,  0.0556,  ...,  0.0234, -0.0645,  0.0961],\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0298,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1055],\n",
      "        ...,\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1055],\n",
      "        [-0.1004, -0.0525,  0.0556,  ...,  0.0234, -0.0645,  0.0961]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.1004, -0.0525,  0.0556,  ...,  0.0234, -0.0645,  0.0961],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1176,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1176,  0.1055],\n",
      "        ...,\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1176,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1176,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1176,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0005, -0.0305, -0.0779,  ...,  0.0106,  0.0620,  0.0506],\n",
      "        ...,\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        ...,\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [-0.1004, -0.0525,  0.0556,  ...,  0.0234, -0.0643,  0.0961],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        ...,\n",
      "        [ 0.0088, -0.1103,  0.0630,  ...,  0.0169,  0.0950, -0.0102],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "3 13 1.791797399520874\n",
      "tensor([[ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0239,  0.0583, -0.1088,  ...,  0.0713,  0.0755,  0.0716],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        ...,\n",
      "        [ 0.0089, -0.1103,  0.0630,  ...,  0.0169,  0.0951, -0.0102],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0910,  0.0325,  0.0701,  ...,  0.0420, -0.0072,  0.0780],\n",
      "        [ 0.0005, -0.0306, -0.0780,  ...,  0.0108,  0.0618,  0.0505],\n",
      "        [ 0.0005, -0.0306, -0.0780,  ...,  0.0108,  0.0618,  0.0505],\n",
      "        ...,\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1054],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1054],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1054]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0111,  0.1072, -0.0113,  ...,  0.0795,  0.0512, -0.0859],\n",
      "        [-0.1005, -0.0524,  0.0556,  ...,  0.0235, -0.0642,  0.0960],\n",
      "        [ 0.0298,  0.0176,  0.1101,  ..., -0.0810,  0.1174,  0.1054],\n",
      "        ...,\n",
      "        [ 0.0298,  0.0176,  0.1101,  ..., -0.0810,  0.1174,  0.1054],\n",
      "        [ 0.0298,  0.0176,  0.1101,  ..., -0.0810,  0.1174,  0.1054],\n",
      "        [ 0.0298,  0.0176,  0.1101,  ..., -0.0810,  0.1174,  0.1054]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1174,  0.1054],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1174,  0.1054],\n",
      "        [ 0.0005, -0.0306, -0.0780,  ...,  0.0108,  0.0617,  0.0505],\n",
      "        ...,\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1174,  0.1054],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1174,  0.1054],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1174,  0.1054]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n"
     ]
    }
   ],
   "source": [
    "seeds = [range(777,782)]\n",
    "for seed in seeds:\n",
    "    !python finetuneRecon.py \\\n",
    "    --task_name bbbp \\\n",
    "    --splitting scaffold \\\n",
    "    --seed {seed}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce4ecd8-eaee-4c19-8a8d-195f56a1af16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 100, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:0', 'task_name': 'esol', 'model_type': 'gin', 'model': {'num_layer': 5, 'emb_dim': 300, 'feat_dim': 300, 'drop_ratio': 0.3, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 750, 'task': 'regression', 'data_path': 'data/esol/esol.csv', 'target': ['measured log solubility in mols per litre']}}\n",
      "Running on: cuda:0\n",
      "1127\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/1127\n",
      "Generating scaffold 1000/1127\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "1.6330338716506958\n",
      "0 0 21.223649978637695\n",
      "1.3430227041244507\n",
      "1.136906385421753\n",
      "1.1001787185668945\n",
      "1.125718116760254\n",
      "0.8750820159912109\n",
      "1.078762412071228\n",
      "0.9390501379966736\n",
      "0.877404510974884\n",
      "0.7573410868644714\n",
      "0.6769704818725586\n",
      "0.8229671716690063\n",
      "0.8603051900863647\n",
      "0.8617355823516846\n",
      "0.729049801826477\n",
      "0.7414910793304443\n",
      "0.6642038226127625\n",
      "0.7754987478256226\n",
      "0.6943560838699341\n",
      "0.6867797374725342\n",
      "0.8501588702201843\n"
     ]
    }
   ],
   "source": [
    "!python finetuneRecon.py \\\n",
    "--task_name esol \\\n",
    "--splitting scaffold \\\n",
    "--seed 750 \\\n",
    "--random_masking 1 \\\n",
    "--epochs 100 \\\n",
    "--mask_edge 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c350a18-74eb-42bb-ac32-ea54a4810977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tf/MolCLR-master - copy2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1537fe0-447a-43d8-90e0-6fa2e27ae65f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49b0a6b-aa9d-4b97-badc-959d13879e14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
