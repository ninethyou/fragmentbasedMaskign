{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cbc95c-8c82-4b39-9acb-eac3d7dea845",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:1', 'task_name': 'freesolv', 'model_type': 'gin', 'model': {'num_layer': 3, 'emb_dim': 64, 'feat_dim': 64, 'drop_ratio': 0.5, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 777, 'task': 'regression', 'data_path': 'data/freesolv/freesolv.csv', 'target': ['expt']}}\n",
      "Running on: cuda:1\n",
      "641\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/641\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 25.32749366760254\n",
      "Validation loss: 64.9162826538086 RMSE: 8.057064\n",
      "Validation loss: 59.7149543762207 RMSE: 7.727545\n",
      "Validation loss: 54.378822326660156 RMSE: 7.3742\n",
      "3 2 5.207518100738525\n",
      "Validation loss: 49.719566345214844 RMSE: 7.0512104\n",
      "Validation loss: 44.988759994506836 RMSE: 6.7073665\n",
      "Validation loss: 46.956520080566406 RMSE: 6.852483\n",
      "6 4 4.574179172515869\n",
      "Validation loss: 46.97561073303223 RMSE: 6.8538756\n",
      "Validation loss: 45.60436820983887 RMSE: 6.753101\n",
      "Validation loss: 50.537498474121094 RMSE: 7.108973\n",
      "9 6 5.422024726867676\n",
      "Validation loss: 43.50664710998535 RMSE: 6.595957\n",
      "Validation loss: 45.60093116760254 RMSE: 6.7528462\n",
      "Validation loss: 43.3640079498291 RMSE: 6.5851355\n",
      "12 8 6.233519554138184\n",
      "Validation loss: 42.54990005493164 RMSE: 6.5230284\n",
      "Validation loss: 46.59244346618652 RMSE: 6.8258657\n",
      "Validation loss: 44.11182403564453 RMSE: 6.641673\n",
      "15 10 6.070601940155029\n",
      "Validation loss: 44.26741600036621 RMSE: 6.6533766\n",
      "Validation loss: 40.890342712402344 RMSE: 6.3945556\n",
      "Validation loss: 41.8270206451416 RMSE: 6.4673815\n",
      "18 12 4.269430637359619\n",
      "Validation loss: 41.126108169555664 RMSE: 6.412964\n",
      "Validation loss: 46.69555854797363 RMSE: 6.833415\n",
      "Validation loss: 43.17250442504883 RMSE: 6.5705786\n",
      "21 14 3.625882625579834\n",
      "Validation loss: 46.73782920837402 RMSE: 6.8365073\n",
      "Validation loss: 43.581403732299805 RMSE: 6.6016216\n",
      "Validation loss: 44.47175121307373 RMSE: 6.6687145\n",
      "Validation loss: 42.491336822509766 RMSE: 6.5185375\n",
      "25 0 5.046657562255859\n",
      "Validation loss: 46.67631721496582 RMSE: 6.8320065\n",
      "Validation loss: 44.27866744995117 RMSE: 6.654222\n",
      "Validation loss: 44.13016605377197 RMSE: 6.643054\n",
      "28 2 2.769848108291626\n",
      "Validation loss: 44.03856945037842 RMSE: 6.6361566\n",
      "Validation loss: 45.573503494262695 RMSE: 6.750815\n",
      "Validation loss: 44.4059944152832 RMSE: 6.663782\n",
      "31 4 4.665074348449707\n",
      "Validation loss: 44.77754783630371 RMSE: 6.6916027\n",
      "Validation loss: 44.06366539001465 RMSE: 6.6380467\n",
      "Validation loss: 41.25924873352051 RMSE: 6.423336\n",
      "34 6 4.162790775299072\n",
      "Validation loss: 47.326175689697266 RMSE: 6.8794026\n",
      "Validation loss: 44.35481834411621 RMSE: 6.6599417\n",
      "Validation loss: 42.54644966125488 RMSE: 6.5227637\n",
      "37 8 3.2433083057403564\n",
      "Validation loss: 45.04207992553711 RMSE: 6.7113395\n",
      "Validation loss: 42.91950702667236 RMSE: 6.551298\n",
      "Validation loss: 38.99664878845215 RMSE: 6.24473\n",
      "40 10 2.2088260650634766\n",
      "Validation loss: 45.8753776550293 RMSE: 6.773136\n",
      "Validation loss: 44.76688194274902 RMSE: 6.690806\n",
      "Validation loss: 43.18782424926758 RMSE: 6.571744\n",
      "43 12 2.2251765727996826\n",
      "Validation loss: 41.6361198425293 RMSE: 6.4526057\n",
      "Validation loss: 38.688209533691406 RMSE: 6.2199845\n",
      "Validation loss: 44.39978790283203 RMSE: 6.6633167\n",
      "46 14 4.151386737823486\n",
      "Validation loss: 42.84095764160156 RMSE: 6.5453005\n",
      "Validation loss: 41.28170204162598 RMSE: 6.4250836\n",
      "Validation loss: 42.432809829711914 RMSE: 6.514047\n",
      "Validation loss: 42.18821144104004 RMSE: 6.495245\n",
      "50 0 4.436469078063965\n",
      "Validation loss: 43.81490898132324 RMSE: 6.619283\n",
      "Validation loss: 43.096900939941406 RMSE: 6.564823\n",
      "Validation loss: 38.92062759399414 RMSE: 6.2386394\n",
      "53 2 1.9085361957550049\n",
      "Validation loss: 38.11320877075195 RMSE: 6.1735897\n",
      "Validation loss: 39.528947830200195 RMSE: 6.2872047\n",
      "Validation loss: 37.702646255493164 RMSE: 6.140248\n",
      "56 4 1.8665415048599243\n",
      "Validation loss: 39.61701011657715 RMSE: 6.2942047\n",
      "Validation loss: 37.31828022003174 RMSE: 6.108869\n",
      "Validation loss: 40.87713050842285 RMSE: 6.3935223\n",
      "59 6 3.312164068222046\n",
      "Validation loss: 35.64491081237793 RMSE: 5.970336\n",
      "Validation loss: 40.570207595825195 RMSE: 6.369475\n",
      "Validation loss: 36.155025482177734 RMSE: 6.012905\n",
      "62 8 2.1154117584228516\n",
      "Validation loss: 36.217891693115234 RMSE: 6.0181303\n",
      "Validation loss: 38.668073654174805 RMSE: 6.218366\n",
      "Validation loss: 36.96232223510742 RMSE: 6.0796647\n",
      "65 10 2.0257391929626465\n",
      "Validation loss: 38.21982192993164 RMSE: 6.182218\n",
      "Validation loss: 36.7620792388916 RMSE: 6.0631742\n",
      "Validation loss: 39.79738998413086 RMSE: 6.3085175\n",
      "68 12 1.9902441501617432\n",
      "Validation loss: 36.696529388427734 RMSE: 6.057766\n",
      "Validation loss: 38.35942363739014 RMSE: 6.1934986\n",
      "Validation loss: 34.44078540802002 RMSE: 5.8686275\n",
      "71 14 2.254169225692749\n",
      "Validation loss: 33.491899490356445 RMSE: 5.7872186\n",
      "Validation loss: 40.32985496520996 RMSE: 6.3505793\n",
      "Validation loss: 38.049217224121094 RMSE: 6.1684046\n",
      "Validation loss: 35.4180793762207 RMSE: 5.951309\n",
      "75 0 1.8885910511016846\n",
      "Validation loss: 36.57289791107178 RMSE: 6.0475526\n",
      "Validation loss: 35.82363700866699 RMSE: 5.9852853\n",
      "Validation loss: 34.353187561035156 RMSE: 5.8611593\n",
      "78 2 1.399882435798645\n",
      "Validation loss: 35.06865119934082 RMSE: 5.9218793\n",
      "Validation loss: 34.15359401702881 RMSE: 5.8441076\n",
      "Validation loss: 36.092960357666016 RMSE: 6.007742\n",
      "81 4 1.4544963836669922\n",
      "Validation loss: 32.82872009277344 RMSE: 5.7296357\n",
      "Validation loss: 31.16120195388794 RMSE: 5.582222\n",
      "Validation loss: 33.27929210662842 RMSE: 5.768821\n",
      "84 6 2.2347962856292725\n",
      "Validation loss: 33.066368103027344 RMSE: 5.750336\n",
      "Validation loss: 35.09730911254883 RMSE: 5.924298\n",
      "Validation loss: 34.80057716369629 RMSE: 5.8992014\n",
      "87 8 2.409445285797119\n",
      "Validation loss: 32.26055145263672 RMSE: 5.679837\n",
      "Validation loss: 35.46672439575195 RMSE: 5.9553943\n",
      "Validation loss: 36.634843826293945 RMSE: 6.0526724\n",
      "90 10 0.9761209487915039\n",
      "Validation loss: 35.27159881591797 RMSE: 5.9389896\n",
      "Validation loss: 34.12389659881592 RMSE: 5.841566\n",
      "Validation loss: 33.73309326171875 RMSE: 5.8080196\n",
      "93 12 2.07479190826416\n",
      "Validation loss: 35.22262954711914 RMSE: 5.9348655\n",
      "Validation loss: 32.350335121154785 RMSE: 5.6877356\n",
      "Validation loss: 33.82548809051514 RMSE: 5.815968\n",
      "96 14 2.5246737003326416\n",
      "Validation loss: 34.235154151916504 RMSE: 5.8510814\n",
      "Validation loss: 34.30763053894043 RMSE: 5.857271\n",
      "Validation loss: 35.39918899536133 RMSE: 5.9497223\n",
      "Validation loss: 32.942240715026855 RMSE: 5.7395334\n",
      "100 0 0.7367951273918152\n",
      "Validation loss: 31.03822612762451 RMSE: 5.571196\n",
      "Validation loss: 33.57386589050293 RMSE: 5.7942963\n",
      "Validation loss: 31.421411514282227 RMSE: 5.6054807\n",
      "103 2 2.852728843688965\n",
      "Validation loss: 32.83754062652588 RMSE: 5.730405\n",
      "Validation loss: 31.342817306518555 RMSE: 5.5984654\n",
      "Validation loss: 32.03880977630615 RMSE: 5.6602836\n",
      "106 4 1.4087629318237305\n",
      "Validation loss: 33.269094467163086 RMSE: 5.7679367\n",
      "Validation loss: 34.7171573638916 RMSE: 5.8921266\n",
      "Validation loss: 34.40304756164551 RMSE: 5.865411\n",
      "109 6 2.9709219932556152\n",
      "Validation loss: 31.883686065673828 RMSE: 5.646564\n",
      "Validation loss: 32.802700996398926 RMSE: 5.727364\n",
      "Validation loss: 32.7764892578125 RMSE: 5.7250752\n",
      "112 8 2.5456950664520264\n",
      "Validation loss: 30.858558654785156 RMSE: 5.555048\n",
      "Validation loss: 30.624208450317383 RMSE: 5.5339146\n",
      "Validation loss: 33.131813049316406 RMSE: 5.756024\n",
      "115 10 1.379805564880371\n",
      "Validation loss: 32.100860595703125 RMSE: 5.665762\n",
      "Validation loss: 34.03514385223389 RMSE: 5.833965\n",
      "Validation loss: 32.94185447692871 RMSE: 5.7394996\n",
      "118 12 2.936830997467041\n",
      "Validation loss: 32.60136318206787 RMSE: 5.70976\n",
      "Validation loss: 33.14593696594238 RMSE: 5.757251\n",
      "Validation loss: 32.64215087890625 RMSE: 5.7133307\n",
      "121 14 1.88895583152771\n",
      "Validation loss: 33.63339424133301 RMSE: 5.7994304\n",
      "Validation loss: 28.59334659576416 RMSE: 5.347275\n",
      "Validation loss: 30.791010856628418 RMSE: 5.548965\n",
      "Validation loss: 30.928641319274902 RMSE: 5.5613527\n",
      "125 0 1.818678855895996\n",
      "Validation loss: 28.231958389282227 RMSE: 5.3133755\n",
      "Validation loss: 32.096455574035645 RMSE: 5.6653733\n",
      "Validation loss: 34.181057929992676 RMSE: 5.846457\n",
      "128 2 2.0560977458953857\n",
      "Validation loss: 30.014419555664062 RMSE: 5.478542\n",
      "Validation loss: 34.89939594268799 RMSE: 5.907571\n",
      "Validation loss: 31.418935775756836 RMSE: 5.60526\n",
      "131 4 2.9875755310058594\n",
      "Validation loss: 31.27010726928711 RMSE: 5.591968\n",
      "Validation loss: 30.093408584594727 RMSE: 5.485746\n",
      "Validation loss: 31.27511978149414 RMSE: 5.5924163\n",
      "134 6 2.26454758644104\n",
      "Validation loss: 32.54837226867676 RMSE: 5.7051177\n",
      "Validation loss: 29.592604637145996 RMSE: 5.4399085\n",
      "Validation loss: 32.010873794555664 RMSE: 5.657815\n",
      "137 8 1.9432646036148071\n",
      "Validation loss: 29.98125457763672 RMSE: 5.475514\n",
      "Validation loss: 26.344572067260742 RMSE: 5.132696\n",
      "Validation loss: 29.21080207824707 RMSE: 5.4047017\n",
      "140 10 1.4993760585784912\n",
      "Validation loss: 28.61625099182129 RMSE: 5.3494163\n",
      "Validation loss: 30.106727600097656 RMSE: 5.4869595\n",
      "Validation loss: 26.015274047851562 RMSE: 5.1005173\n",
      "143 12 0.9645984172821045\n",
      "Validation loss: 30.356802940368652 RMSE: 5.5097013\n",
      "Validation loss: 31.990906715393066 RMSE: 5.65605\n",
      "Validation loss: 29.776939392089844 RMSE: 5.456825\n",
      "146 14 1.9859585762023926\n",
      "Validation loss: 29.976655960083008 RMSE: 5.4750943\n",
      "Validation loss: 27.20369052886963 RMSE: 5.2157154\n",
      "Validation loss: 26.628793716430664 RMSE: 5.1603093\n",
      "Validation loss: 26.10940170288086 RMSE: 5.109736\n",
      "150 0 2.0364041328430176\n",
      "Validation loss: 28.174699783325195 RMSE: 5.307985\n",
      "Validation loss: 27.367371559143066 RMSE: 5.2313833\n",
      "Validation loss: 26.09061908721924 RMSE: 5.1078978\n",
      "153 2 1.0451046228408813\n",
      "Validation loss: 25.55918025970459 RMSE: 5.0556087\n",
      "Validation loss: 26.988966941833496 RMSE: 5.195091\n",
      "Validation loss: 28.24621295928955 RMSE: 5.3147163\n",
      "156 4 2.8729989528656006\n",
      "Validation loss: 25.17129135131836 RMSE: 5.0171\n",
      "Validation loss: 22.15648651123047 RMSE: 4.7070675\n",
      "Validation loss: 25.08225727081299 RMSE: 5.008219\n",
      "159 6 2.9652457237243652\n",
      "Validation loss: 27.302173614501953 RMSE: 5.2251477\n",
      "Validation loss: 31.173431396484375 RMSE: 5.5833173\n",
      "Validation loss: 27.16072654724121 RMSE: 5.211595\n",
      "162 8 1.1752042770385742\n",
      "Validation loss: 27.0127010345459 RMSE: 5.1973743\n",
      "Validation loss: 26.76992416381836 RMSE: 5.173966\n",
      "Validation loss: 22.811108589172363 RMSE: 4.7760973\n",
      "165 10 1.2971999645233154\n",
      "Validation loss: 28.772652626037598 RMSE: 5.3640146\n",
      "Validation loss: 33.823476791381836 RMSE: 5.8157954\n",
      "Validation loss: 28.34844207763672 RMSE: 5.3243256\n",
      "168 12 2.0247998237609863\n",
      "Validation loss: 28.644712448120117 RMSE: 5.3520756\n",
      "Validation loss: 34.12107849121094 RMSE: 5.8413253\n",
      "Validation loss: 33.679908752441406 RMSE: 5.8034396\n",
      "171 14 1.0151214599609375\n",
      "Validation loss: 31.934293746948242 RMSE: 5.6510434\n",
      "Validation loss: 25.6160888671875 RMSE: 5.061234\n",
      "Validation loss: 26.642580032348633 RMSE: 5.1616454\n",
      "Validation loss: 29.195465087890625 RMSE: 5.4032826\n",
      "175 0 2.2026546001434326\n",
      "Validation loss: 29.797910690307617 RMSE: 5.4587464\n",
      "Validation loss: 26.77226448059082 RMSE: 5.174192\n",
      "Validation loss: 29.027085304260254 RMSE: 5.387679\n",
      "178 2 0.947575032711029\n",
      "Validation loss: 24.366077423095703 RMSE: 4.9362006\n",
      "Validation loss: 27.336174488067627 RMSE: 5.2284007\n",
      "Validation loss: 29.4135684967041 RMSE: 5.4234276\n",
      "181 4 1.150662899017334\n",
      "Validation loss: 25.834423065185547 RMSE: 5.0827575\n",
      "Validation loss: 23.691786766052246 RMSE: 4.867421\n",
      "Validation loss: 29.04976463317871 RMSE: 5.3897834\n",
      "184 6 2.195369243621826\n",
      "Validation loss: 29.4750337600708 RMSE: 5.4290915\n",
      "Validation loss: 28.408571243286133 RMSE: 5.329969\n",
      "Validation loss: 30.50018310546875 RMSE: 5.522697\n",
      "187 8 1.6199175119400024\n",
      "Validation loss: 28.659605026245117 RMSE: 5.3534665\n",
      "Validation loss: 25.968002319335938 RMSE: 5.095881\n",
      "Validation loss: 23.77987003326416 RMSE: 4.8764606\n",
      "190 10 2.1760175228118896\n",
      "Validation loss: 26.842208862304688 RMSE: 5.180947\n",
      "Validation loss: 25.78699779510498 RMSE: 5.07809\n",
      "Validation loss: 27.90509033203125 RMSE: 5.282527\n",
      "193 12 1.170555591583252\n",
      "Validation loss: 22.393924713134766 RMSE: 4.732222\n",
      "Validation loss: 27.239375114440918 RMSE: 5.2191358\n",
      "Validation loss: 26.26656150817871 RMSE: 5.1250916\n",
      "196 14 1.5688042640686035\n",
      "Validation loss: 25.131816864013672 RMSE: 5.013164\n",
      "Validation loss: 21.67676544189453 RMSE: 4.6558313\n",
      "Validation loss: 25.133652687072754 RMSE: 5.0133476\n",
      "Validation loss: 24.149900436401367 RMSE: 4.914255\n",
      "Loaded trained model with success.\n",
      "Test loss: 11.82662834754357 Test RMSE: 3.4389868\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:1', 'task_name': 'freesolv', 'model_type': 'gin', 'model': {'num_layer': 3, 'emb_dim': 64, 'feat_dim': 64, 'drop_ratio': 0.5, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 778, 'task': 'regression', 'data_path': 'data/freesolv/freesolv.csv', 'target': ['expt']}}\n",
      "Running on: cuda:1\n",
      "641\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/641\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 22.677589416503906\n",
      "Validation loss: 71.77839279174805 RMSE: 8.472213\n",
      "Validation loss: 66.5051498413086 RMSE: 8.155069\n",
      "Validation loss: 61.29892158508301 RMSE: 7.829363\n",
      "3 2 14.363476753234863\n",
      "Validation loss: 55.108896255493164 RMSE: 7.4235363\n",
      "Validation loss: 44.56052780151367 RMSE: 6.6753674\n",
      "Validation loss: 34.455759048461914 RMSE: 5.8699026\n",
      "6 4 9.241636276245117\n",
      "Validation loss: 42.60310935974121 RMSE: 6.527106\n",
      "Validation loss: 35.93307304382324 RMSE: 5.9944205\n",
      "Validation loss: 32.297149658203125 RMSE: 5.6830583\n",
      "9 6 4.570958614349365\n",
      "Validation loss: 31.567517280578613 RMSE: 5.618498\n",
      "Validation loss: 34.722373962402344 RMSE: 5.8925695\n",
      "Validation loss: 31.164674758911133 RMSE: 5.5825334\n",
      "12 8 5.992387771606445\n",
      "Validation loss: 29.5706729888916 RMSE: 5.437892\n",
      "Validation loss: 28.70838737487793 RMSE: 5.358021\n",
      "Validation loss: 31.244041442871094 RMSE: 5.589637\n",
      "15 10 6.3862714767456055\n",
      "Validation loss: 29.76585578918457 RMSE: 5.4558096\n",
      "Validation loss: 28.68702793121338 RMSE: 5.356027\n",
      "Validation loss: 26.86979866027832 RMSE: 5.183609\n",
      "18 12 8.590579986572266\n",
      "Validation loss: 24.950613975524902 RMSE: 4.995059\n",
      "Validation loss: 24.752254486083984 RMSE: 4.9751635\n",
      "Validation loss: 24.946144104003906 RMSE: 4.9946117\n",
      "21 14 3.4109511375427246\n",
      "Validation loss: 22.32767963409424 RMSE: 4.7252173\n",
      "Validation loss: 23.886347770690918 RMSE: 4.8873663\n",
      "Validation loss: 23.637601852416992 RMSE: 4.8618517\n",
      "Validation loss: 21.238150596618652 RMSE: 4.608487\n",
      "25 0 4.173303127288818\n",
      "Validation loss: 23.160874366760254 RMSE: 4.8125744\n",
      "Validation loss: 20.430745124816895 RMSE: 4.520038\n",
      "Validation loss: 21.04616641998291 RMSE: 4.58761\n",
      "28 2 7.30307674407959\n",
      "Validation loss: 19.799135208129883 RMSE: 4.449622\n",
      "Validation loss: 20.60782527923584 RMSE: 4.539584\n",
      "Validation loss: 18.75683307647705 RMSE: 4.330916\n",
      "31 4 5.636977672576904\n",
      "Validation loss: 19.0224609375 RMSE: 4.361475\n",
      "Validation loss: 20.539737701416016 RMSE: 4.5320787\n",
      "Validation loss: 20.55807399749756 RMSE: 4.5341015\n",
      "34 6 5.1871843338012695\n",
      "Validation loss: 21.139565467834473 RMSE: 4.5977783\n",
      "Validation loss: 20.745566368103027 RMSE: 4.5547304\n",
      "Validation loss: 21.14596652984619 RMSE: 4.5984745\n",
      "37 8 2.4626381397247314\n",
      "Validation loss: 19.49850845336914 RMSE: 4.4157114\n",
      "Validation loss: 20.85533332824707 RMSE: 4.566764\n",
      "Validation loss: 19.673412322998047 RMSE: 4.435472\n",
      "40 10 3.3436620235443115\n",
      "Validation loss: 21.269070625305176 RMSE: 4.6118407\n",
      "Validation loss: 20.28568935394287 RMSE: 4.5039635\n",
      "Validation loss: 20.323345184326172 RMSE: 4.508142\n",
      "43 12 2.11210298538208\n",
      "Validation loss: 20.474868774414062 RMSE: 4.5249166\n",
      "Validation loss: 21.12019157409668 RMSE: 4.595671\n",
      "Validation loss: 20.123029708862305 RMSE: 4.48587\n",
      "46 14 3.9770970344543457\n",
      "Validation loss: 21.47562026977539 RMSE: 4.6341796\n",
      "Validation loss: 21.558433532714844 RMSE: 4.643106\n",
      "Validation loss: 20.53829288482666 RMSE: 4.531919\n",
      "Validation loss: 22.273162841796875 RMSE: 4.719445\n",
      "50 0 3.1196839809417725\n",
      "Validation loss: 21.645706176757812 RMSE: 4.6524944\n",
      "Validation loss: 21.374622344970703 RMSE: 4.6232696\n",
      "Validation loss: 20.899696350097656 RMSE: 4.5716186\n",
      "53 2 2.332399845123291\n",
      "Validation loss: 22.775750160217285 RMSE: 4.7723947\n",
      "Validation loss: 21.645566940307617 RMSE: 4.6524796\n",
      "Validation loss: 21.729660034179688 RMSE: 4.661508\n",
      "56 4 2.4165945053100586\n",
      "Validation loss: 23.36162757873535 RMSE: 4.833387\n",
      "Validation loss: 24.17809009552002 RMSE: 4.917122\n",
      "Validation loss: 21.703035354614258 RMSE: 4.658652\n",
      "59 6 1.918692946434021\n",
      "Validation loss: 20.140514373779297 RMSE: 4.4878182\n",
      "Validation loss: 20.363862991333008 RMSE: 4.512634\n",
      "Validation loss: 22.444594383239746 RMSE: 4.7375727\n",
      "62 8 2.2421412467956543\n",
      "Validation loss: 20.49919033050537 RMSE: 4.527603\n",
      "Validation loss: 22.028939247131348 RMSE: 4.6934996\n",
      "Validation loss: 21.52849531173706 RMSE: 4.639881\n",
      "65 10 2.1388285160064697\n",
      "Validation loss: 21.468467712402344 RMSE: 4.6334076\n",
      "Validation loss: 23.185561180114746 RMSE: 4.815139\n",
      "Validation loss: 22.671591758728027 RMSE: 4.7614694\n",
      "68 12 2.3643875122070312\n",
      "Validation loss: 20.82676124572754 RMSE: 4.563635\n",
      "Validation loss: 23.735279083251953 RMSE: 4.8718867\n",
      "Validation loss: 24.098294258117676 RMSE: 4.9090014\n",
      "71 14 2.193885564804077\n",
      "Validation loss: 21.98717498779297 RMSE: 4.6890483\n",
      "Validation loss: 21.156968116760254 RMSE: 4.5996704\n",
      "Validation loss: 22.065123558044434 RMSE: 4.697353\n",
      "Validation loss: 22.201608657836914 RMSE: 4.7118583\n",
      "75 0 4.600374221801758\n",
      "Validation loss: 21.95411968231201 RMSE: 4.6855226\n",
      "Validation loss: 20.838793754577637 RMSE: 4.564953\n",
      "Validation loss: 22.82493019104004 RMSE: 4.7775445\n",
      "78 2 2.247486114501953\n",
      "Validation loss: 22.211624145507812 RMSE: 4.712921\n",
      "Validation loss: 24.349145889282227 RMSE: 4.934485\n",
      "Validation loss: 21.138745307922363 RMSE: 4.597689\n",
      "81 4 1.7833738327026367\n",
      "Validation loss: 21.650288581848145 RMSE: 4.652987\n",
      "Validation loss: 19.115867614746094 RMSE: 4.3721695\n",
      "Validation loss: 19.9592866897583 RMSE: 4.4675817\n",
      "84 6 1.7758488655090332\n",
      "Validation loss: 20.36069679260254 RMSE: 4.5122833\n",
      "Validation loss: 18.97042465209961 RMSE: 4.355505\n",
      "Validation loss: 21.987919807434082 RMSE: 4.689128\n",
      "87 8 2.027358293533325\n",
      "Validation loss: 20.389805793762207 RMSE: 4.515507\n",
      "Validation loss: 22.001895904541016 RMSE: 4.690618\n",
      "Validation loss: 19.588322639465332 RMSE: 4.42587\n",
      "90 10 2.9666829109191895\n",
      "Validation loss: 20.769386291503906 RMSE: 4.5573444\n",
      "Validation loss: 20.098713874816895 RMSE: 4.4831586\n",
      "Validation loss: 20.382079124450684 RMSE: 4.5146513\n",
      "93 12 2.1526594161987305\n",
      "Validation loss: 21.496294021606445 RMSE: 4.6364098\n",
      "Validation loss: 21.128122329711914 RMSE: 4.5965333\n",
      "Validation loss: 20.244821548461914 RMSE: 4.499425\n",
      "96 14 1.5087132453918457\n",
      "Validation loss: 20.052727699279785 RMSE: 4.478027\n",
      "Validation loss: 19.12290382385254 RMSE: 4.372974\n",
      "Validation loss: 20.233247756958008 RMSE: 4.498138\n",
      "Validation loss: 19.24984121322632 RMSE: 4.387464\n",
      "100 0 3.3066906929016113\n",
      "Validation loss: 19.407238960266113 RMSE: 4.405365\n",
      "Validation loss: 20.907947540283203 RMSE: 4.5725207\n",
      "Validation loss: 20.88072967529297 RMSE: 4.569544\n",
      "103 2 2.188199758529663\n",
      "Validation loss: 20.359950065612793 RMSE: 4.5122004\n",
      "Validation loss: 22.307454109191895 RMSE: 4.723077\n",
      "Validation loss: 19.552786827087402 RMSE: 4.421853\n",
      "106 4 1.5334713459014893\n",
      "Validation loss: 21.389455795288086 RMSE: 4.6248736\n",
      "Validation loss: 21.142958641052246 RMSE: 4.598147\n",
      "Validation loss: 20.61534595489502 RMSE: 4.540413\n",
      "109 6 2.8374783992767334\n",
      "Validation loss: 20.672616004943848 RMSE: 4.546715\n",
      "Validation loss: 22.70767593383789 RMSE: 4.7652574\n",
      "Validation loss: 21.77277183532715 RMSE: 4.66613\n",
      "112 8 0.919394850730896\n",
      "Validation loss: 20.615124702453613 RMSE: 4.540388\n",
      "Validation loss: 19.499144554138184 RMSE: 4.415784\n",
      "Validation loss: 21.023693084716797 RMSE: 4.5851603\n",
      "115 10 1.4535750150680542\n",
      "Validation loss: 20.085603713989258 RMSE: 4.4816966\n",
      "Validation loss: 20.630553245544434 RMSE: 4.542087\n",
      "Validation loss: 20.256755828857422 RMSE: 4.5007505\n",
      "118 12 2.6398837566375732\n",
      "Validation loss: 20.704116821289062 RMSE: 4.5501776\n",
      "Validation loss: 20.80913734436035 RMSE: 4.561703\n",
      "Validation loss: 20.365382194519043 RMSE: 4.512802\n",
      "121 14 1.7153116464614868\n",
      "Validation loss: 21.35899543762207 RMSE: 4.621579\n",
      "Validation loss: 20.246179580688477 RMSE: 4.499575\n",
      "Validation loss: 21.320526123046875 RMSE: 4.617416\n",
      "Validation loss: 20.55983066558838 RMSE: 4.5342946\n",
      "125 0 2.0609960556030273\n",
      "Validation loss: 19.265899658203125 RMSE: 4.389294\n",
      "Validation loss: 18.228058338165283 RMSE: 4.269433\n",
      "Validation loss: 18.493762969970703 RMSE: 4.3004375\n",
      "128 2 1.902334451675415\n",
      "Validation loss: 20.160743713378906 RMSE: 4.4900713\n",
      "Validation loss: 21.221749305725098 RMSE: 4.606707\n",
      "Validation loss: 20.071946144104004 RMSE: 4.4801726\n",
      "131 4 1.5909143686294556\n",
      "Validation loss: 22.67735481262207 RMSE: 4.762075\n",
      "Validation loss: 18.296546936035156 RMSE: 4.2774463\n",
      "Validation loss: 19.51045036315918 RMSE: 4.4170637\n",
      "134 6 2.8080883026123047\n",
      "Validation loss: 20.260498046875 RMSE: 4.5011663\n",
      "Validation loss: 19.55353355407715 RMSE: 4.421938\n",
      "Validation loss: 22.024985313415527 RMSE: 4.6930785\n",
      "137 8 1.5339837074279785\n",
      "Validation loss: 20.525656700134277 RMSE: 4.5305247\n",
      "Validation loss: 19.490933418273926 RMSE: 4.414854\n",
      "Validation loss: 19.00430393218994 RMSE: 4.3593926\n",
      "140 10 1.5760538578033447\n",
      "Validation loss: 18.43329906463623 RMSE: 4.2934017\n",
      "Validation loss: 19.727571487426758 RMSE: 4.4415727\n",
      "Validation loss: 19.816776275634766 RMSE: 4.4516034\n",
      "143 12 2.895109176635742\n",
      "Validation loss: 18.96310043334961 RMSE: 4.3546643\n",
      "Validation loss: 19.14662742614746 RMSE: 4.375686\n",
      "Validation loss: 18.846527099609375 RMSE: 4.3412585\n",
      "146 14 1.497666358947754\n",
      "Validation loss: 19.48151969909668 RMSE: 4.413788\n",
      "Validation loss: 21.048312187194824 RMSE: 4.587844\n",
      "Validation loss: 18.220285415649414 RMSE: 4.2685227\n",
      "Validation loss: 19.143518447875977 RMSE: 4.375331\n",
      "150 0 2.08203387260437\n",
      "Validation loss: 19.73303508758545 RMSE: 4.442188\n",
      "Validation loss: 19.02157688140869 RMSE: 4.3613734\n",
      "Validation loss: 19.45677089691162 RMSE: 4.410983\n",
      "153 2 1.7078112363815308\n",
      "Validation loss: 18.40024757385254 RMSE: 4.289551\n",
      "Validation loss: 19.41801929473877 RMSE: 4.406588\n",
      "Validation loss: 19.84303855895996 RMSE: 4.4545527\n",
      "156 4 0.9098565578460693\n",
      "Validation loss: 18.509485244750977 RMSE: 4.3022647\n",
      "Validation loss: 20.15244197845459 RMSE: 4.489147\n",
      "Validation loss: 21.63260269165039 RMSE: 4.6510863\n",
      "159 6 1.3009297847747803\n",
      "Validation loss: 19.073654174804688 RMSE: 4.3673396\n",
      "Validation loss: 19.456247329711914 RMSE: 4.4109235\n",
      "Validation loss: 18.216376304626465 RMSE: 4.268065\n",
      "162 8 1.8277697563171387\n",
      "Validation loss: 18.01030445098877 RMSE: 4.243855\n",
      "Validation loss: 18.98503589630127 RMSE: 4.357182\n",
      "Validation loss: 17.623432159423828 RMSE: 4.198027\n",
      "165 10 2.2137651443481445\n",
      "Validation loss: 18.807540893554688 RMSE: 4.3367662\n",
      "Validation loss: 19.580097198486328 RMSE: 4.4249406\n",
      "Validation loss: 19.626901626586914 RMSE: 4.430226\n",
      "168 12 3.205897331237793\n",
      "Validation loss: 18.34328317642212 RMSE: 4.282906\n",
      "Validation loss: 18.296794414520264 RMSE: 4.2774754\n",
      "Validation loss: 17.23554801940918 RMSE: 4.1515718\n",
      "171 14 2.4203476905822754\n",
      "Validation loss: 16.801740646362305 RMSE: 4.098993\n",
      "Validation loss: 17.964670181274414 RMSE: 4.238475\n",
      "Validation loss: 19.31058979034424 RMSE: 4.3943815\n",
      "Validation loss: 18.72946262359619 RMSE: 4.327755\n",
      "175 0 1.9062191247940063\n",
      "Validation loss: 20.43636703491211 RMSE: 4.52066\n",
      "Validation loss: 19.85945987701416 RMSE: 4.456395\n",
      "Validation loss: 20.43521499633789 RMSE: 4.5205326\n",
      "178 2 1.4418041706085205\n",
      "Validation loss: 19.819254875183105 RMSE: 4.451882\n",
      "Validation loss: 18.985793113708496 RMSE: 4.3572693\n",
      "Validation loss: 19.98697280883789 RMSE: 4.4706793\n",
      "181 4 1.5970098972320557\n",
      "Validation loss: 19.787944793701172 RMSE: 4.4483643\n",
      "Validation loss: 19.146173000335693 RMSE: 4.375634\n",
      "Validation loss: 19.59740161895752 RMSE: 4.426895\n",
      "184 6 1.3758958578109741\n",
      "Validation loss: 18.5098934173584 RMSE: 4.3023124\n",
      "Validation loss: 19.205707550048828 RMSE: 4.3824315\n",
      "Validation loss: 19.766204833984375 RMSE: 4.44592\n",
      "187 8 1.6566187143325806\n",
      "Validation loss: 19.600881576538086 RMSE: 4.4272885\n",
      "Validation loss: 20.926823139190674 RMSE: 4.5745845\n",
      "Validation loss: 20.688652992248535 RMSE: 4.548478\n",
      "190 10 1.1213265657424927\n",
      "Validation loss: 19.49834632873535 RMSE: 4.415693\n",
      "Validation loss: 19.60010051727295 RMSE: 4.4272\n",
      "Validation loss: 19.261746406555176 RMSE: 4.3888206\n",
      "193 12 1.3387985229492188\n",
      "Validation loss: 19.818238258361816 RMSE: 4.451768\n",
      "Validation loss: 19.837739944458008 RMSE: 4.453958\n",
      "Validation loss: 21.494921684265137 RMSE: 4.636262\n",
      "196 14 1.3344192504882812\n",
      "Validation loss: 20.271109104156494 RMSE: 4.5023446\n",
      "Validation loss: 19.605143547058105 RMSE: 4.427769\n",
      "Validation loss: 19.06397008895874 RMSE: 4.3662305\n",
      "Validation loss: 19.194424629211426 RMSE: 4.3811445\n",
      "Loaded trained model with success.\n",
      "Test loss: 7.424133337461031 Test RMSE: 2.7247264\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:1', 'task_name': 'freesolv', 'model_type': 'gin', 'model': {'num_layer': 3, 'emb_dim': 64, 'feat_dim': 64, 'drop_ratio': 0.5, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 779, 'task': 'regression', 'data_path': 'data/freesolv/freesolv.csv', 'target': ['expt']}}\n",
      "Running on: cuda:1\n",
      "641\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/641\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 17.148149490356445\n",
      "Validation loss: 70.05281829833984 RMSE: 8.369756\n",
      "Validation loss: 64.40346050262451 RMSE: 8.025176\n",
      "Validation loss: 58.14547538757324 RMSE: 7.625318\n",
      "3 2 11.332221031188965\n",
      "Validation loss: 50.83464241027832 RMSE: 7.1298413\n",
      "Validation loss: 42.33867835998535 RMSE: 6.506818\n",
      "Validation loss: 35.46838569641113 RMSE: 5.955534\n",
      "6 4 5.351139068603516\n",
      "Validation loss: 31.444087028503418 RMSE: 5.607503\n",
      "Validation loss: 32.29246139526367 RMSE: 5.682646\n",
      "Validation loss: 31.296499252319336 RMSE: 5.5943274\n",
      "9 6 5.085469722747803\n",
      "Validation loss: 30.666351318359375 RMSE: 5.5377207\n",
      "Validation loss: 30.065868377685547 RMSE: 5.483235\n",
      "Validation loss: 29.80141258239746 RMSE: 5.4590673\n",
      "12 8 4.144587993621826\n",
      "Validation loss: 27.403785705566406 RMSE: 5.2348623\n",
      "Validation loss: 26.091742515563965 RMSE: 5.108008\n",
      "Validation loss: 25.231264114379883 RMSE: 5.023073\n",
      "15 10 4.675271034240723\n",
      "Validation loss: 25.208711624145508 RMSE: 5.020828\n",
      "Validation loss: 24.895896911621094 RMSE: 4.989579\n",
      "Validation loss: 24.934561729431152 RMSE: 4.9934516\n",
      "18 12 7.284787654876709\n",
      "Validation loss: 23.80507755279541 RMSE: 4.879045\n",
      "Validation loss: 23.18026351928711 RMSE: 4.8145885\n",
      "Validation loss: 23.45238208770752 RMSE: 4.842766\n",
      "21 14 3.670153856277466\n",
      "Validation loss: 22.76975727081299 RMSE: 4.7717667\n",
      "Validation loss: 22.71560764312744 RMSE: 4.7660894\n",
      "Validation loss: 23.56796646118164 RMSE: 4.854685\n",
      "Validation loss: 22.328083038330078 RMSE: 4.7252603\n",
      "25 0 3.81467866897583\n",
      "Validation loss: 20.722888946533203 RMSE: 4.55224\n",
      "Validation loss: 22.433406829833984 RMSE: 4.7363915\n",
      "Validation loss: 21.215988159179688 RMSE: 4.6060815\n",
      "28 2 3.512667655944824\n",
      "Validation loss: 22.663623809814453 RMSE: 4.760633\n",
      "Validation loss: 21.41391944885254 RMSE: 4.6275177\n",
      "Validation loss: 20.82039451599121 RMSE: 4.5629373\n",
      "31 4 3.0105040073394775\n",
      "Validation loss: 21.06072235107422 RMSE: 4.589196\n",
      "Validation loss: 22.139066696166992 RMSE: 4.705217\n",
      "Validation loss: 22.282400131225586 RMSE: 4.7204237\n",
      "34 6 3.97733211517334\n",
      "Validation loss: 21.390485763549805 RMSE: 4.624985\n",
      "Validation loss: 23.640661239624023 RMSE: 4.8621664\n",
      "Validation loss: 22.58152198791504 RMSE: 4.7520022\n",
      "37 8 4.128984451293945\n",
      "Validation loss: 21.74993133544922 RMSE: 4.663682\n",
      "Validation loss: 21.68429470062256 RMSE: 4.65664\n",
      "Validation loss: 21.62621784210205 RMSE: 4.6503997\n",
      "40 10 4.982650279998779\n",
      "Validation loss: 22.083502769470215 RMSE: 4.699309\n",
      "Validation loss: 20.75057601928711 RMSE: 4.55528\n",
      "Validation loss: 22.921842575073242 RMSE: 4.7876763\n",
      "43 12 3.937429904937744\n",
      "Validation loss: 21.510818481445312 RMSE: 4.6379757\n",
      "Validation loss: 22.894680976867676 RMSE: 4.7848387\n",
      "Validation loss: 22.09188175201416 RMSE: 4.7002\n",
      "46 14 2.265592575073242\n",
      "Validation loss: 20.76696014404297 RMSE: 4.557078\n",
      "Validation loss: 21.967161178588867 RMSE: 4.686914\n",
      "Validation loss: 21.428866386413574 RMSE: 4.6291323\n",
      "Validation loss: 21.861778259277344 RMSE: 4.675658\n",
      "50 0 3.095219373703003\n",
      "Validation loss: 21.818697929382324 RMSE: 4.6710486\n",
      "Validation loss: 21.979703903198242 RMSE: 4.6882515\n",
      "Validation loss: 20.987964630126953 RMSE: 4.581262\n",
      "53 2 4.5185651779174805\n",
      "Validation loss: 21.45212173461914 RMSE: 4.6316433\n",
      "Validation loss: 23.059545516967773 RMSE: 4.8020353\n",
      "Validation loss: 22.22121810913086 RMSE: 4.7139387\n",
      "56 4 3.071997880935669\n",
      "Validation loss: 21.750267028808594 RMSE: 4.663718\n",
      "Validation loss: 20.736637115478516 RMSE: 4.5537496\n",
      "Validation loss: 20.46916675567627 RMSE: 4.5242863\n",
      "59 6 1.7058790922164917\n",
      "Validation loss: 21.474093437194824 RMSE: 4.634015\n",
      "Validation loss: 22.370397567749023 RMSE: 4.7297354\n",
      "Validation loss: 22.63601589202881 RMSE: 4.7577324\n",
      "62 8 2.957304000854492\n",
      "Validation loss: 22.136326789855957 RMSE: 4.7049255\n",
      "Validation loss: 22.55222511291504 RMSE: 4.7489185\n",
      "Validation loss: 20.27251434326172 RMSE: 4.502501\n",
      "65 10 4.448254585266113\n",
      "Validation loss: 22.285690307617188 RMSE: 4.7207723\n",
      "Validation loss: 20.92691421508789 RMSE: 4.5745945\n",
      "Validation loss: 21.790857315063477 RMSE: 4.6680675\n",
      "68 12 2.450618028640747\n",
      "Validation loss: 22.303449630737305 RMSE: 4.722653\n",
      "Validation loss: 21.724889755249023 RMSE: 4.660997\n",
      "Validation loss: 20.71597194671631 RMSE: 4.5514803\n",
      "71 14 2.0722568035125732\n",
      "Validation loss: 22.139846801757812 RMSE: 4.7053\n",
      "Validation loss: 20.921009063720703 RMSE: 4.573949\n",
      "Validation loss: 22.339104652404785 RMSE: 4.726426\n",
      "Validation loss: 22.770092010498047 RMSE: 4.771802\n",
      "75 0 3.2227938175201416\n",
      "Validation loss: 23.079867362976074 RMSE: 4.804151\n",
      "Validation loss: 22.65528678894043 RMSE: 4.759757\n",
      "Validation loss: 23.16562271118164 RMSE: 4.813068\n",
      "78 2 2.8447351455688477\n",
      "Validation loss: 23.85549831390381 RMSE: 4.884209\n",
      "Validation loss: 22.303568840026855 RMSE: 4.7226653\n",
      "Validation loss: 21.906336784362793 RMSE: 4.6804204\n",
      "81 4 2.6828789710998535\n",
      "Validation loss: 22.148950576782227 RMSE: 4.7062674\n",
      "Validation loss: 22.272069931030273 RMSE: 4.71933\n",
      "Validation loss: 22.675591468811035 RMSE: 4.7618895\n",
      "84 6 1.777479648590088\n",
      "Validation loss: 22.67472267150879 RMSE: 4.7617984\n",
      "Validation loss: 22.624406337738037 RMSE: 4.7565117\n",
      "Validation loss: 21.485531330108643 RMSE: 4.6352487\n",
      "87 8 2.0170207023620605\n",
      "Validation loss: 21.809606552124023 RMSE: 4.670076\n",
      "Validation loss: 21.77199649810791 RMSE: 4.6660476\n",
      "Validation loss: 21.846179962158203 RMSE: 4.6739902\n",
      "90 10 1.6711562871932983\n",
      "Validation loss: 23.061519622802734 RMSE: 4.802241\n",
      "Validation loss: 22.36008071899414 RMSE: 4.728645\n",
      "Validation loss: 21.57766056060791 RMSE: 4.645176\n",
      "93 12 2.8907034397125244\n",
      "Validation loss: 22.15812397003174 RMSE: 4.7072415\n",
      "Validation loss: 21.62519073486328 RMSE: 4.6502895\n",
      "Validation loss: 21.742870330810547 RMSE: 4.6629252\n",
      "96 14 3.1362853050231934\n",
      "Validation loss: 22.379950523376465 RMSE: 4.7307453\n",
      "Validation loss: 20.59882926940918 RMSE: 4.5385933\n",
      "Validation loss: 22.87272548675537 RMSE: 4.7825437\n",
      "Validation loss: 21.308899879455566 RMSE: 4.6161566\n",
      "100 0 2.369356632232666\n",
      "Validation loss: 22.558804512023926 RMSE: 4.749611\n",
      "Validation loss: 21.481348514556885 RMSE: 4.6347976\n",
      "Validation loss: 21.58639907836914 RMSE: 4.6461167\n",
      "103 2 1.8164122104644775\n",
      "Validation loss: 22.791467666625977 RMSE: 4.774041\n",
      "Validation loss: 22.20014190673828 RMSE: 4.711703\n",
      "Validation loss: 22.20379066467285 RMSE: 4.7120895\n",
      "106 4 1.3311659097671509\n",
      "Validation loss: 22.05547332763672 RMSE: 4.6963253\n",
      "Validation loss: 22.812214851379395 RMSE: 4.7762136\n",
      "Validation loss: 22.212793350219727 RMSE: 4.713045\n",
      "109 6 1.4789425134658813\n",
      "Validation loss: 23.603193283081055 RMSE: 4.8583117\n",
      "Validation loss: 20.943864822387695 RMSE: 4.5764465\n",
      "Validation loss: 22.287604331970215 RMSE: 4.720975\n",
      "112 8 2.374872922897339\n",
      "Validation loss: 20.04195547103882 RMSE: 4.4768248\n",
      "Validation loss: 20.805065155029297 RMSE: 4.561257\n",
      "Validation loss: 20.49824333190918 RMSE: 4.5274982\n",
      "115 10 1.842052698135376\n",
      "Validation loss: 20.83063793182373 RMSE: 4.5640593\n",
      "Validation loss: 20.992878913879395 RMSE: 4.5817986\n",
      "Validation loss: 21.454418182373047 RMSE: 4.6318917\n",
      "118 12 1.690314531326294\n",
      "Validation loss: 21.707862854003906 RMSE: 4.6591697\n",
      "Validation loss: 23.534615516662598 RMSE: 4.8512487\n",
      "Validation loss: 21.089113235473633 RMSE: 4.5922885\n",
      "121 14 3.322666645050049\n",
      "Validation loss: 20.637657165527344 RMSE: 4.5428686\n",
      "Validation loss: 20.99252223968506 RMSE: 4.58176\n",
      "Validation loss: 22.23143768310547 RMSE: 4.7150226\n",
      "Validation loss: 21.50786304473877 RMSE: 4.637657\n",
      "125 0 2.242300033569336\n",
      "Validation loss: 21.274049758911133 RMSE: 4.61238\n",
      "Validation loss: 22.072046279907227 RMSE: 4.6980896\n",
      "Validation loss: 23.548830032348633 RMSE: 4.8527136\n",
      "128 2 2.6104912757873535\n",
      "Validation loss: 21.42208194732666 RMSE: 4.6283994\n",
      "Validation loss: 22.43237018585205 RMSE: 4.7362823\n",
      "Validation loss: 23.531163215637207 RMSE: 4.850893\n",
      "131 4 3.2514426708221436\n",
      "Validation loss: 22.575121879577637 RMSE: 4.7513285\n",
      "Validation loss: 23.68949794769287 RMSE: 4.8671856\n",
      "Validation loss: 22.05709934234619 RMSE: 4.696499\n",
      "134 6 1.529489278793335\n",
      "Validation loss: 23.04618263244629 RMSE: 4.800644\n",
      "Validation loss: 21.322099685668945 RMSE: 4.617586\n",
      "Validation loss: 22.281188011169434 RMSE: 4.7202954\n",
      "137 8 1.7460606098175049\n",
      "Validation loss: 20.495838165283203 RMSE: 4.527233\n",
      "Validation loss: 20.822590827941895 RMSE: 4.5631776\n",
      "Validation loss: 22.332457542419434 RMSE: 4.725723\n",
      "140 10 2.8553121089935303\n",
      "Validation loss: 22.21100425720215 RMSE: 4.7128553\n",
      "Validation loss: 21.93729019165039 RMSE: 4.6837263\n",
      "Validation loss: 20.87706470489502 RMSE: 4.5691423\n",
      "143 12 2.1953728199005127\n",
      "Validation loss: 21.287837028503418 RMSE: 4.6138744\n",
      "Validation loss: 20.452475547790527 RMSE: 4.5224414\n",
      "Validation loss: 21.171725273132324 RMSE: 4.6012745\n",
      "146 14 1.7705118656158447\n",
      "Validation loss: 21.61221408843994 RMSE: 4.648894\n",
      "Validation loss: 21.933768272399902 RMSE: 4.68335\n",
      "Validation loss: 22.853031158447266 RMSE: 4.780484\n",
      "Validation loss: 21.88102436065674 RMSE: 4.677716\n",
      "150 0 1.451316237449646\n",
      "Validation loss: 22.61765766143799 RMSE: 4.7558026\n",
      "Validation loss: 23.43753719329834 RMSE: 4.841233\n",
      "Validation loss: 21.518259048461914 RMSE: 4.6387777\n",
      "153 2 1.687941312789917\n",
      "Validation loss: 20.55928134918213 RMSE: 4.534234\n",
      "Validation loss: 20.52556800842285 RMSE: 4.530515\n",
      "Validation loss: 22.47585391998291 RMSE: 4.740871\n",
      "156 4 1.6125949621200562\n",
      "Validation loss: 21.31631565093994 RMSE: 4.6169596\n",
      "Validation loss: 21.73906135559082 RMSE: 4.6625166\n",
      "Validation loss: 22.329418182373047 RMSE: 4.7254014\n",
      "159 6 1.1298940181732178\n",
      "Validation loss: 23.314326286315918 RMSE: 4.828491\n",
      "Validation loss: 21.644551277160645 RMSE: 4.6523705\n",
      "Validation loss: 20.57316493988037 RMSE: 4.535765\n",
      "162 8 1.3476516008377075\n",
      "Validation loss: 22.39623737335205 RMSE: 4.732466\n",
      "Validation loss: 21.52054214477539 RMSE: 4.6390243\n",
      "Validation loss: 20.90830898284912 RMSE: 4.5725603\n",
      "165 10 2.5711448192596436\n",
      "Validation loss: 21.65226936340332 RMSE: 4.6532\n",
      "Validation loss: 21.90602970123291 RMSE: 4.680388\n",
      "Validation loss: 22.299203872680664 RMSE: 4.7222033\n",
      "168 12 1.2181302309036255\n",
      "Validation loss: 20.637588500976562 RMSE: 4.542861\n",
      "Validation loss: 20.323647499084473 RMSE: 4.5081754\n",
      "Validation loss: 21.271892547607422 RMSE: 4.6121464\n",
      "171 14 1.7825591564178467\n",
      "Validation loss: 22.216651916503906 RMSE: 4.7134542\n",
      "Validation loss: 20.426488876342773 RMSE: 4.5195675\n",
      "Validation loss: 21.368962287902832 RMSE: 4.6226573\n",
      "Validation loss: 20.39275598526001 RMSE: 4.515834\n",
      "175 0 2.5331013202667236\n",
      "Validation loss: 20.78868293762207 RMSE: 4.5594606\n",
      "Validation loss: 21.70785427093506 RMSE: 4.6591687\n",
      "Validation loss: 21.443842887878418 RMSE: 4.6307497\n",
      "178 2 1.2620518207550049\n",
      "Validation loss: 22.02976703643799 RMSE: 4.693588\n",
      "Validation loss: 21.459632873535156 RMSE: 4.6324544\n",
      "Validation loss: 21.870614051818848 RMSE: 4.676603\n",
      "181 4 1.5314428806304932\n",
      "Validation loss: 21.377605438232422 RMSE: 4.6235924\n",
      "Validation loss: 20.508481979370117 RMSE: 4.5286293\n",
      "Validation loss: 23.242033004760742 RMSE: 4.820999\n",
      "184 6 2.1827943325042725\n",
      "Validation loss: 22.592740058898926 RMSE: 4.7531824\n",
      "Validation loss: 22.498579025268555 RMSE: 4.743267\n",
      "Validation loss: 21.837364196777344 RMSE: 4.6730466\n",
      "187 8 2.4597225189208984\n",
      "Validation loss: 21.198822021484375 RMSE: 4.604218\n",
      "Validation loss: 21.810958862304688 RMSE: 4.6702204\n",
      "Validation loss: 21.97083854675293 RMSE: 4.687306\n",
      "190 10 1.0253609418869019\n",
      "Validation loss: 22.757307052612305 RMSE: 4.770462\n",
      "Validation loss: 19.944951057434082 RMSE: 4.4659767\n",
      "Validation loss: 22.057856559753418 RMSE: 4.6965795\n",
      "193 12 1.4814984798431396\n",
      "Validation loss: 19.48026466369629 RMSE: 4.4136453\n",
      "Validation loss: 20.468411922454834 RMSE: 4.524203\n",
      "Validation loss: 20.616629600524902 RMSE: 4.540554\n",
      "196 14 1.339630365371704\n",
      "Validation loss: 21.088062286376953 RMSE: 4.592174\n",
      "Validation loss: 22.568782806396484 RMSE: 4.7506614\n",
      "Validation loss: 21.981871604919434 RMSE: 4.6884828\n",
      "Validation loss: 21.96045970916748 RMSE: 4.686199\n",
      "Loaded trained model with success.\n",
      "Test loss: 8.072481046731655 Test RMSE: 2.841211\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:1', 'task_name': 'freesolv', 'model_type': 'gin', 'model': {'num_layer': 3, 'emb_dim': 64, 'feat_dim': 64, 'drop_ratio': 0.5, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 780, 'task': 'regression', 'data_path': 'data/freesolv/freesolv.csv', 'target': ['expt']}}\n",
      "Running on: cuda:1\n",
      "641\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/641\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 17.31785774230957\n",
      "Validation loss: 61.924537658691406 RMSE: 7.8692145\n",
      "Validation loss: 56.7908935546875 RMSE: 7.535973\n",
      "Validation loss: 51.279083251953125 RMSE: 7.1609416\n",
      "3 2 14.043974876403809\n",
      "Validation loss: 45.091227531433105 RMSE: 6.7150006\n",
      "Validation loss: 34.36293888092041 RMSE: 5.861991\n",
      "Validation loss: 33.68380165100098 RMSE: 5.803775\n",
      "6 4 3.5787291526794434\n",
      "Validation loss: 34.484646797180176 RMSE: 5.872363\n",
      "Validation loss: 31.893500328063965 RMSE: 5.647433\n",
      "Validation loss: 34.217668533325195 RMSE: 5.849587\n",
      "9 6 4.932107448577881\n",
      "Validation loss: 34.96511650085449 RMSE: 5.9131308\n",
      "Validation loss: 32.477416038513184 RMSE: 5.698896\n",
      "Validation loss: 35.07940483093262 RMSE: 5.922787\n",
      "12 8 5.723072052001953\n",
      "Validation loss: 37.29858589172363 RMSE: 6.107257\n",
      "Validation loss: 34.18381881713867 RMSE: 5.846693\n",
      "Validation loss: 36.9163293838501 RMSE: 6.075881\n",
      "15 10 6.827978610992432\n",
      "Validation loss: 37.78389549255371 RMSE: 6.14686\n",
      "Validation loss: 34.70031929016113 RMSE: 5.8906975\n",
      "Validation loss: 37.324283599853516 RMSE: 6.1093607\n",
      "18 12 3.2428197860717773\n",
      "Validation loss: 36.64561462402344 RMSE: 6.053562\n",
      "Validation loss: 37.787357330322266 RMSE: 6.1471424\n",
      "Validation loss: 37.50977420806885 RMSE: 6.124522\n",
      "21 14 4.067364692687988\n",
      "Validation loss: 38.50942420959473 RMSE: 6.205596\n",
      "Validation loss: 36.9954776763916 RMSE: 6.082391\n",
      "Validation loss: 36.95189571380615 RMSE: 6.0788074\n",
      "Validation loss: 37.130510330200195 RMSE: 6.0934806\n",
      "25 0 3.8925888538360596\n",
      "Validation loss: 35.45955276489258 RMSE: 5.9547925\n",
      "Validation loss: 34.77706527709961 RMSE: 5.897208\n",
      "Validation loss: 33.6351261138916 RMSE: 5.79958\n",
      "28 2 2.2569055557250977\n",
      "Validation loss: 32.99199104309082 RMSE: 5.7438655\n",
      "Validation loss: 33.71516418457031 RMSE: 5.806476\n",
      "Validation loss: 33.36942481994629 RMSE: 5.7766275\n",
      "31 4 1.986750841140747\n",
      "Validation loss: 32.93651103973389 RMSE: 5.7390337\n",
      "Validation loss: 34.654624938964844 RMSE: 5.886818\n",
      "Validation loss: 35.15043258666992 RMSE: 5.9287796\n",
      "34 6 1.664435863494873\n",
      "Validation loss: 34.17533302307129 RMSE: 5.8459673\n",
      "Validation loss: 35.209954261779785 RMSE: 5.9337974\n",
      "Validation loss: 34.46590518951416 RMSE: 5.870767\n",
      "37 8 3.130559206008911\n",
      "Validation loss: 35.909706115722656 RMSE: 5.9924707\n",
      "Validation loss: 33.313429832458496 RMSE: 5.771779\n",
      "Validation loss: 34.80044746398926 RMSE: 5.8991904\n",
      "40 10 3.1142594814300537\n",
      "Validation loss: 34.4059476852417 RMSE: 5.8656583\n",
      "Validation loss: 33.768099784851074 RMSE: 5.811033\n",
      "Validation loss: 34.017754554748535 RMSE: 5.832474\n",
      "43 12 4.371681213378906\n",
      "Validation loss: 32.60983657836914 RMSE: 5.710502\n",
      "Validation loss: 33.61537551879883 RMSE: 5.797877\n",
      "Validation loss: 34.38711738586426 RMSE: 5.8640532\n",
      "46 14 3.151937484741211\n",
      "Validation loss: 31.239105224609375 RMSE: 5.5891953\n",
      "Validation loss: 32.749990463256836 RMSE: 5.7227607\n",
      "Validation loss: 32.13543701171875 RMSE: 5.6688128\n",
      "Validation loss: 31.6846923828125 RMSE: 5.628916\n",
      "50 0 1.6416831016540527\n",
      "Validation loss: 32.41761302947998 RMSE: 5.693647\n",
      "Validation loss: 32.99658203125 RMSE: 5.744265\n",
      "Validation loss: 30.655729293823242 RMSE: 5.5367618\n",
      "53 2 2.761512517929077\n",
      "Validation loss: 30.318748474121094 RMSE: 5.506246\n",
      "Validation loss: 31.245885848999023 RMSE: 5.589802\n",
      "Validation loss: 30.198514938354492 RMSE: 5.4953175\n",
      "56 4 2.2978601455688477\n",
      "Validation loss: 31.358107566833496 RMSE: 5.599831\n",
      "Validation loss: 31.460416793823242 RMSE: 5.6089582\n",
      "Validation loss: 29.5631160736084 RMSE: 5.4371977\n",
      "59 6 4.484328269958496\n",
      "Validation loss: 30.616782188415527 RMSE: 5.533243\n",
      "Validation loss: 30.29790496826172 RMSE: 5.504353\n",
      "Validation loss: 30.282163619995117 RMSE: 5.5029235\n",
      "62 8 2.4030940532684326\n",
      "Validation loss: 27.683578491210938 RMSE: 5.2615185\n",
      "Validation loss: 28.572080612182617 RMSE: 5.3452854\n",
      "Validation loss: 29.23772430419922 RMSE: 5.4071918\n",
      "65 10 2.7239906787872314\n",
      "Validation loss: 28.987982749938965 RMSE: 5.384049\n",
      "Validation loss: 28.42597770690918 RMSE: 5.3316016\n",
      "Validation loss: 28.489879608154297 RMSE: 5.337591\n",
      "68 12 3.029099464416504\n",
      "Validation loss: 28.434226036071777 RMSE: 5.3323755\n",
      "Validation loss: 27.430718421936035 RMSE: 5.2374344\n",
      "Validation loss: 27.626197814941406 RMSE: 5.256063\n",
      "71 14 1.5752586126327515\n",
      "Validation loss: 26.322021484375 RMSE: 5.1304994\n",
      "Validation loss: 27.779890060424805 RMSE: 5.2706633\n",
      "Validation loss: 29.5640869140625 RMSE: 5.437287\n",
      "Validation loss: 29.01292896270752 RMSE: 5.386365\n",
      "75 0 1.1419453620910645\n",
      "Validation loss: 30.029338836669922 RMSE: 5.479903\n",
      "Validation loss: 29.136877059936523 RMSE: 5.3978586\n",
      "Validation loss: 28.321237564086914 RMSE: 5.32177\n",
      "78 2 1.838526964187622\n",
      "Validation loss: 27.201760292053223 RMSE: 5.2155304\n",
      "Validation loss: 27.255142211914062 RMSE: 5.220646\n",
      "Validation loss: 29.980111122131348 RMSE: 5.47541\n",
      "81 4 1.9283652305603027\n",
      "Validation loss: 27.388429641723633 RMSE: 5.2333956\n",
      "Validation loss: 27.687881469726562 RMSE: 5.2619276\n",
      "Validation loss: 26.608935356140137 RMSE: 5.1583853\n",
      "84 6 2.056312084197998\n",
      "Validation loss: 28.06844997406006 RMSE: 5.2979665\n",
      "Validation loss: 27.645883560180664 RMSE: 5.2579355\n",
      "Validation loss: 27.919607162475586 RMSE: 5.2839007\n",
      "87 8 2.791839122772217\n",
      "Validation loss: 26.1007661819458 RMSE: 5.108891\n",
      "Validation loss: 26.044891357421875 RMSE: 5.10342\n",
      "Validation loss: 24.522014617919922 RMSE: 4.9519706\n",
      "90 10 3.150935649871826\n",
      "Validation loss: 25.12829875946045 RMSE: 5.0128136\n",
      "Validation loss: 25.152292251586914 RMSE: 5.015206\n",
      "Validation loss: 26.026691436767578 RMSE: 5.101636\n",
      "93 12 6.294649124145508\n",
      "Validation loss: 26.000757217407227 RMSE: 5.099094\n",
      "Validation loss: 26.41103172302246 RMSE: 5.1391664\n",
      "Validation loss: 25.436887741088867 RMSE: 5.0435\n",
      "96 14 1.892582654953003\n",
      "Validation loss: 24.684279441833496 RMSE: 4.9683275\n",
      "Validation loss: 27.131990432739258 RMSE: 5.2088375\n",
      "Validation loss: 26.801450729370117 RMSE: 5.177012\n",
      "Validation loss: 26.360414505004883 RMSE: 5.134239\n",
      "100 0 4.663386821746826\n",
      "Validation loss: 27.2252779006958 RMSE: 5.217785\n",
      "Validation loss: 26.698638916015625 RMSE: 5.1670723\n",
      "Validation loss: 25.707688331604004 RMSE: 5.070275\n",
      "103 2 2.105620861053467\n",
      "Validation loss: 28.527652740478516 RMSE: 5.3411283\n",
      "Validation loss: 24.911338806152344 RMSE: 4.991126\n",
      "Validation loss: 25.039621353149414 RMSE: 5.0039606\n",
      "106 4 1.8691411018371582\n",
      "Validation loss: 26.62723731994629 RMSE: 5.1601586\n",
      "Validation loss: 26.595800399780273 RMSE: 5.1571116\n",
      "Validation loss: 27.403959274291992 RMSE: 5.234879\n",
      "109 6 0.9230336546897888\n",
      "Validation loss: 27.360321044921875 RMSE: 5.2307096\n",
      "Validation loss: 27.21688461303711 RMSE: 5.2169805\n",
      "Validation loss: 25.69334888458252 RMSE: 5.0688605\n",
      "112 8 3.049743413925171\n",
      "Validation loss: 27.94076919555664 RMSE: 5.285903\n",
      "Validation loss: 27.286476135253906 RMSE: 5.2236457\n",
      "Validation loss: 29.17881202697754 RMSE: 5.4017415\n",
      "115 10 1.521414041519165\n",
      "Validation loss: 27.404640197753906 RMSE: 5.2349443\n",
      "Validation loss: 27.039566040039062 RMSE: 5.1999583\n",
      "Validation loss: 26.64586067199707 RMSE: 5.1619625\n",
      "118 12 1.5143418312072754\n",
      "Validation loss: 26.263259887695312 RMSE: 5.124769\n",
      "Validation loss: 27.253211975097656 RMSE: 5.220461\n",
      "Validation loss: 26.68862819671631 RMSE: 5.166104\n",
      "121 14 1.6106027364730835\n",
      "Validation loss: 27.61561393737793 RMSE: 5.255056\n",
      "Validation loss: 28.412979125976562 RMSE: 5.330383\n",
      "Validation loss: 26.399621963500977 RMSE: 5.1380563\n",
      "Validation loss: 26.05654525756836 RMSE: 5.1045613\n",
      "125 0 1.5429332256317139\n",
      "Validation loss: 25.20482349395752 RMSE: 5.0204406\n",
      "Validation loss: 27.672311782836914 RMSE: 5.260448\n",
      "Validation loss: 25.942672729492188 RMSE: 5.093395\n",
      "128 2 2.7671971321105957\n",
      "Validation loss: 28.26585865020752 RMSE: 5.3165646\n",
      "Validation loss: 27.102577209472656 RMSE: 5.2060137\n",
      "Validation loss: 26.80862331390381 RMSE: 5.1777043\n",
      "131 4 1.2722140550613403\n",
      "Validation loss: 26.331457138061523 RMSE: 5.131418\n",
      "Validation loss: 25.469316959381104 RMSE: 5.046714\n",
      "Validation loss: 26.77789306640625 RMSE: 5.1747355\n",
      "134 6 2.2949252128601074\n",
      "Validation loss: 25.79423427581787 RMSE: 5.0788026\n",
      "Validation loss: 26.558319091796875 RMSE: 5.1534767\n",
      "Validation loss: 24.99473476409912 RMSE: 4.9994736\n",
      "137 8 1.5267671346664429\n",
      "Validation loss: 26.626867294311523 RMSE: 5.160123\n",
      "Validation loss: 25.485698699951172 RMSE: 5.048336\n",
      "Validation loss: 25.444615364074707 RMSE: 5.0442653\n",
      "140 10 1.5987931489944458\n",
      "Validation loss: 25.437816619873047 RMSE: 5.0435915\n",
      "Validation loss: 25.603240966796875 RMSE: 5.0599647\n",
      "Validation loss: 25.20921516418457 RMSE: 5.020878\n",
      "143 12 2.678967237472534\n",
      "Validation loss: 25.34730815887451 RMSE: 5.0346107\n",
      "Validation loss: 25.81455898284912 RMSE: 5.080803\n",
      "Validation loss: 26.292537689208984 RMSE: 5.127625\n",
      "146 14 2.271012783050537\n",
      "Validation loss: 26.301559448242188 RMSE: 5.1285048\n",
      "Validation loss: 25.247952461242676 RMSE: 5.024734\n",
      "Validation loss: 26.120370864868164 RMSE: 5.110809\n",
      "Validation loss: 26.096177101135254 RMSE: 5.108442\n",
      "150 0 1.1255316734313965\n",
      "Validation loss: 27.352028846740723 RMSE: 5.2299166\n",
      "Validation loss: 25.612550735473633 RMSE: 5.060884\n",
      "Validation loss: 26.818339347839355 RMSE: 5.1786423\n",
      "153 2 2.4430065155029297\n",
      "Validation loss: 26.084388732910156 RMSE: 5.107288\n",
      "Validation loss: 26.441978454589844 RMSE: 5.142176\n",
      "Validation loss: 26.570448875427246 RMSE: 5.154653\n",
      "156 4 2.5229947566986084\n",
      "Validation loss: 26.76783275604248 RMSE: 5.1737638\n",
      "Validation loss: 25.849363327026367 RMSE: 5.084227\n",
      "Validation loss: 26.222203254699707 RMSE: 5.120762\n",
      "159 6 1.686298131942749\n",
      "Validation loss: 26.952345848083496 RMSE: 5.1915646\n",
      "Validation loss: 25.723374366760254 RMSE: 5.0718217\n",
      "Validation loss: 27.4345703125 RMSE: 5.237802\n",
      "162 8 2.1735496520996094\n",
      "Validation loss: 25.743144989013672 RMSE: 5.0737705\n",
      "Validation loss: 24.385330200195312 RMSE: 4.938151\n",
      "Validation loss: 25.282415866851807 RMSE: 5.0281625\n",
      "165 10 1.370370626449585\n",
      "Validation loss: 24.83754062652588 RMSE: 4.983728\n",
      "Validation loss: 23.559197425842285 RMSE: 4.8537817\n",
      "Validation loss: 24.06905174255371 RMSE: 4.906022\n",
      "168 12 1.9352362155914307\n",
      "Validation loss: 24.931442260742188 RMSE: 4.9931397\n",
      "Validation loss: 23.570717811584473 RMSE: 4.854968\n",
      "Validation loss: 22.958231925964355 RMSE: 4.791475\n",
      "171 14 2.0431790351867676\n",
      "Validation loss: 26.189372062683105 RMSE: 5.117555\n",
      "Validation loss: 26.626582145690918 RMSE: 5.160095\n",
      "Validation loss: 26.848477363586426 RMSE: 5.1815515\n",
      "Validation loss: 24.56281566619873 RMSE: 4.956089\n",
      "175 0 1.4939634799957275\n",
      "Validation loss: 25.111281394958496 RMSE: 5.0111156\n",
      "Validation loss: 26.081896781921387 RMSE: 5.1070437\n",
      "Validation loss: 26.45078754425049 RMSE: 5.143033\n",
      "178 2 1.6000661849975586\n",
      "Validation loss: 24.55067539215088 RMSE: 4.9548635\n",
      "Validation loss: 25.886034965515137 RMSE: 5.087832\n",
      "Validation loss: 27.103120803833008 RMSE: 5.2060657\n",
      "181 4 1.5079023838043213\n",
      "Validation loss: 27.673880577087402 RMSE: 5.260597\n",
      "Validation loss: 25.259239196777344 RMSE: 5.025857\n",
      "Validation loss: 26.501550674438477 RMSE: 5.147966\n",
      "184 6 1.9488352537155151\n",
      "Validation loss: 26.676600456237793 RMSE: 5.1649394\n",
      "Validation loss: 25.438634872436523 RMSE: 5.0436726\n",
      "Validation loss: 27.292648315429688 RMSE: 5.2242365\n",
      "187 8 1.7344295978546143\n",
      "Validation loss: 25.388381004333496 RMSE: 5.0386887\n",
      "Validation loss: 24.536136627197266 RMSE: 4.9533963\n",
      "Validation loss: 26.24406623840332 RMSE: 5.122896\n",
      "190 10 1.2876884937286377\n",
      "Validation loss: 25.27645492553711 RMSE: 5.0275693\n",
      "Validation loss: 26.869138717651367 RMSE: 5.183545\n",
      "Validation loss: 25.02603244781494 RMSE: 5.0026026\n",
      "193 12 1.2324879169464111\n",
      "Validation loss: 26.849905014038086 RMSE: 5.1816893\n",
      "Validation loss: 27.19877052307129 RMSE: 5.215244\n",
      "Validation loss: 26.0460147857666 RMSE: 5.10353\n",
      "196 14 1.0642777681350708\n",
      "Validation loss: 26.328076362609863 RMSE: 5.131089\n",
      "Validation loss: 27.576675415039062 RMSE: 5.25135\n",
      "Validation loss: 26.325406074523926 RMSE: 5.130829\n",
      "Validation loss: 26.278849601745605 RMSE: 5.1262903\n",
      "Loaded trained model with success.\n",
      "Test loss: 9.101696946414618 Test RMSE: 3.016902\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:1', 'task_name': 'freesolv', 'model_type': 'gin', 'model': {'num_layer': 3, 'emb_dim': 64, 'feat_dim': 64, 'drop_ratio': 0.5, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 781, 'task': 'regression', 'data_path': 'data/freesolv/freesolv.csv', 'target': ['expt']}}\n",
      "Running on: cuda:1\n",
      "641\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/641\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 19.637737274169922\n",
      "Validation loss: 68.89424896240234 RMSE: 8.300256\n",
      "Validation loss: 63.159847259521484 RMSE: 7.947317\n",
      "Validation loss: 56.80192852020264 RMSE: 7.5367055\n",
      "3 2 13.061576843261719\n",
      "Validation loss: 49.9100284576416 RMSE: 7.064703\n",
      "Validation loss: 40.574219703674316 RMSE: 6.3697896\n",
      "Validation loss: 33.18288040161133 RMSE: 5.7604585\n",
      "6 4 5.462339401245117\n",
      "Validation loss: 37.03763389587402 RMSE: 6.085855\n",
      "Validation loss: 38.307777404785156 RMSE: 6.1893277\n",
      "Validation loss: 35.85563278198242 RMSE: 5.987957\n",
      "9 6 5.542967319488525\n",
      "Validation loss: 34.58525276184082 RMSE: 5.8809233\n",
      "Validation loss: 34.7894229888916 RMSE: 5.898256\n",
      "Validation loss: 35.85024452209473 RMSE: 5.9875073\n",
      "12 8 4.7311906814575195\n",
      "Validation loss: 37.246856689453125 RMSE: 6.10302\n",
      "Validation loss: 32.39378261566162 RMSE: 5.6915536\n",
      "Validation loss: 35.7542667388916 RMSE: 5.979487\n",
      "15 10 5.120952606201172\n",
      "Validation loss: 33.889448165893555 RMSE: 5.821464\n",
      "Validation loss: 33.67607879638672 RMSE: 5.8031096\n",
      "Validation loss: 33.21467399597168 RMSE: 5.763217\n",
      "18 12 7.014519214630127\n",
      "Validation loss: 32.158535957336426 RMSE: 5.67085\n",
      "Validation loss: 36.37765312194824 RMSE: 6.031389\n",
      "Validation loss: 33.762216567993164 RMSE: 5.8105264\n",
      "21 14 4.055285453796387\n",
      "Validation loss: 34.835744857788086 RMSE: 5.9021816\n",
      "Validation loss: 33.446624755859375 RMSE: 5.7833056\n",
      "Validation loss: 34.92037296295166 RMSE: 5.909346\n",
      "Validation loss: 33.043198585510254 RMSE: 5.7483215\n",
      "25 0 3.259397506713867\n",
      "Validation loss: 34.36664009094238 RMSE: 5.862307\n",
      "Validation loss: 37.775513648986816 RMSE: 6.146179\n",
      "Validation loss: 32.94307041168213 RMSE: 5.7396054\n",
      "28 2 3.3748726844787598\n",
      "Validation loss: 34.81723403930664 RMSE: 5.900613\n",
      "Validation loss: 33.5216121673584 RMSE: 5.7897854\n",
      "Validation loss: 36.80616855621338 RMSE: 6.0668087\n",
      "31 4 5.604700088500977\n",
      "Validation loss: 37.13447189331055 RMSE: 6.093806\n",
      "Validation loss: 31.25491428375244 RMSE: 5.5906096\n",
      "Validation loss: 30.787132263183594 RMSE: 5.5486155\n",
      "34 6 2.0328238010406494\n",
      "Validation loss: 32.62133598327637 RMSE: 5.7115088\n",
      "Validation loss: 33.06572914123535 RMSE: 5.750281\n",
      "Validation loss: 34.68506717681885 RMSE: 5.889403\n",
      "37 8 2.4596009254455566\n",
      "Validation loss: 36.413631439208984 RMSE: 6.0343714\n",
      "Validation loss: 33.221567153930664 RMSE: 5.763815\n",
      "Validation loss: 30.841182708740234 RMSE: 5.5534835\n",
      "40 10 3.2785890102386475\n",
      "Validation loss: 34.028982162475586 RMSE: 5.8334365\n",
      "Validation loss: 32.03244495391846 RMSE: 5.6597214\n",
      "Validation loss: 35.299936294555664 RMSE: 5.9413753\n",
      "43 12 2.2560324668884277\n",
      "Validation loss: 37.9885368347168 RMSE: 6.1634836\n",
      "Validation loss: 33.99081802368164 RMSE: 5.8301644\n",
      "Validation loss: 32.362576484680176 RMSE: 5.688812\n",
      "46 14 2.1507070064544678\n",
      "Validation loss: 35.50895881652832 RMSE: 5.958939\n",
      "Validation loss: 36.32896614074707 RMSE: 6.027352\n",
      "Validation loss: 35.81115913391113 RMSE: 5.984243\n",
      "Validation loss: 31.979073524475098 RMSE: 5.6550045\n",
      "50 0 3.9749677181243896\n",
      "Validation loss: 36.79191493988037 RMSE: 6.065634\n",
      "Validation loss: 31.961522102355957 RMSE: 5.653452\n",
      "Validation loss: 32.57847595214844 RMSE: 5.7077556\n",
      "53 2 2.0017175674438477\n",
      "Validation loss: 35.93373203277588 RMSE: 5.994475\n",
      "Validation loss: 36.16761016845703 RMSE: 6.0139513\n",
      "Validation loss: 37.92212104797363 RMSE: 6.158094\n",
      "56 4 2.103768825531006\n",
      "Validation loss: 37.08944129943848 RMSE: 6.090111\n",
      "Validation loss: 34.78550624847412 RMSE: 5.897924\n",
      "Validation loss: 40.235490798950195 RMSE: 6.3431454\n",
      "59 6 2.130485773086548\n",
      "Validation loss: 39.859439849853516 RMSE: 6.313433\n",
      "Validation loss: 37.15416717529297 RMSE: 6.095422\n",
      "Validation loss: 38.74819374084473 RMSE: 6.2248044\n",
      "62 8 1.938522219657898\n",
      "Validation loss: 35.62316036224365 RMSE: 5.968514\n",
      "Validation loss: 34.68300533294678 RMSE: 5.889228\n",
      "Validation loss: 39.88281440734863 RMSE: 6.3152843\n",
      "65 10 1.038594365119934\n",
      "Validation loss: 34.79065990447998 RMSE: 5.8983607\n",
      "Validation loss: 36.600162506103516 RMSE: 6.0498066\n",
      "Validation loss: 37.098764419555664 RMSE: 6.090875\n",
      "68 12 3.232595920562744\n",
      "Validation loss: 37.72922134399414 RMSE: 6.1424117\n",
      "Validation loss: 38.013771057128906 RMSE: 6.1655307\n",
      "Validation loss: 34.97434043884277 RMSE: 5.9139104\n",
      "71 14 1.8807792663574219\n",
      "Validation loss: 39.21903610229492 RMSE: 6.2625103\n",
      "Validation loss: 36.537373542785645 RMSE: 6.0446153\n",
      "Validation loss: 37.2901668548584 RMSE: 6.106568\n",
      "Validation loss: 35.08027648925781 RMSE: 5.9228606\n",
      "75 0 2.6450369358062744\n",
      "Validation loss: 36.20705699920654 RMSE: 6.01723\n",
      "Validation loss: 37.29311752319336 RMSE: 6.1068087\n",
      "Validation loss: 36.46743869781494 RMSE: 6.0388274\n",
      "78 2 2.4883525371551514\n",
      "Validation loss: 35.77413558959961 RMSE: 5.9811482\n",
      "Validation loss: 40.58545684814453 RMSE: 6.3706717\n",
      "Validation loss: 36.73489761352539 RMSE: 6.060932\n",
      "81 4 1.0740289688110352\n",
      "Validation loss: 32.533766746520996 RMSE: 5.703838\n",
      "Validation loss: 32.97359752655029 RMSE: 5.7422643\n",
      "Validation loss: 34.41378211975098 RMSE: 5.8663263\n",
      "84 6 2.02799654006958\n",
      "Validation loss: 32.39423942565918 RMSE: 5.6915936\n",
      "Validation loss: 32.35222816467285 RMSE: 5.687902\n",
      "Validation loss: 34.29355239868164 RMSE: 5.8560696\n",
      "87 8 2.6092607975006104\n",
      "Validation loss: 35.95449256896973 RMSE: 5.9962063\n",
      "Validation loss: 34.475154876708984 RMSE: 5.871555\n",
      "Validation loss: 33.06181716918945 RMSE: 5.7499404\n",
      "90 10 2.8108365535736084\n",
      "Validation loss: 37.00580024719238 RMSE: 6.083239\n",
      "Validation loss: 35.868282318115234 RMSE: 5.9890137\n",
      "Validation loss: 38.1785888671875 RMSE: 6.1788826\n",
      "93 12 4.4388017654418945\n",
      "Validation loss: 35.70894241333008 RMSE: 5.9756956\n",
      "Validation loss: 43.71820068359375 RMSE: 6.6119742\n",
      "Validation loss: 37.469364166259766 RMSE: 6.1212225\n",
      "96 14 1.7279874086380005\n",
      "Validation loss: 38.72824287414551 RMSE: 6.2232018\n",
      "Validation loss: 34.26483154296875 RMSE: 5.853617\n",
      "Validation loss: 33.27994155883789 RMSE: 5.768877\n",
      "Validation loss: 36.18601417541504 RMSE: 6.0154815\n",
      "100 0 1.395535945892334\n",
      "Validation loss: 34.4408016204834 RMSE: 5.868629\n",
      "Validation loss: 32.30733871459961 RMSE: 5.6839547\n",
      "Validation loss: 41.7647705078125 RMSE: 6.462567\n",
      "103 2 3.6820054054260254\n",
      "Validation loss: 36.707237243652344 RMSE: 6.05865\n",
      "Validation loss: 35.98865509033203 RMSE: 5.9990544\n",
      "Validation loss: 40.93933296203613 RMSE: 6.3983855\n",
      "106 4 2.4336423873901367\n",
      "Validation loss: 36.82609176635742 RMSE: 6.068451\n",
      "Validation loss: 38.58121871948242 RMSE: 6.2113776\n",
      "Validation loss: 36.01168441772461 RMSE: 6.0009737\n",
      "109 6 3.2604541778564453\n",
      "Validation loss: 36.07380294799805 RMSE: 6.0061474\n",
      "Validation loss: 34.48048210144043 RMSE: 5.8720083\n",
      "Validation loss: 36.83345031738281 RMSE: 6.069057\n",
      "112 8 1.4629240036010742\n",
      "Validation loss: 36.0130672454834 RMSE: 6.001089\n",
      "Validation loss: 42.86202621459961 RMSE: 6.54691\n",
      "Validation loss: 35.57376670837402 RMSE: 5.964375\n",
      "115 10 2.522294521331787\n",
      "Validation loss: 35.54999542236328 RMSE: 5.962382\n",
      "Validation loss: 36.26446723937988 RMSE: 6.021999\n",
      "Validation loss: 36.32749938964844 RMSE: 6.0272303\n",
      "118 12 1.1750484704971313\n",
      "Validation loss: 35.782440185546875 RMSE: 5.9818425\n",
      "Validation loss: 43.782135009765625 RMSE: 6.616807\n",
      "Validation loss: 37.35003471374512 RMSE: 6.1114674\n",
      "121 14 1.0668963193893433\n",
      "Validation loss: 39.53321647644043 RMSE: 6.2875447\n",
      "Validation loss: 38.1541862487793 RMSE: 6.1769075\n",
      "Validation loss: 35.9311637878418 RMSE: 5.9942613\n",
      "Validation loss: 35.92870903015137 RMSE: 5.994056\n",
      "125 0 1.6983373165130615\n",
      "Validation loss: 41.599552154541016 RMSE: 6.4497714\n",
      "Validation loss: 38.484840393066406 RMSE: 6.203615\n",
      "Validation loss: 34.92779350280762 RMSE: 5.909974\n",
      "128 2 0.930363655090332\n",
      "Validation loss: 41.29852867126465 RMSE: 6.4263935\n",
      "Validation loss: 39.17882537841797 RMSE: 6.2592993\n",
      "Validation loss: 34.07744216918945 RMSE: 5.8375883\n",
      "131 4 2.1484804153442383\n",
      "Validation loss: 36.56504440307617 RMSE: 6.0469036\n",
      "Validation loss: 35.86159324645996 RMSE: 5.9884553\n",
      "Validation loss: 40.54997444152832 RMSE: 6.367886\n",
      "134 6 1.0276066064834595\n",
      "Validation loss: 39.243696212768555 RMSE: 6.2644787\n",
      "Validation loss: 38.44443130493164 RMSE: 6.200357\n",
      "Validation loss: 37.518117904663086 RMSE: 6.1252036\n",
      "137 8 1.5813236236572266\n",
      "Validation loss: 44.0141716003418 RMSE: 6.634318\n",
      "Validation loss: 37.52758026123047 RMSE: 6.125976\n",
      "Validation loss: 37.328121185302734 RMSE: 6.109674\n",
      "140 10 1.1531044244766235\n",
      "Validation loss: 37.36537742614746 RMSE: 6.112723\n",
      "Validation loss: 35.21559143066406 RMSE: 5.934273\n",
      "Validation loss: 38.08978080749512 RMSE: 6.171692\n",
      "143 12 3.3557825088500977\n",
      "Validation loss: 42.368417739868164 RMSE: 6.509103\n",
      "Validation loss: 37.15750217437744 RMSE: 6.0956955\n",
      "Validation loss: 37.85726547241211 RMSE: 6.152826\n",
      "146 14 1.8095933198928833\n",
      "Validation loss: 39.81938171386719 RMSE: 6.31026\n",
      "Validation loss: 30.218061447143555 RMSE: 5.4970956\n",
      "Validation loss: 44.04645919799805 RMSE: 6.636751\n",
      "Validation loss: 45.199968338012695 RMSE: 6.723092\n",
      "150 0 1.4141709804534912\n",
      "Validation loss: 41.09437370300293 RMSE: 6.410489\n",
      "Validation loss: 43.03883934020996 RMSE: 6.5603995\n",
      "Validation loss: 42.45968437194824 RMSE: 6.51611\n",
      "153 2 1.4752508401870728\n",
      "Validation loss: 40.02019119262695 RMSE: 6.326152\n",
      "Validation loss: 36.68546676635742 RMSE: 6.0568523\n",
      "Validation loss: 46.52181816101074 RMSE: 6.82069\n",
      "156 4 0.6268469095230103\n",
      "Validation loss: 38.87199401855469 RMSE: 6.2347407\n",
      "Validation loss: 36.89149475097656 RMSE: 6.073837\n",
      "Validation loss: 40.64910316467285 RMSE: 6.3756647\n",
      "159 6 1.2199602127075195\n",
      "Validation loss: 42.32769775390625 RMSE: 6.505974\n",
      "Validation loss: 39.02597236633301 RMSE: 6.247077\n",
      "Validation loss: 39.452491760253906 RMSE: 6.2811217\n",
      "162 8 1.5747389793395996\n",
      "Validation loss: 35.01665019989014 RMSE: 5.9174867\n",
      "Validation loss: 34.58267307281494 RMSE: 5.8807034\n",
      "Validation loss: 39.14221000671387 RMSE: 6.256374\n",
      "165 10 2.4675803184509277\n",
      "Validation loss: 36.876285552978516 RMSE: 6.0725846\n",
      "Validation loss: 37.115135192871094 RMSE: 6.0922194\n",
      "Validation loss: 38.51365280151367 RMSE: 6.2059364\n",
      "168 12 1.6106276512145996\n",
      "Validation loss: 39.68909454345703 RMSE: 6.2999287\n",
      "Validation loss: 36.263282775878906 RMSE: 6.0219007\n",
      "Validation loss: 41.49670219421387 RMSE: 6.4417934\n",
      "171 14 0.6552164554595947\n",
      "Validation loss: 41.501455307006836 RMSE: 6.442162\n",
      "Validation loss: 41.92092704772949 RMSE: 6.4746375\n",
      "Validation loss: 41.368459701538086 RMSE: 6.431832\n",
      "Validation loss: 43.759878158569336 RMSE: 6.615125\n",
      "175 0 1.3449170589447021\n",
      "Validation loss: 36.72491264343262 RMSE: 6.060108\n",
      "Validation loss: 39.60043144226074 RMSE: 6.2928877\n",
      "Validation loss: 40.5124397277832 RMSE: 6.3649387\n",
      "178 2 1.4594073295593262\n",
      "Validation loss: 39.75741004943848 RMSE: 6.3053474\n",
      "Validation loss: 37.7205924987793 RMSE: 6.1417093\n",
      "Validation loss: 38.69061279296875 RMSE: 6.2201777\n",
      "181 4 2.4676806926727295\n",
      "Validation loss: 37.03815841674805 RMSE: 6.085899\n",
      "Validation loss: 35.59454345703125 RMSE: 5.9661164\n",
      "Validation loss: 39.12590217590332 RMSE: 6.25507\n",
      "184 6 0.8858174085617065\n",
      "Validation loss: 44.94214630126953 RMSE: 6.7038903\n",
      "Validation loss: 39.83127307891846 RMSE: 6.3112025\n",
      "Validation loss: 41.147844314575195 RMSE: 6.4146585\n",
      "187 8 1.537001609802246\n",
      "Validation loss: 40.77705955505371 RMSE: 6.385692\n",
      "Validation loss: 40.53866386413574 RMSE: 6.366998\n",
      "Validation loss: 39.341684341430664 RMSE: 6.272295\n",
      "190 10 0.7291616201400757\n",
      "Validation loss: 41.72828674316406 RMSE: 6.459743\n",
      "Validation loss: 37.69612693786621 RMSE: 6.139717\n",
      "Validation loss: 39.326507568359375 RMSE: 6.2710853\n",
      "193 12 1.5267665386199951\n",
      "Validation loss: 38.795639991760254 RMSE: 6.2286143\n",
      "Validation loss: 32.30387878417969 RMSE: 5.68365\n",
      "Validation loss: 39.81636047363281 RMSE: 6.3100204\n",
      "196 14 0.800096869468689\n",
      "Validation loss: 43.76588439941406 RMSE: 6.615579\n",
      "Validation loss: 38.59702110290527 RMSE: 6.2126503\n",
      "Validation loss: 38.795284271240234 RMSE: 6.228586\n",
      "Validation loss: 38.68513298034668 RMSE: 6.2197375\n",
      "Loaded trained model with success.\n",
      "Test loss: 23.53042994279128 Test RMSE: 4.8508177\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:1', 'task_name': 'esol', 'model_type': 'gin', 'model': {'num_layer': 5, 'emb_dim': 300, 'feat_dim': 300, 'drop_ratio': 0.3, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 777, 'task': 'regression', 'data_path': 'data/esol/esol.csv', 'target': ['measured log solubility in mols per litre']}}\n",
      "Running on: cuda:1\n",
      "1127\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/1127\n",
      "Generating scaffold 1000/1127\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 18.423805236816406\n",
      "Validation loss: 4.604345714096475 RMSE: 2.145774\n",
      "1 21 1.964418649673462\n",
      "Validation loss: 5.770572886002802 RMSE: 2.4022017\n",
      "Validation loss: 6.267575795671581 RMSE: 2.5035126\n",
      "3 13 1.8737335205078125\n",
      "Validation loss: 8.820553108654192 RMSE: 2.9699416\n",
      "Validation loss: 6.022893829683287 RMSE: 2.4541583\n",
      "5 5 1.7932159900665283\n",
      "Validation loss: 5.9734640290251875 RMSE: 2.444067\n",
      "6 26 1.6484352350234985\n",
      "Validation loss: 8.886518334920428 RMSE: 2.9810262\n",
      "Validation loss: 8.348808254815836 RMSE: 2.8894303\n",
      "8 18 0.9550918340682983\n",
      "Validation loss: 5.828820034465958 RMSE: 2.414295\n",
      "Validation loss: 8.51214595811557 RMSE: 2.9175582\n",
      "10 10 0.8756111860275269\n",
      "Validation loss: 5.490328235963805 RMSE: 2.343145\n",
      "Validation loss: 5.874517499873068 RMSE: 2.4237404\n",
      "12 2 0.7662685513496399\n",
      "Validation loss: 4.382126128779048 RMSE: 2.0933528\n",
      "13 23 1.3965473175048828\n",
      "Validation loss: 5.807072719641491 RMSE: 2.409787\n",
      "Validation loss: 4.799919807805424 RMSE: 2.190872\n",
      "15 15 1.420879602432251\n",
      "Validation loss: 4.9322852033429445 RMSE: 2.2208748\n",
      "Validation loss: 5.843570962416387 RMSE: 2.417348\n",
      "17 7 2.0313313007354736\n",
      "Validation loss: 3.9676346272493888 RMSE: 1.9918922\n",
      "18 28 2.7746026515960693\n",
      "Validation loss: 4.966526556858974 RMSE: 2.2285705\n",
      "Validation loss: 6.929661168461353 RMSE: 2.632425\n",
      "20 20 1.145577311515808\n",
      "Validation loss: 5.645413023180666 RMSE: 2.3760078\n",
      "Validation loss: 3.863047979574288 RMSE: 1.9654639\n",
      "22 12 0.381734699010849\n",
      "Validation loss: 5.525394543082313 RMSE: 2.3506157\n",
      "Validation loss: 6.870043703940063 RMSE: 2.6210768\n",
      "24 4 0.7413754463195801\n",
      "Validation loss: 5.003174013796106 RMSE: 2.2367775\n",
      "25 25 0.6460554003715515\n",
      "Validation loss: 5.970608951771154 RMSE: 2.443483\n",
      "Validation loss: 6.1232969317816 RMSE: 2.4745295\n",
      "27 17 0.7498219013214111\n",
      "Validation loss: 3.7930480800898727 RMSE: 1.947575\n",
      "Validation loss: 4.276110315744856 RMSE: 2.0678759\n",
      "29 9 1.0609663724899292\n",
      "Validation loss: 5.150574276932573 RMSE: 2.2694879\n",
      "Validation loss: 4.610645235112283 RMSE: 2.1472414\n",
      "31 1 0.8956303596496582\n",
      "Validation loss: 3.410975714700412 RMSE: 1.8468828\n",
      "32 22 0.570487916469574\n",
      "Validation loss: 4.618973677137257 RMSE: 2.14918\n",
      "Validation loss: 5.126779606911988 RMSE: 2.264239\n",
      "34 14 1.02534019947052\n",
      "Validation loss: 4.536660367408685 RMSE: 2.1299438\n",
      "Validation loss: 3.5965621555800986 RMSE: 1.8964604\n",
      "36 6 1.7257646322250366\n",
      "Validation loss: 5.054553415922992 RMSE: 2.2482333\n",
      "37 27 0.6611768007278442\n",
      "Validation loss: 4.274961513755596 RMSE: 2.067598\n",
      "Validation loss: 4.196449347301922 RMSE: 2.0485237\n",
      "39 19 1.892154335975647\n",
      "Validation loss: 4.351645165840082 RMSE: 2.0860598\n",
      "Validation loss: 4.620641271624945 RMSE: 2.1495676\n",
      "41 11 0.4669916033744812\n",
      "Validation loss: 4.066161565021076 RMSE: 2.0164726\n",
      "Validation loss: 4.021058196515108 RMSE: 2.0052576\n",
      "43 3 0.9236777424812317\n",
      "Validation loss: 4.04405493862861 RMSE: 2.0109837\n",
      "44 24 0.4581928551197052\n",
      "Validation loss: 4.934024186260932 RMSE: 2.2212663\n",
      "Validation loss: 4.440202483033712 RMSE: 2.107179\n",
      "46 16 0.2718189060688019\n",
      "Validation loss: 4.931150284488644 RMSE: 2.2206192\n",
      "Validation loss: 4.6110486309085275 RMSE: 2.1473353\n",
      "48 8 1.0404950380325317\n",
      "Validation loss: 4.637475750087637 RMSE: 2.1534798\n",
      "Validation loss: 5.398850854519194 RMSE: 2.3235428\n",
      "50 0 0.8805153965950012\n",
      "Validation loss: 4.525617540410135 RMSE: 2.1273499\n",
      "51 21 1.1501315832138062\n",
      "Validation loss: 4.464495490082597 RMSE: 2.1129353\n",
      "Validation loss: 3.741131980862238 RMSE: 1.9342006\n",
      "53 13 0.4955908954143524\n",
      "Validation loss: 3.4504494540459287 RMSE: 1.8575385\n",
      "Validation loss: 4.388046581133277 RMSE: 2.0947666\n",
      "55 5 0.6398540735244751\n",
      "Validation loss: 4.573047962863888 RMSE: 2.1384685\n",
      "56 26 0.4152144491672516\n",
      "Validation loss: 2.8563713373336115 RMSE: 1.6900802\n",
      "Validation loss: 4.477807644194207 RMSE: 2.1160831\n",
      "58 18 0.7167611122131348\n",
      "Validation loss: 4.307213002601556 RMSE: 2.0753825\n",
      "Validation loss: 5.235357100984691 RMSE: 2.2880902\n",
      "60 10 0.5411441326141357\n",
      "Validation loss: 4.643311357076189 RMSE: 2.1548343\n",
      "Validation loss: 4.710294666543471 RMSE: 2.1703215\n",
      "62 2 0.5317161083221436\n",
      "Validation loss: 4.8327514386810035 RMSE: 2.198352\n",
      "63 23 0.617778480052948\n",
      "Validation loss: 3.446121781273226 RMSE: 1.8563733\n",
      "Validation loss: 4.198544472719716 RMSE: 2.049035\n",
      "65 15 1.084781289100647\n",
      "Validation loss: 3.1384469428948596 RMSE: 1.7715662\n",
      "Validation loss: 3.97182143894972 RMSE: 1.992943\n",
      "67 7 1.251533031463623\n",
      "Validation loss: 4.323467596442298 RMSE: 2.079295\n",
      "68 28 0.526614785194397\n",
      "Validation loss: 3.7842764137065514 RMSE: 1.9453218\n",
      "Validation loss: 4.778315189665398 RMSE: 2.1859357\n",
      "70 20 1.0943547487258911\n",
      "Validation loss: 4.781673237285783 RMSE: 2.1867037\n",
      "Validation loss: 5.35394744535463 RMSE: 2.3138597\n",
      "72 12 0.39568284153938293\n",
      "Validation loss: 5.290694270513754 RMSE: 2.3001509\n",
      "Validation loss: 6.212810575434592 RMSE: 2.4925508\n",
      "74 4 0.8034099340438843\n",
      "Validation loss: 5.221862134680284 RMSE: 2.2851396\n",
      "75 25 0.5166500806808472\n",
      "Validation loss: 4.078466242393561 RMSE: 2.0195212\n",
      "Validation loss: 3.7556639435017005 RMSE: 1.9379535\n",
      "77 17 0.7339308857917786\n",
      "Validation loss: 5.397720592211833 RMSE: 2.3232996\n",
      "Validation loss: 6.531505601595988 RMSE: 2.555681\n",
      "79 9 0.4904673993587494\n",
      "Validation loss: 4.87699309914513 RMSE: 2.2083914\n",
      "Validation loss: 4.72723471379913 RMSE: 2.1742203\n",
      "81 1 0.61960768699646\n",
      "Validation loss: 5.278299306346252 RMSE: 2.297455\n",
      "82 22 0.3961094319820404\n",
      "Validation loss: 5.3028408911375875 RMSE: 2.3027897\n",
      "Validation loss: 4.311890585232625 RMSE: 2.0765092\n",
      "84 14 0.5337855815887451\n",
      "Validation loss: 4.687024681969026 RMSE: 2.1649537\n",
      "Validation loss: 3.431370102198778 RMSE: 1.8523958\n",
      "86 6 0.562173068523407\n",
      "Validation loss: 4.692593935316643 RMSE: 2.1662395\n",
      "87 27 0.4138858914375305\n",
      "Validation loss: 5.153720767097136 RMSE: 2.2701807\n",
      "Validation loss: 4.902065350946072 RMSE: 2.2140608\n",
      "89 19 1.5099480152130127\n",
      "Validation loss: 4.828806623948359 RMSE: 2.1974547\n",
      "Validation loss: 5.882190417399449 RMSE: 2.4253228\n",
      "91 11 0.6041123270988464\n",
      "Validation loss: 5.949671728421102 RMSE: 2.439195\n",
      "Validation loss: 5.222562384816397 RMSE: 2.2852926\n",
      "93 3 0.8314527869224548\n",
      "Validation loss: 5.963828500393218 RMSE: 2.442095\n",
      "94 24 0.48879241943359375\n",
      "Validation loss: 6.21064384004711 RMSE: 2.4921162\n",
      "Validation loss: 5.505959852606849 RMSE: 2.3464782\n",
      "96 16 0.6439612507820129\n",
      "Validation loss: 3.9145177149139676 RMSE: 1.9785141\n",
      "Validation loss: 4.0333054635377055 RMSE: 2.0083091\n",
      "98 8 0.4015320837497711\n",
      "Validation loss: 3.7517548356435997 RMSE: 1.9369447\n",
      "Validation loss: 3.9822653753567585 RMSE: 1.9955615\n",
      "100 0 0.3185572922229767\n",
      "Validation loss: 6.169851028813725 RMSE: 2.4839184\n",
      "101 21 0.25771844387054443\n",
      "Validation loss: 6.4424381002915645 RMSE: 2.5381958\n",
      "Validation loss: 5.650080942474635 RMSE: 2.3769898\n",
      "103 13 0.32923340797424316\n",
      "Validation loss: 4.621923936151825 RMSE: 2.149866\n",
      "Validation loss: 4.642299603571934 RMSE: 2.1545997\n",
      "105 5 2.0058679580688477\n",
      "Validation loss: 6.6173562834748125 RMSE: 2.5724223\n",
      "106 26 0.3135488033294678\n",
      "Validation loss: 4.7165380663576375 RMSE: 2.1717591\n",
      "Validation loss: 6.0652833280310166 RMSE: 2.4627798\n",
      "108 18 0.4358694553375244\n",
      "Validation loss: 5.387431480188285 RMSE: 2.3210843\n",
      "Validation loss: 7.058680272735326 RMSE: 2.6568177\n",
      "110 10 0.8399337530136108\n",
      "Validation loss: 6.129091935875142 RMSE: 2.4757004\n",
      "Validation loss: 4.999842141581848 RMSE: 2.2360327\n",
      "112 2 0.8311124444007874\n",
      "Validation loss: 7.376794587194392 RMSE: 2.7160254\n",
      "113 23 1.136040210723877\n",
      "Validation loss: 7.950338435384024 RMSE: 2.8196344\n",
      "Validation loss: 4.733290294630337 RMSE: 2.1756124\n",
      "115 15 0.33546608686447144\n",
      "Validation loss: 6.527996907191994 RMSE: 2.5549943\n",
      "Validation loss: 8.279089358000629 RMSE: 2.8773408\n",
      "117 7 0.3484550416469574\n",
      "Validation loss: 8.962463218553932 RMSE: 2.9937372\n",
      "118 28 0.5314090251922607\n",
      "Validation loss: 5.880542008222732 RMSE: 2.4249828\n",
      "Validation loss: 6.127387232485071 RMSE: 2.4753559\n",
      "120 20 1.1519091129302979\n",
      "Validation loss: 5.508955297216905 RMSE: 2.3471162\n",
      "Validation loss: 5.540539032590073 RMSE: 2.353835\n",
      "122 12 0.597217321395874\n",
      "Validation loss: 5.694550683013106 RMSE: 2.3863258\n",
      "Validation loss: 6.201701206443584 RMSE: 2.4903216\n",
      "124 4 0.5831282734870911\n",
      "Validation loss: 7.786495622280425 RMSE: 2.7904294\n",
      "125 25 0.9093800187110901\n",
      "Validation loss: 5.5289062398724855 RMSE: 2.3513627\n",
      "Validation loss: 5.347262082901676 RMSE: 2.312415\n",
      "127 17 0.4210352599620819\n",
      "Validation loss: 6.622986046613845 RMSE: 2.5735164\n",
      "Validation loss: 3.7153405214832946 RMSE: 1.9275218\n",
      "129 9 0.7699083685874939\n",
      "Validation loss: 5.178262069162014 RMSE: 2.2755795\n",
      "Validation loss: 5.896256301255352 RMSE: 2.4282207\n",
      "131 1 0.49844950437545776\n",
      "Validation loss: 6.549788164881479 RMSE: 2.5592554\n",
      "132 22 0.43090584874153137\n",
      "Validation loss: 4.825158764830733 RMSE: 2.1966243\n",
      "Validation loss: 5.359343292438878 RMSE: 2.3150256\n",
      "134 14 0.7220309376716614\n",
      "Validation loss: 5.864875675302692 RMSE: 2.4217505\n",
      "Validation loss: 4.935726714345206 RMSE: 2.2216496\n",
      "136 6 0.4255968928337097\n",
      "Validation loss: 5.439293958444511 RMSE: 2.3322294\n",
      "137 27 0.18232327699661255\n",
      "Validation loss: 5.699972494513587 RMSE: 2.3874614\n",
      "Validation loss: 6.63317624657555 RMSE: 2.5754952\n",
      "139 19 0.6644812822341919\n",
      "Validation loss: 6.153049040684658 RMSE: 2.480534\n",
      "Validation loss: 5.1993019559742075 RMSE: 2.2801976\n",
      "141 11 1.5503023862838745\n",
      "Validation loss: 4.574041130268468 RMSE: 2.1387007\n",
      "Validation loss: 5.2780987984311265 RMSE: 2.2974114\n",
      "143 3 0.9571633338928223\n",
      "Validation loss: 7.588353629660817 RMSE: 2.7546966\n",
      "144 24 0.9621426463127136\n",
      "Validation loss: 4.460790207955689 RMSE: 2.1120582\n",
      "Validation loss: 5.244725708412913 RMSE: 2.2901368\n",
      "146 16 0.3361441493034363\n",
      "Validation loss: 9.7880074492598 RMSE: 3.1285791\n",
      "Validation loss: 3.74524067777448 RMSE: 1.9352624\n",
      "148 8 0.29520413279533386\n",
      "Validation loss: 6.93318115504442 RMSE: 2.6330936\n",
      "Validation loss: 4.43097097261817 RMSE: 2.1049871\n",
      "150 0 0.7889841794967651\n",
      "Validation loss: 5.594157847682987 RMSE: 2.3651972\n",
      "151 21 0.6057075262069702\n",
      "Validation loss: 5.2847319662043475 RMSE: 2.2988546\n",
      "Validation loss: 7.462290941086491 RMSE: 2.7317195\n",
      "153 13 1.1741023063659668\n",
      "Validation loss: 6.684955567385243 RMSE: 2.5855281\n",
      "Validation loss: 7.098306997687415 RMSE: 2.664265\n",
      "155 5 0.4482807219028473\n",
      "Validation loss: 5.239373869600549 RMSE: 2.2889678\n",
      "156 26 0.6762243509292603\n",
      "Validation loss: 4.9459396885559626 RMSE: 2.223947\n",
      "Validation loss: 5.666918480290776 RMSE: 2.3805292\n",
      "158 18 0.6821041703224182\n",
      "Validation loss: 6.291636348825641 RMSE: 2.5083137\n",
      "Validation loss: 7.320552408167746 RMSE: 2.705652\n",
      "160 10 0.45881855487823486\n",
      "Validation loss: 4.57428276222364 RMSE: 2.1387572\n",
      "Validation loss: 7.2295040198132 RMSE: 2.6887736\n",
      "162 2 0.48141932487487793\n",
      "Validation loss: 4.504178895359546 RMSE: 2.122305\n",
      "163 23 0.4306827783584595\n",
      "Validation loss: 6.0679664105440665 RMSE: 2.4633243\n",
      "Validation loss: 5.260897412764288 RMSE: 2.2936647\n",
      "165 15 0.24437378346920013\n",
      "Validation loss: 5.034375718209596 RMSE: 2.2437413\n",
      "Validation loss: 6.3376628149927186 RMSE: 2.5174716\n",
      "167 7 0.287214457988739\n",
      "Validation loss: 7.922124195942836 RMSE: 2.8146267\n",
      "168 28 1.4292696714401245\n",
      "Validation loss: 5.499964494620804 RMSE: 2.3452003\n",
      "Validation loss: 7.44804549428214 RMSE: 2.7291107\n",
      "170 20 0.5842111110687256\n",
      "Validation loss: 5.61300922495074 RMSE: 2.369179\n",
      "Validation loss: 6.879216042240109 RMSE: 2.622826\n",
      "172 12 0.31403955817222595\n",
      "Validation loss: 6.505605364267805 RMSE: 2.5506086\n",
      "Validation loss: 5.446023375587126 RMSE: 2.3336716\n",
      "174 4 0.3078637421131134\n",
      "Validation loss: 6.007801393492032 RMSE: 2.4510818\n",
      "175 25 0.5936342477798462\n",
      "Validation loss: 4.97667902971791 RMSE: 2.2308474\n",
      "Validation loss: 6.303946153252526 RMSE: 2.510766\n",
      "177 17 0.3368300795555115\n",
      "Validation loss: 5.830046518714027 RMSE: 2.4145489\n",
      "Validation loss: 5.588557694865539 RMSE: 2.3640132\n",
      "179 9 0.3264007270336151\n",
      "Validation loss: 5.887511848348431 RMSE: 2.4264197\n",
      "Validation loss: 5.123001410897854 RMSE: 2.263405\n",
      "181 1 0.5878947377204895\n",
      "Validation loss: 6.064442562845956 RMSE: 2.4626088\n",
      "182 22 0.38328027725219727\n",
      "Validation loss: 4.092290317062783 RMSE: 2.022941\n",
      "Validation loss: 6.352555047094294 RMSE: 2.5204277\n",
      "184 14 0.5761188864707947\n",
      "Validation loss: 6.49546910176235 RMSE: 2.548621\n",
      "Validation loss: 7.662252620258163 RMSE: 2.7680774\n",
      "186 6 0.24726589024066925\n",
      "Validation loss: 5.737499498688014 RMSE: 2.3953078\n",
      "187 27 0.25091928243637085\n",
      "Validation loss: 6.68702597744697 RMSE: 2.5859284\n",
      "Validation loss: 7.2725187466207855 RMSE: 2.6967607\n",
      "189 19 0.27722734212875366\n",
      "Validation loss: 7.206541922240131 RMSE: 2.6845002\n",
      "Validation loss: 6.905560105247835 RMSE: 2.6278431\n",
      "191 11 0.43182459473609924\n",
      "Validation loss: 5.856672573933559 RMSE: 2.4200563\n",
      "Validation loss: 8.094304156514395 RMSE: 2.8450491\n",
      "193 3 0.5373433232307434\n",
      "Validation loss: 5.4094610298629355 RMSE: 2.3258247\n",
      "194 24 0.4475337862968445\n",
      "Validation loss: 6.661204987922601 RMSE: 2.580931\n",
      "Validation loss: 9.020984995681628 RMSE: 3.0034955\n",
      "196 16 0.4388963282108307\n",
      "Validation loss: 5.995861306654668 RMSE: 2.4486449\n",
      "Validation loss: 4.809638223816863 RMSE: 2.1930888\n",
      "198 8 1.2889840602874756\n",
      "Validation loss: 6.4152535295064474 RMSE: 2.532835\n",
      "Validation loss: 7.010094402110682 RMSE: 2.6476583\n",
      "Loaded trained model with success.\n",
      "Test loss: 3.3203810168578562 Test RMSE: 1.8221914\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:1', 'task_name': 'esol', 'model_type': 'gin', 'model': {'num_layer': 5, 'emb_dim': 300, 'feat_dim': 300, 'drop_ratio': 0.3, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 778, 'task': 'regression', 'data_path': 'data/esol/esol.csv', 'target': ['measured log solubility in mols per litre']}}\n",
      "Running on: cuda:1\n",
      "1127\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/1127\n",
      "Generating scaffold 1000/1127\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 9.838883399963379\n",
      "Validation loss: 7.623338003074173 RMSE: 2.7610395\n",
      "1 21 1.4741811752319336\n",
      "Validation loss: 9.946068729974527 RMSE: 3.1537387\n",
      "Validation loss: 12.64485386198601 RMSE: 3.5559607\n",
      "3 13 1.614366888999939\n",
      "Validation loss: 17.740253954862073 RMSE: 4.211918\n",
      "Validation loss: 12.844366968205545 RMSE: 3.5839038\n",
      "5 5 1.4409279823303223\n",
      "Validation loss: 9.694746591348563 RMSE: 3.1136386\n",
      "6 26 0.8282309174537659\n",
      "Validation loss: 13.440178947111146 RMSE: 3.6660848\n",
      "Validation loss: 12.215126442698251 RMSE: 3.4950147\n",
      "8 18 1.1066797971725464\n",
      "Validation loss: 12.086330683885423 RMSE: 3.47654\n",
      "Validation loss: 11.285593564531444 RMSE: 3.3594038\n",
      "10 10 1.7755907773971558\n",
      "Validation loss: 8.62791996508573 RMSE: 2.937332\n",
      "Validation loss: 11.52546110828366 RMSE: 3.394917\n",
      "12 2 0.8985940217971802\n",
      "Validation loss: 12.833688955391402 RMSE: 3.582414\n",
      "13 23 0.6675302982330322\n",
      "Validation loss: 13.659330638109056 RMSE: 3.6958532\n",
      "Validation loss: 14.794236090330951 RMSE: 3.8463278\n",
      "15 15 1.0464295148849487\n",
      "Validation loss: 12.590864274354107 RMSE: 3.5483606\n",
      "Validation loss: 12.039462756266635 RMSE: 3.4697928\n",
      "17 7 1.6082736253738403\n",
      "Validation loss: 11.882811761535374 RMSE: 3.4471455\n",
      "18 28 3.133575201034546\n",
      "Validation loss: 12.16135914557803 RMSE: 3.4873142\n",
      "Validation loss: 12.619682185417783 RMSE: 3.5524192\n",
      "20 20 1.2708017826080322\n",
      "Validation loss: 7.971176164340129 RMSE: 2.823327\n",
      "Validation loss: 11.447671569554151 RMSE: 3.3834407\n",
      "22 12 0.797724187374115\n",
      "Validation loss: 10.251304373276971 RMSE: 3.2017658\n",
      "Validation loss: 9.633014628317504 RMSE: 3.10371\n",
      "24 4 1.4064940214157104\n",
      "Validation loss: 10.139505061428103 RMSE: 3.184259\n",
      "25 25 0.6884217858314514\n",
      "Validation loss: 9.166739750752408 RMSE: 3.0276625\n",
      "Validation loss: 10.994405822416322 RMSE: 3.315781\n",
      "27 17 0.34997445344924927\n",
      "Validation loss: 10.337478409826229 RMSE: 3.2151947\n",
      "Validation loss: 8.96243732587426 RMSE: 2.9937332\n",
      "29 9 0.35790473222732544\n",
      "Validation loss: 8.957674342974098 RMSE: 2.9929373\n",
      "Validation loss: 9.382598843194742 RMSE: 3.0631027\n",
      "31 1 0.9690445065498352\n",
      "Validation loss: 10.04960049992114 RMSE: 3.1701105\n",
      "32 22 0.8480420708656311\n",
      "Validation loss: 11.85667462475532 RMSE: 3.443352\n",
      "Validation loss: 7.720690111143399 RMSE: 2.778613\n",
      "34 14 0.8602287769317627\n",
      "Validation loss: 7.7517703275764935 RMSE: 2.7842\n",
      "Validation loss: 8.084245386376844 RMSE: 2.8432808\n",
      "36 6 0.6820483207702637\n",
      "Validation loss: 10.156066907190644 RMSE: 3.1868587\n",
      "37 27 0.9168463945388794\n",
      "Validation loss: 9.59772815113574 RMSE: 3.09802\n",
      "Validation loss: 8.038412060357828 RMSE: 2.8352094\n",
      "39 19 0.44681021571159363\n",
      "Validation loss: 12.82963524033538 RMSE: 3.5818481\n",
      "Validation loss: 10.038136929537343 RMSE: 3.168302\n",
      "41 11 0.45377692580223083\n",
      "Validation loss: 9.051079762720429 RMSE: 3.008501\n",
      "Validation loss: 11.8822439581947 RMSE: 3.4470632\n",
      "43 3 1.0234801769256592\n",
      "Validation loss: 15.033512731569003 RMSE: 3.8773074\n",
      "44 24 0.7431588768959045\n",
      "Validation loss: 10.40720384310832 RMSE: 3.2260199\n",
      "Validation loss: 10.741225225735555 RMSE: 3.277381\n",
      "46 16 0.7651186585426331\n",
      "Validation loss: 12.204556667699222 RMSE: 3.4935021\n",
      "Validation loss: 12.918655336430643 RMSE: 3.594253\n",
      "48 8 0.5805237889289856\n",
      "Validation loss: 13.118142482453743 RMSE: 3.6218977\n",
      "Validation loss: 14.863743773603861 RMSE: 3.8553524\n",
      "50 0 0.5389307737350464\n",
      "Validation loss: 10.097231949325156 RMSE: 3.1776142\n",
      "51 21 0.9307397603988647\n",
      "Validation loss: 12.823315375674087 RMSE: 3.5809658\n",
      "Validation loss: 10.152803429460104 RMSE: 3.1863463\n",
      "53 13 1.0760010480880737\n",
      "Validation loss: 11.386215159323363 RMSE: 3.3743467\n",
      "Validation loss: 11.708180317836526 RMSE: 3.4217217\n",
      "55 5 0.7123939394950867\n",
      "Validation loss: 14.480854473282806 RMSE: 3.8053718\n",
      "56 26 1.0545684099197388\n",
      "Validation loss: 11.810739192287478 RMSE: 3.4366755\n",
      "Validation loss: 13.935694027790982 RMSE: 3.7330542\n",
      "58 18 0.5913627743721008\n",
      "Validation loss: 9.067600140529397 RMSE: 3.0112457\n",
      "Validation loss: 12.60240403740807 RMSE: 3.5499864\n",
      "60 10 0.6018245220184326\n",
      "Validation loss: 15.991866592812327 RMSE: 3.998983\n",
      "Validation loss: 14.718718452791197 RMSE: 3.8364983\n",
      "62 2 0.7226458787918091\n",
      "Validation loss: 14.088015429741514 RMSE: 3.7534003\n",
      "63 23 0.846877932548523\n",
      "Validation loss: 12.63516587282704 RMSE: 3.5545979\n",
      "Validation loss: 14.706392524516687 RMSE: 3.8348916\n",
      "65 15 0.2776222825050354\n",
      "Validation loss: 13.817851370414802 RMSE: 3.7172372\n",
      "Validation loss: 12.258067620539032 RMSE: 3.5011523\n",
      "67 7 0.9250650405883789\n",
      "Validation loss: 18.444291849051957 RMSE: 4.294682\n",
      "68 28 1.588727355003357\n",
      "Validation loss: 14.791832915449564 RMSE: 3.8460152\n",
      "Validation loss: 13.61595590979652 RMSE: 3.6899805\n",
      "70 20 0.8615308403968811\n",
      "Validation loss: 15.304627815179066 RMSE: 3.912113\n",
      "Validation loss: 16.524698341842246 RMSE: 4.065058\n",
      "72 12 0.5163202285766602\n",
      "Validation loss: 14.600581945571225 RMSE: 3.8210707\n",
      "Validation loss: 15.425935002554834 RMSE: 3.9275863\n",
      "74 4 0.35878244042396545\n",
      "Validation loss: 11.635849277530097 RMSE: 3.411136\n",
      "75 25 0.9370598793029785\n",
      "Validation loss: 16.47172678044412 RMSE: 4.0585375\n",
      "Validation loss: 10.403299276807667 RMSE: 3.2254145\n",
      "77 17 0.5081290006637573\n",
      "Validation loss: 12.300498759852047 RMSE: 3.5072067\n",
      "Validation loss: 16.575217019140194 RMSE: 4.071267\n",
      "79 9 1.6304051876068115\n",
      "Validation loss: 15.383073713927143 RMSE: 3.9221263\n",
      "Validation loss: 15.499700736155551 RMSE: 3.936966\n",
      "81 1 0.6244427561759949\n",
      "Validation loss: 13.9849380223097 RMSE: 3.7396443\n",
      "82 22 0.7569403052330017\n",
      "Validation loss: 13.980065506116478 RMSE: 3.7389925\n",
      "Validation loss: 12.911125723239595 RMSE: 3.5932055\n",
      "84 14 1.022202491760254\n",
      "Validation loss: 14.061049748311001 RMSE: 3.7498066\n",
      "Validation loss: 15.841784477233887 RMSE: 3.9801738\n",
      "86 6 0.8344551920890808\n",
      "Validation loss: 14.675884221507385 RMSE: 3.8309116\n",
      "87 27 0.3565634489059448\n",
      "Validation loss: 15.829716657115295 RMSE: 3.9786577\n",
      "Validation loss: 16.78468040871409 RMSE: 4.0969114\n",
      "89 19 0.6089535355567932\n",
      "Validation loss: 17.546124618665306 RMSE: 4.1888094\n",
      "Validation loss: 16.43399451264238 RMSE: 4.0538864\n",
      "91 11 0.48301124572753906\n",
      "Validation loss: 17.71568208880129 RMSE: 4.209\n",
      "Validation loss: 13.666002442351484 RMSE: 3.6967556\n",
      "93 3 0.4937698543071747\n",
      "Validation loss: 18.07410052814315 RMSE: 4.2513647\n",
      "94 24 0.4520724415779114\n",
      "Validation loss: 15.205119749086093 RMSE: 3.899374\n",
      "Validation loss: 15.814177690354068 RMSE: 3.9767046\n",
      "96 16 0.28578993678092957\n",
      "Validation loss: 13.930958401840345 RMSE: 3.73242\n",
      "Validation loss: 14.142819016380647 RMSE: 3.760694\n",
      "98 8 0.5154905915260315\n",
      "Validation loss: 13.548404161908985 RMSE: 3.6808157\n",
      "Validation loss: 12.129447692263442 RMSE: 3.4827354\n",
      "100 0 0.44091808795928955\n",
      "Validation loss: 18.37833266131646 RMSE: 4.286996\n",
      "101 21 0.5886672735214233\n",
      "Validation loss: 17.525774002075195 RMSE: 4.1863794\n",
      "Validation loss: 17.03004528990889 RMSE: 4.1267476\n",
      "103 13 0.645524799823761\n",
      "Validation loss: 20.28981951485693 RMSE: 4.5044227\n",
      "Validation loss: 17.094253160257256 RMSE: 4.1345196\n",
      "105 5 0.4818834960460663\n",
      "Validation loss: 13.462370273286263 RMSE: 3.6691103\n",
      "106 26 0.638257622718811\n",
      "Validation loss: 16.88222473279565 RMSE: 4.1087985\n",
      "Validation loss: 17.335107119737472 RMSE: 4.1635447\n",
      "108 18 0.5482742190361023\n",
      "Validation loss: 17.81224349536727 RMSE: 4.2204556\n",
      "Validation loss: 14.3897407329188 RMSE: 3.793381\n",
      "110 10 1.156815767288208\n",
      "Validation loss: 14.737160809272158 RMSE: 3.838901\n",
      "Validation loss: 16.34324835043038 RMSE: 4.0426784\n",
      "112 2 0.3628092110157013\n",
      "Validation loss: 17.33468595859224 RMSE: 4.1634946\n",
      "113 23 0.7717198133468628\n",
      "Validation loss: 16.602033058098986 RMSE: 4.074559\n",
      "Validation loss: 17.751255811843198 RMSE: 4.213224\n",
      "115 15 0.9256123304367065\n",
      "Validation loss: 17.87171382397677 RMSE: 4.227495\n",
      "Validation loss: 15.914291820694915 RMSE: 3.989272\n",
      "117 7 0.568530261516571\n",
      "Validation loss: 17.204327920896816 RMSE: 4.14781\n",
      "118 28 1.055589199066162\n",
      "Validation loss: 17.069150317031724 RMSE: 4.1314826\n",
      "Validation loss: 16.882383869812553 RMSE: 4.1088176\n",
      "120 20 0.28624334931373596\n",
      "Validation loss: 17.850309760169644 RMSE: 4.2249627\n",
      "Validation loss: 16.833959554148986 RMSE: 4.102921\n",
      "122 12 0.34190627932548523\n",
      "Validation loss: 17.26414820578246 RMSE: 4.155015\n",
      "Validation loss: 15.603003063033112 RMSE: 3.9500637\n",
      "124 4 0.47087061405181885\n",
      "Validation loss: 14.154659828253552 RMSE: 3.7622678\n",
      "125 25 0.6210442185401917\n",
      "Validation loss: 15.099599559750176 RMSE: 3.8858202\n",
      "Validation loss: 12.307810040701806 RMSE: 3.508249\n",
      "127 17 0.5682536959648132\n",
      "Validation loss: 17.121633867246913 RMSE: 4.13783\n",
      "Validation loss: 20.110733150380902 RMSE: 4.484499\n",
      "129 9 0.4597359895706177\n",
      "Validation loss: 15.810245193211378 RMSE: 3.9762099\n",
      "Validation loss: 17.433139986696496 RMSE: 4.175301\n",
      "131 1 0.585588812828064\n",
      "Validation loss: 15.353490238696073 RMSE: 3.9183528\n",
      "132 22 0.24423052370548248\n",
      "Validation loss: 16.15133492292556 RMSE: 4.0188723\n",
      "Validation loss: 17.236921850558932 RMSE: 4.151737\n",
      "134 14 0.6065222024917603\n",
      "Validation loss: 15.616516737811333 RMSE: 3.951774\n",
      "Validation loss: 17.582335564942486 RMSE: 4.1931295\n",
      "136 6 1.0591564178466797\n",
      "Validation loss: 15.09108799959706 RMSE: 3.8847249\n",
      "137 27 0.5127593278884888\n",
      "Validation loss: 16.94093741357854 RMSE: 4.1159368\n",
      "Validation loss: 17.968424349759534 RMSE: 4.238918\n",
      "139 19 1.0843971967697144\n",
      "Validation loss: 15.860931278330035 RMSE: 3.9825785\n",
      "Validation loss: 17.43053861634921 RMSE: 4.1749897\n",
      "141 11 0.5563310980796814\n",
      "Validation loss: 18.899330088522582 RMSE: 4.3473363\n",
      "Validation loss: 15.972472916662166 RMSE: 3.9965575\n",
      "143 3 0.3279536962509155\n",
      "Validation loss: 17.36988148647072 RMSE: 4.167719\n",
      "144 24 0.40921762585639954\n",
      "Validation loss: 16.209943197469794 RMSE: 4.0261574\n",
      "Validation loss: 16.769673828530102 RMSE: 4.0950794\n",
      "146 16 0.7569087147712708\n",
      "Validation loss: 14.614622310199568 RMSE: 3.8229077\n",
      "Validation loss: 14.847013270960446 RMSE: 3.8531823\n",
      "148 8 0.4959387481212616\n",
      "Validation loss: 12.694788717590603 RMSE: 3.5629747\n",
      "Validation loss: 20.650157337695095 RMSE: 4.5442443\n",
      "150 0 0.5437063574790955\n",
      "Validation loss: 15.290199710204538 RMSE: 3.9102685\n",
      "151 21 0.42925241589546204\n",
      "Validation loss: 18.075907943523035 RMSE: 4.2515774\n",
      "Validation loss: 20.53648636404392 RMSE: 4.53172\n",
      "153 13 0.49808308482170105\n",
      "Validation loss: 17.988517170458767 RMSE: 4.2412877\n",
      "Validation loss: 20.757193388137143 RMSE: 4.5560064\n",
      "155 5 0.40567633509635925\n",
      "Validation loss: 20.651815616979007 RMSE: 4.544427\n",
      "156 26 0.8985666036605835\n",
      "Validation loss: 15.752468041614094 RMSE: 3.9689379\n",
      "Validation loss: 16.945368083177414 RMSE: 4.116475\n",
      "158 18 0.26842767000198364\n",
      "Validation loss: 21.6376327919749 RMSE: 4.6516266\n",
      "Validation loss: 18.236674401612408 RMSE: 4.270442\n",
      "160 10 0.5709875226020813\n",
      "Validation loss: 16.06375022483083 RMSE: 4.007961\n",
      "Validation loss: 20.23691845784145 RMSE: 4.498546\n",
      "162 2 0.4106360971927643\n",
      "Validation loss: 21.360248886378464 RMSE: 4.621715\n",
      "163 23 0.37364256381988525\n",
      "Validation loss: 18.528995142573805 RMSE: 4.304532\n",
      "Validation loss: 19.469148635864258 RMSE: 4.412386\n",
      "165 15 0.3437265157699585\n",
      "Validation loss: 19.60436039477323 RMSE: 4.4276814\n",
      "Validation loss: 19.87257861247105 RMSE: 4.457867\n",
      "167 7 0.6416865587234497\n",
      "Validation loss: 21.403097878515194 RMSE: 4.626348\n",
      "168 28 0.5605310797691345\n",
      "Validation loss: 18.94727874013175 RMSE: 4.352847\n",
      "Validation loss: 18.043123844450555 RMSE: 4.24772\n",
      "170 20 0.2773442566394806\n",
      "Validation loss: 16.083149884654357 RMSE: 4.0103803\n",
      "Validation loss: 20.580404433528933 RMSE: 4.5365634\n",
      "172 12 0.2818101942539215\n",
      "Validation loss: 14.427218462513611 RMSE: 3.798318\n",
      "Validation loss: 17.94883293388164 RMSE: 4.2366066\n",
      "174 4 0.3126492500305176\n",
      "Validation loss: 21.581811668598547 RMSE: 4.645623\n",
      "175 25 0.27768778800964355\n",
      "Validation loss: 20.500235160895155 RMSE: 4.5277185\n",
      "Validation loss: 17.55601585860801 RMSE: 4.18999\n",
      "177 17 0.9464219808578491\n",
      "Validation loss: 17.448620779324422 RMSE: 4.1771545\n",
      "Validation loss: 17.41618563221619 RMSE: 4.17327\n",
      "179 9 0.48809781670570374\n",
      "Validation loss: 23.65843496069444 RMSE: 4.8639936\n",
      "Validation loss: 19.687227991829932 RMSE: 4.437029\n",
      "181 1 0.2906285226345062\n",
      "Validation loss: 19.155961619014235 RMSE: 4.3767524\n",
      "182 22 0.48096325993537903\n",
      "Validation loss: 22.868047899904504 RMSE: 4.782055\n",
      "Validation loss: 23.85060165202723 RMSE: 4.8837075\n",
      "184 14 0.4263661801815033\n",
      "Validation loss: 20.39550757619132 RMSE: 4.5161386\n",
      "Validation loss: 21.669560997886997 RMSE: 4.655058\n",
      "186 6 0.5452930331230164\n",
      "Validation loss: 22.049980653070772 RMSE: 4.6957407\n",
      "187 27 0.2597856819629669\n",
      "Validation loss: 18.850533865194404 RMSE: 4.34172\n",
      "Validation loss: 18.460245858251522 RMSE: 4.296539\n",
      "189 19 0.3497912883758545\n",
      "Validation loss: 20.57572273659495 RMSE: 4.5360465\n",
      "Validation loss: 19.92920029901825 RMSE: 4.4642134\n",
      "191 11 0.3169870972633362\n",
      "Validation loss: 19.74973045619188 RMSE: 4.4440665\n",
      "Validation loss: 19.508271284862957 RMSE: 4.4168167\n",
      "193 3 0.9311090111732483\n",
      "Validation loss: 21.360624093925004 RMSE: 4.621755\n",
      "194 24 0.6775383353233337\n",
      "Validation loss: 19.51053210908333 RMSE: 4.417073\n",
      "Validation loss: 15.429680647048276 RMSE: 3.9280632\n",
      "196 16 0.36234819889068604\n",
      "Validation loss: 16.902820148299227 RMSE: 4.111304\n",
      "Validation loss: 19.490895026552995 RMSE: 4.4148493\n",
      "198 8 0.5633448958396912\n",
      "Validation loss: 20.455770053694735 RMSE: 4.522805\n",
      "Validation loss: 24.229620908213928 RMSE: 4.9223595\n",
      "Loaded trained model with success.\n",
      "Test loss: 8.399413767114149 Test RMSE: 2.8981743\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:1', 'task_name': 'esol', 'model_type': 'gin', 'model': {'num_layer': 5, 'emb_dim': 300, 'feat_dim': 300, 'drop_ratio': 0.3, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 779, 'task': 'regression', 'data_path': 'data/esol/esol.csv', 'target': ['measured log solubility in mols per litre']}}\n",
      "Running on: cuda:1\n",
      "1127\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/1127\n",
      "Generating scaffold 1000/1127\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 16.261409759521484\n",
      "Validation loss: 4.662686845897573 RMSE: 2.1593256\n",
      "1 21 3.1850295066833496\n",
      "Validation loss: 7.2815923226618136 RMSE: 2.6984427\n",
      "Validation loss: 8.390473483938031 RMSE: 2.8966315\n",
      "3 13 1.1876850128173828\n",
      "Validation loss: 11.408640346695892 RMSE: 3.377668\n",
      "Validation loss: 12.752730622755744 RMSE: 3.5710967\n",
      "5 5 1.1153843402862549\n",
      "Validation loss: 10.46632440955238 RMSE: 3.23517\n",
      "6 26 0.5620578527450562\n",
      "Validation loss: 14.07970909523753 RMSE: 3.7522936\n",
      "Validation loss: 10.189559928083842 RMSE: 3.1921089\n",
      "8 18 0.974165678024292\n",
      "Validation loss: 15.363070698965966 RMSE: 3.9195752\n",
      "Validation loss: 11.212572207493064 RMSE: 3.348518\n",
      "10 10 0.9833012819290161\n",
      "Validation loss: 12.021021581329075 RMSE: 3.4671347\n",
      "Validation loss: 11.488824456138948 RMSE: 3.3895168\n",
      "12 2 0.8448418378829956\n",
      "Validation loss: 11.451599813140598 RMSE: 3.3840213\n",
      "13 23 1.4703598022460938\n",
      "Validation loss: 10.948571656657531 RMSE: 3.308863\n",
      "Validation loss: 12.54006162153936 RMSE: 3.541195\n",
      "15 15 0.6812912821769714\n",
      "Validation loss: 9.705183012295613 RMSE: 3.1153145\n",
      "Validation loss: 6.733891157977349 RMSE: 2.5949743\n",
      "17 7 1.286316990852356\n",
      "Validation loss: 8.92840753825365 RMSE: 2.988044\n",
      "18 28 1.1196547746658325\n",
      "Validation loss: 9.76262763116212 RMSE: 3.1245203\n",
      "Validation loss: 10.064278792491 RMSE: 3.1724248\n",
      "20 20 0.7800556421279907\n",
      "Validation loss: 5.088436223764335 RMSE: 2.2557561\n",
      "Validation loss: 8.549227007722433 RMSE: 2.923906\n",
      "22 12 0.7474104762077332\n",
      "Validation loss: 11.399106709303053 RMSE: 3.3762562\n",
      "Validation loss: 6.2552310850767965 RMSE: 2.501046\n",
      "24 4 0.9062341451644897\n",
      "Validation loss: 7.407561956253727 RMSE: 2.7216837\n",
      "25 25 0.841290295124054\n",
      "Validation loss: 6.866061413182622 RMSE: 2.620317\n",
      "Validation loss: 7.37706083956018 RMSE: 2.7160745\n",
      "27 17 1.3974357843399048\n",
      "Validation loss: 7.209125181215 RMSE: 2.6849816\n",
      "Validation loss: 6.624539814164153 RMSE: 2.5738182\n",
      "29 9 0.6782394051551819\n",
      "Validation loss: 6.604878898215505 RMSE: 2.5699959\n",
      "Validation loss: 10.970267388672955 RMSE: 3.3121395\n",
      "31 1 0.7689644694328308\n",
      "Validation loss: 8.353776138440697 RMSE: 2.89029\n",
      "32 22 0.7745819091796875\n",
      "Validation loss: 7.557514950237443 RMSE: 2.7490935\n",
      "Validation loss: 7.766640781301312 RMSE: 2.7868695\n",
      "34 14 0.9872134327888489\n",
      "Validation loss: 8.087315825234473 RMSE: 2.8438206\n",
      "Validation loss: 10.134653243343386 RMSE: 3.183497\n",
      "36 6 0.6427193284034729\n",
      "Validation loss: 7.786046323523057 RMSE: 2.7903488\n",
      "37 27 0.818246066570282\n",
      "Validation loss: 5.656357035172724 RMSE: 2.3783097\n",
      "Validation loss: 9.646664585687418 RMSE: 3.105908\n",
      "39 19 1.2930097579956055\n",
      "Validation loss: 5.958047672710587 RMSE: 2.4409113\n",
      "Validation loss: 8.433517515131857 RMSE: 2.904052\n",
      "41 11 0.5337115526199341\n",
      "Validation loss: 8.85096538172359 RMSE: 2.9750571\n",
      "Validation loss: 6.8090859303432225 RMSE: 2.6094224\n",
      "43 3 0.7247084379196167\n",
      "Validation loss: 9.49884425644326 RMSE: 3.0820196\n",
      "44 24 0.6203172206878662\n",
      "Validation loss: 7.233631269066735 RMSE: 2.6895409\n",
      "Validation loss: 9.34186171641392 RMSE: 3.0564458\n",
      "46 16 0.45401716232299805\n",
      "Validation loss: 7.813432115369139 RMSE: 2.7952516\n",
      "Validation loss: 7.132642720652893 RMSE: 2.6707008\n",
      "48 8 1.1263771057128906\n",
      "Validation loss: 8.145795585834875 RMSE: 2.8540838\n",
      "Validation loss: 8.605651990502281 RMSE: 2.9335394\n",
      "50 0 0.455217182636261\n",
      "Validation loss: 9.161639264199586 RMSE: 3.02682\n",
      "51 21 0.8948907852172852\n",
      "Validation loss: 6.661737475774984 RMSE: 2.5810342\n",
      "Validation loss: 8.111150944127445 RMSE: 2.8480084\n",
      "53 13 0.5643520355224609\n",
      "Validation loss: 6.2289477027623 RMSE: 2.495786\n",
      "Validation loss: 7.010587721799327 RMSE: 2.6477513\n",
      "55 5 0.7815256118774414\n",
      "Validation loss: 8.93999270211279 RMSE: 2.9899821\n",
      "56 26 0.5537273287773132\n",
      "Validation loss: 9.318039413047048 RMSE: 3.0525465\n",
      "Validation loss: 7.524551897977306 RMSE: 2.7430916\n",
      "58 18 0.705268383026123\n",
      "Validation loss: 6.191350337678352 RMSE: 2.4882424\n",
      "Validation loss: 6.673229643728881 RMSE: 2.5832596\n",
      "60 10 0.46305614709854126\n",
      "Validation loss: 7.7606330171095586 RMSE: 2.7857912\n",
      "Validation loss: 10.002128790965122 RMSE: 3.1626143\n",
      "62 2 1.9409465789794922\n",
      "Validation loss: 5.566315368213485 RMSE: 2.359304\n",
      "63 23 0.8508031368255615\n",
      "Validation loss: 8.731386648870147 RMSE: 2.9548922\n",
      "Validation loss: 6.080912100530304 RMSE: 2.4659505\n",
      "65 15 0.542455792427063\n",
      "Validation loss: 7.722389136795449 RMSE: 2.7789187\n",
      "Validation loss: 8.187180379850675 RMSE: 2.861325\n",
      "67 7 0.9020496606826782\n",
      "Validation loss: 5.61115494027602 RMSE: 2.3687875\n",
      "68 28 0.21396541595458984\n",
      "Validation loss: 9.201133407322706 RMSE: 3.0333366\n",
      "Validation loss: 8.200967965927799 RMSE: 2.8637333\n",
      "70 20 0.4829544425010681\n",
      "Validation loss: 8.347591239794166 RMSE: 2.88922\n",
      "Validation loss: 7.53261280903774 RMSE: 2.7445605\n",
      "72 12 0.5588904023170471\n",
      "Validation loss: 7.4273955589902085 RMSE: 2.7253249\n",
      "Validation loss: 9.384153070703016 RMSE: 3.0633566\n",
      "74 4 1.0319485664367676\n",
      "Validation loss: 9.314355217250048 RMSE: 3.051943\n",
      "75 25 0.4380528926849365\n",
      "Validation loss: 9.140073725607543 RMSE: 3.0232553\n",
      "Validation loss: 8.519350110957053 RMSE: 2.9187925\n",
      "77 17 0.7781509160995483\n",
      "Validation loss: 9.93626119394218 RMSE: 3.1521838\n",
      "Validation loss: 7.199800744520879 RMSE: 2.6832445\n",
      "79 9 0.9618350863456726\n",
      "Validation loss: 8.98739930380762 RMSE: 2.997899\n",
      "Validation loss: 8.565254827516268 RMSE: 2.9266458\n",
      "81 1 0.29979944229125977\n",
      "Validation loss: 8.084347104604266 RMSE: 2.8432987\n",
      "82 22 0.4228317141532898\n",
      "Validation loss: 6.833992768177944 RMSE: 2.6141906\n",
      "Validation loss: 9.385892522018567 RMSE: 3.0636404\n",
      "84 14 0.9630357027053833\n",
      "Validation loss: 7.252152577965661 RMSE: 2.6929822\n",
      "Validation loss: 9.083556069736987 RMSE: 3.0138938\n",
      "86 6 1.3103508949279785\n",
      "Validation loss: 7.912593326737396 RMSE: 2.8129332\n",
      "87 27 0.6699016094207764\n",
      "Validation loss: 8.3064501222256 RMSE: 2.8820913\n",
      "Validation loss: 8.077217443854408 RMSE: 2.8420446\n",
      "89 19 0.551056981086731\n",
      "Validation loss: 11.054001242713591 RMSE: 3.324756\n",
      "Validation loss: 6.612368473964455 RMSE: 2.5714526\n",
      "91 11 0.6622021198272705\n",
      "Validation loss: 6.9184323412127195 RMSE: 2.6302915\n",
      "Validation loss: 5.686058006455413 RMSE: 2.3845456\n",
      "93 3 1.6736371517181396\n",
      "Validation loss: 8.653604499006693 RMSE: 2.9417007\n",
      "94 24 0.5468230843544006\n",
      "Validation loss: 7.02630457413935 RMSE: 2.6507177\n",
      "Validation loss: 7.287132917252262 RMSE: 2.699469\n",
      "96 16 0.657698392868042\n",
      "Validation loss: 11.4653862640921 RMSE: 3.3860576\n",
      "Validation loss: 8.149900183213495 RMSE: 2.854803\n",
      "98 8 0.46892687678337097\n",
      "Validation loss: 11.059107366916352 RMSE: 3.3255236\n",
      "Validation loss: 9.528310345337454 RMSE: 3.0867963\n",
      "100 0 0.9907686710357666\n",
      "Validation loss: 5.905531490798545 RMSE: 2.4301298\n",
      "101 21 0.7656269669532776\n",
      "Validation loss: 6.552615448436906 RMSE: 2.5598078\n",
      "Validation loss: 7.168815946156999 RMSE: 2.6774645\n",
      "103 13 0.9317187070846558\n",
      "Validation loss: 11.67506866961454 RMSE: 3.41688\n",
      "Validation loss: 7.335086354112203 RMSE: 2.7083366\n",
      "105 5 0.42181360721588135\n",
      "Validation loss: 7.614819855816596 RMSE: 2.7594965\n",
      "106 26 0.9592740535736084\n",
      "Validation loss: 10.161341726252463 RMSE: 3.1876862\n",
      "Validation loss: 8.952067510216636 RMSE: 2.9920006\n",
      "108 18 0.4715310335159302\n",
      "Validation loss: 8.824428203886589 RMSE: 2.970594\n",
      "Validation loss: 6.755143055873635 RMSE: 2.5990658\n",
      "110 10 0.7674908638000488\n",
      "Validation loss: 10.226127666709697 RMSE: 3.1978316\n",
      "Validation loss: 9.914093346722359 RMSE: 3.1486652\n",
      "112 2 0.3719564974308014\n",
      "Validation loss: 7.997147581218618 RMSE: 2.8279228\n",
      "113 23 0.4797741174697876\n",
      "Validation loss: 8.341280042597678 RMSE: 2.8881276\n",
      "Validation loss: 8.3216805162683 RMSE: 2.8847325\n",
      "115 15 0.6225734353065491\n",
      "Validation loss: 9.265827854122735 RMSE: 3.0439823\n",
      "Validation loss: 8.80543163603386 RMSE: 2.9673948\n",
      "117 7 0.3896957039833069\n",
      "Validation loss: 9.569820420931926 RMSE: 3.0935128\n",
      "118 28 2.2479846477508545\n",
      "Validation loss: 9.115107063698558 RMSE: 3.0191233\n",
      "Validation loss: 6.110152839559369 RMSE: 2.4718723\n",
      "120 20 1.1717052459716797\n",
      "Validation loss: 13.54538222962776 RMSE: 3.6804051\n",
      "Validation loss: 8.157812523630868 RMSE: 2.8561883\n",
      "122 12 1.5318694114685059\n",
      "Validation loss: 9.041131509088837 RMSE: 3.0068474\n",
      "Validation loss: 12.298095281145214 RMSE: 3.506864\n",
      "124 4 0.380665123462677\n",
      "Validation loss: 9.235658544354735 RMSE: 3.0390227\n",
      "125 25 0.5934364795684814\n",
      "Validation loss: 11.598395275858651 RMSE: 3.4056418\n",
      "Validation loss: 12.994686793437046 RMSE: 3.6048143\n",
      "127 17 0.433345228433609\n",
      "Validation loss: 8.651557074183911 RMSE: 2.9413528\n",
      "Validation loss: 11.291938899892621 RMSE: 3.360348\n",
      "129 9 0.3363744020462036\n",
      "Validation loss: 12.290664942918626 RMSE: 3.5058045\n",
      "Validation loss: 9.840458228524808 RMSE: 3.1369505\n",
      "131 1 0.31831449270248413\n",
      "Validation loss: 8.19149802638366 RMSE: 2.8620794\n",
      "132 22 0.4800730049610138\n",
      "Validation loss: 10.432855979531212 RMSE: 3.229993\n",
      "Validation loss: 7.4257928375649245 RMSE: 2.725031\n",
      "134 14 0.9006035327911377\n",
      "Validation loss: 10.260211834865334 RMSE: 3.2031565\n",
      "Validation loss: 10.700360348794312 RMSE: 3.2711403\n",
      "136 6 1.0972545146942139\n",
      "Validation loss: 8.842066882985883 RMSE: 2.9735613\n",
      "137 27 1.1559417247772217\n",
      "Validation loss: 10.480196530840038 RMSE: 3.2373133\n",
      "Validation loss: 10.560739171188489 RMSE: 3.249729\n",
      "139 19 0.2980749011039734\n",
      "Validation loss: 12.558179965061424 RMSE: 3.5437522\n",
      "Validation loss: 10.192377963952259 RMSE: 3.1925502\n",
      "141 11 0.8203982710838318\n",
      "Validation loss: 8.973326796978975 RMSE: 2.995551\n",
      "Validation loss: 10.564913471188165 RMSE: 3.250371\n",
      "143 3 0.22013208270072937\n",
      "Validation loss: 8.533831980376117 RMSE: 2.9212723\n",
      "144 24 0.4028984308242798\n",
      "Validation loss: 11.04703877879455 RMSE: 3.3237085\n",
      "Validation loss: 12.783864780864885 RMSE: 3.575453\n",
      "146 16 0.2696673572063446\n",
      "Validation loss: 10.811543987915579 RMSE: 3.2880914\n",
      "Validation loss: 12.025037394160718 RMSE: 3.4677136\n",
      "148 8 0.45611217617988586\n",
      "Validation loss: 9.504430011310408 RMSE: 3.0829256\n",
      "Validation loss: 11.266074640560994 RMSE: 3.3564973\n",
      "150 0 0.4858857989311218\n",
      "Validation loss: 10.186949573786913 RMSE: 3.1917002\n",
      "151 21 0.4621642827987671\n",
      "Validation loss: 8.833724443891407 RMSE: 2.9721582\n",
      "Validation loss: 8.543599373471421 RMSE: 2.9229436\n",
      "153 13 0.4213351011276245\n",
      "Validation loss: 9.884315330370338 RMSE: 3.143933\n",
      "Validation loss: 9.152066914381178 RMSE: 3.0252383\n",
      "155 5 0.6013873815536499\n",
      "Validation loss: 6.670709014993854 RMSE: 2.5827715\n",
      "156 26 1.0044351816177368\n",
      "Validation loss: 11.014679799037697 RMSE: 3.3188372\n",
      "Validation loss: 9.064105603547223 RMSE: 3.0106652\n",
      "158 18 0.495658814907074\n",
      "Validation loss: 9.129286559282152 RMSE: 3.021471\n",
      "Validation loss: 8.29202148977634 RMSE: 2.879587\n",
      "160 10 0.4158082604408264\n",
      "Validation loss: 9.773205638986774 RMSE: 3.1262128\n",
      "Validation loss: 10.341690932754922 RMSE: 3.2158499\n",
      "162 2 0.23800426721572876\n",
      "Validation loss: 11.806403041940875 RMSE: 3.436045\n",
      "163 23 0.3968792259693146\n",
      "Validation loss: 10.988718716444167 RMSE: 3.3149238\n",
      "Validation loss: 9.150663899109427 RMSE: 3.0250065\n",
      "165 15 0.3329602777957916\n",
      "Validation loss: 9.273622158354362 RMSE: 3.0452623\n",
      "Validation loss: 9.128962871247689 RMSE: 3.0214174\n",
      "167 7 0.7733281254768372\n",
      "Validation loss: 10.674188137054443 RMSE: 3.2671375\n",
      "168 28 1.8604990243911743\n",
      "Validation loss: 8.934427666453134 RMSE: 2.9890513\n",
      "Validation loss: 10.54470157623291 RMSE: 3.2472606\n",
      "170 20 0.8013900518417358\n",
      "Validation loss: 12.754652765999854 RMSE: 3.5713656\n",
      "Validation loss: 12.284053346752065 RMSE: 3.504861\n",
      "172 12 0.6687169671058655\n",
      "Validation loss: 7.967974848451868 RMSE: 2.82276\n",
      "Validation loss: 11.25858609866252 RMSE: 3.3553817\n",
      "174 4 0.4838516116142273\n",
      "Validation loss: 8.427754798821644 RMSE: 2.9030597\n",
      "175 25 0.620868980884552\n",
      "Validation loss: 12.458461575803504 RMSE: 3.5296547\n",
      "Validation loss: 12.116871749405313 RMSE: 3.4809299\n",
      "177 17 0.31786370277404785\n",
      "Validation loss: 10.436466276118185 RMSE: 3.2305522\n",
      "Validation loss: 9.037944624909258 RMSE: 3.0063174\n",
      "179 9 0.2753937542438507\n",
      "Validation loss: 13.017528044438995 RMSE: 3.6079812\n",
      "Validation loss: 9.461320792679238 RMSE: 3.075926\n",
      "181 1 0.47463181614875793\n",
      "Validation loss: 11.442255956936727 RMSE: 3.3826404\n",
      "182 22 0.25718241930007935\n",
      "Validation loss: 10.571362132519747 RMSE: 3.251363\n",
      "Validation loss: 10.1511995602498 RMSE: 3.1860945\n",
      "184 14 0.26382461190223694\n",
      "Validation loss: 10.613579961050927 RMSE: 3.257849\n",
      "Validation loss: 11.75729704325178 RMSE: 3.4288917\n",
      "186 6 0.40790003538131714\n",
      "Validation loss: 11.170993425149833 RMSE: 3.3423038\n",
      "187 27 0.5838799476623535\n",
      "Validation loss: 13.174917280146506 RMSE: 3.6297271\n",
      "Validation loss: 11.988107655955627 RMSE: 3.4623847\n",
      "189 19 0.7065718173980713\n",
      "Validation loss: 12.919032350050665 RMSE: 3.5943058\n",
      "Validation loss: 8.670389107898274 RMSE: 2.9445524\n",
      "191 11 0.4878856837749481\n",
      "Validation loss: 13.137331430890919 RMSE: 3.6245456\n",
      "Validation loss: 12.005918460609639 RMSE: 3.4649558\n",
      "193 3 0.520051896572113\n",
      "Validation loss: 11.53041755625632 RMSE: 3.3956468\n",
      "194 24 0.15272295475006104\n",
      "Validation loss: 11.984702996448078 RMSE: 3.4618928\n",
      "Validation loss: 11.425026353481597 RMSE: 3.3800926\n",
      "196 16 0.32449814677238464\n",
      "Validation loss: 9.379393180914684 RMSE: 3.0625796\n",
      "Validation loss: 12.462703198458241 RMSE: 3.5302553\n",
      "198 8 1.0201338529586792\n",
      "Validation loss: 12.575259385910709 RMSE: 3.5461614\n",
      "Validation loss: 11.573789503722065 RMSE: 3.4020274\n",
      "Loaded trained model with success.\n",
      "Test loss: 5.291520764342452 Test RMSE: 2.3003306\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:1', 'task_name': 'esol', 'model_type': 'gin', 'model': {'num_layer': 5, 'emb_dim': 300, 'feat_dim': 300, 'drop_ratio': 0.3, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 780, 'task': 'regression', 'data_path': 'data/esol/esol.csv', 'target': ['measured log solubility in mols per litre']}}\n",
      "Running on: cuda:1\n",
      "1127\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/1127\n",
      "Generating scaffold 1000/1127\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 8.488154411315918\n",
      "Validation loss: 6.846353724994491 RMSE: 2.6165538\n",
      "1 21 2.4553744792938232\n",
      "Validation loss: 9.418539806804826 RMSE: 3.0689638\n",
      "Validation loss: 9.22767018427891 RMSE: 3.037708\n",
      "3 13 2.3348910808563232\n",
      "Validation loss: 8.762732510018138 RMSE: 2.9601912\n",
      "Validation loss: 7.78489067280187 RMSE: 2.7901418\n",
      "5 5 2.4915425777435303\n",
      "Validation loss: 9.406735563700177 RMSE: 3.0670404\n",
      "6 26 0.6781152486801147\n",
      "Validation loss: 9.058493386327694 RMSE: 3.0097332\n",
      "Validation loss: 13.798023569900378 RMSE: 3.714569\n",
      "8 18 1.2296888828277588\n",
      "Validation loss: 11.682935605006936 RMSE: 3.418031\n",
      "Validation loss: 7.653509258168988 RMSE: 2.7664979\n",
      "10 10 0.9399132132530212\n",
      "Validation loss: 7.69889002774669 RMSE: 2.7746873\n",
      "Validation loss: 11.808739273949007 RMSE: 3.4363847\n",
      "12 2 1.1728942394256592\n",
      "Validation loss: 7.107199884093968 RMSE: 2.6659331\n",
      "13 23 0.9271973371505737\n",
      "Validation loss: 5.522383407153915 RMSE: 2.349975\n",
      "Validation loss: 6.249855362208543 RMSE: 2.499971\n",
      "15 15 0.6208361387252808\n",
      "Validation loss: 6.833866575122935 RMSE: 2.6141665\n",
      "Validation loss: 6.133860697788474 RMSE: 2.4766634\n",
      "17 7 0.9123390316963196\n",
      "Validation loss: 6.376299014133689 RMSE: 2.5251334\n",
      "18 28 2.8600800037384033\n",
      "Validation loss: 8.1375813273202 RMSE: 2.8526444\n",
      "Validation loss: 6.69715351341045 RMSE: 2.5878859\n",
      "20 20 0.688256561756134\n",
      "Validation loss: 7.646617188917852 RMSE: 2.7652516\n",
      "Validation loss: 6.578605748910819 RMSE: 2.5648792\n",
      "22 12 1.135056734085083\n",
      "Validation loss: 7.804174613108677 RMSE: 2.7935953\n",
      "Validation loss: 6.5897771278313835 RMSE: 2.5670562\n",
      "24 4 0.9101400375366211\n",
      "Validation loss: 4.217754794433054 RMSE: 2.0537171\n",
      "25 25 1.2821834087371826\n",
      "Validation loss: 7.483803926315983 RMSE: 2.7356544\n",
      "Validation loss: 5.719932522393961 RMSE: 2.391638\n",
      "27 17 1.1423470973968506\n",
      "Validation loss: 7.382239050569788 RMSE: 2.7170277\n",
      "Validation loss: 6.709158458540925 RMSE: 2.5902045\n",
      "29 9 1.1134378910064697\n",
      "Validation loss: 8.018146898894184 RMSE: 2.8316333\n",
      "Validation loss: 6.4158625138544405 RMSE: 2.5329554\n",
      "31 1 1.0134997367858887\n",
      "Validation loss: 5.8720993025113 RMSE: 2.4232414\n",
      "32 22 0.5897430777549744\n",
      "Validation loss: 7.799626970713118 RMSE: 2.792781\n",
      "Validation loss: 5.604084171025099 RMSE: 2.3672948\n",
      "34 14 0.6349347233772278\n",
      "Validation loss: 3.7867630013322406 RMSE: 1.9459608\n",
      "Validation loss: 4.33481256096764 RMSE: 2.0820212\n",
      "36 6 1.5004640817642212\n",
      "Validation loss: 7.544199597519056 RMSE: 2.7466705\n",
      "37 27 0.6672159433364868\n",
      "Validation loss: 8.658848787830994 RMSE: 2.9425921\n",
      "Validation loss: 5.9508928619654835 RMSE: 2.439445\n",
      "39 19 1.1817444562911987\n",
      "Validation loss: 7.356433045547621 RMSE: 2.7122746\n",
      "Validation loss: 5.774675373482493 RMSE: 2.4030554\n",
      "41 11 0.7642608880996704\n",
      "Validation loss: 8.400541748620768 RMSE: 2.8983688\n",
      "Validation loss: 7.437214674147884 RMSE: 2.7271256\n",
      "43 3 0.4243224859237671\n",
      "Validation loss: 6.373880301956582 RMSE: 2.5246546\n",
      "44 24 0.7052769660949707\n",
      "Validation loss: 6.700497112442962 RMSE: 2.588532\n",
      "Validation loss: 9.071552993976965 RMSE: 3.0119019\n",
      "46 16 0.6867020726203918\n",
      "Validation loss: 11.322632198840116 RMSE: 3.364912\n",
      "Validation loss: 8.995786827222435 RMSE: 2.9992976\n",
      "48 8 1.0905207395553589\n",
      "Validation loss: 9.484319416822585 RMSE: 3.0796623\n",
      "Validation loss: 7.948602802985538 RMSE: 2.8193266\n",
      "50 0 1.0296984910964966\n",
      "Validation loss: 13.304927041045332 RMSE: 3.6475918\n",
      "51 21 0.7310747504234314\n",
      "Validation loss: 11.164369051435353 RMSE: 3.3413122\n",
      "Validation loss: 9.255401442536211 RMSE: 3.0422692\n",
      "53 13 0.5437337160110474\n",
      "Validation loss: 12.84482216624032 RMSE: 3.5839674\n",
      "Validation loss: 9.815483405526761 RMSE: 3.1329672\n",
      "55 5 0.626700222492218\n",
      "Validation loss: 7.895206746801866 RMSE: 2.8098412\n",
      "56 26 1.122983455657959\n",
      "Validation loss: 7.206753701235341 RMSE: 2.6845398\n",
      "Validation loss: 11.307469190749448 RMSE: 3.362658\n",
      "58 18 0.921672523021698\n",
      "Validation loss: 8.980520813865999 RMSE: 2.9967515\n",
      "Validation loss: 12.249813974431131 RMSE: 3.4999735\n",
      "60 10 0.6346232891082764\n",
      "Validation loss: 9.984605274369232 RMSE: 3.1598427\n",
      "Validation loss: 11.069739662440478 RMSE: 3.327122\n",
      "62 2 0.6579108238220215\n",
      "Validation loss: 7.245846123821967 RMSE: 2.6918108\n",
      "63 23 0.703533947467804\n",
      "Validation loss: 7.722215863455713 RMSE: 2.7788875\n",
      "Validation loss: 9.613160791650282 RMSE: 3.1005096\n",
      "65 15 1.2882579565048218\n",
      "Validation loss: 12.345019804692901 RMSE: 3.5135481\n",
      "Validation loss: 8.95505422406492 RMSE: 2.9924996\n",
      "67 7 0.9218940734863281\n",
      "Validation loss: 7.480330070563122 RMSE: 2.7350192\n",
      "68 28 0.6925370693206787\n",
      "Validation loss: 10.668559154578015 RMSE: 3.266276\n",
      "Validation loss: 8.496040588986558 RMSE: 2.9147968\n",
      "70 20 0.5430574417114258\n",
      "Validation loss: 4.750888794924306 RMSE: 2.1796532\n",
      "Validation loss: 7.370448479610207 RMSE: 2.714857\n",
      "72 12 0.4104083776473999\n",
      "Validation loss: 6.133196780111938 RMSE: 2.4765294\n",
      "Validation loss: 8.53820461509502 RMSE: 2.9220207\n",
      "74 4 0.3288443088531494\n",
      "Validation loss: 6.971781241155304 RMSE: 2.640413\n",
      "75 25 0.7518443465232849\n",
      "Validation loss: 7.670746524777033 RMSE: 2.7696114\n",
      "Validation loss: 10.023678737404072 RMSE: 3.1660194\n",
      "77 17 2.10444974899292\n",
      "Validation loss: 9.339173384472332 RMSE: 3.0560062\n",
      "Validation loss: 5.746047391300708 RMSE: 2.3970916\n",
      "79 9 0.7145379185676575\n",
      "Validation loss: 10.28664728181552 RMSE: 3.2072802\n",
      "Validation loss: 15.230734766057108 RMSE: 3.9026575\n",
      "81 1 0.4517553448677063\n",
      "Validation loss: 9.675953400873505 RMSE: 3.1106193\n",
      "82 22 0.6583828330039978\n",
      "Validation loss: 10.904789173497562 RMSE: 3.3022401\n",
      "Validation loss: 9.045816227398088 RMSE: 3.0076263\n",
      "84 14 0.5416055917739868\n",
      "Validation loss: 7.916320408340049 RMSE: 2.8135955\n",
      "Validation loss: 6.8933793380197175 RMSE: 2.6255245\n",
      "86 6 0.6812360286712646\n",
      "Validation loss: 8.332349275065734 RMSE: 2.886581\n",
      "87 27 0.6890107989311218\n",
      "Validation loss: 10.488974773778324 RMSE: 3.2386687\n",
      "Validation loss: 7.3422921653342454 RMSE: 2.7096665\n",
      "89 19 0.6515329480171204\n",
      "Validation loss: 6.026346206665039 RMSE: 2.4548619\n",
      "Validation loss: 6.887274210431935 RMSE: 2.6243615\n",
      "91 11 1.064634084701538\n",
      "Validation loss: 7.390002065000281 RMSE: 2.7184558\n",
      "Validation loss: 8.771400422121571 RMSE: 2.961655\n",
      "93 3 0.3262120187282562\n",
      "Validation loss: 4.98501485427924 RMSE: 2.2327147\n",
      "94 24 0.45590394735336304\n",
      "Validation loss: 10.99576291784776 RMSE: 3.315986\n",
      "Validation loss: 5.070879079599296 RMSE: 2.2518613\n",
      "96 16 1.5477466583251953\n",
      "Validation loss: 8.663688895976648 RMSE: 2.9434144\n",
      "Validation loss: 8.675442611221719 RMSE: 2.9454105\n",
      "98 8 0.6111932992935181\n",
      "Validation loss: 6.524742898687852 RMSE: 2.5543578\n",
      "Validation loss: 4.997456065321391 RMSE: 2.2354991\n",
      "100 0 0.3281944990158081\n",
      "Validation loss: 8.096101743985066 RMSE: 2.8453648\n",
      "101 21 0.6207330822944641\n",
      "Validation loss: 14.168051171091806 RMSE: 3.7640471\n",
      "Validation loss: 7.4268830054629165 RMSE: 2.725231\n",
      "103 13 0.4305010437965393\n",
      "Validation loss: 7.934985042673297 RMSE: 2.8169105\n",
      "Validation loss: 7.972533217573588 RMSE: 2.8235674\n",
      "105 5 0.5735244750976562\n",
      "Validation loss: 8.649568895323087 RMSE: 2.941015\n",
      "106 26 0.6253174543380737\n",
      "Validation loss: 12.406952655420895 RMSE: 3.5223505\n",
      "Validation loss: 12.971560393814492 RMSE: 3.6016052\n",
      "108 18 0.3583603501319885\n",
      "Validation loss: 12.569700654629058 RMSE: 3.5453775\n",
      "Validation loss: 13.663294690900145 RMSE: 3.6963894\n",
      "110 10 0.3581041991710663\n",
      "Validation loss: 14.53098676900948 RMSE: 3.811953\n",
      "Validation loss: 11.929054142099567 RMSE: 3.4538462\n",
      "112 2 0.4190811812877655\n",
      "Validation loss: 9.681173392101726 RMSE: 3.1114585\n",
      "113 23 0.5220209956169128\n",
      "Validation loss: 13.11106355211376 RMSE: 3.6209204\n",
      "Validation loss: 15.379807852010812 RMSE: 3.9217098\n",
      "115 15 1.0658730268478394\n",
      "Validation loss: 11.323464351417744 RMSE: 3.3650355\n",
      "Validation loss: 9.81285548421134 RMSE: 3.1325476\n",
      "117 7 0.5934862494468689\n",
      "Validation loss: 12.39755465077088 RMSE: 3.5210161\n",
      "118 28 3.6677305698394775\n",
      "Validation loss: 11.933285679437418 RMSE: 3.4544587\n",
      "Validation loss: 9.656838180744543 RMSE: 3.1075454\n",
      "120 20 0.640655517578125\n",
      "Validation loss: 12.750523195857495 RMSE: 3.5707874\n",
      "Validation loss: 8.15267182240444 RMSE: 2.8552885\n",
      "122 12 0.7543997168540955\n",
      "Validation loss: 9.089929960470284 RMSE: 3.0149512\n",
      "Validation loss: 10.92296526917314 RMSE: 3.304991\n",
      "124 4 1.0550650358200073\n",
      "Validation loss: 10.467612258100932 RMSE: 3.2353687\n",
      "125 25 0.399623841047287\n",
      "Validation loss: 10.925330491192573 RMSE: 3.3053486\n",
      "Validation loss: 10.973777078949245 RMSE: 3.312669\n",
      "127 17 0.44066980481147766\n",
      "Validation loss: 11.249956468565275 RMSE: 3.3540955\n",
      "Validation loss: 10.023967692282348 RMSE: 3.166065\n",
      "129 9 0.4981299340724945\n",
      "Validation loss: 9.077129338694885 RMSE: 3.0128276\n",
      "Validation loss: 12.934947629945468 RMSE: 3.5965188\n",
      "131 1 0.44593507051467896\n",
      "Validation loss: 15.57626828894151 RMSE: 3.9466782\n",
      "132 22 0.7470808625221252\n",
      "Validation loss: 12.163369845500034 RMSE: 3.4876022\n",
      "Validation loss: 11.17275925020201 RMSE: 3.3425677\n",
      "134 14 0.4483450651168823\n",
      "Validation loss: 12.230451963644112 RMSE: 3.4972064\n",
      "Validation loss: 12.52685200007616 RMSE: 3.539329\n",
      "136 6 0.8502964377403259\n",
      "Validation loss: 10.44245621166398 RMSE: 3.2314792\n",
      "137 27 0.7605030536651611\n",
      "Validation loss: 11.875816665919482 RMSE: 3.4461308\n",
      "Validation loss: 9.112520015345211 RMSE: 3.0186949\n",
      "139 19 0.25624316930770874\n",
      "Validation loss: 8.184405609569719 RMSE: 2.86084\n",
      "Validation loss: 9.559327328099613 RMSE: 3.091816\n",
      "141 11 0.46172231435775757\n",
      "Validation loss: 8.020102273046444 RMSE: 2.8319786\n",
      "Validation loss: 8.650184006817573 RMSE: 2.9411197\n",
      "143 3 0.3601950705051422\n",
      "Validation loss: 8.885934504787478 RMSE: 2.9809284\n",
      "144 24 0.3507613241672516\n",
      "Validation loss: 9.674295687042507 RMSE: 3.110353\n",
      "Validation loss: 9.379095550132009 RMSE: 3.0625308\n",
      "146 16 0.6019730567932129\n",
      "Validation loss: 8.451501137387437 RMSE: 2.9071465\n",
      "Validation loss: 9.902118210243968 RMSE: 3.146763\n",
      "148 8 0.5138280987739563\n",
      "Validation loss: 11.682546615600586 RMSE: 3.417974\n",
      "Validation loss: 9.934987101934652 RMSE: 3.1519814\n",
      "150 0 0.39424294233322144\n",
      "Validation loss: 10.813127635854535 RMSE: 3.2883322\n",
      "151 21 0.4013451337814331\n",
      "Validation loss: 9.685948608195886 RMSE: 3.1122255\n",
      "Validation loss: 12.36795664255598 RMSE: 3.5168107\n",
      "153 13 0.2203248143196106\n",
      "Validation loss: 10.716668289319603 RMSE: 3.2736323\n",
      "Validation loss: 13.662217292110476 RMSE: 3.6962435\n",
      "155 5 0.30970755219459534\n",
      "Validation loss: 11.328638397486865 RMSE: 3.3658042\n",
      "156 26 0.3199070394039154\n",
      "Validation loss: 10.536882037610079 RMSE: 3.2460566\n",
      "Validation loss: 9.85873869668066 RMSE: 3.1398628\n",
      "158 18 0.2883663475513458\n",
      "Validation loss: 9.24024473038395 RMSE: 3.039777\n",
      "Validation loss: 9.363141878516274 RMSE: 3.059925\n",
      "160 10 0.4722653329372406\n",
      "Validation loss: 9.22932256850521 RMSE: 3.03798\n",
      "Validation loss: 7.3821890881631225 RMSE: 2.7170184\n",
      "162 2 0.6685654520988464\n",
      "Validation loss: 9.765016792094812 RMSE: 3.1249027\n",
      "163 23 0.5407613515853882\n",
      "Validation loss: 8.955581276817659 RMSE: 2.9925878\n",
      "Validation loss: 5.806636683708798 RMSE: 2.4096963\n",
      "165 15 0.29007112979888916\n",
      "Validation loss: 7.180755640553162 RMSE: 2.6796935\n",
      "Validation loss: 10.697852379452867 RMSE: 3.2707572\n",
      "167 7 0.34963124990463257\n",
      "Validation loss: 11.544220198572209 RMSE: 3.3976789\n",
      "168 28 0.45033517479896545\n",
      "Validation loss: 10.945256089742205 RMSE: 3.3083618\n",
      "Validation loss: 10.300080029310378 RMSE: 3.2093737\n",
      "170 20 0.36103934049606323\n",
      "Validation loss: 5.854408939327814 RMSE: 2.4195886\n",
      "Validation loss: 7.371735479979389 RMSE: 2.7150939\n",
      "172 12 0.4489496350288391\n",
      "Validation loss: 10.457893785122222 RMSE: 3.2338667\n",
      "Validation loss: 9.269083246720575 RMSE: 3.0445168\n",
      "174 4 0.6629477739334106\n",
      "Validation loss: 7.052662925382631 RMSE: 2.655685\n",
      "175 25 0.6371884942054749\n",
      "Validation loss: 9.082588077646442 RMSE: 3.0137331\n",
      "Validation loss: 9.073674750539054 RMSE: 3.0122538\n",
      "177 17 0.5679768323898315\n",
      "Validation loss: 7.47380874009259 RMSE: 2.7338269\n",
      "Validation loss: 8.411639838092094 RMSE: 2.9002826\n",
      "179 9 0.926272988319397\n",
      "Validation loss: 7.146493641676101 RMSE: 2.6732926\n",
      "Validation loss: 8.35344478725332 RMSE: 2.8902328\n",
      "181 1 0.6159707903862\n",
      "Validation loss: 8.46144765246231 RMSE: 2.9088569\n",
      "182 22 0.5779752135276794\n",
      "Validation loss: 4.449351057542109 RMSE: 2.1093488\n",
      "Validation loss: 7.329929617653906 RMSE: 2.7073846\n",
      "184 14 0.5746431350708008\n",
      "Validation loss: 7.800251618950768 RMSE: 2.7928932\n",
      "Validation loss: 7.3867840851302695 RMSE: 2.7178638\n",
      "186 6 0.3047384023666382\n",
      "Validation loss: 7.471410489715306 RMSE: 2.7333882\n",
      "187 27 0.4161697030067444\n",
      "Validation loss: 10.899825391516222 RMSE: 3.3014884\n",
      "Validation loss: 12.521189191700083 RMSE: 3.5385294\n",
      "189 19 0.3799557685852051\n",
      "Validation loss: 10.38440422463206 RMSE: 3.2224844\n",
      "Validation loss: 8.391873030535942 RMSE: 2.8968728\n",
      "191 11 0.2970106899738312\n",
      "Validation loss: 9.190443085358206 RMSE: 3.0315742\n",
      "Validation loss: 9.10589899215023 RMSE: 3.0175982\n",
      "193 3 0.3286495804786682\n",
      "Validation loss: 10.10603622841624 RMSE: 3.1789992\n",
      "194 24 0.584380030632019\n",
      "Validation loss: 10.159466152697538 RMSE: 3.1873918\n",
      "Validation loss: 8.717925097035096 RMSE: 2.9526134\n",
      "196 16 1.1876360177993774\n",
      "Validation loss: 9.137438715031717 RMSE: 3.0228198\n",
      "Validation loss: 10.111126051539868 RMSE: 3.1797996\n",
      "198 8 0.5581521391868591\n",
      "Validation loss: 10.36064175377905 RMSE: 3.218795\n",
      "Validation loss: 9.363960886423566 RMSE: 3.0600588\n",
      "Loaded trained model with success.\n",
      "Test loss: 3.988818250926195 Test RMSE: 1.9972026\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:1', 'task_name': 'esol', 'model_type': 'gin', 'model': {'num_layer': 5, 'emb_dim': 300, 'feat_dim': 300, 'drop_ratio': 0.3, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 781, 'task': 'regression', 'data_path': 'data/esol/esol.csv', 'target': ['measured log solubility in mols per litre']}}\n",
      "Running on: cuda:1\n",
      "1127\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/1127\n",
      "Generating scaffold 1000/1127\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 13.651568412780762\n",
      "Validation loss: 4.86226948594625 RMSE: 2.2050555\n",
      "1 21 2.256969690322876\n",
      "Validation loss: 6.022658082236231 RMSE: 2.4541104\n",
      "Validation loss: 6.622486253755283 RMSE: 2.573419\n",
      "3 13 2.109445333480835\n",
      "Validation loss: 8.005507452298055 RMSE: 2.8294008\n",
      "Validation loss: 10.222821581680163 RMSE: 3.1973147\n",
      "5 5 2.7589056491851807\n",
      "Validation loss: 4.85172911871851 RMSE: 2.2026641\n",
      "6 26 1.4128705263137817\n",
      "Validation loss: 5.868681962511181 RMSE: 2.4225364\n",
      "Validation loss: 6.038166126318737 RMSE: 2.457268\n",
      "8 18 1.6827497482299805\n",
      "Validation loss: 4.352764146517863 RMSE: 2.0863278\n",
      "Validation loss: 8.696198429681559 RMSE: 2.9489317\n",
      "10 10 1.1138845682144165\n",
      "Validation loss: 5.608844571408972 RMSE: 2.3683\n",
      "Validation loss: 5.0448078898202 RMSE: 2.2460651\n",
      "12 2 1.0578722953796387\n",
      "Validation loss: 4.908262491226196 RMSE: 2.2154598\n",
      "13 23 1.4184083938598633\n",
      "Validation loss: 7.231021923301494 RMSE: 2.689056\n",
      "Validation loss: 7.158570711591603 RMSE: 2.6755505\n",
      "15 15 0.7095008492469788\n",
      "Validation loss: 8.439008468020278 RMSE: 2.904997\n",
      "Validation loss: 7.290966084573121 RMSE: 2.7001789\n",
      "17 7 0.9155540466308594\n",
      "Validation loss: 7.777203606293265 RMSE: 2.788764\n",
      "18 28 2.0666863918304443\n",
      "Validation loss: 10.11122273976824 RMSE: 3.1798148\n",
      "Validation loss: 7.516108040261058 RMSE: 2.741552\n",
      "20 20 0.6088041663169861\n",
      "Validation loss: 7.73705697692601 RMSE: 2.7815566\n",
      "Validation loss: 10.00557311235276 RMSE: 3.1631587\n",
      "22 12 1.020979881286621\n",
      "Validation loss: 11.553127390093508 RMSE: 3.3989892\n",
      "Validation loss: 11.227219429691282 RMSE: 3.3507044\n",
      "24 4 1.1845107078552246\n",
      "Validation loss: 8.413614990436926 RMSE: 2.9006233\n",
      "25 25 0.45961469411849976\n",
      "Validation loss: 9.711014338299236 RMSE: 3.1162503\n",
      "Validation loss: 9.123003065058615 RMSE: 3.020431\n",
      "27 17 0.70528244972229\n",
      "Validation loss: 8.471507089327922 RMSE: 2.9105854\n",
      "Validation loss: 9.666116511927243 RMSE: 3.1090379\n",
      "29 9 0.5831129550933838\n",
      "Validation loss: 8.165891140963124 RMSE: 2.8576024\n",
      "Validation loss: 7.2842775741509636 RMSE: 2.69894\n",
      "31 1 0.8688613772392273\n",
      "Validation loss: 7.182727016178908 RMSE: 2.6800609\n",
      "32 22 0.9955456852912903\n",
      "Validation loss: 8.284707841619982 RMSE: 2.8783169\n",
      "Validation loss: 6.8232128092672975 RMSE: 2.612128\n",
      "34 14 1.148715615272522\n",
      "Validation loss: 6.891541029499695 RMSE: 2.6251745\n",
      "Validation loss: 7.8248936053925915 RMSE: 2.7973013\n",
      "36 6 1.428039789199829\n",
      "Validation loss: 7.0307814252060075 RMSE: 2.6515622\n",
      "37 27 1.1386274099349976\n",
      "Validation loss: 7.177944740362927 RMSE: 2.6791687\n",
      "Validation loss: 6.388193864737992 RMSE: 2.5274875\n",
      "39 19 0.5462741851806641\n",
      "Validation loss: 9.810069565224437 RMSE: 3.132103\n",
      "Validation loss: 7.07986326977215 RMSE: 2.6608014\n",
      "41 11 0.5959132313728333\n",
      "Validation loss: 7.42635089739234 RMSE: 2.7251332\n",
      "Validation loss: 10.692648220906216 RMSE: 3.2699614\n",
      "43 3 0.45175492763519287\n",
      "Validation loss: 9.044967887675867 RMSE: 3.0074852\n",
      "44 24 0.5385080575942993\n",
      "Validation loss: 10.49131450821868 RMSE: 3.23903\n",
      "Validation loss: 7.796527465887829 RMSE: 2.7922263\n",
      "46 16 0.8061509132385254\n",
      "Validation loss: 8.083426289853797 RMSE: 2.8431365\n",
      "Validation loss: 7.28519661658633 RMSE: 2.6991105\n",
      "48 8 0.5519139766693115\n",
      "Validation loss: 8.724958031578401 RMSE: 2.953804\n",
      "Validation loss: 6.252495613773312 RMSE: 2.500499\n",
      "50 0 0.5855355858802795\n",
      "Validation loss: 5.669375955531027 RMSE: 2.381045\n",
      "51 21 1.4605515003204346\n",
      "Validation loss: 7.017524132686379 RMSE: 2.6490612\n",
      "Validation loss: 7.726025729052789 RMSE: 2.779573\n",
      "53 13 0.4658614993095398\n",
      "Validation loss: 7.630922828100424 RMSE: 2.7624125\n",
      "Validation loss: 6.977321000225776 RMSE: 2.641462\n",
      "55 5 0.6251709461212158\n",
      "Validation loss: 11.0179975813469 RMSE: 3.319337\n",
      "56 26 0.8113086223602295\n",
      "Validation loss: 7.369191456685024 RMSE: 2.7146254\n",
      "Validation loss: 7.611599947498963 RMSE: 2.7589128\n",
      "58 18 1.044203519821167\n",
      "Validation loss: 5.833175652849991 RMSE: 2.415197\n",
      "Validation loss: 7.152088570383798 RMSE: 2.6743388\n",
      "60 10 0.6516397595405579\n",
      "Validation loss: 6.714115412889329 RMSE: 2.5911608\n",
      "Validation loss: 7.443839393885789 RMSE: 2.7283401\n",
      "62 2 1.5835877656936646\n",
      "Validation loss: 6.627683158469411 RMSE: 2.5744288\n",
      "63 23 0.6602221727371216\n",
      "Validation loss: 5.468885907029684 RMSE: 2.3385649\n",
      "Validation loss: 5.482742161877387 RMSE: 2.3415256\n",
      "65 15 0.7603268623352051\n",
      "Validation loss: 8.194896676899058 RMSE: 2.862673\n",
      "Validation loss: 8.00691024392052 RMSE: 2.8296485\n",
      "67 7 0.9441511034965515\n",
      "Validation loss: 5.828766535868687 RMSE: 2.414284\n",
      "68 28 0.3317134976387024\n",
      "Validation loss: 6.984764850245113 RMSE: 2.6428704\n",
      "Validation loss: 7.524295722488809 RMSE: 2.743045\n",
      "70 20 0.9330649971961975\n",
      "Validation loss: 9.331515843889354 RMSE: 3.054753\n",
      "Validation loss: 5.840161583064932 RMSE: 2.4166427\n",
      "72 12 0.7949005365371704\n",
      "Validation loss: 8.585802432710091 RMSE: 2.9301538\n",
      "Validation loss: 6.94224506352855 RMSE: 2.634814\n",
      "74 4 0.705497682094574\n",
      "Validation loss: 7.483420372009277 RMSE: 2.735584\n",
      "75 25 0.5034146904945374\n",
      "Validation loss: 8.437195904487002 RMSE: 2.9046853\n",
      "Validation loss: 6.755899948356426 RMSE: 2.5992115\n",
      "77 17 0.8111669421195984\n",
      "Validation loss: 8.047633955963946 RMSE: 2.8368351\n",
      "Validation loss: 9.22559248662628 RMSE: 3.037366\n",
      "79 9 0.42241954803466797\n",
      "Validation loss: 6.489428186838606 RMSE: 2.5474355\n",
      "Validation loss: 5.804749054191387 RMSE: 2.4093046\n",
      "81 1 0.4943190813064575\n",
      "Validation loss: 6.104544736642753 RMSE: 2.4707377\n",
      "82 22 0.50825035572052\n",
      "Validation loss: 9.794027518382114 RMSE: 3.1295412\n",
      "Validation loss: 6.628014480118203 RMSE: 2.5744932\n",
      "84 14 0.39455950260162354\n",
      "Validation loss: 6.228475380787807 RMSE: 2.4956913\n",
      "Validation loss: 7.9274033571766545 RMSE: 2.8155644\n",
      "86 6 0.5025238394737244\n",
      "Validation loss: 7.726326655497593 RMSE: 2.779627\n",
      "87 27 0.6784420013427734\n",
      "Validation loss: 7.070053218740277 RMSE: 2.6589572\n",
      "Validation loss: 7.948583476311337 RMSE: 2.8193233\n",
      "89 19 0.7078030705451965\n",
      "Validation loss: 6.567095566639858 RMSE: 2.5626345\n",
      "Validation loss: 7.349716308897576 RMSE: 2.711036\n",
      "91 11 1.3568236827850342\n",
      "Validation loss: 7.775656476484991 RMSE: 2.7884865\n",
      "Validation loss: 6.500404130041072 RMSE: 2.5495892\n",
      "93 3 0.7838788628578186\n",
      "Validation loss: 7.042798911575723 RMSE: 2.6538272\n",
      "94 24 1.2364082336425781\n",
      "Validation loss: 9.066858949914442 RMSE: 3.0111227\n",
      "Validation loss: 7.443529382216192 RMSE: 2.7282834\n",
      "96 16 0.6015822887420654\n",
      "Validation loss: 7.57816653125054 RMSE: 2.752847\n",
      "Validation loss: 7.914422128052838 RMSE: 2.8132582\n",
      "98 8 0.6441203355789185\n",
      "Validation loss: 7.050302286063675 RMSE: 2.6552405\n",
      "Validation loss: 7.176736342168487 RMSE: 2.6789432\n",
      "100 0 0.823037326335907\n",
      "Validation loss: 6.1010268093210405 RMSE: 2.4700258\n",
      "101 21 0.5435317754745483\n",
      "Validation loss: 8.13640140432172 RMSE: 2.852438\n",
      "Validation loss: 7.046205039572927 RMSE: 2.6544688\n",
      "103 13 0.5718899369239807\n",
      "Validation loss: 5.78935651441591 RMSE: 2.4061081\n",
      "Validation loss: 6.295897907915369 RMSE: 2.509163\n",
      "105 5 0.8030193448066711\n",
      "Validation loss: 6.025150590238318 RMSE: 2.4546182\n",
      "106 26 0.5618191361427307\n",
      "Validation loss: 4.86685885792285 RMSE: 2.206096\n",
      "Validation loss: 8.157273672323312 RMSE: 2.8560941\n",
      "108 18 0.4549710154533386\n",
      "Validation loss: 8.375531264111004 RMSE: 2.894051\n",
      "Validation loss: 4.254407209632671 RMSE: 2.0626216\n",
      "110 10 0.3733203709125519\n",
      "Validation loss: 4.435787074333798 RMSE: 2.1061308\n",
      "Validation loss: 6.580828358641768 RMSE: 2.5653124\n",
      "112 2 0.8501861095428467\n",
      "Validation loss: 5.6396960621386505 RMSE: 2.3748045\n",
      "113 23 0.5712655186653137\n",
      "Validation loss: 5.974434173212642 RMSE: 2.4442656\n",
      "Validation loss: 6.690418471277288 RMSE: 2.5865843\n",
      "115 15 0.2858164310455322\n",
      "Validation loss: 7.34962735977848 RMSE: 2.7110195\n",
      "Validation loss: 6.059048618890543 RMSE: 2.4615135\n",
      "117 7 0.505399227142334\n",
      "Validation loss: 7.590307117563434 RMSE: 2.7550511\n",
      "118 28 1.8643468618392944\n",
      "Validation loss: 6.209316595465736 RMSE: 2.4918501\n",
      "Validation loss: 7.538346780084931 RMSE: 2.745605\n",
      "120 20 0.17485183477401733\n",
      "Validation loss: 7.679023704697601 RMSE: 2.771105\n",
      "Validation loss: 7.834461558181627 RMSE: 2.7990108\n",
      "122 12 0.43675899505615234\n",
      "Validation loss: 7.881561228659301 RMSE: 2.8074117\n",
      "Validation loss: 9.937017761500536 RMSE: 3.1523035\n",
      "124 4 0.41461730003356934\n",
      "Validation loss: 7.577985501922337 RMSE: 2.752814\n",
      "125 25 0.9419490694999695\n",
      "Validation loss: 3.7044069049632653 RMSE: 1.9246836\n",
      "Validation loss: 9.456720917625764 RMSE: 3.0751781\n",
      "127 17 0.9142759442329407\n",
      "Validation loss: 7.518558624571404 RMSE: 2.7419991\n",
      "Validation loss: 6.331858212968944 RMSE: 2.5163186\n",
      "129 9 0.3739255666732788\n",
      "Validation loss: 7.077095985412598 RMSE: 2.6602812\n",
      "Validation loss: 8.450861070008404 RMSE: 2.9070365\n",
      "131 1 0.6367603540420532\n",
      "Validation loss: 7.512690510370035 RMSE: 2.7409286\n",
      "132 22 0.4895555078983307\n",
      "Validation loss: 7.196905667802929 RMSE: 2.682705\n",
      "Validation loss: 7.070235501348445 RMSE: 2.6589916\n",
      "134 14 0.7080796957015991\n",
      "Validation loss: 7.228577065256845 RMSE: 2.6886013\n",
      "Validation loss: 8.490421392221366 RMSE: 2.9138327\n",
      "136 6 0.5883029103279114\n",
      "Validation loss: 7.763445153700567 RMSE: 2.786296\n",
      "137 27 0.5392194390296936\n",
      "Validation loss: 7.798063961805496 RMSE: 2.7925014\n",
      "Validation loss: 6.922583655973451 RMSE: 2.6310804\n",
      "139 19 0.6388746500015259\n",
      "Validation loss: 7.185812291845811 RMSE: 2.6806364\n",
      "Validation loss: 10.9956983549405 RMSE: 3.3159764\n",
      "141 11 0.5996173620223999\n",
      "Validation loss: 10.002393536863075 RMSE: 3.162656\n",
      "Validation loss: 7.222407332563822 RMSE: 2.6874537\n",
      "143 3 0.7913365364074707\n",
      "Validation loss: 8.401570252612629 RMSE: 2.8985462\n",
      "144 24 0.4004998207092285\n",
      "Validation loss: 9.998019311280377 RMSE: 3.1619644\n",
      "Validation loss: 9.165047223589061 RMSE: 3.0273829\n",
      "146 16 0.40955641865730286\n",
      "Validation loss: 7.58893908863574 RMSE: 2.754803\n",
      "Validation loss: 8.162534266446544 RMSE: 2.857015\n",
      "148 8 0.43755218386650085\n",
      "Validation loss: 6.340698140912352 RMSE: 2.5180743\n",
      "Validation loss: 8.053845030016603 RMSE: 2.8379297\n",
      "150 0 0.39454615116119385\n",
      "Validation loss: 8.093842379814756 RMSE: 2.8449678\n",
      "151 21 0.29920369386672974\n",
      "Validation loss: 7.863595848589872 RMSE: 2.8042104\n",
      "Validation loss: 8.180693347897149 RMSE: 2.860191\n",
      "153 13 0.4774446487426758\n",
      "Validation loss: 7.406415829616311 RMSE: 2.7214732\n",
      "Validation loss: 8.613156200510211 RMSE: 2.934818\n",
      "155 5 0.8925331234931946\n",
      "Validation loss: 8.399253777698078 RMSE: 2.8981466\n",
      "156 26 1.25496506690979\n",
      "Validation loss: 7.075003556445637 RMSE: 2.6598878\n",
      "Validation loss: 6.374663112437831 RMSE: 2.5248094\n",
      "158 18 0.6745055317878723\n",
      "Validation loss: 8.751543078802328 RMSE: 2.9583008\n",
      "Validation loss: 9.22259509668941 RMSE: 3.0368724\n",
      "160 10 0.6167344450950623\n",
      "Validation loss: 10.510707306650888 RMSE: 3.2420223\n",
      "Validation loss: 7.979861833352958 RMSE: 2.8248649\n",
      "162 2 2.0110931396484375\n",
      "Validation loss: 7.509749167788345 RMSE: 2.7403922\n",
      "163 23 0.8624352812767029\n",
      "Validation loss: 8.224540056380551 RMSE: 2.867846\n",
      "Validation loss: 7.513355390160485 RMSE: 2.74105\n",
      "165 15 0.790854811668396\n",
      "Validation loss: 6.870000104988571 RMSE: 2.6210685\n",
      "Validation loss: 11.379112066420834 RMSE: 3.3732939\n",
      "167 7 0.669606626033783\n",
      "Validation loss: 7.242437337352111 RMSE: 2.6911776\n",
      "168 28 0.13916826248168945\n",
      "Validation loss: 9.607261227295462 RMSE: 3.0995584\n",
      "Validation loss: 7.442336002282337 RMSE: 2.7280645\n",
      "170 20 0.6640139222145081\n",
      "Validation loss: 7.641050887318839 RMSE: 2.7642453\n",
      "Validation loss: 8.135195959985783 RMSE: 2.8522265\n",
      "172 12 0.5598295331001282\n",
      "Validation loss: 8.475761270101092 RMSE: 2.9113162\n",
      "Validation loss: 8.9919770839995 RMSE: 2.9986625\n",
      "174 4 0.6510844230651855\n",
      "Validation loss: 10.776687951214546 RMSE: 3.2827868\n",
      "175 25 0.8584025502204895\n",
      "Validation loss: 10.304406858123508 RMSE: 3.2100477\n",
      "Validation loss: 11.094906904001151 RMSE: 3.3309019\n",
      "177 17 0.41584229469299316\n",
      "Validation loss: 10.279097151967276 RMSE: 3.206103\n",
      "Validation loss: 11.72431200584479 RMSE: 3.4240782\n",
      "179 9 0.3399633765220642\n",
      "Validation loss: 10.503574616086167 RMSE: 3.2409217\n",
      "Validation loss: 9.34003159008195 RMSE: 3.0561466\n",
      "181 1 0.6782183647155762\n",
      "Validation loss: 9.286176981124203 RMSE: 3.047323\n",
      "182 22 0.35407888889312744\n",
      "Validation loss: 10.825635277064501 RMSE: 3.2902334\n",
      "Validation loss: 10.724939464467816 RMSE: 3.2748952\n",
      "184 14 0.4511178135871887\n",
      "Validation loss: 13.453256446703346 RMSE: 3.6678681\n",
      "Validation loss: 9.081640657070464 RMSE: 3.013576\n",
      "186 6 0.4512789249420166\n",
      "Validation loss: 11.727339871161806 RMSE: 3.4245203\n",
      "187 27 0.6208335757255554\n",
      "Validation loss: 7.564700016933205 RMSE: 2.7504\n",
      "Validation loss: 9.86557094607733 RMSE: 3.1409507\n",
      "189 19 0.3149523437023163\n",
      "Validation loss: 11.747603019781872 RMSE: 3.4274774\n",
      "Validation loss: 6.811101086371768 RMSE: 2.6098084\n",
      "191 11 0.4796155095100403\n",
      "Validation loss: 7.476972706549991 RMSE: 2.7344053\n",
      "Validation loss: 6.791337679972691 RMSE: 2.6060195\n",
      "193 3 0.36558032035827637\n",
      "Validation loss: 6.42786031064734 RMSE: 2.5353224\n",
      "194 24 0.51065593957901\n",
      "Validation loss: 7.199437711091168 RMSE: 2.6831768\n",
      "Validation loss: 8.285394752975058 RMSE: 2.878436\n",
      "196 16 0.9519442915916443\n",
      "Validation loss: 9.23225038241496 RMSE: 3.038462\n",
      "Validation loss: 6.742625401083347 RMSE: 2.5966566\n",
      "198 8 0.6613746285438538\n",
      "Validation loss: 8.467006717107992 RMSE: 2.909812\n",
      "Validation loss: 9.965325823927348 RMSE: 3.1567903\n",
      "Loaded trained model with success.\n",
      "Test loss: 4.219412753012328 Test RMSE: 2.054121\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:1', 'task_name': 'lipo', 'model_type': 'gin', 'model': {'num_layer': 5, 'emb_dim': 300, 'feat_dim': 300, 'drop_ratio': 0.3, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 777, 'task': 'regression', 'data_path': 'data/lipophilicity/Lipophilicity.csv', 'target': ['exp']}}\n",
      "Running on: cuda:1\n",
      "4199\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/4199\n",
      "Generating scaffold 1000/4199\n",
      "Generating scaffold 2000/4199\n",
      "Generating scaffold 3000/4199\n",
      "Generating scaffold 4000/4199\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 5.3704752922058105\n",
      "0 50 1.585079550743103\n",
      "0 100 1.5538074970245361\n",
      "Validation loss: 1.3856529179073516 RMSE: 1.1771376\n",
      "1 45 1.4087337255477905\n",
      "1 95 1.0529446601867676\n",
      "Validation loss: 1.3059768676757812 RMSE: 1.1427934\n",
      "2 40 0.9132834672927856\n",
      "2 90 0.7038244009017944\n",
      "Validation loss: 4.225264131455194 RMSE: 2.0555446\n",
      "3 35 0.9512184858322144\n",
      "3 85 0.9490811824798584\n",
      "Validation loss: 2.1164168380555655 RMSE: 1.454791\n",
      "4 30 1.1133085489273071\n",
      "4 80 0.9080443978309631\n",
      "Validation loss: 1.5083366166977656 RMSE: 1.2281436\n",
      "5 25 1.3664393424987793\n",
      "5 75 1.1553163528442383\n",
      "Validation loss: 2.1657342337426684 RMSE: 1.4716433\n",
      "6 20 0.9096383452415466\n",
      "6 70 0.843056321144104\n",
      "Validation loss: 4.952726327805292 RMSE: 2.2254722\n",
      "7 15 0.9134048819541931\n",
      "7 65 0.7855589985847473\n",
      "Validation loss: 1.321777976126898 RMSE: 1.1496861\n",
      "8 10 0.5338469743728638\n",
      "8 60 0.7848691940307617\n",
      "Validation loss: 1.5156350601287114 RMSE: 1.2311113\n",
      "9 5 0.6636173129081726\n",
      "9 55 0.9043872356414795\n",
      "Validation loss: 2.697800926935105 RMSE: 1.6424984\n",
      "10 0 0.9875481128692627\n",
      "10 50 0.43611887097358704\n",
      "10 100 0.35359373688697815\n",
      "Validation loss: 4.384180697940645 RMSE: 2.0938435\n",
      "11 45 1.0520904064178467\n",
      "11 95 0.5598217248916626\n",
      "Validation loss: 2.77405614285242 RMSE: 1.6655499\n",
      "12 40 0.6149636507034302\n",
      "12 90 0.5644789338111877\n",
      "Validation loss: 2.2818375700996034 RMSE: 1.5105752\n",
      "13 35 0.8281404376029968\n",
      "13 85 0.6488385200500488\n",
      "Validation loss: 2.053289949326288 RMSE: 1.4329305\n",
      "14 30 0.8522904515266418\n",
      "14 80 0.6882457137107849\n",
      "Validation loss: 4.9749646050589424 RMSE: 2.230463\n",
      "15 25 0.5054041147232056\n",
      "15 75 0.5072879791259766\n",
      "Validation loss: 3.3928340355555218 RMSE: 1.8419647\n",
      "16 20 0.7420864701271057\n",
      "16 70 0.5886962413787842\n",
      "Validation loss: 2.701361283801851 RMSE: 1.6435819\n",
      "17 15 0.6494413614273071\n",
      "17 65 0.4951997399330139\n",
      "Validation loss: 3.225316656203497 RMSE: 1.7959167\n",
      "18 10 0.9369534254074097\n",
      "18 60 0.41267064213752747\n",
      "Validation loss: 1.194197344212305 RMSE: 1.0927933\n",
      "19 5 0.38415080308914185\n",
      "19 55 0.8137584924697876\n",
      "Validation loss: 1.5664090633392334 RMSE: 1.2515626\n",
      "20 0 0.350055992603302\n",
      "20 50 0.46146634221076965\n",
      "20 100 0.6499344706535339\n",
      "Validation loss: 2.710740450450352 RMSE: 1.6464328\n",
      "21 45 0.5424968004226685\n",
      "21 95 0.5834105014801025\n",
      "Validation loss: 3.5131823834918796 RMSE: 1.8743486\n",
      "22 40 0.4311060309410095\n",
      "22 90 0.5542936325073242\n",
      "Validation loss: 4.316930598304385 RMSE: 2.0777225\n",
      "23 35 0.41336023807525635\n",
      "23 85 0.5249998569488525\n",
      "Validation loss: 3.9789461794353667 RMSE: 1.9947295\n",
      "24 30 0.6063011884689331\n",
      "24 80 0.46805790066719055\n",
      "Validation loss: 4.032620012192499 RMSE: 2.0081384\n",
      "25 25 0.47702398896217346\n",
      "25 75 0.5477479696273804\n",
      "Validation loss: 2.9024220829918272 RMSE: 1.7036496\n",
      "26 20 0.31782156229019165\n",
      "26 70 0.3500695824623108\n",
      "Validation loss: 3.4403355280558268 RMSE: 1.8548142\n",
      "27 15 0.632933497428894\n",
      "27 65 0.47518518567085266\n",
      "Validation loss: 3.5377748307727632 RMSE: 1.8808974\n",
      "28 10 0.34799572825431824\n",
      "28 60 0.4521881937980652\n",
      "Validation loss: 4.476748580024356 RMSE: 2.1158328\n",
      "29 5 0.6042276620864868\n",
      "29 55 0.7007229924201965\n",
      "Validation loss: 5.042599382854643 RMSE: 2.2455733\n",
      "30 0 0.5056914687156677\n",
      "30 50 0.44916364550590515\n",
      "30 100 0.41652345657348633\n",
      "Validation loss: 4.123338635762533 RMSE: 2.0306005\n",
      "31 45 0.6591374278068542\n",
      "31 95 0.5159047245979309\n",
      "Validation loss: 2.475361814953032 RMSE: 1.5733283\n",
      "32 40 0.5626840591430664\n",
      "32 90 0.48769262433052063\n",
      "Validation loss: 5.195229587100801 RMSE: 2.2793047\n",
      "33 35 0.3678117096424103\n",
      "33 85 0.5019879937171936\n",
      "Validation loss: 3.9295162564232236 RMSE: 1.9823008\n",
      "34 30 0.39104706048965454\n",
      "34 80 0.4467066824436188\n",
      "Validation loss: 4.183052056176322 RMSE: 2.0452511\n",
      "35 25 0.7687265276908875\n",
      "35 75 0.5354301333427429\n",
      "Validation loss: 4.225171534220378 RMSE: 2.0555222\n",
      "36 20 0.4826987087726593\n",
      "36 70 0.411527544260025\n",
      "Validation loss: 3.914517525264195 RMSE: 1.978514\n",
      "37 15 0.3864744305610657\n",
      "37 65 0.6715748310089111\n",
      "Validation loss: 4.132231312706357 RMSE: 2.032789\n",
      "38 10 0.475876122713089\n",
      "38 60 0.43459245562553406\n",
      "Validation loss: 3.7906094187781925 RMSE: 1.9469488\n",
      "39 5 0.5294547080993652\n",
      "39 55 0.4732263684272766\n",
      "Validation loss: 4.442220029376802 RMSE: 2.1076574\n",
      "40 0 0.28452515602111816\n",
      "40 50 0.7198948264122009\n",
      "40 100 0.3191712498664856\n",
      "Validation loss: 3.9951254526774087 RMSE: 1.998781\n",
      "41 45 0.3108212351799011\n",
      "41 95 0.33176496624946594\n",
      "Validation loss: 3.6053274972098213 RMSE: 1.89877\n",
      "42 40 0.31247207522392273\n",
      "42 90 0.47653868794441223\n",
      "Validation loss: 3.1439009779975526 RMSE: 1.7731049\n",
      "43 35 0.36033129692077637\n",
      "43 85 0.5002614259719849\n",
      "Validation loss: 6.1526674406869075 RMSE: 2.480457\n",
      "44 30 0.5765909552574158\n",
      "44 80 0.31036147475242615\n",
      "Validation loss: 3.019286482674735 RMSE: 1.7376095\n",
      "45 25 0.44644519686698914\n",
      "45 75 0.3149605393409729\n",
      "Validation loss: 3.541654113928477 RMSE: 1.8819283\n",
      "46 20 0.3099789321422577\n",
      "46 70 0.31083181500434875\n",
      "Validation loss: 6.6423327900114515 RMSE: 2.5772724\n",
      "47 15 0.40696942806243896\n",
      "47 65 0.708179235458374\n",
      "Validation loss: 3.965035915374756 RMSE: 1.9912398\n",
      "48 10 0.31909650564193726\n",
      "48 60 0.4792066812515259\n",
      "Validation loss: 3.0056656530925205 RMSE: 1.7336855\n",
      "49 5 0.42415884137153625\n",
      "49 55 0.531937301158905\n",
      "Validation loss: 6.895109505880447 RMSE: 2.625854\n",
      "50 0 0.5058025121688843\n",
      "50 50 0.4545295238494873\n",
      "50 100 0.9548732042312622\n",
      "Validation loss: 6.6320425805591405 RMSE: 2.5752754\n",
      "51 45 0.409274160861969\n",
      "51 95 0.2780279219150543\n",
      "Validation loss: 6.857746750967843 RMSE: 2.61873\n",
      "52 40 0.3978811502456665\n",
      "52 90 0.28651857376098633\n",
      "Validation loss: 5.4291163944062735 RMSE: 2.3300464\n",
      "53 35 0.44521382451057434\n",
      "53 85 0.26816174387931824\n",
      "Validation loss: 6.661829921177455 RMSE: 2.5810523\n",
      "54 30 0.2912103533744812\n",
      "54 80 0.6049266457557678\n",
      "Validation loss: 8.516628837585449 RMSE: 2.9183264\n",
      "55 25 0.3910534977912903\n",
      "55 75 0.5295476913452148\n",
      "Validation loss: 9.098385965256464 RMSE: 3.0163531\n",
      "56 20 0.3243175148963928\n",
      "56 70 0.4553382992744446\n",
      "Validation loss: 7.06252088546753 RMSE: 2.6575403\n",
      "57 15 0.2925577759742737\n",
      "57 65 0.36697661876678467\n",
      "Validation loss: 5.066342317490351 RMSE: 2.2508538\n",
      "58 10 0.3090844452381134\n",
      "58 60 0.43956080079078674\n",
      "Validation loss: 8.38992832274664 RMSE: 2.8965373\n",
      "59 5 0.4349370002746582\n",
      "59 55 0.39411893486976624\n",
      "Validation loss: 7.746938546498616 RMSE: 2.783332\n",
      "60 0 0.49684974551200867\n",
      "60 50 0.3101050853729248\n",
      "60 100 0.34472930431365967\n",
      "Validation loss: 7.0501995994931175 RMSE: 2.6552212\n",
      "61 45 0.29230454564094543\n",
      "61 95 0.4037955403327942\n",
      "Validation loss: 7.097261437915621 RMSE: 2.6640687\n",
      "62 40 0.3775877058506012\n",
      "62 90 0.482028067111969\n",
      "Validation loss: 7.393914086478097 RMSE: 2.719175\n",
      "63 35 0.5761921405792236\n",
      "63 85 0.327190101146698\n",
      "Validation loss: 6.419432163238525 RMSE: 2.53366\n",
      "64 30 0.2619890868663788\n",
      "64 80 0.30441156029701233\n",
      "Validation loss: 5.619778569539388 RMSE: 2.3706074\n",
      "65 25 0.44947898387908936\n",
      "65 75 0.33773964643478394\n",
      "Validation loss: 6.9990949676150365 RMSE: 2.6455803\n",
      "66 20 0.3559255301952362\n",
      "66 70 0.255935937166214\n",
      "Validation loss: 8.55475924355643 RMSE: 2.9248521\n",
      "67 15 0.17043159902095795\n",
      "67 65 0.2923741936683655\n",
      "Validation loss: 6.871202078319731 RMSE: 2.6212978\n",
      "68 10 0.3725968897342682\n",
      "68 60 0.3282473087310791\n",
      "Validation loss: 6.9909755252656485 RMSE: 2.6440454\n",
      "69 5 0.287222295999527\n",
      "69 55 0.6507219076156616\n",
      "Validation loss: 8.586049715677897 RMSE: 2.9301963\n",
      "70 0 0.42243942618370056\n",
      "70 50 0.24751178920269012\n",
      "70 100 0.3803761899471283\n",
      "Validation loss: 5.342951597486223 RMSE: 2.3114824\n",
      "71 45 0.25744062662124634\n",
      "71 95 0.4292900860309601\n",
      "Validation loss: 7.166247481391544 RMSE: 2.6769845\n",
      "72 40 0.2712177634239197\n",
      "72 90 0.23101010918617249\n",
      "Validation loss: 9.726264435904367 RMSE: 3.118696\n",
      "73 35 0.2637956440448761\n",
      "73 85 0.27026084065437317\n",
      "Validation loss: 7.594881757100423 RMSE: 2.7558813\n",
      "74 30 0.3442304730415344\n",
      "74 80 0.43362143635749817\n",
      "Validation loss: 9.358906691414969 RMSE: 3.059233\n",
      "75 25 0.3059682250022888\n",
      "75 75 0.3725148141384125\n",
      "Validation loss: 8.541530236743746 RMSE: 2.9225895\n",
      "76 20 0.2726954221725464\n",
      "76 70 0.3440828025341034\n",
      "Validation loss: 10.628983511243547 RMSE: 3.2602124\n",
      "77 15 0.5703482031822205\n",
      "77 65 0.242847740650177\n",
      "Validation loss: 6.534094824109759 RMSE: 2.5561876\n",
      "78 10 0.24401500821113586\n",
      "78 60 0.36459022760391235\n",
      "Validation loss: 7.382318860008603 RMSE: 2.7170422\n",
      "79 5 0.7746584415435791\n",
      "79 55 0.2896175980567932\n",
      "Validation loss: 8.127798993246897 RMSE: 2.8509295\n",
      "80 0 0.24226777255535126\n",
      "80 50 0.45991939306259155\n",
      "80 100 0.34695205092430115\n",
      "Validation loss: 5.324994807016282 RMSE: 2.307595\n",
      "81 45 0.4027383327484131\n",
      "81 95 0.16911540925502777\n",
      "Validation loss: 6.787379605429513 RMSE: 2.60526\n",
      "82 40 0.16472144424915314\n",
      "82 90 0.37264418601989746\n",
      "Validation loss: 7.543584551130023 RMSE: 2.7465587\n",
      "83 35 0.2861616313457489\n",
      "83 85 0.31052395701408386\n",
      "Validation loss: 8.763886533464705 RMSE: 2.9603863\n",
      "84 30 0.31416693329811096\n",
      "84 80 0.4453517496585846\n",
      "Validation loss: 8.549101947602772 RMSE: 2.9238846\n",
      "85 25 0.35170960426330566\n",
      "85 75 0.2506621778011322\n",
      "Validation loss: 10.7541624886649 RMSE: 3.2793539\n",
      "86 20 0.2330368310213089\n",
      "86 70 0.33311784267425537\n",
      "Validation loss: 8.803768684750512 RMSE: 2.9671147\n",
      "87 15 0.4539998173713684\n",
      "87 65 0.36874663829803467\n",
      "Validation loss: 8.1309355872018 RMSE: 2.8514795\n",
      "88 10 0.3492494821548462\n",
      "88 60 0.3005872964859009\n",
      "Validation loss: 6.051203771999904 RMSE: 2.4599195\n",
      "89 5 0.3835829794406891\n",
      "89 55 0.36356937885284424\n",
      "Validation loss: 5.690019162495931 RMSE: 2.3853762\n",
      "90 0 0.27485594153404236\n",
      "90 50 0.3154982328414917\n",
      "90 100 0.36052799224853516\n",
      "Validation loss: 6.730394994644891 RMSE: 2.5943005\n",
      "91 45 0.16749362647533417\n",
      "91 95 0.30733588337898254\n",
      "Validation loss: 9.632817618052165 RMSE: 3.103678\n",
      "92 40 0.24905280768871307\n",
      "92 90 0.27580052614212036\n",
      "Validation loss: 6.7052195957728795 RMSE: 2.589444\n",
      "93 35 0.4433552324771881\n",
      "93 85 0.1350075751543045\n",
      "Validation loss: 8.873557681129093 RMSE: 2.9788518\n",
      "94 30 0.27299565076828003\n",
      "94 80 0.29516857862472534\n",
      "Validation loss: 9.22513549441383 RMSE: 3.0372908\n",
      "95 25 0.1890457272529602\n",
      "95 75 0.23651626706123352\n",
      "Validation loss: 9.677008665175665 RMSE: 3.110789\n",
      "96 20 0.24954070150852203\n",
      "96 70 0.4924394190311432\n",
      "Validation loss: 6.687781422478812 RMSE: 2.5860746\n",
      "97 15 0.23372536897659302\n",
      "97 65 0.3212457597255707\n",
      "Validation loss: 8.22500354221889 RMSE: 2.8679266\n",
      "98 10 0.16870486736297607\n",
      "98 60 0.4452945291996002\n",
      "Validation loss: 8.133075228191558 RMSE: 2.8518548\n",
      "99 5 0.2869744598865509\n",
      "99 55 0.21833491325378418\n",
      "Validation loss: 7.578127888270787 RMSE: 2.7528398\n",
      "100 0 0.23961974680423737\n",
      "100 50 0.209203839302063\n",
      "100 100 0.28356456756591797\n",
      "Validation loss: 6.692806007748558 RMSE: 2.5870457\n",
      "101 45 0.330028235912323\n",
      "101 95 0.2584160566329956\n",
      "Validation loss: 7.78305112747919 RMSE: 2.789812\n",
      "102 40 0.22580111026763916\n",
      "102 90 0.3356820344924927\n",
      "Validation loss: 8.01553536369687 RMSE: 2.831172\n",
      "103 35 0.2659243047237396\n",
      "103 85 0.29841193556785583\n",
      "Validation loss: 7.1333809716360905 RMSE: 2.6708388\n",
      "104 30 0.14664967358112335\n",
      "104 80 0.19146843254566193\n",
      "Validation loss: 4.856049882797968 RMSE: 2.2036448\n",
      "105 25 0.3227091133594513\n",
      "105 75 0.27554911375045776\n",
      "Validation loss: 7.108500607808431 RMSE: 2.666177\n",
      "106 20 0.1756429225206375\n",
      "106 70 0.253719687461853\n",
      "Validation loss: 5.76022739864531 RMSE: 2.4000473\n",
      "107 15 0.22285427153110504\n",
      "107 65 0.3500935435295105\n",
      "Validation loss: 10.745800763084775 RMSE: 3.2780788\n",
      "108 10 0.3532170057296753\n",
      "108 60 0.21463923156261444\n",
      "Validation loss: 9.711762682596843 RMSE: 3.1163702\n",
      "109 5 0.25625360012054443\n",
      "109 55 0.27521228790283203\n",
      "Validation loss: 10.507127425784157 RMSE: 3.24147\n",
      "110 0 0.408905029296875\n",
      "110 50 0.3659110367298126\n",
      "110 100 0.4054034948348999\n",
      "Validation loss: 5.289243589128767 RMSE: 2.2998357\n",
      "111 45 0.2516665756702423\n",
      "111 95 0.3216012120246887\n",
      "Validation loss: 7.787567429315477 RMSE: 2.7906213\n",
      "112 40 0.2920917570590973\n",
      "112 90 0.15518979728221893\n",
      "Validation loss: 6.941268702915736 RMSE: 2.6346288\n",
      "113 35 0.24840287864208221\n",
      "113 85 0.12177697569131851\n",
      "Validation loss: 9.104315040225075 RMSE: 3.0173357\n",
      "114 30 0.3455204665660858\n",
      "114 80 0.3340816795825958\n",
      "Validation loss: 8.05906814393543 RMSE: 2.8388498\n",
      "115 25 0.17481037974357605\n",
      "115 75 0.15457627177238464\n",
      "Validation loss: 8.349791317894345 RMSE: 2.8896005\n",
      "116 20 0.21346838772296906\n",
      "116 70 0.24995388090610504\n",
      "Validation loss: 8.449337832132976 RMSE: 2.9067743\n",
      "117 15 0.3951956629753113\n",
      "117 65 0.23917348682880402\n",
      "Validation loss: 10.10715054557437 RMSE: 3.1791744\n",
      "118 10 0.37801340222358704\n",
      "118 60 0.37894508242607117\n",
      "Validation loss: 10.082366026015508 RMSE: 3.1752741\n",
      "119 5 0.20965805649757385\n",
      "119 55 0.23670339584350586\n",
      "Validation loss: 6.192417365028745 RMSE: 2.488457\n",
      "120 0 0.23285549879074097\n",
      "120 50 0.2362779825925827\n",
      "120 100 0.21330609917640686\n",
      "Validation loss: 8.858213288443428 RMSE: 2.9762752\n",
      "121 45 0.2026214748620987\n",
      "121 95 0.2881135940551758\n",
      "Validation loss: 8.71384894507272 RMSE: 2.951923\n",
      "122 40 0.2342851310968399\n",
      "122 90 0.1515461504459381\n",
      "Validation loss: 9.459898821512859 RMSE: 3.0756948\n",
      "123 35 0.16862374544143677\n",
      "123 85 0.21570263803005219\n",
      "Validation loss: 8.252249844868977 RMSE: 2.872673\n",
      "124 30 0.14969611167907715\n",
      "124 80 0.18688230216503143\n",
      "Validation loss: 8.561167099362327 RMSE: 2.9259472\n",
      "125 25 0.1635533720254898\n",
      "125 75 0.261871337890625\n",
      "Validation loss: 8.397362936110724 RMSE: 2.8978205\n",
      "126 20 0.2480703890323639\n",
      "126 70 0.20945776998996735\n",
      "Validation loss: 6.065718114943731 RMSE: 2.462868\n",
      "127 15 0.22157487273216248\n",
      "127 65 0.2312832623720169\n",
      "Validation loss: 9.251887657528831 RMSE: 3.0416918\n",
      "128 10 0.18784327805042267\n",
      "128 60 0.35231977701187134\n",
      "Validation loss: 7.794046456473215 RMSE: 2.791782\n",
      "129 5 0.15750104188919067\n",
      "129 55 0.18142525851726532\n",
      "Validation loss: 7.692304692949567 RMSE: 2.7735004\n",
      "130 0 0.35005152225494385\n",
      "130 50 0.2509928047657013\n",
      "130 100 0.17915605008602142\n",
      "Validation loss: 9.260113800139655 RMSE: 3.0430436\n",
      "131 45 0.1785648763179779\n",
      "131 95 0.2553437054157257\n",
      "Validation loss: 5.998635596320742 RMSE: 2.4492111\n",
      "132 40 0.18141144514083862\n",
      "132 90 0.1740158051252365\n",
      "Validation loss: 8.254815737406412 RMSE: 2.8731196\n",
      "133 35 0.4423992335796356\n",
      "133 85 0.16974623501300812\n",
      "Validation loss: 6.223755109877813 RMSE: 2.4947455\n",
      "134 30 0.27068158984184265\n",
      "134 80 0.23113815486431122\n",
      "Validation loss: 8.907028538840157 RMSE: 2.9844644\n",
      "135 25 0.2216210663318634\n",
      "135 75 0.18441230058670044\n",
      "Validation loss: 7.042575636364165 RMSE: 2.6537852\n",
      "136 20 0.23502331972122192\n",
      "136 70 0.23568478226661682\n",
      "Validation loss: 8.489054107666016 RMSE: 2.9135983\n",
      "137 15 0.23498810827732086\n",
      "137 65 0.2943641245365143\n",
      "Validation loss: 6.891920920780727 RMSE: 2.6252468\n",
      "138 10 0.25656160712242126\n",
      "138 60 0.3158028721809387\n",
      "Validation loss: 9.398403476533435 RMSE: 3.0656817\n",
      "139 5 0.18258023262023926\n",
      "139 55 0.27148738503456116\n",
      "Validation loss: 8.262744086129326 RMSE: 2.8744988\n",
      "140 0 0.20383596420288086\n",
      "140 50 0.19914716482162476\n",
      "140 100 0.27233341336250305\n",
      "Validation loss: 7.298482127416701 RMSE: 2.7015703\n",
      "141 45 0.2417033463716507\n",
      "141 95 0.1685153990983963\n",
      "Validation loss: 6.750784506116594 RMSE: 2.598227\n",
      "142 40 0.2976739704608917\n",
      "142 90 0.22491241991519928\n",
      "Validation loss: 7.0544772011893135 RMSE: 2.6560266\n",
      "143 35 0.2527769207954407\n",
      "143 85 0.27538466453552246\n",
      "Validation loss: 8.753977693830217 RMSE: 2.958712\n",
      "144 30 0.20157214999198914\n",
      "144 80 0.19172854721546173\n",
      "Validation loss: 8.380541120256696 RMSE: 2.8949163\n",
      "145 25 0.29853054881095886\n",
      "145 75 0.32648566365242004\n",
      "Validation loss: 7.8697863260904946 RMSE: 2.8053138\n",
      "146 20 0.23699535429477692\n",
      "146 70 0.1571793258190155\n",
      "Validation loss: 8.689472280229841 RMSE: 2.947791\n",
      "147 15 0.17316728830337524\n",
      "147 65 0.15680649876594543\n",
      "Validation loss: 7.981835764930362 RMSE: 2.8252141\n",
      "148 10 0.12460318207740784\n",
      "148 60 0.25893324613571167\n",
      "Validation loss: 5.060660829998198 RMSE: 2.2495914\n",
      "149 5 0.1974894106388092\n",
      "149 55 0.20253443717956543\n",
      "Validation loss: 7.768317728950864 RMSE: 2.7871702\n",
      "150 0 0.29596173763275146\n",
      "150 50 0.2830263674259186\n",
      "150 100 0.17197127640247345\n",
      "Validation loss: 7.006399431682769 RMSE: 2.6469605\n",
      "151 45 0.13750207424163818\n",
      "151 95 0.26272860169410706\n",
      "Validation loss: 7.05998627117702 RMSE: 2.6570635\n",
      "152 40 0.20799925923347473\n",
      "152 90 0.32957398891448975\n",
      "Validation loss: 9.97028842653547 RMSE: 3.157576\n",
      "153 35 0.25741201639175415\n",
      "153 85 0.27675116062164307\n",
      "Validation loss: 7.519934844970703 RMSE: 2.74225\n",
      "154 30 0.12676416337490082\n",
      "154 80 0.24818989634513855\n",
      "Validation loss: 7.523342187064035 RMSE: 2.742871\n",
      "155 25 0.1966330111026764\n",
      "155 75 0.11925232410430908\n",
      "Validation loss: 4.912885084606352 RMSE: 2.216503\n",
      "156 20 0.5388765335083008\n",
      "156 70 0.20920100808143616\n",
      "Validation loss: 8.080251330421085 RMSE: 2.8425784\n",
      "157 15 0.2200758308172226\n",
      "157 65 0.23926986753940582\n",
      "Validation loss: 7.874710518973214 RMSE: 2.8061914\n",
      "158 10 0.402281790971756\n",
      "158 60 0.15050944685935974\n",
      "Validation loss: 8.084174156188965 RMSE: 2.8432682\n",
      "159 5 0.2643771171569824\n",
      "159 55 0.22279998660087585\n",
      "Validation loss: 5.861281531197684 RMSE: 2.4210083\n",
      "160 0 0.2760636806488037\n",
      "160 50 0.32396236062049866\n",
      "160 100 0.21532690525054932\n",
      "Validation loss: 7.248134608495803 RMSE: 2.6922362\n",
      "161 45 0.21454301476478577\n",
      "161 95 0.24355407059192657\n",
      "Validation loss: 5.415176318940662 RMSE: 2.327053\n",
      "162 40 0.23861412703990936\n",
      "162 90 0.1909310668706894\n",
      "Validation loss: 6.351008515130906 RMSE: 2.5201206\n",
      "163 35 0.23126503825187683\n",
      "163 85 0.165316641330719\n",
      "Validation loss: 6.178675569806781 RMSE: 2.4856944\n",
      "164 30 0.27459588646888733\n",
      "164 80 0.1463300734758377\n",
      "Validation loss: 8.831179282778786 RMSE: 2.97173\n",
      "165 25 0.1776156723499298\n",
      "165 75 0.19117914140224457\n",
      "Validation loss: 8.101468440464565 RMSE: 2.8463078\n",
      "166 20 0.14965346455574036\n",
      "166 70 0.12106314301490784\n",
      "Validation loss: 8.719454411097935 RMSE: 2.9528723\n",
      "167 15 0.20480160415172577\n",
      "167 65 0.10819300264120102\n",
      "Validation loss: 8.283943457830519 RMSE: 2.878184\n",
      "168 10 0.15073876082897186\n",
      "168 60 0.1516904979944229\n",
      "Validation loss: 8.378812939780099 RMSE: 2.8946178\n",
      "169 5 0.09738264977931976\n",
      "169 55 0.17367224395275116\n",
      "Validation loss: 5.277769874391102 RMSE: 2.29734\n",
      "170 0 0.13838715851306915\n",
      "170 50 0.12444570660591125\n",
      "170 100 0.1980881690979004\n",
      "Validation loss: 7.909634989783878 RMSE: 2.8124075\n",
      "171 45 0.16856102645397186\n",
      "171 95 0.23701880872249603\n",
      "Validation loss: 6.025744601658412 RMSE: 2.454739\n",
      "172 40 0.17448113858699799\n",
      "172 90 0.1316373348236084\n",
      "Validation loss: 5.896406636919294 RMSE: 2.4282517\n",
      "173 35 0.1754951924085617\n",
      "173 85 0.23971112072467804\n",
      "Validation loss: 7.1352284749348955 RMSE: 2.6711848\n",
      "174 30 0.21032920479774475\n",
      "174 80 0.18598411977291107\n",
      "Validation loss: 6.190898409343901 RMSE: 2.4881518\n",
      "175 25 0.17782793939113617\n",
      "175 75 0.15083912014961243\n",
      "Validation loss: 11.561453801109677 RMSE: 3.4002137\n",
      "176 20 0.13822895288467407\n",
      "176 70 0.13540081679821014\n",
      "Validation loss: 12.234088588896253 RMSE: 3.4977262\n",
      "177 15 0.1035473495721817\n",
      "177 65 0.1696995496749878\n",
      "Validation loss: 8.716079711914062 RMSE: 2.9523008\n",
      "178 10 0.2030072957277298\n",
      "178 60 0.30654701590538025\n",
      "Validation loss: 9.358899143763951 RMSE: 3.0592318\n",
      "179 5 0.16569505631923676\n",
      "179 55 0.16657383739948273\n",
      "Validation loss: 7.172150639125279 RMSE: 2.6780872\n",
      "180 0 0.2262316197156906\n",
      "180 50 0.16646315157413483\n",
      "180 100 0.2207934558391571\n",
      "Validation loss: 10.37280402410598 RMSE: 3.2206838\n",
      "181 45 0.1226438358426094\n",
      "181 95 0.20757991075515747\n",
      "Validation loss: 11.56748659043085 RMSE: 3.4011009\n",
      "182 40 0.2615283131599426\n",
      "182 90 0.1620544195175171\n",
      "Validation loss: 10.884119433448427 RMSE: 3.299109\n",
      "183 35 0.14794276654720306\n",
      "183 85 0.1631227433681488\n",
      "Validation loss: 9.998906526111421 RMSE: 3.1621046\n",
      "184 30 0.12760870158672333\n",
      "184 80 0.11852842569351196\n",
      "Validation loss: 8.6061659631275 RMSE: 2.9336267\n",
      "185 25 0.14042077958583832\n",
      "185 75 0.13958586752414703\n",
      "Validation loss: 6.284064422334944 RMSE: 2.5068038\n",
      "186 20 0.1571771651506424\n",
      "186 70 0.1898740530014038\n",
      "Validation loss: 8.819250642685663 RMSE: 2.9697223\n",
      "187 15 0.17621111869812012\n",
      "187 65 0.14859773218631744\n",
      "Validation loss: 9.275802158174061 RMSE: 3.04562\n",
      "188 10 0.17959292232990265\n",
      "188 60 0.2863466739654541\n",
      "Validation loss: 9.118732570466541 RMSE: 3.019724\n",
      "189 5 0.2406764179468155\n",
      "189 55 0.25506171584129333\n",
      "Validation loss: 8.42657012939453 RMSE: 2.9028556\n",
      "190 0 0.37498795986175537\n",
      "190 50 0.10528510808944702\n",
      "190 100 0.13389793038368225\n",
      "Validation loss: 8.469623824528286 RMSE: 2.9102619\n",
      "191 45 0.2617632746696472\n",
      "191 95 0.20185121893882751\n",
      "Validation loss: 8.980737277439662 RMSE: 2.9967878\n",
      "192 40 0.22882121801376343\n",
      "192 90 0.15021996200084686\n",
      "Validation loss: 6.3222625823248 RMSE: 2.514411\n",
      "193 35 0.14056076109409332\n",
      "193 85 0.3306756019592285\n",
      "Validation loss: 8.106661115373884 RMSE: 2.84722\n",
      "194 30 0.18436649441719055\n",
      "194 80 0.09317127615213394\n",
      "Validation loss: 5.8275576636904765 RMSE: 2.4140334\n",
      "195 25 0.10889076441526413\n",
      "195 75 0.16054823994636536\n",
      "Validation loss: 10.084486861456009 RMSE: 3.1756082\n",
      "196 20 0.18205563724040985\n",
      "196 70 0.30821943283081055\n",
      "Validation loss: 6.979070499965123 RMSE: 2.641793\n",
      "197 15 0.18724994361400604\n",
      "197 65 0.33909377455711365\n",
      "Validation loss: 12.752298554920015 RMSE: 3.571036\n",
      "198 10 0.16375011205673218\n",
      "198 60 0.19661585986614227\n",
      "Validation loss: 7.467119798206148 RMSE: 2.732603\n",
      "199 5 0.14143869280815125\n",
      "199 55 0.12869654595851898\n",
      "Validation loss: 8.768698265438989 RMSE: 2.9611988\n",
      "Loaded trained model with success.\n",
      "Test loss: 1.160211639744895 Test RMSE: 1.0771312\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:1', 'task_name': 'lipo', 'model_type': 'gin', 'model': {'num_layer': 5, 'emb_dim': 300, 'feat_dim': 300, 'drop_ratio': 0.3, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 778, 'task': 'regression', 'data_path': 'data/lipophilicity/Lipophilicity.csv', 'target': ['exp']}}\n",
      "Running on: cuda:1\n",
      "4199\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/4199\n",
      "Generating scaffold 1000/4199\n",
      "Generating scaffold 2000/4199\n",
      "Generating scaffold 3000/4199\n",
      "Generating scaffold 4000/4199\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 8.89050006866455\n",
      "0 50 1.957584023475647\n",
      "0 100 2.044323205947876\n",
      "Validation loss: 1.5014728478022985 RMSE: 1.225346\n",
      "1 45 0.9201608300209045\n",
      "1 95 0.8966129422187805\n",
      "Validation loss: 1.8616596494402204 RMSE: 1.3644265\n",
      "2 40 0.8441185355186462\n",
      "2 90 0.9097780585289001\n",
      "Validation loss: 1.3292590686253138 RMSE: 1.1529349\n",
      "3 35 0.8255192041397095\n",
      "3 85 0.8202798366546631\n",
      "Validation loss: 3.09753444762457 RMSE: 1.7599814\n",
      "4 30 1.177309274673462\n",
      "4 80 0.6923457980155945\n",
      "Validation loss: 1.6246165956769671 RMSE: 1.2746044\n",
      "5 25 0.6421681642532349\n",
      "5 75 0.8439679145812988\n",
      "Validation loss: 1.6363580578849428 RMSE: 1.2792021\n",
      "6 20 0.6785181164741516\n",
      "6 70 0.729661226272583\n",
      "Validation loss: 1.6562090465000698 RMSE: 1.2869378\n",
      "7 15 0.8052675724029541\n",
      "7 65 0.4908236265182495\n",
      "Validation loss: 1.4308535018137523 RMSE: 1.196183\n",
      "8 10 0.7454406023025513\n",
      "8 60 1.13600754737854\n",
      "Validation loss: 1.6700324671609061 RMSE: 1.2922974\n",
      "9 5 0.6826833486557007\n",
      "9 55 0.4214157462120056\n",
      "Validation loss: 1.240341976143065 RMSE: 1.1137064\n",
      "10 0 0.8030998110771179\n",
      "10 50 0.5926729440689087\n",
      "10 100 0.8741741180419922\n",
      "Validation loss: 3.4240648519425165 RMSE: 1.8504229\n",
      "11 45 0.611114501953125\n",
      "11 95 0.6475972533226013\n",
      "Validation loss: 1.89980134055728 RMSE: 1.3783329\n",
      "12 40 0.5298528671264648\n",
      "12 90 0.5494871735572815\n",
      "Validation loss: 1.1314159824734642 RMSE: 1.0636804\n",
      "13 35 0.4130915403366089\n",
      "13 85 0.6737871170043945\n",
      "Validation loss: 1.5483974252428327 RMSE: 1.2443461\n",
      "14 30 0.8200922012329102\n",
      "14 80 0.5975213646888733\n",
      "Validation loss: 1.3678356965382894 RMSE: 1.169545\n",
      "15 25 0.5730108022689819\n",
      "15 75 0.6344195008277893\n",
      "Validation loss: 1.9268856139410109 RMSE: 1.388123\n",
      "16 20 0.3561071753501892\n",
      "16 70 0.5792666673660278\n",
      "Validation loss: 1.2119506767817907 RMSE: 1.1008863\n",
      "17 15 0.6440637707710266\n",
      "17 65 0.6945263743400574\n",
      "Validation loss: 1.3617341688701086 RMSE: 1.1669337\n",
      "18 10 0.6333191394805908\n",
      "18 60 0.38507506251335144\n",
      "Validation loss: 1.3179290456431252 RMSE: 1.1480108\n",
      "19 5 0.3773816227912903\n",
      "19 55 0.817196249961853\n",
      "Validation loss: 1.3678043479011173 RMSE: 1.1695317\n",
      "20 0 1.0198918581008911\n",
      "20 50 0.5844699740409851\n",
      "20 100 0.5843024253845215\n",
      "Validation loss: 1.641737170162655 RMSE: 1.2813029\n",
      "21 45 0.44900551438331604\n",
      "21 95 0.8332873582839966\n",
      "Validation loss: 1.7726622399829683 RMSE: 1.3314135\n",
      "22 40 0.2873505651950836\n",
      "22 90 0.2901436388492584\n",
      "Validation loss: 1.627662039370764 RMSE: 1.2757986\n",
      "23 35 0.3936764597892761\n",
      "23 85 0.29389646649360657\n",
      "Validation loss: 1.2396604310898554 RMSE: 1.1134003\n",
      "24 30 0.3878626227378845\n",
      "24 80 0.5677552223205566\n",
      "Validation loss: 1.4029590595336188 RMSE: 1.1844656\n",
      "25 25 0.7076685428619385\n",
      "25 75 0.53294837474823\n",
      "Validation loss: 1.854372719355992 RMSE: 1.3617536\n",
      "26 20 0.34270238876342773\n",
      "26 70 0.5098666548728943\n",
      "Validation loss: 1.3490607238951184 RMSE: 1.1614907\n",
      "27 15 0.5571892857551575\n",
      "27 65 0.5861009955406189\n",
      "Validation loss: 1.6510366400082905 RMSE: 1.2849268\n",
      "28 10 0.36898383498191833\n",
      "28 60 0.3811092674732208\n",
      "Validation loss: 1.782409786042713 RMSE: 1.3350692\n",
      "29 5 0.32515475153923035\n",
      "29 55 0.5180630683898926\n",
      "Validation loss: 1.1799002766609192 RMSE: 1.0862322\n",
      "30 0 0.40186282992362976\n",
      "30 50 0.37769514322280884\n",
      "30 100 0.3827904462814331\n",
      "Validation loss: 1.2069149914241972 RMSE: 1.0985968\n",
      "31 45 0.33414730429649353\n",
      "31 95 0.32634997367858887\n",
      "Validation loss: 1.4895398162660145 RMSE: 1.220467\n",
      "32 40 0.4519262909889221\n",
      "32 90 0.7983781695365906\n",
      "Validation loss: 1.163969069435483 RMSE: 1.078874\n",
      "33 35 1.0010490417480469\n",
      "33 85 0.48864197731018066\n",
      "Validation loss: 1.2248272532508486 RMSE: 1.1067193\n",
      "34 30 0.5568205714225769\n",
      "34 80 0.36904069781303406\n",
      "Validation loss: 1.23146424974714 RMSE: 1.1097137\n",
      "35 25 0.29699891805648804\n",
      "35 75 0.4315526783466339\n",
      "Validation loss: 1.2532101086207799 RMSE: 1.1194687\n",
      "36 20 0.34132301807403564\n",
      "36 70 0.6147813200950623\n",
      "Validation loss: 1.2384617987133208 RMSE: 1.112862\n",
      "37 15 0.2819511294364929\n",
      "37 65 0.3685897886753082\n",
      "Validation loss: 1.5459063666207449 RMSE: 1.2433448\n",
      "38 10 0.6205793619155884\n",
      "38 60 0.46097850799560547\n",
      "Validation loss: 1.708943441935948 RMSE: 1.3072656\n",
      "57 65 0.3958178758621216\n",
      "Validation loss: 1.6202119946479798 RMSE: 1.2728754\n",
      "58 10 0.4228697419166565\n",
      "58 60 0.5850346684455872\n",
      "Validation loss: 2.2116345473698207 RMSE: 1.4871565\n",
      "59 5 0.5192150473594666\n",
      "59 55 0.305878609418869\n",
      "Validation loss: 1.5659606002625965 RMSE: 1.2513835\n",
      "60 0 0.31490856409072876\n",
      "60 50 0.5624887943267822\n",
      "60 100 0.3681659996509552\n",
      "Validation loss: 1.409949659165882 RMSE: 1.187413\n",
      "61 45 0.24017590284347534\n",
      "61 95 0.6278648972511292\n",
      "Validation loss: 2.457975228627523 RMSE: 1.5677931\n",
      "62 40 0.49837571382522583\n",
      "62 90 0.2626940906047821\n",
      "Validation loss: 1.364129652863457 RMSE: 1.1679596\n",
      "63 35 0.5768073201179504\n",
      "63 85 0.5706603527069092\n",
      "Validation loss: 3.9874632517496744 RMSE: 1.9968632\n",
      "64 30 0.3406238853931427\n",
      "64 80 0.32491153478622437\n",
      "Validation loss: 2.9100939705258324 RMSE: 1.7058997\n",
      "65 25 0.48426616191864014\n",
      "65 75 0.37617555260658264\n",
      "Validation loss: 1.9215405969392685 RMSE: 1.3861965\n",
      "66 20 0.386229544878006\n",
      "66 70 0.4980468153953552\n",
      "Validation loss: 2.1650979587009975 RMSE: 1.4714272\n",
      "67 15 0.2935906648635864\n",
      "67 65 0.48941442370414734\n",
      "Validation loss: 1.4044228962489538 RMSE: 1.1850835\n",
      "68 10 0.25151312351226807\n",
      "68 60 0.2673335373401642\n",
      "Validation loss: 1.2351001217251731 RMSE: 1.1113507\n",
      "69 5 0.4167453944683075\n",
      "69 55 0.4698772430419922\n",
      "Validation loss: 1.1252553474335443 RMSE: 1.0607805\n",
      "70 0 0.4355385899543762\n",
      "70 50 0.2529313266277313\n",
      "70 100 0.36927998065948486\n",
      "Validation loss: 1.941058601651873 RMSE: 1.3932188\n",
      "71 45 0.2484690248966217\n",
      "71 95 0.32431167364120483\n",
      "Validation loss: 1.9006844770340692 RMSE: 1.3786532\n",
      "72 40 0.25235116481781006\n",
      "72 90 0.20773206651210785\n",
      "Validation loss: 1.539935014361427 RMSE: 1.2409412\n",
      "73 35 0.2237633615732193\n",
      "73 85 0.32240983843803406\n",
      "Validation loss: 3.507592028663272 RMSE: 1.8728566\n",
      "74 30 0.37422290444374084\n",
      "74 80 0.2773909568786621\n",
      "Validation loss: 1.8856788294655935 RMSE: 1.3732002\n",
      "75 25 0.4853636622428894\n",
      "75 75 0.26499834656715393\n",
      "Validation loss: 3.3273379462105885 RMSE: 1.8240992\n",
      "76 20 0.3071472942829132\n",
      "76 70 0.36759695410728455\n",
      "Validation loss: 4.197225016639346 RMSE: 2.048713\n",
      "77 15 0.27345672249794006\n",
      "77 65 0.18829984962940216\n",
      "Validation loss: 1.781464982032776 RMSE: 1.3347154\n",
      "78 10 0.5347127914428711\n",
      "78 60 0.37275323271751404\n",
      "Validation loss: 1.1888058151517595 RMSE: 1.0903237\n",
      "79 5 0.20711028575897217\n",
      "79 55 0.5524954199790955\n",
      "Validation loss: 2.184085187457857 RMSE: 1.477865\n",
      "80 0 0.24611924588680267\n",
      "80 50 0.2850032448768616\n",
      "80 100 0.3201565146446228\n",
      "Validation loss: 1.2729108288174584 RMSE: 1.1282336\n",
      "81 45 0.3032715320587158\n",
      "81 95 0.3813105523586273\n",
      "Validation loss: 2.3451944124130977 RMSE: 1.5314028\n",
      "82 40 0.23470422625541687\n",
      "82 90 0.2727414667606354\n",
      "Validation loss: 1.418424577940078 RMSE: 1.1909763\n",
      "83 35 0.2585424482822418\n",
      "83 85 0.17742674052715302\n",
      "Validation loss: 2.185920183999198 RMSE: 1.4784857\n",
      "84 30 0.4251859784126282\n",
      "84 80 0.31394052505493164\n",
      "Validation loss: 1.524455029623849 RMSE: 1.2346883\n",
      "85 25 0.46445176005363464\n",
      "85 75 0.43739211559295654\n",
      "Validation loss: 1.2031648272559756 RMSE: 1.0968887\n",
      "86 20 0.2603147327899933\n",
      "86 70 0.3760676383972168\n",
      "Validation loss: 1.2386309589658464 RMSE: 1.1129379\n",
      "87 15 0.26979702711105347\n",
      "87 65 0.5678094625473022\n",
      "Validation loss: 1.292183864684332 RMSE: 1.1367426\n",
      "88 10 0.260175496339798\n",
      "88 60 0.31528258323669434\n",
      "Validation loss: 2.527800983474368 RMSE: 1.5899061\n",
      "89 5 0.2508764863014221\n",
      "89 55 0.32857614755630493\n",
      "Validation loss: 2.288913004738944 RMSE: 1.5129154\n",
      "90 0 0.3474785387516022\n",
      "90 50 0.20263956487178802\n",
      "90 100 0.4027123749256134\n",
      "Validation loss: 4.1721748420170375 RMSE: 2.0425901\n",
      "91 45 0.16709326207637787\n",
      "91 95 0.2604290246963501\n",
      "Validation loss: 3.023997942606608 RMSE: 1.7389646\n",
      "92 40 0.30783334374427795\n",
      "92 90 0.2970053553581238\n",
      "Validation loss: 2.8547055789402553 RMSE: 1.6895874\n",
      "93 35 0.29141965508461\n",
      "93 85 0.43472978472709656\n",
      "Validation loss: 3.478831386566162 RMSE: 1.8651626\n",
      "94 30 0.22768263518810272\n",
      "94 80 0.39539486169815063\n",
      "Validation loss: 2.322047594615391 RMSE: 1.5238266\n",
      "95 25 0.20108260214328766\n",
      "95 75 0.20685049891471863\n",
      "Validation loss: 3.1090261096046086 RMSE: 1.7632431\n",
      "96 20 0.2623347342014313\n",
      "96 70 0.22750349342823029\n",
      "Validation loss: 3.0774275643484934 RMSE: 1.7542598\n",
      "97 15 0.472825825214386\n",
      "97 65 0.22105133533477783\n",
      "Validation loss: 1.5114262609254747 RMSE: 1.2294008\n",
      "98 10 0.21054325997829437\n",
      "98 60 0.26806244254112244\n",
      "Validation loss: 1.5381637686774845 RMSE: 1.2402273\n",
      "99 5 0.27708637714385986\n",
      "99 55 0.2609565854072571\n",
      "Validation loss: 1.6240189620426724 RMSE: 1.27437\n",
      "100 0 0.25335678458213806\n",
      "100 50 0.49009832739830017\n",
      "100 100 0.3050243556499481\n",
      "Validation loss: 1.668818864368257 RMSE: 1.2918277\n",
      "101 45 0.2856430113315582\n",
      "101 95 0.16726164519786835\n",
      "Validation loss: 4.314940434410459 RMSE: 2.0772436\n",
      "102 40 0.32542330026626587\n",
      "102 90 0.3308905363082886\n",
      "Validation loss: 2.476732721782866 RMSE: 1.5737638\n",
      "103 35 0.26154643297195435\n",
      "103 85 0.29219746589660645\n",
      "Validation loss: 2.8910758449917746 RMSE: 1.7003164\n",
      "104 30 0.11344397068023682\n",
      "104 80 0.29752758145332336\n",
      "Validation loss: 6.398830309368315 RMSE: 2.5295908\n",
      "105 25 0.2788461446762085\n",
      "105 75 0.3109520375728607\n",
      "Validation loss: 7.570144349052793 RMSE: 2.7513895\n",
      "106 20 0.33708417415618896\n",
      "106 70 0.4439448416233063\n",
      "Validation loss: 6.865666026160831 RMSE: 2.6202416\n",
      "107 15 0.2789580225944519\n",
      "107 65 0.371599018573761\n",
      "Validation loss: 7.3414456639971055 RMSE: 2.7095103\n",
      "108 10 0.2260502278804779\n",
      "108 60 0.3130909502506256\n",
      "Validation loss: 2.1782890955607095 RMSE: 1.4759028\n",
      "109 5 0.24282892048358917\n",
      "109 55 0.2461140751838684\n",
      "Validation loss: 4.732126045227051 RMSE: 2.1753452\n",
      "110 0 0.4626119136810303\n",
      "110 50 0.30836787819862366\n",
      "110 100 0.23792782425880432\n",
      "Validation loss: 1.379871948560079 RMSE: 1.1746795\n",
      "111 45 0.29984837770462036\n",
      "111 95 0.20666444301605225\n",
      "Validation loss: 1.9343736682619368 RMSE: 1.3908175\n",
      "112 40 0.24986767768859863\n",
      "112 90 0.169941708445549\n",
      "Validation loss: 1.9027492375600905 RMSE: 1.3794018\n",
      "113 35 0.22694966197013855\n",
      "113 85 0.23192697763442993\n",
      "Validation loss: 1.472726397287278 RMSE: 1.2135594\n",
      "114 30 0.2840000092983246\n",
      "114 80 0.29273858666419983\n",
      "Validation loss: 5.343997823624384 RMSE: 2.3117087\n",
      "115 25 0.2886621654033661\n",
      "115 75 0.24825450778007507\n",
      "Validation loss: 3.4928463686080207 RMSE: 1.8689159\n",
      "116 20 0.30326446890830994\n",
      "116 70 0.2485039383172989\n",
      "Validation loss: 1.979570644242423 RMSE: 1.4069722\n",
      "117 15 0.1889030933380127\n",
      "117 65 0.17767949402332306\n",
      "Validation loss: 1.5705751850491478 RMSE: 1.2532259\n",
      "118 10 0.29649385809898376\n",
      "118 60 0.23279502987861633\n",
      "Validation loss: 3.0704078038533527 RMSE: 1.752258\n",
      "119 5 0.18204574286937714\n",
      "119 55 0.40534237027168274\n",
      "Validation loss: 3.6028047516232444 RMSE: 1.8981055\n",
      "120 0 0.1774601936340332\n",
      "120 50 0.2537178695201874\n",
      "120 100 0.33494633436203003\n",
      "Validation loss: 4.394931384495327 RMSE: 2.096409\n",
      "121 45 0.3753502368927002\n",
      "121 95 0.2633703351020813\n",
      "Validation loss: 2.2584307534354076 RMSE: 1.5028076\n",
      "122 40 0.16963209211826324\n",
      "122 90 0.2615046501159668\n",
      "Validation loss: 1.7043707802182153 RMSE: 1.3055155\n",
      "123 35 0.2817132771015167\n",
      "123 85 0.2561390697956085\n",
      "Validation loss: 4.054343400682722 RMSE: 2.01354\n",
      "124 30 0.2625656723976135\n",
      "124 80 0.22196583449840546\n",
      "Validation loss: 2.597914074716114 RMSE: 1.6118046\n",
      "125 25 0.10143090039491653\n",
      "125 75 0.1587819904088974\n",
      "Validation loss: 4.1301947139558335 RMSE: 2.032288\n",
      "126 20 0.34037497639656067\n",
      "126 70 0.4054720103740692\n",
      "Validation loss: 1.433860135929925 RMSE: 1.1974391\n",
      "127 15 0.17073723673820496\n",
      "127 65 0.24078288674354553\n",
      "Validation loss: 1.6052883619353884 RMSE: 1.2669997\n",
      "128 10 0.18970680236816406\n",
      "128 60 0.21701739728450775\n",
      "Validation loss: 2.176443800472078 RMSE: 1.4752774\n",
      "129 5 0.289488822221756\n",
      "129 55 0.26227495074272156\n",
      "Validation loss: 2.384726501646496 RMSE: 1.5442559\n",
      "130 0 0.2266903966665268\n",
      "130 50 0.17045171558856964\n",
      "130 100 0.38108816742897034\n",
      "Validation loss: 1.8546431677682058 RMSE: 1.3618529\n",
      "131 45 0.2755049765110016\n",
      "131 95 0.33457517623901367\n",
      "Validation loss: 1.5855521304266793 RMSE: 1.2591871\n",
      "132 40 0.27670422196388245\n",
      "132 90 0.09260079264640808\n",
      "Validation loss: 2.239244942438035 RMSE: 1.4964107\n",
      "133 35 0.10727379471063614\n",
      "133 85 0.319007009267807\n",
      "Validation loss: 2.5243022600809732 RMSE: 1.5888053\n",
      "134 30 0.10351653397083282\n",
      "134 80 0.2718677520751953\n",
      "Validation loss: 2.1630903016953242 RMSE: 1.4707447\n",
      "135 25 0.248525470495224\n",
      "135 75 0.28568223118782043\n",
      "Validation loss: 1.7782732725143433 RMSE: 1.3335191\n",
      "136 20 0.21604527533054352\n",
      "136 70 0.1419508457183838\n",
      "Validation loss: 4.296503130594889 RMSE: 2.0728009\n",
      "137 15 0.1179487332701683\n",
      "137 65 0.22621802985668182\n",
      "Validation loss: 3.6253799938020252 RMSE: 1.9040432\n",
      "138 10 0.253342866897583\n",
      "138 60 0.1729399859905243\n",
      "Validation loss: 2.0912932668413435 RMSE: 1.4461305\n",
      "139 5 0.4279278814792633\n",
      "139 55 0.17271795868873596\n",
      "Validation loss: 1.4029675665355863 RMSE: 1.1844693\n",
      "140 0 0.13427351415157318\n",
      "140 50 0.28120794892311096\n",
      "140 100 0.17763768136501312\n",
      "Validation loss: 2.460538895924886 RMSE: 1.5686104\n",
      "141 45 0.21763333678245544\n",
      "141 95 0.32348406314849854\n",
      "Validation loss: 1.989614793232509 RMSE: 1.410537\n",
      "142 40 0.18341517448425293\n",
      "142 90 0.11680396646261215\n",
      "Validation loss: 2.411610018071674 RMSE: 1.552936\n",
      "143 35 0.2201952487230301\n",
      "143 85 0.18183313310146332\n",
      "Validation loss: 2.4362999303000312 RMSE: 1.5608652\n",
      "144 30 0.17669638991355896\n",
      "144 80 0.23151390254497528\n",
      "Validation loss: 1.419717074008215 RMSE: 1.1915188\n",
      "145 25 0.18254537880420685\n",
      "145 75 0.18408890068531036\n",
      "Validation loss: 1.877129111971174 RMSE: 1.3700836\n",
      "146 20 0.22953210771083832\n",
      "146 70 0.2156786024570465\n",
      "Validation loss: 1.5632817029953003 RMSE: 1.2503127\n",
      "147 15 0.2165827751159668\n",
      "147 65 0.3361625671386719\n",
      "Validation loss: 2.7757608867826917 RMSE: 1.6660615\n",
      "148 10 0.19690635800361633\n",
      "148 60 0.31378328800201416\n",
      "Validation loss: 4.927819052196685 RMSE: 2.2198691\n",
      "149 5 0.17773987352848053\n",
      "149 55 0.15259942412376404\n",
      "Validation loss: 4.816398557027181 RMSE: 2.1946294\n",
      "150 0 0.22910982370376587\n",
      "150 50 0.18992401659488678\n",
      "150 100 0.3025747239589691\n",
      "Validation loss: 3.9981352579025997 RMSE: 1.9995338\n",
      "151 45 0.22829346358776093\n",
      "151 95 0.18441253900527954\n",
      "Validation loss: 1.894013164156959 RMSE: 1.3762316\n",
      "152 40 0.3087749779224396\n",
      "152 90 0.23266112804412842\n",
      "Validation loss: 1.362946841830299 RMSE: 1.1674532\n",
      "153 35 0.209755077958107\n",
      "153 85 0.35382795333862305\n",
      "Validation loss: 3.5812038966587614 RMSE: 1.8924068\n",
      "154 30 0.17012667655944824\n",
      "154 80 0.19558227062225342\n",
      "Validation loss: 2.5030302252088275 RMSE: 1.5820968\n",
      "155 25 0.19977697730064392\n",
      "155 75 0.2653118669986725\n",
      "Validation loss: 2.2176216397966657 RMSE: 1.4891682\n",
      "156 20 0.30344051122665405\n",
      "156 70 0.2879635691642761\n",
      "Validation loss: 3.1186319362549555 RMSE: 1.7659649\n",
      "157 15 0.2821434438228607\n",
      "157 65 0.14012576639652252\n",
      "Validation loss: 2.8253080595107307 RMSE: 1.6808653\n",
      "158 10 0.251094788312912\n",
      "158 60 0.13277070224285126\n",
      "Validation loss: 1.207481791291918 RMSE: 1.0988548\n",
      "159 5 0.250715970993042\n",
      "159 55 0.1954626739025116\n",
      "Validation loss: 4.289844694591704 RMSE: 2.0711942\n",
      "160 0 0.2598535120487213\n",
      "160 50 0.1897164285182953\n",
      "160 100 0.21444356441497803\n",
      "Validation loss: 3.377863039289202 RMSE: 1.8378963\n",
      "161 45 0.13108588755130768\n",
      "161 95 0.16111597418785095\n",
      "Validation loss: 3.1575116725195023 RMSE: 1.7769388\n",
      "162 40 0.42392200231552124\n",
      "162 90 0.1958596259355545\n",
      "Validation loss: 1.7340807846614292 RMSE: 1.316845\n",
      "163 35 0.19907045364379883\n",
      "163 85 0.22812488675117493\n",
      "Validation loss: 1.5207086290631975 RMSE: 1.2331702\n",
      "164 30 0.18697810173034668\n",
      "164 80 0.12414464354515076\n",
      "Validation loss: 2.0458923044658843 RMSE: 1.4303468\n",
      "165 25 0.19536368548870087\n",
      "165 75 0.25548672676086426\n",
      "Validation loss: 2.07880745615278 RMSE: 1.441807\n",
      "166 20 0.19594219326972961\n",
      "166 70 0.2561844289302826\n",
      "Validation loss: 1.8730612925120762 RMSE: 1.3685982\n",
      "167 15 0.24775490164756775\n",
      "167 65 0.20319436490535736\n",
      "Validation loss: 1.8514117252259028 RMSE: 1.3606659\n",
      "168 10 0.22537516057491302\n",
      "168 60 0.21784673631191254\n",
      "Validation loss: 2.063529900142125 RMSE: 1.4364992\n",
      "169 5 0.2854858636856079\n",
      "169 55 0.1722271889448166\n",
      "Validation loss: 3.1557635647909983 RMSE: 1.7764468\n",
      "170 0 0.12006792426109314\n",
      "170 50 0.30570852756500244\n",
      "170 100 0.2230997532606125\n",
      "Validation loss: 2.390940546989441 RMSE: 1.5462666\n",
      "171 45 0.1752343624830246\n",
      "171 95 0.21928925812244415\n",
      "Validation loss: 2.8337120396750315 RMSE: 1.6833633\n",
      "172 40 0.15558649599552155\n",
      "172 90 0.33371701836586\n",
      "Validation loss: 1.4720831663835616 RMSE: 1.2132944\n",
      "173 35 0.12354779988527298\n",
      "173 85 0.16036689281463623\n",
      "Validation loss: 2.466910302355176 RMSE: 1.5706401\n",
      "174 30 0.30849429965019226\n",
      "174 80 0.1169763132929802\n",
      "Validation loss: 2.1159412176836105 RMSE: 1.4546275\n",
      "175 25 0.3357311487197876\n",
      "175 75 0.20029033720493317\n",
      "Validation loss: 2.6853036449069068 RMSE: 1.6386898\n",
      "176 20 0.10313832759857178\n",
      "176 70 0.16811947524547577\n",
      "Validation loss: 3.283875288282122 RMSE: 1.8121465\n",
      "177 15 0.1486305296421051\n",
      "177 65 0.17175106704235077\n",
      "Validation loss: 1.6689134529658727 RMSE: 1.2918643\n",
      "178 10 0.17593073844909668\n",
      "178 60 0.11846473067998886\n",
      "Validation loss: 2.9866702851795015 RMSE: 1.7281985\n",
      "179 5 0.16257750988006592\n",
      "179 55 0.13711154460906982\n",
      "Validation loss: 4.081060037158784 RMSE: 2.0201633\n",
      "180 0 0.14904727041721344\n",
      "180 50 0.13974817097187042\n",
      "180 100 0.17149454355239868\n",
      "Validation loss: 2.269900390080043 RMSE: 1.5066189\n",
      "181 45 0.16841717064380646\n",
      "181 95 0.2230592519044876\n",
      "Validation loss: 2.118864193416777 RMSE: 1.4556319\n",
      "182 40 0.1487916111946106\n",
      "182 90 0.26682645082473755\n",
      "Validation loss: 1.9596205370766775 RMSE: 1.3998644\n",
      "183 35 0.2741621434688568\n",
      "183 85 0.12655457854270935\n",
      "Validation loss: 1.3522293022700718 RMSE: 1.162854\n",
      "184 30 0.11335618793964386\n",
      "184 80 0.1589723825454712\n",
      "Validation loss: 1.9161857060023717 RMSE: 1.3842635\n",
      "185 25 0.21875466406345367\n",
      "185 75 0.3224608600139618\n",
      "Validation loss: 1.8002005929038638 RMSE: 1.3417156\n",
      "186 20 0.14002551138401031\n",
      "186 70 0.13049834966659546\n",
      "Validation loss: 2.6274960018339613 RMSE: 1.6209552\n",
      "187 15 0.16004537045955658\n",
      "187 65 0.30646514892578125\n",
      "Validation loss: 1.7060164244402023 RMSE: 1.3061457\n",
      "188 10 0.13746121525764465\n",
      "188 60 0.179242804646492\n",
      "Validation loss: 1.6216168471745083 RMSE: 1.2734272\n",
      "189 5 0.14772553741931915\n",
      "189 55 0.37087807059288025\n",
      "Validation loss: 2.4535297348385767 RMSE: 1.5663748\n",
      "190 0 0.2325012981891632\n",
      "190 50 0.15831100940704346\n",
      "190 100 0.20411457121372223\n",
      "Validation loss: 3.180613817487444 RMSE: 1.7834275\n",
      "191 45 0.18143746256828308\n",
      "191 95 0.18237397074699402\n",
      "Validation loss: 2.2967335814521426 RMSE: 1.5154979\n",
      "192 40 0.1632060706615448\n",
      "192 90 0.22255739569664001\n"
     ]
    }
   ],
   "source": [
    "datasets = ['FreeSolv', 'ESOL', 'Lipo', 'qm7', \"bace\",  \"bbbp\",  'tox21', 'sider',]\n",
    "seeds = [777, 778, 779, 780, 781]\n",
    "\n",
    "for dataset in datasets:\n",
    "    for seed in seeds:\n",
    "            if dataset == 'FreeSolv':\n",
    "            # FreeSolv 데이터셋에 대한 특정 옵션을 적용\n",
    "                !python finetuneReconEmbeddingBias.py \\\n",
    "                --task_name {dataset} \\\n",
    "                --seed {seed} \\\n",
    "                --dropout 0.5 \\\n",
    "                --num_layer 3 \\\n",
    "                --emb_dim 64 \\\n",
    "                --feat_dim 64 \\\n",
    "                --alpha 0.1 \\\n",
    "                --gpu cuda:1 \\\n",
    "                \n",
    "            else:\n",
    "                !python finetuneReconEmbeddingBias.py \\\n",
    "                --task_name {dataset} \\\n",
    "                --seed {seed} \\\n",
    "                --alpha 0.1 \\\n",
    "                --gpu cuda:1 \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2a46ae-a6b6-4a66-befc-c4e38c83fdb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:0', 'task_name': 'freesolv', 'model_type': 'gin', 'model': {'num_layer': 5, 'emb_dim': 300, 'feat_dim': 300, 'drop_ratio': 0.3, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'random', 'seed': 777, 'task': 'regression', 'data_path': 'data/freesolv/freesolv.csv', 'target': ['expt']}}\n",
      "Running on: cuda:0\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 14.321107864379883\n",
      "Validation loss: 20.91863441467285 RMSE: 4.5736895\n",
      "Validation loss: 17.256500959396362 RMSE: 4.154094\n",
      "2 16 1.6740686893463135\n",
      "Validation loss: 9.5014009475708 RMSE: 3.0824342\n",
      "Validation loss: 18.261054039001465 RMSE: 4.2732954\n",
      "Validation loss: 4.354099988937378 RMSE: 2.0866482\n",
      "5 15 2.0978336334228516\n",
      "Validation loss: 2.7761409282684326 RMSE: 1.6661755\n",
      "Validation loss: 6.134705543518066 RMSE: 2.4768338\n",
      "Validation loss: 6.321134567260742 RMSE: 2.5141869\n",
      "8 14 3.4172286987304688\n",
      "Validation loss: 5.5209267139434814 RMSE: 2.3496654\n",
      "Validation loss: 13.93534231185913 RMSE: 3.7330072\n",
      "Validation loss: 5.228390216827393 RMSE: 2.2865674\n",
      "11 13 3.121549129486084\n",
      "Validation loss: 7.373233079910278 RMSE: 2.7153695\n",
      "Validation loss: 4.980787515640259 RMSE: 2.231768\n",
      "Validation loss: 4.149728178977966 RMSE: 2.0370882\n",
      "14 12 3.0783309936523438\n",
      "Validation loss: 5.393917560577393 RMSE: 2.322481\n",
      "Validation loss: 9.424787402153015 RMSE: 3.069982\n",
      "Validation loss: 3.304729461669922 RMSE: 1.8178914\n",
      "17 11 2.1237094402313232\n",
      "Validation loss: 13.586473822593689 RMSE: 3.6859837\n",
      "Validation loss: 2.705667734146118 RMSE: 1.6448911\n",
      "Validation loss: 4.860611200332642 RMSE: 2.204679\n",
      "20 10 2.180887460708618\n",
      "Validation loss: 9.186617851257324 RMSE: 3.0309432\n",
      "Validation loss: 6.59820419549942 RMSE: 2.5686972\n",
      "Validation loss: 5.45841646194458 RMSE: 2.3363254\n",
      "23 9 2.2084107398986816\n",
      "Validation loss: 10.025069236755371 RMSE: 3.1662388\n",
      "Validation loss: 2.3286338448524475 RMSE: 1.5259862\n",
      "Validation loss: 8.440638542175293 RMSE: 2.9052775\n",
      "26 8 1.3372224569320679\n",
      "Validation loss: 6.463868141174316 RMSE: 2.5424137\n",
      "Validation loss: 12.901103496551514 RMSE: 3.5918105\n",
      "Validation loss: 12.712461471557617 RMSE: 3.565454\n",
      "29 7 3.387467861175537\n",
      "Validation loss: 1.6997678875923157 RMSE: 1.3037512\n",
      "Validation loss: 8.209753513336182 RMSE: 2.865267\n",
      "Validation loss: 3.7261006832122803 RMSE: 1.9303108\n",
      "32 6 1.3978638648986816\n",
      "Validation loss: 3.634792923927307 RMSE: 1.9065133\n",
      "Validation loss: 23.70071315765381 RMSE: 4.8683376\n",
      "Validation loss: 2.1626139879226685 RMSE: 1.4705827\n",
      "35 5 2.1506173610687256\n",
      "Validation loss: 7.431595802307129 RMSE: 2.7260954\n",
      "Validation loss: 6.668334126472473 RMSE: 2.5823119\n",
      "Validation loss: 2.303868055343628 RMSE: 1.5178498\n",
      "38 4 5.720208168029785\n",
      "Validation loss: 10.526123046875 RMSE: 3.2443988\n",
      "Validation loss: 4.498493194580078 RMSE: 2.1209652\n",
      "Validation loss: 4.741388440132141 RMSE: 2.1774728\n",
      "41 3 1.1128249168395996\n",
      "Validation loss: 15.646967649459839 RMSE: 3.9556246\n",
      "Validation loss: 1.95924711227417 RMSE: 1.3997312\n",
      "Validation loss: 3.023181438446045 RMSE: 1.7387301\n",
      "44 2 1.488884449005127\n",
      "Validation loss: 8.31072986125946 RMSE: 2.8828337\n",
      "Validation loss: 3.8074792623519897 RMSE: 1.9512762\n",
      "Validation loss: 15.148390293121338 RMSE: 3.8920937\n",
      "47 1 1.3609793186187744\n",
      "Validation loss: 11.078186750411987 RMSE: 3.3283908\n",
      "Validation loss: 2.7154356241226196 RMSE: 1.6478577\n",
      "Validation loss: 9.06560730934143 RMSE: 3.0109143\n",
      "50 0 1.7610065937042236\n",
      "Validation loss: 5.494647741317749 RMSE: 2.3440666\n",
      "Validation loss: 4.9150495529174805 RMSE: 2.216991\n",
      "52 16 4.530332565307617\n",
      "Validation loss: 2.4536731243133545 RMSE: 1.5664204\n",
      "Validation loss: 7.778294801712036 RMSE: 2.7889593\n",
      "Validation loss: 11.89563512802124 RMSE: 3.4490044\n",
      "55 15 1.5943224430084229\n",
      "Validation loss: 8.33542251586914 RMSE: 2.887113\n",
      "Validation loss: 4.603169918060303 RMSE: 2.1454997\n",
      "Validation loss: 8.7169189453125 RMSE: 2.9524424\n",
      "58 14 6.113883972167969\n",
      "Validation loss: 3.524573802947998 RMSE: 1.8773848\n",
      "Validation loss: 3.1536667346954346 RMSE: 1.7758563\n",
      "Validation loss: 2.669476866722107 RMSE: 1.6338534\n",
      "61 13 3.593710422515869\n",
      "Validation loss: 2.729556918144226 RMSE: 1.6521369\n",
      "Validation loss: 1.4966734647750854 RMSE: 1.223386\n",
      "Validation loss: 10.484611511230469 RMSE: 3.2379944\n",
      "64 12 1.4421100616455078\n",
      "Validation loss: 15.941698551177979 RMSE: 3.9927056\n",
      "Validation loss: 2.03757643699646 RMSE: 1.4274371\n",
      "Validation loss: 4.891714930534363 RMSE: 2.2117226\n",
      "67 11 0.8361790180206299\n",
      "Validation loss: 1.9478195905685425 RMSE: 1.3956432\n",
      "Validation loss: 4.604289650917053 RMSE: 2.1457608\n",
      "Validation loss: 6.783350348472595 RMSE: 2.6044862\n",
      "70 10 2.0118415355682373\n",
      "Validation loss: 5.839918375015259 RMSE: 2.4165924\n",
      "Validation loss: 8.688260555267334 RMSE: 2.9475856\n",
      "Validation loss: 2.0918793082237244 RMSE: 1.446333\n",
      "73 9 0.996292233467102\n",
      "Validation loss: 2.770226240158081 RMSE: 1.6643996\n",
      "Validation loss: 3.364711880683899 RMSE: 1.8343152\n",
      "Validation loss: 2.3308213353157043 RMSE: 1.5267028\n",
      "76 8 1.6383395195007324\n",
      "Validation loss: 2.394361436367035 RMSE: 1.5473725\n",
      "Validation loss: 1.2451589703559875 RMSE: 1.1158668\n",
      "Validation loss: 9.45081090927124 RMSE: 3.0742173\n",
      "79 7 2.138673782348633\n",
      "Validation loss: 6.314155101776123 RMSE: 2.5127983\n",
      "Validation loss: 11.514304161071777 RMSE: 3.3932729\n",
      "Validation loss: 1.5443845093250275 RMSE: 1.2427326\n",
      "82 6 1.8259228467941284\n",
      "Validation loss: 2.806889295578003 RMSE: 1.6753772\n",
      "Validation loss: 2.180079460144043 RMSE: 1.4765092\n",
      "Validation loss: 13.70202922821045 RMSE: 3.7016258\n",
      "85 5 1.7427281141281128\n",
      "Validation loss: 2.6307966709136963 RMSE: 1.6219732\n",
      "Validation loss: 5.62000185251236 RMSE: 2.370654\n",
      "Validation loss: 2.6636844873428345 RMSE: 1.6320797\n",
      "88 4 0.8470762968063354\n",
      "Validation loss: 1.5560477375984192 RMSE: 1.2474165\n",
      "Validation loss: 6.000298738479614 RMSE: 2.4495509\n",
      "Validation loss: 3.4865431785583496 RMSE: 1.867229\n",
      "91 3 0.5117723345756531\n",
      "Validation loss: 1.3801007866859436 RMSE: 1.1747768\n",
      "Validation loss: 3.1592814922332764 RMSE: 1.7774367\n",
      "Validation loss: 2.77260684967041 RMSE: 1.6651146\n",
      "94 2 2.832580327987671\n",
      "Validation loss: 2.2825942039489746 RMSE: 1.5108255\n",
      "Validation loss: 1.2492355108261108 RMSE: 1.117692\n",
      "Validation loss: 4.164891719818115 RMSE: 2.0408063\n",
      "97 1 2.0177931785583496\n",
      "Validation loss: 3.6904574632644653 RMSE: 1.9210564\n",
      "Validation loss: 5.913467675447464 RMSE: 2.4317622\n",
      "Validation loss: 1.072115421295166 RMSE: 1.0354302\n",
      "100 0 1.43095862865448\n",
      "Validation loss: 2.359165281057358 RMSE: 1.5359572\n",
      "Validation loss: 3.742803692817688 RMSE: 1.9346328\n",
      "102 16 0.6668446660041809\n",
      "Validation loss: 2.897221565246582 RMSE: 1.7021226\n",
      "Validation loss: 1.9340905249118805 RMSE: 1.3907158\n",
      "Validation loss: 3.806449294090271 RMSE: 1.9510124\n",
      "105 15 2.747955322265625\n",
      "Validation loss: 2.2267810106277466 RMSE: 1.4922402\n",
      "Validation loss: 2.8945993185043335 RMSE: 1.7013521\n",
      "Validation loss: 2.2886324524879456 RMSE: 1.5128231\n",
      "108 14 1.856547236442566\n",
      "Validation loss: 5.9955174922943115 RMSE: 2.4485748\n",
      "Validation loss: 1.249184787273407 RMSE: 1.1176693\n",
      "Validation loss: 3.0336804389953613 RMSE: 1.7417461\n",
      "111 13 0.8801333904266357\n",
      "Validation loss: 1.4764221906661987 RMSE: 1.2150811\n",
      "Validation loss: 6.9158570766448975 RMSE: 2.6298018\n",
      "Validation loss: 1.338023066520691 RMSE: 1.1567296\n",
      "114 12 1.0879697799682617\n",
      "Validation loss: 2.7781455516815186 RMSE: 1.6667773\n",
      "Validation loss: 4.137959718704224 RMSE: 2.0341976\n",
      "Validation loss: 2.805605858564377 RMSE: 1.6749942\n",
      "117 11 1.7006199359893799\n",
      "Validation loss: 2.56577730178833 RMSE: 1.6018044\n",
      "Validation loss: 1.2324445247650146 RMSE: 1.1101552\n",
      "Validation loss: 1.9795788526535034 RMSE: 1.4069748\n",
      "120 10 1.2716553211212158\n",
      "Validation loss: 1.340436190366745 RMSE: 1.157772\n",
      "Validation loss: 1.4255233108997345 RMSE: 1.1939527\n",
      "Validation loss: 6.794361591339111 RMSE: 2.6065993\n",
      "123 9 1.2550132274627686\n",
      "Validation loss: 1.4356001019477844 RMSE: 1.1981653\n",
      "Validation loss: 5.750680923461914 RMSE: 2.3980577\n",
      "Validation loss: 1.3114582300186157 RMSE: 1.145189\n",
      "126 8 1.7500109672546387\n",
      "Validation loss: 4.8761069774627686 RMSE: 2.208191\n",
      "Validation loss: 1.45756334066391 RMSE: 1.2072957\n",
      "Validation loss: 1.1161895394325256 RMSE: 1.0564988\n",
      "129 7 0.6116553544998169\n",
      "Validation loss: 1.739623486995697 RMSE: 1.3189479\n",
      "Validation loss: 3.356816291809082 RMSE: 1.8321614\n",
      "Validation loss: 3.7187633514404297 RMSE: 1.9284096\n",
      "132 6 1.253115177154541\n",
      "Validation loss: 1.6520788371562958 RMSE: 1.2853323\n",
      "Validation loss: 1.0393221378326416 RMSE: 1.0194714\n",
      "Validation loss: 2.311925768852234 RMSE: 1.5205017\n",
      "135 5 0.5788388848304749\n",
      "Validation loss: 2.79392671585083 RMSE: 1.6715044\n",
      "Validation loss: 2.091659724712372 RMSE: 1.4462571\n",
      "Validation loss: 1.4883313179016113 RMSE: 1.2199719\n",
      "138 4 1.0223125219345093\n",
      "Validation loss: 2.0054184198379517 RMSE: 1.4161279\n",
      "Validation loss: 1.3556776642799377 RMSE: 1.1643357\n",
      "Validation loss: 1.5639651715755463 RMSE: 1.2505859\n",
      "141 3 1.521355152130127\n",
      "Validation loss: 0.9317711591720581 RMSE: 0.9652828\n",
      "Validation loss: 1.3833388686180115 RMSE: 1.1761543\n",
      "Validation loss: 1.6674079895019531 RMSE: 1.2912816\n",
      "144 2 1.359778642654419\n",
      "Validation loss: 6.275326251983643 RMSE: 2.5050602\n",
      "Validation loss: 2.056834638118744 RMSE: 1.4341669\n",
      "Validation loss: 1.3124879002571106 RMSE: 1.1456387\n",
      "147 1 1.4902451038360596\n",
      "Validation loss: 1.7479283809661865 RMSE: 1.3220924\n",
      "Validation loss: 3.0501595735549927 RMSE: 1.7464706\n",
      "Validation loss: 2.12426096200943 RMSE: 1.4574845\n",
      "150 0 0.7807320356369019\n",
      "Validation loss: 1.9108963012695312 RMSE: 1.3823519\n",
      "Validation loss: 1.4102342128753662 RMSE: 1.1875328\n",
      "152 16 0.3569975197315216\n",
      "Validation loss: 1.5386036038398743 RMSE: 1.2404047\n",
      "Validation loss: 0.9323343336582184 RMSE: 0.9655746\n",
      "Validation loss: 1.196922391653061 RMSE: 1.0940394\n",
      "155 15 1.0107722282409668\n",
      "Validation loss: 3.7611218690872192 RMSE: 1.9393616\n",
      "Validation loss: 3.757067382335663 RMSE: 1.9383152\n",
      "Validation loss: 5.740548729896545 RMSE: 2.395944\n",
      "158 14 2.9125609397888184\n",
      "Validation loss: 1.1788890063762665 RMSE: 1.0857666\n",
      "Validation loss: 1.6328120231628418 RMSE: 1.2778153\n",
      "Validation loss: 1.2574752569198608 RMSE: 1.1213721\n",
      "161 13 1.1761541366577148\n",
      "Validation loss: 4.028734087944031 RMSE: 2.0071704\n",
      "Validation loss: 0.9544443190097809 RMSE: 0.97695667\n",
      "Validation loss: 1.6894562244415283 RMSE: 1.2997906\n",
      "164 12 0.6288465261459351\n",
      "Validation loss: 1.0672178864479065 RMSE: 1.0330625\n",
      "Validation loss: 0.966910719871521 RMSE: 0.98331624\n",
      "Validation loss: 1.1631883382797241 RMSE: 1.0785121\n",
      "167 11 0.9410708546638489\n",
      "Validation loss: 1.181836485862732 RMSE: 1.087123\n",
      "Validation loss: 1.3985340595245361 RMSE: 1.1825962\n",
      "Validation loss: 3.3499534130096436 RMSE: 1.8302876\n",
      "170 10 1.0085904598236084\n",
      "Validation loss: 2.5982906818389893 RMSE: 1.6119213\n",
      "Validation loss: 1.89385986328125 RMSE: 1.3761758\n",
      "Validation loss: 1.6625536680221558 RMSE: 1.2894006\n",
      "173 9 0.9965384006500244\n",
      "Validation loss: 4.122945308685303 RMSE: 2.0305035\n",
      "Validation loss: 1.4736483097076416 RMSE: 1.2139392\n",
      "Validation loss: 0.7938440442085266 RMSE: 0.8909793\n",
      "176 8 1.9352898597717285\n",
      "Validation loss: 1.3663355112075806 RMSE: 1.1689036\n",
      "Validation loss: 1.0736965537071228 RMSE: 1.0361933\n",
      "Validation loss: 5.307312250137329 RMSE: 2.3037603\n",
      "179 7 1.3857712745666504\n",
      "Validation loss: 1.7182368636131287 RMSE: 1.3108155\n",
      "Validation loss: 1.4312275648117065 RMSE: 1.1963391\n",
      "Validation loss: 0.9997207522392273 RMSE: 0.9998604\n",
      "182 6 0.9685672521591187\n",
      "Validation loss: 1.9417122602462769 RMSE: 1.3934534\n",
      "Validation loss: 1.6379902362823486 RMSE: 1.27984\n",
      "Validation loss: 1.0898262858390808 RMSE: 1.0439473\n",
      "185 5 1.3943133354187012\n",
      "Validation loss: 1.1425244808197021 RMSE: 1.0688894\n",
      "Validation loss: 1.9647348523139954 RMSE: 1.4016898\n",
      "Validation loss: 1.6245195865631104 RMSE: 1.2745663\n",
      "188 4 0.5735785961151123\n",
      "Validation loss: 2.1779500246047974 RMSE: 1.4757879\n",
      "Validation loss: 1.380947470664978 RMSE: 1.1751372\n",
      "Validation loss: 1.5370616912841797 RMSE: 1.239783\n",
      "191 3 0.5352512001991272\n",
      "Validation loss: 2.5072742700576782 RMSE: 1.5834376\n",
      "Validation loss: 1.66990065574646 RMSE: 1.2922465\n",
      "Validation loss: 3.1005266904830933 RMSE: 1.7608316\n",
      "194 2 1.3924925327301025\n",
      "Validation loss: 4.185925126075745 RMSE: 2.0459528\n",
      "Validation loss: 0.7411659359931946 RMSE: 0.86090994\n",
      "Validation loss: 4.66570520401001 RMSE: 2.1600242\n",
      "197 1 1.9714056253433228\n",
      "Validation loss: 2.1266363859176636 RMSE: 1.4582992\n",
      "Validation loss: 0.8483651876449585 RMSE: 0.9210674\n",
      "Validation loss: 3.0864381194114685 RMSE: 1.7568266\n",
      "Loaded trained model with success.\n",
      "Test loss: 1.9685813188552856 Test RMSE: 1.4030615\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:0', 'task_name': 'freesolv', 'model_type': 'gin', 'model': {'num_layer': 5, 'emb_dim': 300, 'feat_dim': 300, 'drop_ratio': 0.3, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'random', 'seed': 778, 'task': 'regression', 'data_path': 'data/freesolv/freesolv.csv', 'target': ['expt']}}\n",
      "Running on: cuda:0\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 16.177167892456055\n",
      "Validation loss: 24.261737823486328 RMSE: 4.9256206\n",
      "Validation loss: 19.728859424591064 RMSE: 4.441718\n",
      "2 16 17.045284271240234\n",
      "Validation loss: 19.012720108032227 RMSE: 4.360358\n",
      "Validation loss: 6.400212526321411 RMSE: 2.5298643\n",
      "Validation loss: 17.22266435623169 RMSE: 4.1500196\n",
      "5 15 3.8309154510498047\n",
      "Validation loss: 5.235531568527222 RMSE: 2.2881284\n",
      "Validation loss: 27.575143814086914 RMSE: 5.251204\n",
      "Validation loss: 5.97870135307312 RMSE: 2.4451385\n",
      "8 14 3.077336311340332\n",
      "Validation loss: 6.119457244873047 RMSE: 2.4737537\n",
      "Validation loss: 5.8354332447052 RMSE: 2.4156642\n",
      "Validation loss: 7.723174333572388 RMSE: 2.7790601\n",
      "11 13 3.958364963531494\n",
      "Validation loss: 6.013454914093018 RMSE: 2.4522345\n",
      "Validation loss: 5.204346179962158 RMSE: 2.2813036\n",
      "Validation loss: 6.283418893814087 RMSE: 2.506675\n",
      "14 12 2.3135015964508057\n",
      "Validation loss: 4.100240707397461 RMSE: 2.0249052\n",
      "Validation loss: 9.616214275360107 RMSE: 3.1010022\n",
      "Validation loss: 7.279036045074463 RMSE: 2.6979687\n",
      "17 11 3.8547940254211426\n",
      "Validation loss: 9.974107682704926 RMSE: 3.1581812\n",
      "Validation loss: 3.30516254901886 RMSE: 1.8180106\n",
      "Validation loss: 2.5077884197235107 RMSE: 1.5835998\n",
      "20 10 2.145199775695801\n",
      "Validation loss: 3.2945258617401123 RMSE: 1.815083\n",
      "Validation loss: 7.0554587841033936 RMSE: 2.6562114\n",
      "Validation loss: 3.3896178603172302 RMSE: 1.8410914\n",
      "23 9 1.5921759605407715\n",
      "Validation loss: 5.975407600402832 RMSE: 2.4444647\n",
      "Validation loss: 3.8915568590164185 RMSE: 1.9727031\n",
      "Validation loss: 3.271517276763916 RMSE: 1.8087337\n",
      "26 8 1.3833270072937012\n",
      "Validation loss: 3.5469539165496826 RMSE: 1.8833357\n",
      "Validation loss: 3.483122944831848 RMSE: 1.8663127\n",
      "Validation loss: 6.065205097198486 RMSE: 2.4627638\n",
      "29 7 1.1739394664764404\n",
      "Validation loss: 6.60438871383667 RMSE: 2.5699005\n",
      "Validation loss: 3.480749785900116 RMSE: 1.8656768\n",
      "Validation loss: 2.2743804454803467 RMSE: 1.508105\n",
      "32 6 1.3863418102264404\n",
      "Validation loss: 8.4001704454422 RMSE: 2.8983047\n",
      "Validation loss: 3.368551254272461 RMSE: 1.8353615\n",
      "Validation loss: 7.511385917663574 RMSE: 2.7406907\n",
      "35 5 2.5965898036956787\n",
      "Validation loss: 3.83659827709198 RMSE: 1.9587238\n",
      "Validation loss: 3.257595717906952 RMSE: 1.804881\n",
      "Validation loss: 2.791750907897949 RMSE: 1.6708534\n",
      "38 4 1.881471037864685\n",
      "Validation loss: 9.365322828292847 RMSE: 3.0602813\n",
      "Validation loss: 11.13070011138916 RMSE: 3.3362706\n",
      "Validation loss: 6.849674940109253 RMSE: 2.6171882\n",
      "41 3 3.5577640533447266\n",
      "Validation loss: 4.5565197467803955 RMSE: 2.1346006\n",
      "Validation loss: 5.489298582077026 RMSE: 2.3429246\n",
      "Validation loss: 2.858259618282318 RMSE: 1.6906388\n",
      "44 2 2.3805320262908936\n",
      "Validation loss: 4.33515477180481 RMSE: 2.0821033\n",
      "Validation loss: 3.2206575870513916 RMSE: 1.7946192\n",
      "Validation loss: 5.4128406047821045 RMSE: 2.326551\n",
      "47 1 1.824760913848877\n",
      "Validation loss: 3.548068881034851 RMSE: 1.883632\n",
      "Validation loss: 4.326249122619629 RMSE: 2.0799637\n",
      "Validation loss: 3.7500126361846924 RMSE: 1.9364951\n",
      "50 0 2.920224189758301\n",
      "Validation loss: 2.8610774278640747 RMSE: 1.6914718\n",
      "Validation loss: 4.7691570520401 RMSE: 2.18384\n",
      "52 16 8.06419563293457\n",
      "Validation loss: 5.873327732086182 RMSE: 2.423495\n",
      "Validation loss: 4.766313314437866 RMSE: 2.1831892\n",
      "Validation loss: 5.336477994918823 RMSE: 2.310082\n",
      "55 15 0.8392414450645447\n",
      "Validation loss: 4.3221787214279175 RMSE: 2.0789852\n",
      "Validation loss: 4.687639951705933 RMSE: 2.1650958\n",
      "Validation loss: 5.042495012283325 RMSE: 2.2455502\n",
      "58 14 0.9096124768257141\n",
      "Validation loss: 3.4262540340423584 RMSE: 1.8510143\n",
      "Validation loss: 3.1890887022018433 RMSE: 1.7858019\n",
      "Validation loss: 8.866388320922852 RMSE: 2.9776487\n",
      "61 13 1.245629072189331\n",
      "Validation loss: 4.22921359539032 RMSE: 2.0565054\n",
      "Validation loss: 5.495846509933472 RMSE: 2.344322\n",
      "Validation loss: 3.5939204692840576 RMSE: 1.8957638\n",
      "64 12 0.957255482673645\n",
      "Validation loss: 3.9504001140594482 RMSE: 1.9875616\n",
      "Validation loss: 3.2795557975769043 RMSE: 1.8109546\n",
      "Validation loss: 6.611280679702759 RMSE: 2.5712411\n",
      "67 11 0.6362749338150024\n",
      "Validation loss: 3.502771496772766 RMSE: 1.8715692\n",
      "Validation loss: 4.1181793212890625 RMSE: 2.0293298\n",
      "Validation loss: 7.210947036743164 RMSE: 2.6853206\n",
      "70 10 2.4402623176574707\n",
      "Validation loss: 2.392658770084381 RMSE: 1.5468222\n",
      "Validation loss: 4.481552720069885 RMSE: 2.1169677\n",
      "Validation loss: 4.823304891586304 RMSE: 2.1962025\n",
      "73 9 0.842302680015564\n",
      "Validation loss: 7.994356632232666 RMSE: 2.8274293\n",
      "Validation loss: 2.0276458263397217 RMSE: 1.4239542\n",
      "Validation loss: 2.9081162214279175 RMSE: 1.70532\n",
      "76 8 1.0372307300567627\n",
      "Validation loss: 2.5272045135498047 RMSE: 1.5897183\n",
      "Validation loss: 3.901069402694702 RMSE: 1.9751126\n",
      "Validation loss: 1.6934426426887512 RMSE: 1.3013235\n",
      "79 7 5.582481861114502\n",
      "Validation loss: 4.278210818767548 RMSE: 2.0683837\n",
      "Validation loss: 2.150050461292267 RMSE: 1.4663051\n",
      "Validation loss: 4.639265656471252 RMSE: 2.1538954\n",
      "82 6 1.0610411167144775\n",
      "Validation loss: 3.7479405403137207 RMSE: 1.9359597\n",
      "Validation loss: 2.750980257987976 RMSE: 1.658608\n",
      "Validation loss: 7.1140453815460205 RMSE: 2.6672168\n",
      "85 5 2.820385456085205\n",
      "Validation loss: 2.742839813232422 RMSE: 1.6561521\n",
      "Validation loss: 6.140491247177124 RMSE: 2.4780014\n",
      "Validation loss: 4.836590051651001 RMSE: 2.199225\n",
      "88 4 1.0982080698013306\n",
      "Validation loss: 3.1317538022994995 RMSE: 1.7696763\n",
      "Validation loss: 6.137798070907593 RMSE: 2.477458\n",
      "Validation loss: 2.7881836891174316 RMSE: 1.6697855\n",
      "91 3 0.6676109433174133\n",
      "Validation loss: 3.0917364358901978 RMSE: 1.7583334\n",
      "Validation loss: 3.5074254274368286 RMSE: 1.8728125\n",
      "Validation loss: 3.1494717597961426 RMSE: 1.7746753\n",
      "94 2 1.818185567855835\n",
      "Validation loss: 3.318026840686798 RMSE: 1.8215451\n",
      "Validation loss: 3.2647088766098022 RMSE: 1.8068506\n",
      "Validation loss: 3.2623045444488525 RMSE: 1.806185\n",
      "97 1 3.451174736022949\n",
      "Validation loss: 4.204235553741455 RMSE: 2.0504236\n",
      "Validation loss: 2.0708935260772705 RMSE: 1.4390599\n",
      "Validation loss: 2.3481072187423706 RMSE: 1.5323536\n",
      "100 0 0.7608904838562012\n",
      "Validation loss: 1.9471333026885986 RMSE: 1.3953973\n",
      "Validation loss: 1.894763708114624 RMSE: 1.3765043\n",
      "102 16 0.40318411588668823\n",
      "Validation loss: 3.6346575021743774 RMSE: 1.906478\n",
      "Validation loss: 2.1790401935577393 RMSE: 1.4761574\n",
      "Validation loss: 2.420938491821289 RMSE: 1.5559366\n",
      "105 15 6.433834075927734\n",
      "Validation loss: 1.8929924964904785 RMSE: 1.3758606\n",
      "Validation loss: 2.8596739768981934 RMSE: 1.6910571\n",
      "Validation loss: 8.327637195587158 RMSE: 2.8857648\n",
      "108 14 0.6014808416366577\n",
      "Validation loss: 14.256386280059814 RMSE: 3.775763\n",
      "Validation loss: 15.315126657485962 RMSE: 3.9134548\n",
      "Validation loss: 2.776508629322052 RMSE: 1.666286\n",
      "111 13 1.560638666152954\n",
      "Validation loss: 4.6485559940338135 RMSE: 2.156051\n",
      "Validation loss: 2.097421407699585 RMSE: 1.4482478\n",
      "Validation loss: 2.6311044096946716 RMSE: 1.6220679\n",
      "114 12 1.1249645948410034\n",
      "Validation loss: 2.067474067211151 RMSE: 1.4378713\n",
      "Validation loss: 2.8440637588500977 RMSE: 1.6864352\n",
      "Validation loss: 10.749679565429688 RMSE: 3.2786703\n",
      "117 11 0.7704058885574341\n",
      "Validation loss: 2.4998421669006348 RMSE: 1.5810889\n",
      "Validation loss: 2.1385265588760376 RMSE: 1.4623704\n",
      "Validation loss: 2.2349127531051636 RMSE: 1.4949625\n",
      "120 10 2.030350923538208\n",
      "Validation loss: 2.8448140621185303 RMSE: 1.6866575\n",
      "Validation loss: 6.282335042953491 RMSE: 2.506459\n",
      "Validation loss: 3.8403666019439697 RMSE: 1.9596854\n",
      "123 9 0.34163355827331543\n",
      "Validation loss: 4.635653853416443 RMSE: 2.153057\n",
      "Validation loss: 3.7509816884994507 RMSE: 1.936745\n",
      "Validation loss: 4.0451420545578 RMSE: 2.011254\n",
      "126 8 0.39654847979545593\n",
      "Validation loss: 2.977458953857422 RMSE: 1.7255315\n",
      "Validation loss: 1.9943581819534302 RMSE: 1.4122175\n",
      "Validation loss: 9.022719502449036 RMSE: 3.0037842\n",
      "129 7 0.6537832021713257\n",
      "Validation loss: 2.475521445274353 RMSE: 1.573379\n",
      "Validation loss: 3.081158995628357 RMSE: 1.7553228\n",
      "Validation loss: 6.174708366394043 RMSE: 2.484896\n",
      "132 6 2.7577638626098633\n",
      "Validation loss: 6.466942071914673 RMSE: 2.5430183\n",
      "Validation loss: 2.8816038370132446 RMSE: 1.697529\n",
      "Validation loss: 2.4262691736221313 RMSE: 1.5576488\n",
      "135 5 1.3890268802642822\n",
      "Validation loss: 3.71411395072937 RMSE: 1.9272038\n",
      "Validation loss: 5.127269744873047 RMSE: 2.2643476\n",
      "Validation loss: 6.524946093559265 RMSE: 2.554398\n",
      "138 4 0.7051113247871399\n",
      "Validation loss: 2.26407253742218 RMSE: 1.5046835\n",
      "Validation loss: 5.4928752183914185 RMSE: 2.3436882\n",
      "Validation loss: 5.883825302124023 RMSE: 2.4256597\n",
      "141 3 3.058467149734497\n",
      "Validation loss: 2.166216492652893 RMSE: 1.4718072\n",
      "Validation loss: 2.9101377725601196 RMSE: 1.7059125\n",
      "Validation loss: 6.473652362823486 RMSE: 2.5443375\n",
      "144 2 1.248665452003479\n",
      "Validation loss: 3.6994166374206543 RMSE: 1.9233868\n",
      "Validation loss: 3.130277633666992 RMSE: 1.7692591\n",
      "Validation loss: 4.930642366409302 RMSE: 2.2205055\n",
      "147 1 1.0608155727386475\n",
      "Validation loss: 5.398394346237183 RMSE: 2.3234441\n",
      "Validation loss: 1.9192267656326294 RMSE: 1.3853614\n",
      "Validation loss: 2.368110775947571 RMSE: 1.5388666\n",
      "150 0 0.34353500604629517\n",
      "Validation loss: 3.953379511833191 RMSE: 1.9883107\n",
      "Validation loss: 2.4039742946624756 RMSE: 1.5504756\n",
      "152 16 8.636552810668945\n",
      "Validation loss: 2.7699525356292725 RMSE: 1.6643175\n",
      "Validation loss: 2.876649796962738 RMSE: 1.696069\n",
      "Validation loss: 6.884784698486328 RMSE: 2.6238875\n",
      "155 15 0.9985655546188354\n",
      "Validation loss: 1.9693673849105835 RMSE: 1.4033415\n",
      "Validation loss: 2.6073597073554993 RMSE: 1.6147323\n",
      "Validation loss: 2.4745023250579834 RMSE: 1.5730551\n",
      "158 14 1.2581570148468018\n",
      "Validation loss: 4.624174475669861 RMSE: 2.1503892\n",
      "Validation loss: 2.2568711042404175 RMSE: 1.5022886\n",
      "Validation loss: 3.3900887966156006 RMSE: 1.8412194\n",
      "161 13 0.8555150032043457\n",
      "Validation loss: 2.448672890663147 RMSE: 1.5648235\n",
      "Validation loss: 4.446964740753174 RMSE: 2.1087828\n",
      "Validation loss: 3.0658514499664307 RMSE: 1.7509575\n",
      "164 12 0.4280672073364258\n",
      "Validation loss: 2.7690131664276123 RMSE: 1.6640353\n",
      "Validation loss: 2.9017393589019775 RMSE: 1.7034489\n",
      "Validation loss: 2.7440223693847656 RMSE: 1.6565093\n",
      "167 11 4.516729354858398\n",
      "Validation loss: 16.870887279510498 RMSE: 4.1074195\n",
      "Validation loss: 4.072938561439514 RMSE: 2.0181525\n",
      "Validation loss: 3.5237984657287598 RMSE: 1.8771784\n",
      "170 10 3.036881923675537\n",
      "Validation loss: 3.036983370780945 RMSE: 1.7426944\n",
      "Validation loss: 2.44356369972229 RMSE: 1.5631903\n",
      "Validation loss: 3.3137908577919006 RMSE: 1.820382\n",
      "173 9 0.9306402206420898\n",
      "Validation loss: 3.2512301206588745 RMSE: 1.803117\n",
      "Validation loss: 2.7262980937957764 RMSE: 1.6511506\n",
      "Validation loss: 3.3111889362335205 RMSE: 1.819667\n",
      "176 8 1.9048376083374023\n",
      "Validation loss: 3.0413849353790283 RMSE: 1.7439568\n",
      "Validation loss: 2.903261661529541 RMSE: 1.703896\n",
      "Validation loss: 2.482684373855591 RMSE: 1.5756537\n",
      "179 7 1.1275672912597656\n",
      "Validation loss: 3.1045680046081543 RMSE: 1.7619786\n",
      "Validation loss: 2.8164854645729065 RMSE: 1.6782386\n",
      "Validation loss: 3.712785482406616 RMSE: 1.926859\n",
      "182 6 0.6586277484893799\n",
      "Validation loss: 10.226084232330322 RMSE: 3.197825\n",
      "Validation loss: 1.4486350417137146 RMSE: 1.2035925\n",
      "Validation loss: 2.521359086036682 RMSE: 1.5878788\n",
      "185 5 0.3939151465892792\n",
      "Validation loss: 2.095668315887451 RMSE: 1.4476424\n",
      "Validation loss: 2.631614923477173 RMSE: 1.6222253\n",
      "Validation loss: 2.4122458696365356 RMSE: 1.5531406\n",
      "188 4 0.7573904991149902\n",
      "Validation loss: 3.088388741016388 RMSE: 1.7573813\n",
      "Validation loss: 2.1823490858078003 RMSE: 1.4772775\n",
      "Validation loss: 2.7811315059661865 RMSE: 1.6676725\n",
      "191 3 0.6242269277572632\n",
      "Validation loss: 2.3720672130584717 RMSE: 1.5401515\n",
      "Validation loss: 1.6066965460777283 RMSE: 1.2675554\n",
      "Validation loss: 1.9161116480827332 RMSE: 1.3842369\n",
      "194 2 0.40893393754959106\n",
      "Validation loss: 2.829254627227783 RMSE: 1.682039\n",
      "Validation loss: 3.5460554361343384 RMSE: 1.883097\n",
      "Validation loss: 3.258698523044586 RMSE: 1.8051865\n",
      "197 1 0.43936824798583984\n",
      "Validation loss: 2.2613322734832764 RMSE: 1.5037729\n",
      "Validation loss: 3.0301384925842285 RMSE: 1.7407295\n",
      "Validation loss: 2.5140167474746704 RMSE: 1.5855651\n",
      "Loaded trained model with success.\n",
      "Test loss: 1.296982854604721 Test RMSE: 1.1388515\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:0', 'task_name': 'freesolv', 'model_type': 'gin', 'model': {'num_layer': 5, 'emb_dim': 300, 'feat_dim': 300, 'drop_ratio': 0.3, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'random', 'seed': 779, 'task': 'regression', 'data_path': 'data/freesolv/freesolv.csv', 'target': ['expt']}}\n",
      "Running on: cuda:0\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 25.34465789794922\n",
      "Validation loss: 12.407707452774048 RMSE: 3.5224576\n",
      "Validation loss: 10.950044393539429 RMSE: 3.3090851\n",
      "2 16 9.865896224975586\n",
      "Validation loss: 7.281047344207764 RMSE: 2.6983416\n",
      "Validation loss: 3.1205453872680664 RMSE: 1.7665067\n",
      "Validation loss: 8.189875364303589 RMSE: 2.8617957\n",
      "5 15 5.4235687255859375\n",
      "Validation loss: 11.925300598144531 RMSE: 3.4533033\n",
      "Validation loss: 11.280821323394775 RMSE: 3.3586934\n",
      "Validation loss: 9.048739194869995 RMSE: 3.0081117\n",
      "8 14 3.725132942199707\n",
      "Validation loss: 8.653762340545654 RMSE: 2.9417279\n",
      "Validation loss: 8.24027967453003 RMSE: 2.8705888\n",
      "Validation loss: 13.310264706611633 RMSE: 3.6483235\n",
      "11 13 3.842151165008545\n",
      "Validation loss: 13.993754386901855 RMSE: 3.7408228\n",
      "Validation loss: 40.17470169067383 RMSE: 6.3383517\n",
      "Validation loss: 5.9025468826293945 RMSE: 2.429516\n",
      "14 12 2.875326633453369\n",
      "Validation loss: 15.69568932056427 RMSE: 3.9617782\n",
      "Validation loss: 12.538475036621094 RMSE: 3.5409706\n",
      "Validation loss: 2.867928624153137 RMSE: 1.6934958\n",
      "17 11 1.5011425018310547\n",
      "Validation loss: 11.986506223678589 RMSE: 3.4621537\n",
      "Validation loss: 19.080206632614136 RMSE: 4.368089\n",
      "Validation loss: 9.991400957107544 RMSE: 3.1609185\n",
      "20 10 2.6338210105895996\n",
      "Validation loss: 7.560392141342163 RMSE: 2.7496164\n",
      "Validation loss: 6.972215056419373 RMSE: 2.640495\n",
      "Validation loss: 20.8851158618927 RMSE: 4.5700235\n",
      "23 9 2.8116395473480225\n",
      "Validation loss: 8.898826122283936 RMSE: 2.9830902\n",
      "Validation loss: 13.516381740570068 RMSE: 3.6764634\n",
      "Validation loss: 8.907856225967407 RMSE: 2.9846036\n",
      "26 8 3.271012306213379\n",
      "Validation loss: 4.919792026281357 RMSE: 2.21806\n",
      "Validation loss: 15.52579927444458 RMSE: 3.9402792\n",
      "Validation loss: 16.0887770652771 RMSE: 4.0110817\n",
      "29 7 2.6327834129333496\n",
      "Validation loss: 9.56667709350586 RMSE: 3.093005\n",
      "Validation loss: 18.736005783081055 RMSE: 4.328511\n",
      "Validation loss: 9.579350233078003 RMSE: 3.0950525\n",
      "32 6 7.236246109008789\n",
      "Validation loss: 13.506068885326385 RMSE: 3.6750605\n",
      "Validation loss: 7.609158158302307 RMSE: 2.7584705\n",
      "Validation loss: 9.251606613397598 RMSE: 3.0416455\n",
      "35 5 2.20535945892334\n",
      "Validation loss: 9.817100524902344 RMSE: 3.1332252\n",
      "Validation loss: 5.903929233551025 RMSE: 2.4298003\n",
      "Validation loss: 8.708036184310913 RMSE: 2.9509382\n",
      "38 4 2.9949190616607666\n",
      "Validation loss: 7.169894695281982 RMSE: 2.6776655\n",
      "Validation loss: 12.196779012680054 RMSE: 3.4923887\n",
      "Validation loss: 17.020306825637817 RMSE: 4.1255674\n",
      "41 3 0.8254646062850952\n",
      "Validation loss: 11.572498798370361 RMSE: 3.4018376\n",
      "Validation loss: 10.738515615463257 RMSE: 3.2769675\n",
      "Validation loss: 12.545060276985168 RMSE: 3.5419002\n",
      "44 2 1.2029037475585938\n",
      "Validation loss: 15.750840663909912 RMSE: 3.9687326\n",
      "Validation loss: 23.04930591583252 RMSE: 4.800969\n",
      "Validation loss: 5.263496160507202 RMSE: 2.2942312\n",
      "47 1 2.828524112701416\n",
      "Validation loss: 5.428640604019165 RMSE: 2.3299444\n",
      "Validation loss: 8.358381688594818 RMSE: 2.8910866\n",
      "Validation loss: 12.04343867301941 RMSE: 3.4703658\n",
      "50 0 3.5597305297851562\n",
      "Validation loss: 4.632461130619049 RMSE: 2.1523154\n",
      "Validation loss: 10.834916353225708 RMSE: 3.2916436\n",
      "52 16 0.014107247814536095\n",
      "Validation loss: 7.682161629199982 RMSE: 2.7716713\n",
      "Validation loss: 9.323116421699524 RMSE: 3.0533776\n",
      "Validation loss: 10.95346474647522 RMSE: 3.3096018\n",
      "55 15 1.3543438911437988\n",
      "Validation loss: 10.487406373023987 RMSE: 3.2384264\n",
      "Validation loss: 12.428297877311707 RMSE: 3.5253794\n",
      "Validation loss: 17.106172800064087 RMSE: 4.135961\n",
      "58 14 1.1939666271209717\n",
      "Validation loss: 3.819244623184204 RMSE: 1.9542888\n",
      "Validation loss: 8.40926730632782 RMSE: 2.8998737\n",
      "Validation loss: 4.636030197143555 RMSE: 2.1531446\n",
      "61 13 0.6500518321990967\n",
      "Validation loss: 11.045481145381927 RMSE: 3.3234744\n",
      "Validation loss: 12.095356941223145 RMSE: 3.477838\n",
      "Validation loss: 2.532334089279175 RMSE: 1.591331\n",
      "64 12 1.9962137937545776\n",
      "Validation loss: 12.296108186244965 RMSE: 3.5065806\n",
      "Validation loss: 9.195169925689697 RMSE: 3.0323539\n",
      "Validation loss: 7.6196160316467285 RMSE: 2.7603652\n",
      "67 11 0.8398916721343994\n",
      "Validation loss: 11.689944744110107 RMSE: 3.419056\n",
      "Validation loss: 11.393112003803253 RMSE: 3.3753684\n",
      "Validation loss: 8.976502358913422 RMSE: 2.996081\n",
      "70 10 0.8679982423782349\n",
      "Validation loss: 4.520162433385849 RMSE: 2.1260674\n",
      "Validation loss: 13.051826000213623 RMSE: 3.612731\n",
      "Validation loss: 5.0417174100875854 RMSE: 2.2453768\n",
      "73 9 3.008023262023926\n",
      "Validation loss: 13.149206697940826 RMSE: 3.6261835\n",
      "Validation loss: 6.895186543464661 RMSE: 2.6258693\n",
      "Validation loss: 10.904086887836456 RMSE: 3.3021333\n",
      "76 8 3.3582215309143066\n",
      "Validation loss: 6.922343492507935 RMSE: 2.6310346\n",
      "Validation loss: 8.562838077545166 RMSE: 2.9262328\n",
      "Validation loss: 9.650052189826965 RMSE: 3.1064532\n",
      "79 7 0.5592973232269287\n",
      "Validation loss: 5.833475112915039 RMSE: 2.415259\n",
      "Validation loss: 9.370941936969757 RMSE: 3.0611997\n",
      "Validation loss: 2.1663047075271606 RMSE: 1.471837\n",
      "82 6 2.181236505508423\n",
      "Validation loss: 8.572536408901215 RMSE: 2.9278898\n",
      "Validation loss: 14.920010566711426 RMSE: 3.862643\n",
      "Validation loss: 7.8420023918151855 RMSE: 2.800358\n",
      "85 5 1.2096376419067383\n",
      "Validation loss: 7.246490836143494 RMSE: 2.6919308\n",
      "Validation loss: 6.272541761398315 RMSE: 2.5045042\n",
      "Validation loss: 2.543134033679962 RMSE: 1.5947206\n",
      "88 4 1.7187529802322388\n",
      "Validation loss: 6.0916712284088135 RMSE: 2.4681315\n",
      "Validation loss: 9.425927877426147 RMSE: 3.0701678\n",
      "Validation loss: 11.839131355285645 RMSE: 3.4408035\n",
      "91 3 1.6918084621429443\n",
      "Validation loss: 4.942620515823364 RMSE: 2.2232006\n",
      "Validation loss: 8.239064931869507 RMSE: 2.870377\n",
      "Validation loss: 6.177694797515869 RMSE: 2.485497\n",
      "94 2 3.3657453060150146\n",
      "Validation loss: 5.081050276756287 RMSE: 2.254119\n",
      "Validation loss: 6.333524644374847 RMSE: 2.5166495\n",
      "Validation loss: 4.724359154701233 RMSE: 2.173559\n",
      "97 1 1.5374150276184082\n",
      "Validation loss: 10.146036028862 RMSE: 3.1852846\n",
      "Validation loss: 3.6952667236328125 RMSE: 1.9223077\n",
      "Validation loss: 4.303432643413544 RMSE: 2.0744717\n",
      "100 0 1.144290804862976\n",
      "Validation loss: 3.2733246088027954 RMSE: 1.8092335\n",
      "Validation loss: 6.557059973478317 RMSE: 2.5606756\n",
      "102 16 26.50275993347168\n",
      "Validation loss: 4.829246401786804 RMSE: 2.1975548\n",
      "Validation loss: 4.797077119350433 RMSE: 2.190223\n",
      "Validation loss: 1.6351289749145508 RMSE: 1.2787216\n",
      "105 15 1.4952147006988525\n",
      "Validation loss: 16.360883355140686 RMSE: 4.044859\n",
      "Validation loss: 13.22340178489685 RMSE: 3.6363995\n",
      "Validation loss: 2.176056385040283 RMSE: 1.4751463\n",
      "108 14 1.1397364139556885\n",
      "Validation loss: 2.25118088722229 RMSE: 1.5003936\n",
      "Validation loss: 2.4837217330932617 RMSE: 1.5759828\n",
      "Validation loss: 2.353823661804199 RMSE: 1.5342176\n",
      "111 13 1.7176434993743896\n",
      "Validation loss: 5.941526770591736 RMSE: 2.4375246\n",
      "Validation loss: 7.349184036254883 RMSE: 2.710938\n",
      "Validation loss: 5.214462041854858 RMSE: 2.2835197\n",
      "114 12 1.4763509035110474\n",
      "Validation loss: 3.8320605754852295 RMSE: 1.9575648\n",
      "Validation loss: 4.372048258781433 RMSE: 2.0909443\n",
      "Validation loss: 4.4103734493255615 RMSE: 2.100089\n",
      "117 11 1.1774027347564697\n",
      "Validation loss: 8.59807574748993 RMSE: 2.9322476\n",
      "Validation loss: 3.3164197206497192 RMSE: 1.8211039\n",
      "Validation loss: 7.782244324684143 RMSE: 2.7896674\n",
      "120 10 1.4214004278182983\n",
      "Validation loss: 2.061392664909363 RMSE: 1.4357553\n",
      "Validation loss: 3.192260265350342 RMSE: 1.7866898\n",
      "Validation loss: 2.5014848709106445 RMSE: 1.5816084\n",
      "123 9 1.549255132675171\n",
      "Validation loss: 7.7422440350055695 RMSE: 2.7824883\n",
      "Validation loss: 4.844755828380585 RMSE: 2.201081\n",
      "Validation loss: 2.65641325712204 RMSE: 1.6298505\n",
      "126 8 1.752204418182373\n",
      "Validation loss: 7.358728766441345 RMSE: 2.7126977\n",
      "Validation loss: 5.25514554977417 RMSE: 2.2924106\n",
      "Validation loss: 3.194562792778015 RMSE: 1.787334\n",
      "129 7 1.192499041557312\n",
      "Validation loss: 2.7923807501792908 RMSE: 1.6710418\n",
      "Validation loss: 13.555424094200134 RMSE: 3.6817691\n",
      "Validation loss: 3.234817624092102 RMSE: 1.7985598\n",
      "132 6 1.3933422565460205\n",
      "Validation loss: 3.969472259283066 RMSE: 1.9923533\n",
      "Validation loss: 4.381146192550659 RMSE: 2.093119\n",
      "Validation loss: 5.872615694999695 RMSE: 2.423348\n",
      "135 5 1.9760018587112427\n",
      "Validation loss: 3.626494348049164 RMSE: 1.9043357\n",
      "Validation loss: 3.935815393924713 RMSE: 1.9838895\n",
      "Validation loss: 8.364284634590149 RMSE: 2.8921072\n",
      "138 4 2.033379316329956\n",
      "Validation loss: 6.116712212562561 RMSE: 2.4731987\n",
      "Validation loss: 4.067933976650238 RMSE: 2.0169122\n",
      "Validation loss: 5.310504674911499 RMSE: 2.3044527\n",
      "141 3 1.7096061706542969\n",
      "Validation loss: 1.2207102179527283 RMSE: 1.1048576\n",
      "Validation loss: 4.561801373958588 RMSE: 2.135837\n",
      "Validation loss: 2.648259162902832 RMSE: 1.6273472\n",
      "144 2 0.7973589897155762\n",
      "Validation loss: 2.383908271789551 RMSE: 1.5439911\n",
      "Validation loss: 5.260348558425903 RMSE: 2.293545\n",
      "Validation loss: 4.5857884883880615 RMSE: 2.1414452\n",
      "147 1 0.9492155313491821\n",
      "Validation loss: 6.9498714208602905 RMSE: 2.6362607\n",
      "Validation loss: 3.611623227596283 RMSE: 1.9004271\n",
      "Validation loss: 2.5452182292938232 RMSE: 1.595374\n",
      "150 0 0.3967480957508087\n",
      "Validation loss: 3.5009982585906982 RMSE: 1.8710954\n",
      "Validation loss: 3.1697704792022705 RMSE: 1.7803854\n",
      "152 16 28.2015380859375\n",
      "Validation loss: 7.195996642112732 RMSE: 2.682536\n",
      "Validation loss: 2.9755430817604065 RMSE: 1.7249761\n",
      "Validation loss: 1.8819925785064697 RMSE: 1.3718574\n",
      "155 15 1.3871794939041138\n",
      "Validation loss: 9.145497441291809 RMSE: 3.0241523\n",
      "Validation loss: 2.7685552835464478 RMSE: 1.6638978\n",
      "Validation loss: 9.539920210838318 RMSE: 3.088676\n",
      "158 14 0.4960346817970276\n",
      "Validation loss: 1.184550404548645 RMSE: 1.0883704\n",
      "Validation loss: 4.768923878669739 RMSE: 2.1837866\n",
      "Validation loss: 4.139249265193939 RMSE: 2.0345144\n",
      "161 13 2.6640396118164062\n",
      "Validation loss: 3.756318509578705 RMSE: 1.9381224\n",
      "Validation loss: 2.733267068862915 RMSE: 1.6532598\n"
     ]
    }
   ],
   "source": [
    "seeds = list(range(777,782))\n",
    "# datasets = [\"bace\",  \"bbbp\", \"tox21\", \"toxcast\", \"sider\",  ]\n",
    "datasets = [\"freeSolv\",  \"lipo\", \"esol\", \"qm7\", \"bace\",  \"bbbp\", ]\n",
    "\n",
    "for dataset in datasets:\n",
    "    for seed in seeds: \n",
    "        !python finetuneRecon.py \\\n",
    "        --task_name {dataset} \\\n",
    "        --splitting scaffold \\\n",
    "        --seed {seed} \\\n",
    "        --alpha 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6764b07a-c44b-44d3-ab6b-161100c0d53b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:0', 'task_name': 'esol', 'model_type': 'gin', 'model': {'num_layer': 5, 'emb_dim': 300, 'feat_dim': 300, 'drop_ratio': 0.3, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 751, 'task': 'regression', 'data_path': 'data/esol/esol.csv', 'target': ['measured log solubility in mols per litre']}}\n",
      "Running on: cuda:0\n",
      "1127\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/1127\n",
      "Generating scaffold 1000/1127\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "tensor([[ 0.0806, -0.0410, -0.0872,  ..., -0.0789, -0.0887, -0.1107],\n",
      "        [ 0.0313,  0.0164,  0.1077,  ..., -0.0785,  0.1171,  0.1046],\n",
      "        [-0.0002, -0.0291, -0.0762,  ...,  0.0110,  0.0630,  0.0511],\n",
      "        ...,\n",
      "        [ 0.0313,  0.0164,  0.1077,  ..., -0.0785,  0.1171,  0.1046],\n",
      "        [-0.0175,  0.1190, -0.0096,  ...,  0.0821,  0.0621,  0.0488],\n",
      "        [ 0.0313,  0.0164,  0.1077,  ..., -0.0785,  0.1171,  0.1046]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "0 0 8.955981254577637\n",
      "tensor([[ 0.0312,  0.0165,  0.1078,  ..., -0.0786,  0.1172,  0.1047],\n",
      "        [ 0.0312,  0.0165,  0.1078,  ..., -0.0786,  0.1172,  0.1047],\n",
      "        [ 0.0312,  0.0165,  0.1078,  ..., -0.0786,  0.1172,  0.1047],\n",
      "        ...,\n",
      "        [ 0.0312,  0.0165,  0.1078,  ..., -0.0786,  0.1172,  0.1047],\n",
      "        [ 0.0312,  0.0165,  0.1078,  ..., -0.0786,  0.1172,  0.1047],\n",
      "        [ 0.0312,  0.0165,  0.1078,  ..., -0.0786,  0.1172,  0.1047]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0004, -0.0289, -0.0764,  ...,  0.0108,  0.0629,  0.0509],\n",
      "        [-0.0004, -0.0289, -0.0764,  ...,  0.0108,  0.0629,  0.0509],\n",
      "        [-0.0004, -0.0289, -0.0764,  ...,  0.0108,  0.0629,  0.0509],\n",
      "        ...,\n",
      "        [ 0.0311,  0.0166,  0.1079,  ..., -0.0787,  0.1173,  0.1048],\n",
      "        [ 0.0311,  0.0166,  0.1079,  ..., -0.0787,  0.1173,  0.1048],\n",
      "        [ 0.0311,  0.0166,  0.1079,  ..., -0.0787,  0.1173,  0.1048]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0004, -0.0288, -0.0764,  ...,  0.0108,  0.0629,  0.0509],\n",
      "        [-0.0004, -0.0288, -0.0764,  ...,  0.0108,  0.0629,  0.0509],\n",
      "        [ 0.0311,  0.0167,  0.1079,  ..., -0.0788,  0.1174,  0.1048],\n",
      "        ...,\n",
      "        [ 0.0187, -0.0968, -0.0305,  ...,  0.0381, -0.0655, -0.0279],\n",
      "        [ 0.0311,  0.0167,  0.1079,  ..., -0.0788,  0.1174,  0.1048],\n",
      "        [ 0.0311,  0.0167,  0.1079,  ..., -0.0788,  0.1174,  0.1048]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0310,  0.0168,  0.1079,  ..., -0.0789,  0.1174,  0.1049],\n",
      "        [-0.0003, -0.0287, -0.0764,  ...,  0.0107,  0.0629,  0.0509],\n",
      "        [-0.0003, -0.0287, -0.0764,  ...,  0.0107,  0.0629,  0.0509],\n",
      "        ...,\n",
      "        [ 0.0310,  0.0168,  0.1079,  ..., -0.0789,  0.1174,  0.1049],\n",
      "        [ 0.0310,  0.0168,  0.1079,  ..., -0.0789,  0.1174,  0.1049],\n",
      "        [ 0.0310,  0.0168,  0.1079,  ..., -0.0789,  0.1174,  0.1049]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0309,  0.0168,  0.1080,  ..., -0.0789,  0.1175,  0.1050],\n",
      "        [ 0.0309,  0.0168,  0.1080,  ..., -0.0789,  0.1175,  0.1050],\n",
      "        [-0.0961, -0.0527,  0.0569,  ...,  0.0218, -0.0656,  0.0982],\n",
      "        ...,\n",
      "        [ 0.0309,  0.0168,  0.1080,  ..., -0.0789,  0.1175,  0.1050],\n",
      "        [-0.0961, -0.0527,  0.0569,  ...,  0.0218, -0.0656,  0.0982],\n",
      "        [ 0.0065, -0.1107,  0.0619,  ...,  0.0173,  0.0921, -0.0094]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0065, -0.1107,  0.0619,  ...,  0.0174,  0.0921, -0.0095],\n",
      "        [ 0.0065, -0.1107,  0.0619,  ...,  0.0174,  0.0921, -0.0095],\n",
      "        [ 0.0309,  0.0169,  0.1080,  ..., -0.0790,  0.1176,  0.1050],\n",
      "        ...,\n",
      "        [-0.0961, -0.0528,  0.0569,  ...,  0.0219, -0.0655,  0.0981],\n",
      "        [ 0.0309,  0.0169,  0.1080,  ..., -0.0790,  0.1176,  0.1050],\n",
      "        [ 0.0065, -0.1107,  0.0619,  ...,  0.0174,  0.0921, -0.0095]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0002, -0.0285, -0.0763,  ...,  0.0108,  0.0630,  0.0507],\n",
      "        [-0.0002, -0.0285, -0.0763,  ...,  0.0108,  0.0630,  0.0507],\n",
      "        [ 0.0309,  0.0169,  0.1081,  ..., -0.0791,  0.1177,  0.1051],\n",
      "        ...,\n",
      "        [ 0.0309,  0.0169,  0.1081,  ..., -0.0791,  0.1177,  0.1051],\n",
      "        [ 0.0309,  0.0169,  0.1081,  ..., -0.0791,  0.1177,  0.1051],\n",
      "        [-0.0123,  0.1087, -0.0120,  ...,  0.0790,  0.0517, -0.0861]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0961, -0.0528,  0.0569,  ...,  0.0220, -0.0653,  0.0979],\n",
      "        [-0.0961, -0.0528,  0.0569,  ...,  0.0220, -0.0653,  0.0979],\n",
      "        [ 0.0804, -0.0412, -0.0873,  ..., -0.0789, -0.0891, -0.1105],\n",
      "        ...,\n",
      "        [ 0.0309,  0.0170,  0.1081,  ..., -0.0791,  0.1178,  0.1052],\n",
      "        [ 0.0309,  0.0170,  0.1081,  ..., -0.0791,  0.1178,  0.1052],\n",
      "        [-0.0961, -0.0528,  0.0569,  ...,  0.0220, -0.0653,  0.0979]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0002, -0.0284, -0.0764,  ...,  0.0108,  0.0631,  0.0506],\n",
      "        [-0.0002, -0.0284, -0.0764,  ...,  0.0108,  0.0631,  0.0506],\n",
      "        [ 0.0309,  0.0170,  0.1082,  ..., -0.0792,  0.1179,  0.1052],\n",
      "        ...,\n",
      "        [ 0.0309,  0.0170,  0.1082,  ..., -0.0792,  0.1179,  0.1052],\n",
      "        [ 0.0309,  0.0170,  0.1082,  ..., -0.0792,  0.1179,  0.1052],\n",
      "        [ 0.0309,  0.0170,  0.1082,  ..., -0.0792,  0.1179,  0.1052]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0310,  0.0170,  0.1083,  ..., -0.0792,  0.1179,  0.1053],\n",
      "        [-0.0002, -0.0284, -0.0765,  ...,  0.0108,  0.0631,  0.0505],\n",
      "        [-0.0002, -0.0284, -0.0765,  ...,  0.0108,  0.0631,  0.0505],\n",
      "        ...,\n",
      "        [ 0.0310,  0.0170,  0.1083,  ..., -0.0792,  0.1179,  0.1053],\n",
      "        [ 0.0310,  0.0170,  0.1083,  ..., -0.0792,  0.1179,  0.1053],\n",
      "        [ 0.0310,  0.0170,  0.1083,  ..., -0.0792,  0.1179,  0.1053]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0310,  0.0171,  0.1083,  ..., -0.0793,  0.1179,  0.1054],\n",
      "        [ 0.0070, -0.1110,  0.0617,  ...,  0.0178,  0.0925, -0.0099],\n",
      "        [ 0.0310,  0.0171,  0.1083,  ..., -0.0793,  0.1179,  0.1054],\n",
      "        ...,\n",
      "        [ 0.0310,  0.0171,  0.1083,  ..., -0.0793,  0.1179,  0.1054],\n",
      "        [ 0.0310,  0.0171,  0.1083,  ..., -0.0793,  0.1179,  0.1054],\n",
      "        [-0.0961, -0.0529,  0.0568,  ...,  0.0223, -0.0651,  0.0977]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0309,  0.0171,  0.1084,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [ 0.0309,  0.0171,  0.1084,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [-0.0961, -0.0529,  0.0568,  ...,  0.0223, -0.0650,  0.0976],\n",
      "        ...,\n",
      "        [ 0.0309,  0.0171,  0.1084,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [ 0.0309,  0.0171,  0.1084,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [ 0.0309,  0.0171,  0.1084,  ..., -0.0794,  0.1179,  0.1054]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0961, -0.0529,  0.0568,  ...,  0.0224, -0.0649,  0.0975],\n",
      "        [-0.0003, -0.0284, -0.0766,  ...,  0.0107,  0.0633,  0.0504],\n",
      "        [-0.0003, -0.0284, -0.0766,  ...,  0.0107,  0.0633,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0309,  0.0171,  0.1085,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [ 0.0071, -0.1112,  0.0617,  ...,  0.0179,  0.0926, -0.0100],\n",
      "        [ 0.0309,  0.0171,  0.1085,  ..., -0.0794,  0.1179,  0.1054]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0309,  0.0171,  0.1085,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [ 0.0309,  0.0171,  0.1085,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [ 0.0309,  0.0171,  0.1085,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        ...,\n",
      "        [ 0.0071, -0.1112,  0.0616,  ...,  0.0180,  0.0927, -0.0100],\n",
      "        [ 0.0309,  0.0171,  0.1085,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [ 0.0309,  0.0171,  0.1085,  ..., -0.0794,  0.1179,  0.1054]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0309,  0.0171,  0.1086,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [ 0.0309,  0.0171,  0.1086,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [ 0.0309,  0.0171,  0.1086,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        ...,\n",
      "        [ 0.0309,  0.0171,  0.1086,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [-0.0962, -0.0529,  0.0568,  ...,  0.0225, -0.0647,  0.0974],\n",
      "        [-0.0127,  0.1084, -0.0120,  ...,  0.0788,  0.0512, -0.0857]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0309,  0.0171,  0.1087,  ..., -0.0795,  0.1179,  0.1054],\n",
      "        [-0.0002, -0.0284, -0.0766,  ...,  0.0107,  0.0632,  0.0503],\n",
      "        [-0.0002, -0.0284, -0.0766,  ...,  0.0107,  0.0632,  0.0503],\n",
      "        ...,\n",
      "        [ 0.0309,  0.0171,  0.1087,  ..., -0.0795,  0.1179,  0.1054],\n",
      "        [ 0.0309,  0.0171,  0.1087,  ..., -0.0795,  0.1179,  0.1054],\n",
      "        [ 0.0309,  0.0171,  0.1087,  ..., -0.0795,  0.1179,  0.1054]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 3.0917e-02,  1.7134e-02,  1.0878e-01,  ..., -7.9471e-02,\n",
      "          1.1795e-01,  1.0532e-01],\n",
      "        [-1.1599e-04, -2.8463e-02, -7.6550e-02,  ...,  1.0742e-02,\n",
      "          6.3165e-02,  5.0226e-02],\n",
      "        [-1.1599e-04, -2.8463e-02, -7.6550e-02,  ...,  1.0742e-02,\n",
      "          6.3165e-02,  5.0226e-02],\n",
      "        ...,\n",
      "        [ 3.0917e-02,  1.7134e-02,  1.0878e-01,  ..., -7.9471e-02,\n",
      "          1.1795e-01,  1.0532e-01],\n",
      "        [ 3.0917e-02,  1.7134e-02,  1.0878e-01,  ..., -7.9471e-02,\n",
      "          1.1795e-01,  1.0532e-01],\n",
      "        [ 3.0917e-02,  1.7134e-02,  1.0878e-01,  ..., -7.9471e-02,\n",
      "          1.1795e-01,  1.0532e-01]], device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0309,  0.0172,  0.1088,  ..., -0.0795,  0.1180,  0.1053],\n",
      "        [ 0.0309,  0.0172,  0.1088,  ..., -0.0795,  0.1180,  0.1053],\n",
      "        [ 0.0309,  0.0172,  0.1088,  ..., -0.0795,  0.1180,  0.1053],\n",
      "        ...,\n",
      "        [ 0.0074, -0.1113,  0.0618,  ...,  0.0180,  0.0929, -0.0100],\n",
      "        [ 0.0309,  0.0172,  0.1088,  ..., -0.0795,  0.1180,  0.1053],\n",
      "        [-0.0964, -0.0527,  0.0568,  ...,  0.0224, -0.0647,  0.0975]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-9.6424e-02, -5.2667e-02,  5.6764e-02,  ...,  2.2343e-02,\n",
      "         -6.4659e-02,  9.7490e-02],\n",
      "        [ 3.0807e-02,  1.7243e-02,  1.0880e-01,  ..., -7.9564e-02,\n",
      "          1.1803e-01,  1.0522e-01],\n",
      "        [-1.7022e-05, -2.8596e-02, -7.6470e-02,  ...,  1.0681e-02,\n",
      "          6.3049e-02,  5.0292e-02],\n",
      "        ...,\n",
      "        [ 3.0807e-02,  1.7243e-02,  1.0880e-01,  ..., -7.9564e-02,\n",
      "          1.1803e-01,  1.0522e-01],\n",
      "        [-1.7267e-02,  1.1858e-01, -1.0125e-02,  ...,  8.2536e-02,\n",
      "          6.1876e-02,  4.8307e-02],\n",
      "        [ 3.0807e-02,  1.7243e-02,  1.0880e-01,  ..., -7.9564e-02,\n",
      "          1.1803e-01,  1.0522e-01]], device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 3.0753e-02,  1.7299e-02,  1.0883e-01,  ..., -7.9583e-02,\n",
      "          1.1807e-01,  1.0516e-01],\n",
      "        [ 3.0753e-02,  1.7299e-02,  1.0883e-01,  ..., -7.9583e-02,\n",
      "          1.1807e-01,  1.0516e-01],\n",
      "        [ 4.1295e-05, -2.8627e-02, -7.6411e-02,  ...,  1.0639e-02,\n",
      "          6.3006e-02,  5.0348e-02],\n",
      "        ...,\n",
      "        [ 3.0753e-02,  1.7299e-02,  1.0883e-01,  ..., -7.9583e-02,\n",
      "          1.1807e-01,  1.0516e-01],\n",
      "        [-1.2426e-02,  1.0806e-01, -1.2240e-02,  ...,  7.9107e-02,\n",
      "          5.1456e-02, -8.5975e-02],\n",
      "        [ 3.0753e-02,  1.7299e-02,  1.0883e-01,  ..., -7.9583e-02,\n",
      "          1.1807e-01,  1.0516e-01]], device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0307,  0.0173,  0.1089,  ..., -0.0796,  0.1181,  0.1051],\n",
      "        [ 0.0307,  0.0173,  0.1089,  ..., -0.0796,  0.1181,  0.1051],\n",
      "        [ 0.0307,  0.0173,  0.1089,  ..., -0.0796,  0.1181,  0.1051],\n",
      "        ...,\n",
      "        [ 0.0076, -0.1114,  0.0619,  ...,  0.0181,  0.0930, -0.0100],\n",
      "        [ 0.0307,  0.0173,  0.1089,  ..., -0.0796,  0.1181,  0.1051],\n",
      "        [ 0.0307,  0.0173,  0.1089,  ..., -0.0796,  0.1181,  0.1051]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0307,  0.0174,  0.1089,  ..., -0.0796,  0.1181,  0.1051],\n",
      "        [ 0.0808, -0.0411, -0.0881,  ..., -0.0790, -0.0893, -0.1108],\n",
      "        [ 0.0076, -0.1113,  0.0619,  ...,  0.0181,  0.0930, -0.0100],\n",
      "        ...,\n",
      "        [ 0.0307,  0.0174,  0.1089,  ..., -0.0796,  0.1181,  0.1051],\n",
      "        [-0.0966, -0.0525,  0.0567,  ...,  0.0222, -0.0646,  0.0975],\n",
      "        [-0.0966, -0.0525,  0.0567,  ...,  0.0222, -0.0646,  0.0975]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0001, -0.0288, -0.0764,  ...,  0.0106,  0.0630,  0.0504],\n",
      "        [ 0.0898,  0.0321,  0.0691,  ...,  0.0426, -0.0070,  0.0773],\n",
      "        [ 0.0001, -0.0288, -0.0764,  ...,  0.0106,  0.0630,  0.0504],\n",
      "        ...,\n",
      "        [-0.0967, -0.0524,  0.0567,  ...,  0.0222, -0.0646,  0.0975],\n",
      "        [ 0.0307,  0.0174,  0.1089,  ..., -0.0797,  0.1182,  0.1051],\n",
      "        [-0.0967, -0.0524,  0.0567,  ...,  0.0222, -0.0646,  0.0975]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0968, -0.0523,  0.0567,  ...,  0.0222, -0.0646,  0.0974],\n",
      "        [ 0.0001, -0.0288, -0.0764,  ...,  0.0106,  0.0630,  0.0504],\n",
      "        [ 0.0001, -0.0288, -0.0764,  ...,  0.0106,  0.0630,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0307,  0.0174,  0.1090,  ..., -0.0797,  0.1182,  0.1052],\n",
      "        [-0.0968, -0.0523,  0.0567,  ...,  0.0222, -0.0646,  0.0974],\n",
      "        [ 0.0307,  0.0174,  0.1090,  ..., -0.0797,  0.1182,  0.1052]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0306,  0.0174,  0.1090,  ..., -0.0798,  0.1182,  0.1052],\n",
      "        [ 0.0001, -0.0289, -0.0764,  ...,  0.0106,  0.0630,  0.0504],\n",
      "        [ 0.0001, -0.0289, -0.0764,  ...,  0.0106,  0.0630,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0306,  0.0174,  0.1090,  ..., -0.0798,  0.1182,  0.1052],\n",
      "        [-0.0968, -0.0523,  0.0566,  ...,  0.0222, -0.0646,  0.0974],\n",
      "        [-0.0122,  0.1078, -0.0123,  ...,  0.0793,  0.0515, -0.0860]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0122,  0.1078, -0.0123,  ...,  0.0793,  0.0516, -0.0860],\n",
      "        [ 0.0306,  0.0174,  0.1091,  ..., -0.0798,  0.1182,  0.1053],\n",
      "        [ 0.0306,  0.0174,  0.1091,  ..., -0.0798,  0.1182,  0.1053],\n",
      "        ...,\n",
      "        [ 0.0306,  0.0174,  0.1091,  ..., -0.0798,  0.1182,  0.1053],\n",
      "        [ 0.0306,  0.0174,  0.1091,  ..., -0.0798,  0.1182,  0.1053],\n",
      "        [ 0.0306,  0.0174,  0.1091,  ..., -0.0798,  0.1182,  0.1053]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0306,  0.0174,  0.1091,  ..., -0.0799,  0.1182,  0.1053],\n",
      "        [-0.0121,  0.1078, -0.0122,  ...,  0.0793,  0.0516, -0.0860],\n",
      "        [ 0.0306,  0.0174,  0.1091,  ..., -0.0799,  0.1182,  0.1053],\n",
      "        ...,\n",
      "        [ 0.0306,  0.0174,  0.1091,  ..., -0.0799,  0.1182,  0.1053],\n",
      "        [ 0.0306,  0.0174,  0.1091,  ..., -0.0799,  0.1182,  0.1053],\n",
      "        [-0.0170,  0.1189, -0.0102,  ...,  0.0827,  0.0622,  0.0487]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0121,  0.1077, -0.0122,  ...,  0.0793,  0.0516, -0.0860],\n",
      "        [ 0.0305,  0.0174,  0.1092,  ..., -0.0799,  0.1182,  0.1053],\n",
      "        [ 0.0305,  0.0174,  0.1092,  ..., -0.0799,  0.1182,  0.1053],\n",
      "        ...,\n",
      "        [ 0.0305,  0.0174,  0.1092,  ..., -0.0799,  0.1182,  0.1053],\n",
      "        [ 0.0001, -0.0291, -0.0765,  ...,  0.0107,  0.0630,  0.0504],\n",
      "        [ 0.0001, -0.0291, -0.0765,  ...,  0.0107,  0.0630,  0.0504]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Validation loss: 2.2496231530619935 RMSE: 1.4998744\n",
      "tensor([[ 0.0305,  0.0174,  0.1092,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [-0.0970, -0.0521,  0.0565,  ...,  0.0224, -0.0646,  0.0973],\n",
      "        [ 0.0001, -0.0292, -0.0766,  ...,  0.0107,  0.0630,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0305,  0.0174,  0.1092,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [ 0.0305,  0.0174,  0.1092,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [ 0.0305,  0.0174,  0.1092,  ..., -0.0800,  0.1182,  0.1054]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0002, -0.0292, -0.0766,  ...,  0.0108,  0.0630,  0.0504],\n",
      "        [ 0.0305,  0.0174,  0.1092,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [ 0.0305,  0.0174,  0.1092,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        ...,\n",
      "        [ 0.0305,  0.0174,  0.1092,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [-0.0971, -0.0521,  0.0565,  ...,  0.0224, -0.0646,  0.0972],\n",
      "        [ 0.0078, -0.1111,  0.0621,  ...,  0.0182,  0.0933, -0.0101]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0971, -0.0521,  0.0565,  ...,  0.0224, -0.0647,  0.0972],\n",
      "        [ 0.0002, -0.0292, -0.0766,  ...,  0.0108,  0.0630,  0.0504],\n",
      "        [ 0.0002, -0.0292, -0.0766,  ...,  0.0108,  0.0630,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0305,  0.0174,  0.1093,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [ 0.0305,  0.0174,  0.1093,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [ 0.0078, -0.1111,  0.0621,  ...,  0.0182,  0.0934, -0.0101]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0304,  0.0174,  0.1093,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [ 0.0304,  0.0174,  0.1093,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [ 0.0002, -0.0293, -0.0766,  ...,  0.0108,  0.0630,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0304,  0.0174,  0.1093,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [ 0.0304,  0.0174,  0.1093,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [ 0.0304,  0.0174,  0.1093,  ..., -0.0800,  0.1182,  0.1054]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [-0.0120,  0.1077, -0.0121,  ...,  0.0793,  0.0515, -0.0860],\n",
      "        ...,\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [ 0.0809, -0.0410, -0.0884,  ..., -0.0791, -0.0894, -0.1111],\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [ 0.0003, -0.0293, -0.0766,  ...,  0.0108,  0.0631,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [-0.0120,  0.1076, -0.0121,  ...,  0.0793,  0.0515, -0.0860]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        ...,\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0304,  0.0174,  0.1095,  ..., -0.0801,  0.1182,  0.1055],\n",
      "        [ 0.0304,  0.0174,  0.1095,  ..., -0.0801,  0.1182,  0.1055],\n",
      "        [ 0.0304,  0.0174,  0.1095,  ..., -0.0801,  0.1182,  0.1055],\n",
      "        ...,\n",
      "        [-0.0974, -0.0521,  0.0564,  ...,  0.0225, -0.0648,  0.0970],\n",
      "        [ 0.0233,  0.0582, -0.1101,  ...,  0.0704,  0.0737,  0.0701],\n",
      "        [-0.0974, -0.0521,  0.0564,  ...,  0.0225, -0.0648,  0.0970]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0303,  0.0174,  0.1095,  ..., -0.0801,  0.1182,  0.1055],\n",
      "        [ 0.0303,  0.0174,  0.1095,  ..., -0.0801,  0.1182,  0.1055],\n",
      "        [ 0.0003, -0.0294, -0.0767,  ...,  0.0108,  0.0631,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0303,  0.0174,  0.1095,  ..., -0.0801,  0.1182,  0.1055],\n",
      "        [ 0.0303,  0.0174,  0.1095,  ..., -0.0801,  0.1182,  0.1055],\n",
      "        [-0.0974, -0.0522,  0.0564,  ...,  0.0225, -0.0649,  0.0970]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0003, -0.0295, -0.0767,  ...,  0.0108,  0.0631,  0.0503],\n",
      "        [ 0.0003, -0.0295, -0.0767,  ...,  0.0108,  0.0631,  0.0503],\n",
      "        [ 0.0303,  0.0174,  0.1096,  ..., -0.0801,  0.1182,  0.1055],\n",
      "        ...,\n",
      "        [ 0.0303,  0.0174,  0.1096,  ..., -0.0801,  0.1182,  0.1055],\n",
      "        [ 0.0303,  0.0174,  0.1096,  ..., -0.0801,  0.1182,  0.1055],\n",
      "        [ 0.0303,  0.0174,  0.1096,  ..., -0.0801,  0.1182,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0303,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1055],\n",
      "        [ 0.0003, -0.0295, -0.0767,  ...,  0.0108,  0.0631,  0.0503],\n",
      "        [ 0.0003, -0.0295, -0.0767,  ...,  0.0108,  0.0631,  0.0503],\n",
      "        ...,\n",
      "        [-0.0167,  0.1191, -0.0102,  ...,  0.0827,  0.0622,  0.0491],\n",
      "        [-0.0167,  0.1191, -0.0102,  ...,  0.0827,  0.0622,  0.0491],\n",
      "        [ 0.0303,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0302,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1055],\n",
      "        [ 0.0004, -0.0296, -0.0768,  ...,  0.0108,  0.0631,  0.0503],\n",
      "        [ 0.0004, -0.0296, -0.0768,  ...,  0.0108,  0.0631,  0.0503],\n",
      "        ...,\n",
      "        [ 0.0302,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1055],\n",
      "        [ 0.0302,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1055],\n",
      "        [ 0.0302,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0079, -0.1110,  0.0623,  ...,  0.0182,  0.0935, -0.0101],\n",
      "        [-0.0976, -0.0523,  0.0563,  ...,  0.0225, -0.0650,  0.0970],\n",
      "        [ 0.0302,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0302,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0302,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0302,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0302,  0.0174,  0.1097,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0302,  0.0174,  0.1097,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0302,  0.0174,  0.1097,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        ...,\n",
      "        [-0.0977, -0.0523,  0.0563,  ...,  0.0225, -0.0649,  0.0970],\n",
      "        [ 0.0302,  0.0174,  0.1097,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0302,  0.0174,  0.1097,  ..., -0.0802,  0.1182,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [-0.0977, -0.0524,  0.0563,  ...,  0.0225, -0.0649,  0.0969],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0118,  0.1075, -0.0120,  ...,  0.0793,  0.0513, -0.0859],\n",
      "        [-0.0118,  0.1075, -0.0120,  ...,  0.0793,  0.0513, -0.0859],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0803,  0.1182,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0803,  0.1182,  0.1056],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0803,  0.1182,  0.1056],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0803,  0.1182,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0003, -0.0297, -0.0768,  ...,  0.0108,  0.0631,  0.0504],\n",
      "        [ 0.0003, -0.0297, -0.0768,  ...,  0.0108,  0.0631,  0.0504],\n",
      "        [ 0.0003, -0.0297, -0.0768,  ...,  0.0108,  0.0631,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0082, -0.1109,  0.0626,  ...,  0.0178,  0.0937, -0.0099],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0803,  0.1181,  0.1056],\n",
      "        [ 0.0813, -0.0407, -0.0882,  ..., -0.0792, -0.0893, -0.1111]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0300,  0.0174,  0.1096,  ..., -0.0804,  0.1181,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0804,  0.1181,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0804,  0.1181,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0804,  0.1181,  0.1056],\n",
      "        [ 0.0082, -0.1109,  0.0626,  ...,  0.0178,  0.0938, -0.0099],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0804,  0.1181,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0002, -0.0298, -0.0769,  ...,  0.0107,  0.0631,  0.0504],\n",
      "        [ 0.0002, -0.0298, -0.0769,  ...,  0.0107,  0.0631,  0.0504],\n",
      "        [ 0.0002, -0.0298, -0.0769,  ...,  0.0107,  0.0631,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0804,  0.1181,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0804,  0.1181,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0804,  0.1181,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0300,  0.0174,  0.1096,  ..., -0.0805,  0.1181,  0.1055],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0805,  0.1181,  0.1055],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0805,  0.1181,  0.1055],\n",
      "        ...,\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0805,  0.1181,  0.1055],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0805,  0.1181,  0.1055],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0805,  0.1181,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0174,  0.1097,  ..., -0.0805,  0.1181,  0.1055],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0805,  0.1181,  0.1055],\n",
      "        [ 0.0001, -0.0298, -0.0769,  ...,  0.0107,  0.0631,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0805,  0.1181,  0.1055],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0805,  0.1181,  0.1055],\n",
      "        [ 0.0900,  0.0317,  0.0689,  ...,  0.0427, -0.0067,  0.0771]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "1 21 1.6086289882659912\n",
      "tensor([[ 0.0299,  0.0174,  0.1097,  ..., -0.0806,  0.1181,  0.1055],\n",
      "        [ 0.0083, -0.1108,  0.0627,  ...,  0.0176,  0.0939, -0.0098],\n",
      "        [ 0.0083, -0.1108,  0.0627,  ...,  0.0176,  0.0939, -0.0098],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0806,  0.1181,  0.1055],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0806,  0.1181,  0.1055],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0806,  0.1181,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0174,  0.1097,  ..., -0.0806,  0.1181,  0.1056],\n",
      "        [-0.0983, -0.0523,  0.0559,  ...,  0.0224, -0.0645,  0.0968],\n",
      "        [-0.0983, -0.0523,  0.0559,  ...,  0.0224, -0.0645,  0.0968],\n",
      "        ...,\n",
      "        [ 0.0083, -0.1107,  0.0627,  ...,  0.0175,  0.0939, -0.0097],\n",
      "        [-0.0983, -0.0523,  0.0559,  ...,  0.0224, -0.0645,  0.0968],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0806,  0.1181,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0174,  0.1097,  ..., -0.0807,  0.1182,  0.1056],\n",
      "        [ 0.0002, -0.0300, -0.0769,  ...,  0.0107,  0.0631,  0.0505],\n",
      "        [ 0.0002, -0.0300, -0.0769,  ...,  0.0107,  0.0631,  0.0505],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0807,  0.1182,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0807,  0.1182,  0.1056],\n",
      "        [ 0.0901,  0.0316,  0.0689,  ...,  0.0427, -0.0068,  0.0770]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0174,  0.1097,  ..., -0.0807,  0.1182,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0807,  0.1182,  0.1056],\n",
      "        [ 0.0003, -0.0300, -0.0769,  ...,  0.0106,  0.0631,  0.0506],\n",
      "        ...,\n",
      "        [ 0.0901,  0.0316,  0.0689,  ...,  0.0427, -0.0069,  0.0771],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0807,  0.1182,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0807,  0.1182,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0174,  0.1097,  ..., -0.0808,  0.1182,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0808,  0.1182,  0.1056],\n",
      "        [-0.0116,  0.1073, -0.0120,  ...,  0.0794,  0.0513, -0.0860],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0808,  0.1182,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0808,  0.1182,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0808,  0.1182,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0986, -0.0523,  0.0560,  ...,  0.0223, -0.0644,  0.0966],\n",
      "        [ 0.0299,  0.0175,  0.1097,  ..., -0.0808,  0.1182,  0.1056],\n",
      "        [ 0.0299,  0.0175,  0.1097,  ..., -0.0808,  0.1182,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0175,  0.1097,  ..., -0.0808,  0.1182,  0.1056],\n",
      "        [ 0.0299,  0.0175,  0.1097,  ..., -0.0808,  0.1182,  0.1056],\n",
      "        [ 0.0299,  0.0175,  0.1097,  ..., -0.0808,  0.1182,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0082, -0.1106,  0.0628,  ...,  0.0174,  0.0939, -0.0098],\n",
      "        [-0.0987, -0.0523,  0.0560,  ...,  0.0223, -0.0644,  0.0966],\n",
      "        [ 0.0004, -0.0301, -0.0769,  ...,  0.0105,  0.0631,  0.0508],\n",
      "        ...,\n",
      "        [ 0.0004, -0.0301, -0.0769,  ...,  0.0105,  0.0631,  0.0508],\n",
      "        [ 0.0004, -0.0301, -0.0769,  ...,  0.0105,  0.0631,  0.0508],\n",
      "        [ 0.0004, -0.0301, -0.0769,  ...,  0.0105,  0.0631,  0.0508]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Validation loss: 17.010034324848547 RMSE: 4.1243224\n",
      "tensor([[ 0.0004, -0.0302, -0.0769,  ...,  0.0105,  0.0631,  0.0508],\n",
      "        [-0.0115,  0.1073, -0.0120,  ...,  0.0794,  0.0512, -0.0860],\n",
      "        [ 0.0299,  0.0175,  0.1096,  ..., -0.0809,  0.1182,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0082, -0.1106,  0.0628,  ...,  0.0174,  0.0939, -0.0097],\n",
      "        [ 0.0299,  0.0175,  0.1096,  ..., -0.0809,  0.1182,  0.1056],\n",
      "        [ 0.0299,  0.0175,  0.1096,  ..., -0.0809,  0.1182,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0175,  0.1096,  ..., -0.0809,  0.1181,  0.1056],\n",
      "        [ 0.0004, -0.0302, -0.0769,  ...,  0.0105,  0.0631,  0.0508],\n",
      "        [ 0.0004, -0.0302, -0.0769,  ...,  0.0105,  0.0631,  0.0508],\n",
      "        ...,\n",
      "        [-0.0989, -0.0523,  0.0560,  ...,  0.0224, -0.0644,  0.0966],\n",
      "        [ 0.0299,  0.0175,  0.1096,  ..., -0.0809,  0.1181,  0.1056],\n",
      "        [ 0.0299,  0.0175,  0.1096,  ..., -0.0809,  0.1181,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0175,  0.1096,  ..., -0.0810,  0.1181,  0.1056],\n",
      "        [ 0.0004, -0.0302, -0.0769,  ...,  0.0105,  0.0631,  0.0508],\n",
      "        [ 0.0004, -0.0302, -0.0769,  ...,  0.0105,  0.0631,  0.0508],\n",
      "        ...,\n",
      "        [-0.0989, -0.0523,  0.0560,  ...,  0.0225, -0.0644,  0.0966],\n",
      "        [ 0.0299,  0.0175,  0.1096,  ..., -0.0810,  0.1181,  0.1056],\n",
      "        [-0.0115,  0.1073, -0.0119,  ...,  0.0795,  0.0512, -0.0860]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0174,  0.1096,  ..., -0.0810,  0.1181,  0.1056],\n",
      "        [ 0.0004, -0.0303, -0.0770,  ...,  0.0104,  0.0630,  0.0508],\n",
      "        [ 0.0004, -0.0303, -0.0770,  ...,  0.0104,  0.0630,  0.0508],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0810,  0.1181,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0810,  0.1181,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0810,  0.1181,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056],\n",
      "        [-0.0115,  0.1073, -0.0118,  ...,  0.0795,  0.0512, -0.0859],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0300,  0.0174,  0.1096,  ..., -0.0811,  0.1179,  0.1056],\n",
      "        [-0.0993, -0.0524,  0.0558,  ...,  0.0228, -0.0645,  0.0966],\n",
      "        [ 0.0004, -0.0304, -0.0772,  ...,  0.0104,  0.0629,  0.0507],\n",
      "        ...,\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0811,  0.1179,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0811,  0.1179,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0811,  0.1179,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1179,  0.1056],\n",
      "        [-0.0115,  0.1073, -0.0117,  ...,  0.0795,  0.0512, -0.0859],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1179,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1179,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1179,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1179,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0994, -0.0524,  0.0558,  ...,  0.0229, -0.0646,  0.0966],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1179,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1179,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1179,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1179,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1179,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1178,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1178,  0.1056],\n",
      "        [ 0.0003, -0.0305, -0.0773,  ...,  0.0104,  0.0628,  0.0506],\n",
      "        ...,\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1178,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1178,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0995, -0.0524,  0.0558,  ...,  0.0229, -0.0646,  0.0966],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        ...,\n",
      "        [-0.0995, -0.0524,  0.0558,  ...,  0.0229, -0.0646,  0.0966],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0174,  0.1097,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0004, -0.0305, -0.0774,  ...,  0.0104,  0.0628,  0.0506],\n",
      "        [ 0.0004, -0.0305, -0.0774,  ...,  0.0104,  0.0628,  0.0506],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0165,  0.1185, -0.0105,  ...,  0.0828,  0.0629,  0.0487],\n",
      "        [ 0.0299,  0.0174,  0.1098,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0004, -0.0305, -0.0774,  ...,  0.0104,  0.0627,  0.0506],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0174,  0.1098,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1098,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1098,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0115,  0.1073, -0.0117,  ...,  0.0795,  0.0511, -0.0859],\n",
      "        [ 0.0299,  0.0174,  0.1098,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [-0.0997, -0.0524,  0.0558,  ...,  0.0230, -0.0647,  0.0966],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0174,  0.1098,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1098,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1098,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0298,  0.0175,  0.1099,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        ...,\n",
      "        [-0.0998, -0.0524,  0.0558,  ...,  0.0230, -0.0647,  0.0966],\n",
      "        [ 0.0234,  0.0589, -0.1091,  ...,  0.0714,  0.0753,  0.0714],\n",
      "        [ 0.0808, -0.0412, -0.0879,  ..., -0.0785, -0.0889, -0.1103]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0298,  0.0175,  0.1100,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1100,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0004, -0.0305, -0.0774,  ...,  0.0103,  0.0626,  0.0507],\n",
      "        ...,\n",
      "        [ 0.0298,  0.0175,  0.1100,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1100,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1100,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0298,  0.0175,  0.1100,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1100,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1100,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        ...,\n",
      "        [-0.0998, -0.0524,  0.0558,  ...,  0.0230, -0.0647,  0.0966],\n",
      "        [ 0.0298,  0.0175,  0.1100,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1100,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0005, -0.0305, -0.0774,  ...,  0.0103,  0.0626,  0.0507],\n",
      "        [ 0.0005, -0.0305, -0.0774,  ...,  0.0103,  0.0626,  0.0507],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0005, -0.0306, -0.0774,  ...,  0.0102,  0.0626,  0.0507],\n",
      "        [ 0.0005, -0.0306, -0.0774,  ...,  0.0102,  0.0626,  0.0507],\n",
      "        [ 0.0005, -0.0306, -0.0774,  ...,  0.0102,  0.0626,  0.0507],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.1000, -0.0523,  0.0557,  ...,  0.0230, -0.0646,  0.0964],\n",
      "        [ 0.0005, -0.0306, -0.0775,  ...,  0.0103,  0.0625,  0.0507],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0806, -0.0413, -0.0879,  ..., -0.0785, -0.0888, -0.1102],\n",
      "        [-0.1000, -0.0523,  0.0557,  ...,  0.0230, -0.0646,  0.0964],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0005, -0.0305, -0.0775,  ...,  0.0103,  0.0625,  0.0507],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0005, -0.0305, -0.0776,  ...,  0.0103,  0.0625,  0.0507],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [ 0.0005, -0.0305, -0.0776,  ...,  0.0103,  0.0625,  0.0507],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [-0.1001, -0.0524,  0.0557,  ...,  0.0231, -0.0646,  0.0963],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0112,  0.1071, -0.0116,  ...,  0.0795,  0.0511, -0.0860],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [ 0.0087, -0.1107,  0.0631,  ...,  0.0173,  0.0944, -0.0097],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [-0.1001, -0.0524,  0.0557,  ...,  0.0232, -0.0646,  0.0963],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.1001, -0.0524,  0.0556,  ...,  0.0232, -0.0645,  0.0963],\n",
      "        [-0.1001, -0.0524,  0.0556,  ...,  0.0232, -0.0645,  0.0963],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [ 0.0005, -0.0306, -0.0777,  ...,  0.0103,  0.0624,  0.0507],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [-0.1002, -0.0524,  0.0556,  ...,  0.0232, -0.0645,  0.0962],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1177,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.1002, -0.0524,  0.0556,  ...,  0.0233, -0.0645,  0.0962],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1177,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1176,  0.1056],\n",
      "        [ 0.0005, -0.0305, -0.0778,  ...,  0.0104,  0.0624,  0.0506],\n",
      "        [ 0.0089, -0.1107,  0.0631,  ...,  0.0172,  0.0945, -0.0097],\n",
      "        ...,\n",
      "        [-0.1002, -0.0524,  0.0556,  ...,  0.0233, -0.0645,  0.0962],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1176,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Validation loss: 3.044654910543324 RMSE: 1.744894\n",
      "tensor([[ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1176,  0.1056],\n",
      "        ...,\n",
      "        [-0.1002, -0.0523,  0.0556,  ...,  0.0233, -0.0645,  0.0962],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1176,  0.1056],\n",
      "        [ 0.0089, -0.1107,  0.0632,  ...,  0.0171,  0.0946, -0.0098]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0005, -0.0306, -0.0779,  ...,  0.0104,  0.0623,  0.0507],\n",
      "        [ 0.0005, -0.0306, -0.0779,  ...,  0.0104,  0.0623,  0.0507],\n",
      "        [ 0.0297,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0005, -0.0306, -0.0779,  ...,  0.0105,  0.0622,  0.0507],\n",
      "        [ 0.0005, -0.0306, -0.0779,  ...,  0.0105,  0.0622,  0.0507],\n",
      "        [ 0.0005, -0.0306, -0.0779,  ...,  0.0105,  0.0622,  0.0507],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0297,  0.0175,  0.1100,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0175,  0.1100,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0175,  0.1100,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0175,  0.1100,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        [-0.1002, -0.0523,  0.0556,  ...,  0.0233, -0.0645,  0.0962],\n",
      "        [ 0.0297,  0.0175,  0.1100,  ..., -0.0811,  0.1176,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0297,  0.0175,  0.1099,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        [-0.1002, -0.0523,  0.0556,  ...,  0.0233, -0.0645,  0.0962],\n",
      "        [ 0.0005, -0.0306, -0.0779,  ...,  0.0105,  0.0621,  0.0507],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0175,  0.1099,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0175,  0.1099,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0175,  0.1099,  ..., -0.0811,  0.1176,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0113,  0.1071, -0.0114,  ...,  0.0795,  0.0511, -0.0859],\n",
      "        [ 0.0297,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [ 0.0005, -0.0306, -0.0779,  ...,  0.0105,  0.0621,  0.0507],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0297,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [ 0.0005, -0.0306, -0.0779,  ...,  0.0105,  0.0621,  0.0507],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [-0.1003, -0.0524,  0.0556,  ...,  0.0234, -0.0646,  0.0962],\n",
      "        [ 0.0297,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0909,  0.0324,  0.0702,  ...,  0.0420, -0.0075,  0.0781],\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0298,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [-0.1003, -0.0525,  0.0556,  ...,  0.0234, -0.0645,  0.0961],\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0298,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1055],\n",
      "        ...,\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1055],\n",
      "        [-0.1004, -0.0525,  0.0556,  ...,  0.0234, -0.0645,  0.0961]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.1004, -0.0525,  0.0556,  ...,  0.0234, -0.0645,  0.0961],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1176,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1176,  0.1055],\n",
      "        ...,\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1176,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1176,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1176,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0005, -0.0305, -0.0779,  ...,  0.0106,  0.0620,  0.0506],\n",
      "        ...,\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        ...,\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [-0.1004, -0.0525,  0.0556,  ...,  0.0234, -0.0643,  0.0961],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        ...,\n",
      "        [ 0.0088, -0.1103,  0.0630,  ...,  0.0169,  0.0950, -0.0102],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "3 13 1.791797399520874\n",
      "tensor([[ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0239,  0.0583, -0.1088,  ...,  0.0713,  0.0755,  0.0716],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        ...,\n",
      "        [ 0.0089, -0.1103,  0.0630,  ...,  0.0169,  0.0951, -0.0102],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0910,  0.0325,  0.0701,  ...,  0.0420, -0.0072,  0.0780],\n",
      "        [ 0.0005, -0.0306, -0.0780,  ...,  0.0108,  0.0618,  0.0505],\n",
      "        [ 0.0005, -0.0306, -0.0780,  ...,  0.0108,  0.0618,  0.0505],\n",
      "        ...,\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1054],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1054],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1054]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0111,  0.1072, -0.0113,  ...,  0.0795,  0.0512, -0.0859],\n",
      "        [-0.1005, -0.0524,  0.0556,  ...,  0.0235, -0.0642,  0.0960],\n",
      "        [ 0.0298,  0.0176,  0.1101,  ..., -0.0810,  0.1174,  0.1054],\n",
      "        ...,\n",
      "        [ 0.0298,  0.0176,  0.1101,  ..., -0.0810,  0.1174,  0.1054],\n",
      "        [ 0.0298,  0.0176,  0.1101,  ..., -0.0810,  0.1174,  0.1054],\n",
      "        [ 0.0298,  0.0176,  0.1101,  ..., -0.0810,  0.1174,  0.1054]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1174,  0.1054],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1174,  0.1054],\n",
      "        [ 0.0005, -0.0306, -0.0780,  ...,  0.0108,  0.0617,  0.0505],\n",
      "        ...,\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1174,  0.1054],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1174,  0.1054],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1174,  0.1054]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n"
     ]
    }
   ],
   "source": [
    "seeds = [range(777,782)]\n",
    "for seed in seeds:\n",
    "    !python finetuneRecon.py \\\n",
    "    --task_name bbbp \\\n",
    "    --splitting scaffold \\\n",
    "    --seed {seed}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c350a18-74eb-42bb-ac32-ea54a4810977",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
