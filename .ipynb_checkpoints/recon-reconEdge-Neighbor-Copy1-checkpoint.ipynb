{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78cbc95c-8c82-4b39-9acb-eac3d7dea845",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': 1e-06, 'gpu': 'cuda:1', 'task_name': 'freesolv', 'model_type': 'gin', 'model': {'num_layer': 3, 'emb_dim': 64, 'feat_dim': 64, 'drop_ratio': 0.5, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 777, 'task': 'regression', 'data_path': 'data/freesolv/freesolv.csv', 'target': ['expt']}}\n",
      "Running on: cuda:1\n",
      "641\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/641\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 22.779953002929688\n",
      "Validation loss: 67.77315902709961 RMSE: 8.232446\n",
      "Validation loss: 61.71573829650879 RMSE: 7.8559365\n",
      "Validation loss: 52.876834869384766 RMSE: 7.271646\n",
      "3 2 12.713257789611816\n",
      "Validation loss: 39.21852493286133 RMSE: 6.26247\n",
      "Validation loss: 14.895552635192871 RMSE: 3.8594756\n",
      "Validation loss: 12.062166213989258 RMSE: 3.473063\n",
      "6 4 5.002315998077393\n",
      "Validation loss: 13.25211238861084 RMSE: 3.6403449\n",
      "Validation loss: 10.653528213500977 RMSE: 3.2639747\n",
      "Validation loss: 12.243656158447266 RMSE: 3.4990935\n",
      "9 6 5.6970415115356445\n",
      "Validation loss: 11.077747821807861 RMSE: 3.328325\n",
      "Validation loss: 13.473960399627686 RMSE: 3.6706893\n",
      "Validation loss: 11.589026927947998 RMSE: 3.4042659\n",
      "12 8 4.886643409729004\n",
      "Validation loss: 12.388232707977295 RMSE: 3.519692\n",
      "Validation loss: 13.121169567108154 RMSE: 3.6223156\n",
      "Validation loss: 13.723459720611572 RMSE: 3.7045188\n",
      "15 10 5.041806221008301\n",
      "Validation loss: 12.60991096496582 RMSE: 3.5510437\n",
      "Validation loss: 11.381659030914307 RMSE: 3.3736713\n",
      "Validation loss: 11.977782249450684 RMSE: 3.4608932\n",
      "18 12 5.7920379638671875\n",
      "Validation loss: 11.714958667755127 RMSE: 3.4227123\n",
      "Validation loss: 14.490486145019531 RMSE: 3.806637\n",
      "Validation loss: 14.239026546478271 RMSE: 3.7734635\n",
      "21 14 4.566793441772461\n",
      "Validation loss: 11.00649356842041 RMSE: 3.3176038\n",
      "Validation loss: 11.476335525512695 RMSE: 3.387674\n",
      "Validation loss: 12.705564975738525 RMSE: 3.5644867\n",
      "Validation loss: 14.886786460876465 RMSE: 3.85834\n",
      "25 0 5.120304107666016\n",
      "Validation loss: 11.359424591064453 RMSE: 3.3703744\n",
      "Validation loss: 12.578572750091553 RMSE: 3.5466282\n",
      "Validation loss: 11.438767433166504 RMSE: 3.3821247\n",
      "28 2 4.28645658493042\n",
      "Validation loss: 10.86487102508545 RMSE: 3.2961903\n",
      "Validation loss: 12.691813945770264 RMSE: 3.5625572\n",
      "Validation loss: 11.692421913146973 RMSE: 3.4194183\n",
      "31 4 4.921440124511719\n",
      "Validation loss: 16.889065265655518 RMSE: 4.1096306\n",
      "Validation loss: 11.334606170654297 RMSE: 3.3666906\n",
      "Validation loss: 13.305307388305664 RMSE: 3.647644\n",
      "34 6 6.271999835968018\n",
      "Validation loss: 12.275371074676514 RMSE: 3.5036225\n",
      "Validation loss: 14.827122688293457 RMSE: 3.8506002\n",
      "Validation loss: 14.595048904418945 RMSE: 3.8203468\n",
      "37 8 3.5112404823303223\n",
      "Validation loss: 11.310861110687256 RMSE: 3.3631625\n",
      "Validation loss: 12.66931438446045 RMSE: 3.559398\n",
      "Validation loss: 11.79943561553955 RMSE: 3.4350305\n",
      "40 10 4.630180358886719\n",
      "Validation loss: 9.89644169807434 RMSE: 3.1458611\n",
      "Validation loss: 11.793173789978027 RMSE: 3.4341192\n",
      "Validation loss: 12.923505306243896 RMSE: 3.5949278\n",
      "43 12 2.664228916168213\n",
      "Validation loss: 12.692230224609375 RMSE: 3.5626156\n",
      "Validation loss: 12.446499347686768 RMSE: 3.5279598\n",
      "Validation loss: 11.853596687316895 RMSE: 3.4429052\n",
      "46 14 2.2454440593719482\n",
      "Validation loss: 11.892475605010986 RMSE: 3.4485466\n",
      "Validation loss: 12.24877643585205 RMSE: 3.4998252\n",
      "Validation loss: 10.318016052246094 RMSE: 3.2121668\n",
      "Validation loss: 12.54276418685913 RMSE: 3.5415766\n",
      "50 0 4.101531982421875\n",
      "Validation loss: 10.259845733642578 RMSE: 3.2030995\n",
      "Validation loss: 18.110301971435547 RMSE: 4.25562\n",
      "Validation loss: 11.716913938522339 RMSE: 3.422998\n",
      "53 2 1.8090072870254517\n",
      "Validation loss: 12.92298936843872 RMSE: 3.594856\n",
      "Validation loss: 15.88447093963623 RMSE: 3.9855328\n",
      "Validation loss: 12.232727289199829 RMSE: 3.4975314\n",
      "56 4 2.918971300125122\n",
      "Validation loss: 13.727979183197021 RMSE: 3.7051287\n",
      "Validation loss: 11.11337661743164 RMSE: 3.3336732\n",
      "Validation loss: 13.05991506576538 RMSE: 3.6138508\n",
      "59 6 4.031835556030273\n",
      "Validation loss: 8.301502704620361 RMSE: 2.8812327\n",
      "Validation loss: 10.410968542098999 RMSE: 3.2266033\n",
      "Validation loss: 12.05345606803894 RMSE: 3.4718087\n",
      "62 8 2.2886483669281006\n",
      "Validation loss: 13.65312385559082 RMSE: 3.6950133\n",
      "Validation loss: 15.779284954071045 RMSE: 3.9723148\n",
      "Validation loss: 10.136800765991211 RMSE: 3.1838346\n",
      "65 10 2.135326385498047\n",
      "Validation loss: 10.94153356552124 RMSE: 3.3077989\n",
      "Validation loss: 14.731008291244507 RMSE: 3.8380992\n",
      "Validation loss: 12.910686016082764 RMSE: 3.5931442\n",
      "68 12 2.1420657634735107\n",
      "Validation loss: 14.67130422592163 RMSE: 3.8303137\n",
      "Validation loss: 13.287248611450195 RMSE: 3.6451678\n",
      "Validation loss: 17.469604969024658 RMSE: 4.1796656\n",
      "71 14 1.8821630477905273\n",
      "Validation loss: 10.527352333068848 RMSE: 3.244588\n",
      "Validation loss: 12.223406791687012 RMSE: 3.4961987\n",
      "Validation loss: 13.777413845062256 RMSE: 3.7117941\n",
      "Validation loss: 12.13545274734497 RMSE: 3.4835975\n",
      "75 0 1.8345767259597778\n",
      "Validation loss: 11.348179578781128 RMSE: 3.3687057\n",
      "Validation loss: 13.08760404586792 RMSE: 3.6176794\n",
      "Validation loss: 12.371789932250977 RMSE: 3.5173552\n",
      "78 2 1.9326746463775635\n",
      "Validation loss: 11.344198226928711 RMSE: 3.3681152\n",
      "Validation loss: 11.816449642181396 RMSE: 3.4375064\n",
      "Validation loss: 10.810451030731201 RMSE: 3.2879252\n",
      "81 4 1.9163886308670044\n",
      "Validation loss: 14.543217182159424 RMSE: 3.8135567\n",
      "Validation loss: 9.105223178863525 RMSE: 3.0174863\n",
      "Validation loss: 15.62938117980957 RMSE: 3.9534013\n",
      "84 6 2.7738282680511475\n",
      "Validation loss: 16.461520671844482 RMSE: 4.0572796\n",
      "Validation loss: 13.013273239135742 RMSE: 3.6073914\n",
      "Validation loss: 10.27790117263794 RMSE: 3.205917\n",
      "87 8 1.9850084781646729\n",
      "Validation loss: 10.283532857894897 RMSE: 3.2067945\n",
      "Validation loss: 10.49489974975586 RMSE: 3.2395835\n",
      "Validation loss: 12.854413509368896 RMSE: 3.5853052\n",
      "90 10 2.9354095458984375\n",
      "Validation loss: 11.051042079925537 RMSE: 3.3243105\n",
      "Validation loss: 10.491153717041016 RMSE: 3.239005\n",
      "Validation loss: 12.145262241363525 RMSE: 3.4850051\n",
      "93 12 2.3593573570251465\n",
      "Validation loss: 16.431443214416504 RMSE: 4.0535717\n",
      "Validation loss: 12.572171211242676 RMSE: 3.5457258\n",
      "Validation loss: 13.624984741210938 RMSE: 3.6912036\n",
      "96 14 2.1567318439483643\n",
      "Validation loss: 13.231602907180786 RMSE: 3.6375268\n",
      "Validation loss: 14.760388851165771 RMSE: 3.8419251\n",
      "Validation loss: 14.030950784683228 RMSE: 3.7457912\n",
      "Validation loss: 14.559786319732666 RMSE: 3.8157287\n",
      "100 0 2.562896490097046\n",
      "Validation loss: 16.593142986297607 RMSE: 4.073468\n",
      "Validation loss: 14.639019966125488 RMSE: 3.8260972\n",
      "Validation loss: 14.113982677459717 RMSE: 3.7568583\n",
      "103 2 2.152271032333374\n",
      "Validation loss: 14.171558380126953 RMSE: 3.764513\n",
      "Validation loss: 12.492556095123291 RMSE: 3.5344813\n",
      "Validation loss: 13.564045906066895 RMSE: 3.6829402\n",
      "106 4 6.408669471740723\n",
      "Validation loss: 18.179428100585938 RMSE: 4.263734\n",
      "Validation loss: 16.553876876831055 RMSE: 4.068646\n",
      "Validation loss: 11.732721090316772 RMSE: 3.425306\n",
      "109 6 2.0860517024993896\n",
      "Validation loss: 14.335044622421265 RMSE: 3.786165\n",
      "Validation loss: 15.137131690979004 RMSE: 3.8906472\n",
      "Validation loss: 17.289371490478516 RMSE: 4.1580496\n",
      "112 8 1.625004768371582\n",
      "Validation loss: 14.272300720214844 RMSE: 3.77787\n",
      "Validation loss: 15.34418249130249 RMSE: 3.917165\n",
      "Validation loss: 13.283124923706055 RMSE: 3.644602\n",
      "115 10 1.7819000482559204\n",
      "Validation loss: 14.808581352233887 RMSE: 3.8481922\n",
      "Validation loss: 11.605114459991455 RMSE: 3.406628\n",
      "Validation loss: 15.581732034683228 RMSE: 3.9473705\n",
      "118 12 2.409705400466919\n",
      "Validation loss: 15.647778987884521 RMSE: 3.9557273\n",
      "Validation loss: 18.10683536529541 RMSE: 4.255213\n",
      "Validation loss: 18.083741188049316 RMSE: 4.252498\n",
      "121 14 5.324584484100342\n",
      "Validation loss: 14.124500751495361 RMSE: 3.7582576\n",
      "Validation loss: 20.021867275238037 RMSE: 4.47458\n",
      "Validation loss: 14.42641544342041 RMSE: 3.798212\n",
      "Validation loss: 12.89774751663208 RMSE: 3.5913436\n",
      "125 0 3.0368833541870117\n",
      "Validation loss: 12.596378326416016 RMSE: 3.5491376\n",
      "Validation loss: 16.437193393707275 RMSE: 4.054281\n",
      "Validation loss: 13.404252529144287 RMSE: 3.6611817\n",
      "128 2 2.198568105697632\n",
      "Validation loss: 15.105765342712402 RMSE: 3.886614\n",
      "Validation loss: 16.788530349731445 RMSE: 4.097381\n",
      "Validation loss: 15.748067378997803 RMSE: 3.9683838\n",
      "131 4 2.0258500576019287\n",
      "Validation loss: 13.77161979675293 RMSE: 3.7110133\n",
      "Validation loss: 12.158030033111572 RMSE: 3.4868364\n",
      "Validation loss: 14.645142555236816 RMSE: 3.8268974\n",
      "134 6 1.4686908721923828\n",
      "Validation loss: 11.284870624542236 RMSE: 3.359296\n",
      "Validation loss: 11.186949729919434 RMSE: 3.3446898\n",
      "Validation loss: 19.240336418151855 RMSE: 4.3863807\n",
      "137 8 1.5348180532455444\n",
      "Validation loss: 15.057888507843018 RMSE: 3.8804495\n",
      "Validation loss: 13.103674411773682 RMSE: 3.6198997\n",
      "Validation loss: 13.884074687957764 RMSE: 3.7261338\n",
      "140 10 1.9248714447021484\n",
      "Validation loss: 13.809291362762451 RMSE: 3.7160857\n",
      "Validation loss: 14.884839057922363 RMSE: 3.8580873\n",
      "Validation loss: 14.52125358581543 RMSE: 3.810676\n",
      "143 12 2.5530598163604736\n",
      "Validation loss: 13.204588890075684 RMSE: 3.633812\n",
      "Validation loss: 13.646732330322266 RMSE: 3.6941483\n",
      "Validation loss: 16.52766704559326 RMSE: 4.0654235\n",
      "146 14 1.4532299041748047\n",
      "Validation loss: 15.06623649597168 RMSE: 3.8815248\n",
      "Validation loss: 16.2120418548584 RMSE: 4.026418\n",
      "Validation loss: 14.789255619049072 RMSE: 3.8456805\n",
      "Validation loss: 19.328657150268555 RMSE: 4.3964367\n",
      "150 0 2.115750789642334\n",
      "Validation loss: 15.661234855651855 RMSE: 3.9574277\n",
      "Validation loss: 13.286067962646484 RMSE: 3.645006\n",
      "Validation loss: 14.45044231414795 RMSE: 3.8013737\n",
      "153 2 2.7599146366119385\n",
      "Validation loss: 15.168867111206055 RMSE: 3.894723\n",
      "Validation loss: 15.477789402008057 RMSE: 3.934182\n",
      "Validation loss: 13.830552101135254 RMSE: 3.7189445\n",
      "156 4 1.9452651739120483\n",
      "Validation loss: 18.811011791229248 RMSE: 4.3371663\n",
      "Validation loss: 13.442116260528564 RMSE: 3.666349\n",
      "Validation loss: 15.52681827545166 RMSE: 3.9404085\n",
      "159 6 1.68309485912323\n",
      "Validation loss: 16.452962398529053 RMSE: 4.0562253\n",
      "Validation loss: 24.350507736206055 RMSE: 4.9346232\n",
      "Validation loss: 18.80702781677246 RMSE: 4.3367066\n",
      "162 8 1.8899730443954468\n",
      "Validation loss: 13.894059181213379 RMSE: 3.7274733\n",
      "Validation loss: 15.679866313934326 RMSE: 3.959781\n",
      "Validation loss: 17.887552738189697 RMSE: 4.2293677\n",
      "165 10 1.9302078485488892\n",
      "Validation loss: 15.027225017547607 RMSE: 3.8764963\n",
      "Validation loss: 12.625516414642334 RMSE: 3.5532403\n",
      "Validation loss: 15.517487525939941 RMSE: 3.9392242\n",
      "168 12 1.6924679279327393\n",
      "Validation loss: 15.69288158416748 RMSE: 3.9614244\n",
      "Validation loss: 15.112870216369629 RMSE: 3.8875277\n",
      "Validation loss: 16.209275245666504 RMSE: 4.0260744\n",
      "171 14 2.426086187362671\n",
      "Validation loss: 18.39227867126465 RMSE: 4.288621\n",
      "Validation loss: 12.04541540145874 RMSE: 3.4706504\n",
      "Validation loss: 12.486444473266602 RMSE: 3.533616\n",
      "Validation loss: 22.335501194000244 RMSE: 4.7260447\n",
      "175 0 0.9915956258773804\n",
      "Validation loss: 16.678757667541504 RMSE: 4.083964\n",
      "Validation loss: 20.262885093688965 RMSE: 4.5014315\n",
      "Validation loss: 15.97849178314209 RMSE: 3.9973104\n",
      "178 2 0.8838363885879517\n",
      "Validation loss: 15.54406213760376 RMSE: 3.9425957\n",
      "Validation loss: 21.943507194519043 RMSE: 4.68439\n",
      "Validation loss: 19.386661529541016 RMSE: 4.403029\n",
      "181 4 1.9768669605255127\n",
      "Validation loss: 24.145451545715332 RMSE: 4.9138026\n",
      "Validation loss: 25.744966506958008 RMSE: 5.07395\n",
      "Validation loss: 17.306519031524658 RMSE: 4.1601105\n",
      "184 6 2.0865325927734375\n",
      "Validation loss: 18.121851444244385 RMSE: 4.2569766\n",
      "Validation loss: 16.358405590057373 RMSE: 4.044553\n",
      "Validation loss: 20.766561031341553 RMSE: 4.557034\n",
      "187 8 1.7964537143707275\n",
      "Validation loss: 23.392900466918945 RMSE: 4.8366213\n",
      "Validation loss: 17.369750499725342 RMSE: 4.1677027\n",
      "Validation loss: 25.211363792419434 RMSE: 5.0210915\n",
      "190 10 2.9378485679626465\n",
      "Validation loss: 19.356277465820312 RMSE: 4.3995767\n",
      "Validation loss: 14.46076774597168 RMSE: 3.8027318\n",
      "Validation loss: 18.456960678100586 RMSE: 4.296157\n",
      "193 12 1.9417965412139893\n",
      "Validation loss: 15.511348247528076 RMSE: 3.9384449\n",
      "Validation loss: 14.95199728012085 RMSE: 3.8667812\n",
      "Validation loss: 18.301128387451172 RMSE: 4.2779818\n",
      "196 14 1.9635870456695557\n",
      "Validation loss: 19.42578411102295 RMSE: 4.4074693\n",
      "Validation loss: 15.486928462982178 RMSE: 3.9353435\n",
      "Validation loss: 12.662050724029541 RMSE: 3.5583775\n",
      "Validation loss: 12.979454040527344 RMSE: 3.6027007\n",
      "Loaded trained model with success.\n",
      "Test loss: 9.28871101966271 Test RMSE: 3.0477388\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': 1e-06, 'gpu': 'cuda:1', 'task_name': 'freesolv', 'model_type': 'gin', 'model': {'num_layer': 3, 'emb_dim': 64, 'feat_dim': 64, 'drop_ratio': 0.5, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 778, 'task': 'regression', 'data_path': 'data/freesolv/freesolv.csv', 'target': ['expt']}}\n",
      "Running on: cuda:1\n",
      "641\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/641\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 18.894298553466797\n",
      "Validation loss: 63.34828567504883 RMSE: 7.9591637\n",
      "Validation loss: 58.237531661987305 RMSE: 7.631352\n",
      "Validation loss: 51.02623176574707 RMSE: 7.143265\n",
      "3 2 13.389998435974121\n",
      "Validation loss: 39.60048294067383 RMSE: 6.292892\n",
      "Validation loss: 29.507853507995605 RMSE: 5.432113\n",
      "Validation loss: 17.117634773254395 RMSE: 4.1373463\n",
      "6 4 11.218884468078613\n",
      "Validation loss: 16.384268760681152 RMSE: 4.0477486\n",
      "Validation loss: 15.741950988769531 RMSE: 3.9676127\n",
      "Validation loss: 12.909311771392822 RMSE: 3.592953\n",
      "9 6 4.950159549713135\n",
      "Validation loss: 13.464061737060547 RMSE: 3.6693408\n",
      "Validation loss: 10.730175971984863 RMSE: 3.2756948\n",
      "Validation loss: 11.562463760375977 RMSE: 3.4003623\n",
      "12 8 8.840912818908691\n",
      "Validation loss: 13.304579734802246 RMSE: 3.6475444\n",
      "Validation loss: 10.206638813018799 RMSE: 3.194783\n",
      "Validation loss: 11.306111812591553 RMSE: 3.362456\n",
      "15 10 4.1609697341918945\n",
      "Validation loss: 11.559683799743652 RMSE: 3.3999534\n",
      "Validation loss: 9.725580215454102 RMSE: 3.118586\n",
      "Validation loss: 11.475449085235596 RMSE: 3.3875432\n",
      "18 12 6.259697914123535\n",
      "Validation loss: 11.80241346359253 RMSE: 3.435464\n",
      "Validation loss: 12.867005348205566 RMSE: 3.587061\n",
      "Validation loss: 13.042588233947754 RMSE: 3.6114523\n",
      "21 14 3.982607126235962\n",
      "Validation loss: 11.861506462097168 RMSE: 3.444054\n",
      "Validation loss: 13.250354766845703 RMSE: 3.6401036\n",
      "Validation loss: 13.155285835266113 RMSE: 3.6270216\n",
      "Validation loss: 11.65983772277832 RMSE: 3.4146502\n",
      "25 0 3.6975040435791016\n",
      "Validation loss: 10.150853157043457 RMSE: 3.1860404\n",
      "Validation loss: 11.942151069641113 RMSE: 3.4557416\n",
      "Validation loss: 12.239219665527344 RMSE: 3.4984598\n",
      "28 2 3.1558501720428467\n",
      "Validation loss: 11.036237716674805 RMSE: 3.3220832\n",
      "Validation loss: 12.295268058776855 RMSE: 3.5064607\n",
      "Validation loss: 14.358727931976318 RMSE: 3.7892914\n",
      "31 4 4.913081169128418\n",
      "Validation loss: 11.61171293258667 RMSE: 3.4075963\n",
      "Validation loss: 11.876763820648193 RMSE: 3.4462683\n",
      "Validation loss: 14.910078287124634 RMSE: 3.8613572\n",
      "34 6 4.130626678466797\n",
      "Validation loss: 15.8333740234375 RMSE: 3.9791174\n",
      "Validation loss: 13.868478775024414 RMSE: 3.7240407\n",
      "Validation loss: 13.709115028381348 RMSE: 3.7025824\n",
      "37 8 3.19685697555542\n",
      "Validation loss: 12.188570976257324 RMSE: 3.4912133\n",
      "Validation loss: 11.910435199737549 RMSE: 3.4511497\n",
      "Validation loss: 12.143351554870605 RMSE: 3.484731\n",
      "40 10 2.321047067642212\n",
      "Validation loss: 12.8681321144104 RMSE: 3.5872178\n",
      "Validation loss: 13.801491737365723 RMSE: 3.715036\n",
      "Validation loss: 13.939822673797607 RMSE: 3.733607\n",
      "43 12 3.0101730823516846\n",
      "Validation loss: 13.50724458694458 RMSE: 3.6752203\n",
      "Validation loss: 17.09040880203247 RMSE: 4.1340547\n",
      "Validation loss: 12.372355937957764 RMSE: 3.5174356\n",
      "46 14 2.2543790340423584\n",
      "Validation loss: 15.805449962615967 RMSE: 3.9756074\n",
      "Validation loss: 14.073537588119507 RMSE: 3.7514715\n",
      "Validation loss: 13.55455207824707 RMSE: 3.6816509\n",
      "Validation loss: 14.055359840393066 RMSE: 3.7490478\n",
      "50 0 2.3614437580108643\n",
      "Validation loss: 16.85777974128723 RMSE: 4.1058226\n",
      "Validation loss: 15.715876817703247 RMSE: 3.9643254\n",
      "Validation loss: 17.30724573135376 RMSE: 4.1601977\n",
      "53 2 2.7974348068237305\n",
      "Validation loss: 13.28706407546997 RMSE: 3.6451426\n",
      "Validation loss: 14.151002883911133 RMSE: 3.761782\n",
      "Validation loss: 17.32562494277954 RMSE: 4.162406\n",
      "56 4 3.1787970066070557\n",
      "Validation loss: 25.511242866516113 RMSE: 5.0508657\n",
      "Validation loss: 13.597808361053467 RMSE: 3.6875207\n",
      "Validation loss: 10.170461177825928 RMSE: 3.1891162\n",
      "59 6 1.6218738555908203\n",
      "Validation loss: 12.575358390808105 RMSE: 3.546175\n",
      "Validation loss: 14.362904071807861 RMSE: 3.7898424\n",
      "Validation loss: 12.396145582199097 RMSE: 3.5208158\n",
      "62 8 2.1955857276916504\n",
      "Validation loss: 11.65990924835205 RMSE: 3.4146612\n",
      "Validation loss: 16.656185150146484 RMSE: 4.081199\n",
      "Validation loss: 11.605192422866821 RMSE: 3.4066393\n",
      "65 10 4.192624568939209\n",
      "Validation loss: 14.293545722961426 RMSE: 3.7806804\n",
      "Validation loss: 12.983227252960205 RMSE: 3.6032245\n",
      "Validation loss: 10.258127689361572 RMSE: 3.202831\n",
      "68 12 2.066666841506958\n",
      "Validation loss: 13.769550323486328 RMSE: 3.7107344\n",
      "Validation loss: 17.76339554786682 RMSE: 4.2146645\n",
      "Validation loss: 14.649150848388672 RMSE: 3.8274212\n",
      "71 14 2.820585012435913\n",
      "Validation loss: 16.019678592681885 RMSE: 4.002459\n",
      "Validation loss: 13.839856147766113 RMSE: 3.7201958\n",
      "Validation loss: 18.2549991607666 RMSE: 4.272587\n",
      "Validation loss: 15.629754066467285 RMSE: 3.9534485\n",
      "75 0 1.7602354288101196\n",
      "Validation loss: 19.731337547302246 RMSE: 4.4419966\n",
      "Validation loss: 15.688325881958008 RMSE: 3.9608495\n",
      "Validation loss: 12.104686737060547 RMSE: 3.4791791\n",
      "78 2 2.2111892700195312\n",
      "Validation loss: 10.80271053314209 RMSE: 3.2867477\n",
      "Validation loss: 23.697945594787598 RMSE: 4.8680544\n",
      "Validation loss: 15.130930423736572 RMSE: 3.8898497\n",
      "81 4 4.693848609924316\n",
      "Validation loss: 25.052950859069824 RMSE: 5.0052924\n",
      "Validation loss: 18.62233066558838 RMSE: 4.3153596\n",
      "Validation loss: 16.06877565383911 RMSE: 4.008588\n",
      "84 6 2.2895331382751465\n",
      "Validation loss: 17.709985733032227 RMSE: 4.2083235\n",
      "Validation loss: 12.001441478729248 RMSE: 3.4643097\n",
      "Validation loss: 14.064404964447021 RMSE: 3.7502537\n",
      "87 8 2.9190356731414795\n",
      "Validation loss: 19.010716915130615 RMSE: 4.360128\n",
      "Validation loss: 19.267702341079712 RMSE: 4.389499\n",
      "Validation loss: 20.725940704345703 RMSE: 4.552574\n",
      "90 10 3.041013717651367\n",
      "Validation loss: 11.466351985931396 RMSE: 3.3862002\n",
      "Validation loss: 12.868599891662598 RMSE: 3.587283\n",
      "Validation loss: 10.766103744506836 RMSE: 3.2811742\n",
      "93 12 2.4235472679138184\n",
      "Validation loss: 12.670152187347412 RMSE: 3.5595155\n",
      "Validation loss: 11.306164741516113 RMSE: 3.3624642\n",
      "Validation loss: 12.748286247253418 RMSE: 3.5704741\n",
      "96 14 2.532425880432129\n",
      "Validation loss: 11.580400943756104 RMSE: 3.4029987\n",
      "Validation loss: 13.233141422271729 RMSE: 3.6377387\n",
      "Validation loss: 11.228755950927734 RMSE: 3.3509336\n",
      "Validation loss: 19.38841438293457 RMSE: 4.403228\n",
      "100 0 2.516749382019043\n",
      "Validation loss: 16.530246019363403 RMSE: 4.06574\n",
      "Validation loss: 12.875133514404297 RMSE: 3.5881941\n",
      "Validation loss: 12.219086647033691 RMSE: 3.495581\n",
      "103 2 1.3347859382629395\n",
      "Validation loss: 15.433070182800293 RMSE: 3.928495\n",
      "Validation loss: 15.844706058502197 RMSE: 3.9805408\n",
      "Validation loss: 18.50460147857666 RMSE: 4.3016973\n",
      "106 4 4.441149711608887\n",
      "Validation loss: 23.97240447998047 RMSE: 4.896163\n",
      "Validation loss: 13.726564407348633 RMSE: 3.704938\n",
      "Validation loss: 14.82215428352356 RMSE: 3.849955\n",
      "109 6 1.6443874835968018\n",
      "Validation loss: 14.494660377502441 RMSE: 3.8071854\n",
      "Validation loss: 17.965068817138672 RMSE: 4.2385216\n",
      "Validation loss: 14.065354347229004 RMSE: 3.7503805\n",
      "112 8 2.5037007331848145\n",
      "Validation loss: 15.7903733253479 RMSE: 3.97371\n",
      "Validation loss: 14.161922931671143 RMSE: 3.7632332\n",
      "Validation loss: 15.136908531188965 RMSE: 3.8906183\n",
      "115 10 1.0149600505828857\n",
      "Validation loss: 17.192662239074707 RMSE: 4.1464033\n",
      "Validation loss: 17.86983823776245 RMSE: 4.227273\n",
      "Validation loss: 17.562594413757324 RMSE: 4.190775\n",
      "118 12 2.2067952156066895\n",
      "Validation loss: 18.62496566772461 RMSE: 4.315665\n",
      "Validation loss: 15.585774421691895 RMSE: 3.9478822\n",
      "Validation loss: 15.789971351623535 RMSE: 3.9736595\n",
      "121 14 2.397670269012451\n",
      "Validation loss: 13.282308101654053 RMSE: 3.64449\n",
      "Validation loss: 15.792628765106201 RMSE: 3.973994\n",
      "Validation loss: 16.833255529403687 RMSE: 4.102835\n",
      "Validation loss: 18.777356147766113 RMSE: 4.3332844\n",
      "125 0 1.5601922273635864\n",
      "Validation loss: 22.64983367919922 RMSE: 4.7591844\n",
      "Validation loss: 16.17573070526123 RMSE: 4.0219064\n",
      "Validation loss: 17.50654411315918 RMSE: 4.184082\n",
      "128 2 2.097773551940918\n",
      "Validation loss: 19.999348640441895 RMSE: 4.4720626\n",
      "Validation loss: 18.497017860412598 RMSE: 4.300816\n",
      "Validation loss: 17.095262050628662 RMSE: 4.1346416\n",
      "131 4 1.6536176204681396\n",
      "Validation loss: 19.638925552368164 RMSE: 4.4315825\n",
      "Validation loss: 22.11753749847412 RMSE: 4.7029285\n",
      "Validation loss: 19.70145606994629 RMSE: 4.438632\n",
      "134 6 3.1303844451904297\n",
      "Validation loss: 17.64210796356201 RMSE: 4.200251\n",
      "Validation loss: 21.88474464416504 RMSE: 4.6781135\n",
      "Validation loss: 13.339073181152344 RMSE: 3.6522698\n",
      "137 8 2.8888540267944336\n",
      "Validation loss: 14.669016361236572 RMSE: 3.8300152\n",
      "Validation loss: 15.77651071548462 RMSE: 3.9719656\n",
      "Validation loss: 14.338481903076172 RMSE: 3.7866187\n",
      "140 10 1.7186717987060547\n",
      "Validation loss: 13.966326236724854 RMSE: 3.7371547\n",
      "Validation loss: 16.087040424346924 RMSE: 4.010865\n",
      "Validation loss: 21.853023529052734 RMSE: 4.674722\n",
      "143 12 2.544287919998169\n",
      "Validation loss: 15.474622249603271 RMSE: 3.9337795\n",
      "Validation loss: 19.542487144470215 RMSE: 4.420688\n",
      "Validation loss: 20.297495365142822 RMSE: 4.5052743\n",
      "146 14 1.7247414588928223\n",
      "Validation loss: 18.189639568328857 RMSE: 4.264931\n",
      "Validation loss: 13.936553478240967 RMSE: 3.7331693\n",
      "Validation loss: 15.157152652740479 RMSE: 3.8932188\n",
      "Validation loss: 19.445151329040527 RMSE: 4.4096656\n",
      "150 0 1.251786470413208\n",
      "Validation loss: 14.300910949707031 RMSE: 3.7816546\n",
      "Validation loss: 23.623153686523438 RMSE: 4.8603654\n",
      "Validation loss: 18.349787712097168 RMSE: 4.283665\n",
      "153 2 3.0769636631011963\n",
      "Validation loss: 18.589012145996094 RMSE: 4.3114977\n",
      "Validation loss: 17.170503616333008 RMSE: 4.1437306\n",
      "Validation loss: 17.501309394836426 RMSE: 4.1834564\n",
      "156 4 1.9680904150009155\n",
      "Validation loss: 19.421491622924805 RMSE: 4.4069824\n",
      "Validation loss: 16.720669269561768 RMSE: 4.0890913\n",
      "Validation loss: 14.719427585601807 RMSE: 3.8365905\n",
      "159 6 1.9110289812088013\n",
      "Validation loss: 15.423064947128296 RMSE: 3.9272206\n",
      "Validation loss: 14.110371112823486 RMSE: 3.7563777\n",
      "Validation loss: 14.693613052368164 RMSE: 3.8332248\n",
      "162 8 2.116633892059326\n",
      "Validation loss: 17.003697395324707 RMSE: 4.123554\n",
      "Validation loss: 13.916289806365967 RMSE: 3.7304544\n",
      "Validation loss: 17.278498649597168 RMSE: 4.156741\n",
      "165 10 1.7009403705596924\n",
      "Validation loss: 17.722812175750732 RMSE: 4.209847\n",
      "Validation loss: 15.556922912597656 RMSE: 3.9442265\n",
      "Validation loss: 15.64428997039795 RMSE: 3.9552863\n",
      "168 12 1.619901180267334\n",
      "Validation loss: 15.660888195037842 RMSE: 3.9573834\n",
      "Validation loss: 18.29771852493286 RMSE: 4.2775836\n",
      "Validation loss: 19.70942783355713 RMSE: 4.43953\n",
      "171 14 2.9422106742858887\n",
      "Validation loss: 18.646126747131348 RMSE: 4.3181157\n",
      "Validation loss: 25.788195610046387 RMSE: 5.0782075\n",
      "Validation loss: 22.090559005737305 RMSE: 4.70006\n",
      "Validation loss: 19.811463356018066 RMSE: 4.451007\n",
      "175 0 2.2199127674102783\n",
      "Validation loss: 17.04442024230957 RMSE: 4.1284885\n",
      "Validation loss: 20.11130428314209 RMSE: 4.4845634\n",
      "Validation loss: 18.039329528808594 RMSE: 4.247273\n",
      "178 2 2.495725393295288\n",
      "Validation loss: 17.92482852935791 RMSE: 4.2337723\n",
      "Validation loss: 21.817670822143555 RMSE: 4.670939\n",
      "Validation loss: 19.91159439086914 RMSE: 4.462241\n",
      "181 4 1.4257348775863647\n",
      "Validation loss: 19.177154541015625 RMSE: 4.379173\n",
      "Validation loss: 20.492068767547607 RMSE: 4.5268164\n",
      "Validation loss: 16.260708808898926 RMSE: 4.032457\n",
      "184 6 2.3592491149902344\n",
      "Validation loss: 20.507143020629883 RMSE: 4.5284815\n",
      "Validation loss: 17.45681381225586 RMSE: 4.1781354\n",
      "Validation loss: 24.527155876159668 RMSE: 4.95249\n",
      "187 8 1.9598135948181152\n",
      "Validation loss: 15.28377103805542 RMSE: 3.9094462\n",
      "Validation loss: 15.400676250457764 RMSE: 3.9243696\n",
      "Validation loss: 24.540164947509766 RMSE: 4.953803\n",
      "190 10 1.2403110265731812\n",
      "Validation loss: 17.899885177612305 RMSE: 4.2308254\n",
      "Validation loss: 29.37108278274536 RMSE: 5.4195094\n",
      "Validation loss: 24.419371604919434 RMSE: 4.941596\n",
      "193 12 1.681318759918213\n",
      "Validation loss: 25.240153312683105 RMSE: 5.0239577\n",
      "Validation loss: 19.12177848815918 RMSE: 4.3728456\n",
      "Validation loss: 18.08216667175293 RMSE: 4.252313\n",
      "196 14 2.0278360843658447\n",
      "Validation loss: 14.826367378234863 RMSE: 3.8505027\n",
      "Validation loss: 17.284008979797363 RMSE: 4.157404\n",
      "Validation loss: 17.28327751159668 RMSE: 4.157316\n",
      "Validation loss: 15.19888973236084 RMSE: 3.898575\n",
      "Loaded trained model with success.\n",
      "Test loss: 9.431634131933635 Test RMSE: 3.0710967\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': 1e-06, 'gpu': 'cuda:1', 'task_name': 'freesolv', 'model_type': 'gin', 'model': {'num_layer': 3, 'emb_dim': 64, 'feat_dim': 64, 'drop_ratio': 0.5, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 779, 'task': 'regression', 'data_path': 'data/freesolv/freesolv.csv', 'target': ['expt']}}\n",
      "Running on: cuda:1\n",
      "641\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/641\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 25.75737953186035\n",
      "Validation loss: 72.05214309692383 RMSE: 8.488354\n",
      "Validation loss: 66.32022476196289 RMSE: 8.143723\n",
      "Validation loss: 58.54023361206055 RMSE: 7.651159\n",
      "3 2 14.149141311645508\n",
      "Validation loss: 47.135019302368164 RMSE: 6.865495\n",
      "Validation loss: 27.35358715057373 RMSE: 5.230066\n",
      "Validation loss: 14.80754804611206 RMSE: 3.8480577\n",
      "6 4 7.455792427062988\n",
      "Validation loss: 13.377796173095703 RMSE: 3.657567\n",
      "Validation loss: 16.44920253753662 RMSE: 4.0557613\n",
      "Validation loss: 13.161904335021973 RMSE: 3.6279337\n",
      "9 6 5.669268608093262\n",
      "Validation loss: 12.737295150756836 RMSE: 3.5689347\n",
      "Validation loss: 14.678515434265137 RMSE: 3.831255\n",
      "Validation loss: 13.556469917297363 RMSE: 3.6819112\n",
      "12 8 6.0637969970703125\n",
      "Validation loss: 15.292414665222168 RMSE: 3.9105518\n",
      "Validation loss: 13.28567123413086 RMSE: 3.6449516\n",
      "Validation loss: 14.01899528503418 RMSE: 3.744195\n",
      "15 10 4.030287742614746\n",
      "Validation loss: 12.303210258483887 RMSE: 3.5075934\n",
      "Validation loss: 12.134353160858154 RMSE: 3.4834397\n",
      "Validation loss: 12.737168788909912 RMSE: 3.568917\n",
      "18 12 4.359004020690918\n",
      "Validation loss: 13.665350914001465 RMSE: 3.6966674\n",
      "Validation loss: 15.840023040771484 RMSE: 3.9799528\n",
      "Validation loss: 14.50853681564331 RMSE: 3.8090072\n",
      "21 14 4.379636287689209\n",
      "Validation loss: 16.674243927001953 RMSE: 4.0834107\n",
      "Validation loss: 16.710938453674316 RMSE: 4.0879016\n",
      "Validation loss: 16.02599859237671 RMSE: 4.0032487\n",
      "Validation loss: 13.845696449279785 RMSE: 3.7209806\n",
      "25 0 3.828291654586792\n",
      "Validation loss: 16.504881858825684 RMSE: 4.0626197\n",
      "Validation loss: 16.451913833618164 RMSE: 4.056096\n",
      "Validation loss: 17.98634624481201 RMSE: 4.241031\n",
      "28 2 3.210005521774292\n",
      "Validation loss: 18.314186096191406 RMSE: 4.2795076\n",
      "Validation loss: 16.796048164367676 RMSE: 4.098298\n",
      "Validation loss: 19.883426666259766 RMSE: 4.4590836\n",
      "31 4 2.2087042331695557\n",
      "Validation loss: 19.26007080078125 RMSE: 4.38863\n",
      "Validation loss: 22.8837947845459 RMSE: 4.783701\n",
      "Validation loss: 19.977962493896484 RMSE: 4.4696712\n",
      "34 6 2.940758228302002\n",
      "Validation loss: 21.040406703948975 RMSE: 4.5869827\n",
      "Validation loss: 19.694589614868164 RMSE: 4.437859\n",
      "Validation loss: 20.15786838531494 RMSE: 4.489752\n",
      "37 8 2.4194228649139404\n",
      "Validation loss: 18.91873264312744 RMSE: 4.349567\n",
      "Validation loss: 16.286368370056152 RMSE: 4.0356374\n",
      "Validation loss: 23.31178569793701 RMSE: 4.828228\n",
      "40 10 3.117722511291504\n",
      "Validation loss: 17.784013271331787 RMSE: 4.2171097\n",
      "Validation loss: 16.457518577575684 RMSE: 4.056787\n",
      "Validation loss: 19.80891704559326 RMSE: 4.4507213\n",
      "43 12 5.7327799797058105\n",
      "Validation loss: 22.17891788482666 RMSE: 4.70945\n",
      "Validation loss: 18.376497745513916 RMSE: 4.286782\n",
      "Validation loss: 17.065892219543457 RMSE: 4.1310883\n",
      "46 14 3.3715977668762207\n",
      "Validation loss: 26.403745651245117 RMSE: 5.1384573\n",
      "Validation loss: 23.88093376159668 RMSE: 4.886812\n",
      "Validation loss: 20.13563346862793 RMSE: 4.4872746\n",
      "Validation loss: 25.08502960205078 RMSE: 5.0084953\n",
      "50 0 3.625814914703369\n",
      "Validation loss: 23.260175704956055 RMSE: 4.8228803\n",
      "Validation loss: 18.596717834472656 RMSE: 4.3123913\n",
      "Validation loss: 20.32637596130371 RMSE: 4.508478\n",
      "53 2 1.979562520980835\n",
      "Validation loss: 25.47387981414795 RMSE: 5.0471654\n",
      "Validation loss: 24.100975036621094 RMSE: 4.9092746\n",
      "Validation loss: 15.408441543579102 RMSE: 3.9253588\n",
      "56 4 2.6506235599517822\n",
      "Validation loss: 18.484840393066406 RMSE: 4.2993994\n",
      "Validation loss: 17.914207458496094 RMSE: 4.232518\n",
      "Validation loss: 18.626431465148926 RMSE: 4.3158355\n",
      "59 6 2.7000553607940674\n",
      "Validation loss: 20.695390701293945 RMSE: 4.5492187\n",
      "Validation loss: 21.106200218200684 RMSE: 4.5941486\n",
      "Validation loss: 18.828856468200684 RMSE: 4.339223\n",
      "62 8 4.249803066253662\n",
      "Validation loss: 21.186206817626953 RMSE: 4.602848\n",
      "Validation loss: 20.671656608581543 RMSE: 4.5466094\n",
      "Validation loss: 26.368760108947754 RMSE: 5.135052\n",
      "65 10 1.958534836769104\n",
      "Validation loss: 17.102710723876953 RMSE: 4.135543\n",
      "Validation loss: 31.32351064682007 RMSE: 5.5967417\n",
      "Validation loss: 17.991938591003418 RMSE: 4.2416906\n",
      "68 12 2.7798516750335693\n",
      "Validation loss: 17.091766357421875 RMSE: 4.134219\n",
      "Validation loss: 18.044562339782715 RMSE: 4.247889\n",
      "Validation loss: 17.700830459594727 RMSE: 4.207236\n",
      "71 14 3.418794631958008\n",
      "Validation loss: 19.580097198486328 RMSE: 4.4249406\n",
      "Validation loss: 21.2781343460083 RMSE: 4.6128225\n",
      "Validation loss: 19.19881248474121 RMSE: 4.381645\n",
      "Validation loss: 22.83232021331787 RMSE: 4.778318\n",
      "75 0 2.8645620346069336\n",
      "Validation loss: 21.607871055603027 RMSE: 4.6484265\n",
      "Validation loss: 18.822062015533447 RMSE: 4.3384395\n",
      "Validation loss: 19.93766450881958 RMSE: 4.4651613\n",
      "78 2 3.7466514110565186\n",
      "Validation loss: 21.99202537536621 RMSE: 4.6895657\n",
      "Validation loss: 19.409829139709473 RMSE: 4.4056587\n",
      "Validation loss: 16.866209983825684 RMSE: 4.106849\n",
      "81 4 3.2812798023223877\n",
      "Validation loss: 20.87509536743164 RMSE: 4.5689273\n",
      "Validation loss: 22.278478145599365 RMSE: 4.7200084\n",
      "Validation loss: 29.02798557281494 RMSE: 5.387763\n",
      "84 6 3.4082541465759277\n",
      "Validation loss: 18.04353427886963 RMSE: 4.2477684\n",
      "Validation loss: 17.449350357055664 RMSE: 4.1772413\n",
      "Validation loss: 21.316686630249023 RMSE: 4.617\n",
      "87 8 2.3936800956726074\n",
      "Validation loss: 22.347009658813477 RMSE: 4.7272615\n",
      "Validation loss: 19.102062225341797 RMSE: 4.3705907\n",
      "Validation loss: 17.303467273712158 RMSE: 4.159744\n",
      "90 10 1.7229856252670288\n",
      "Validation loss: 21.617691040039062 RMSE: 4.6494827\n",
      "Validation loss: 19.54648780822754 RMSE: 4.4211407\n",
      "Validation loss: 30.414043426513672 RMSE: 5.514893\n",
      "93 12 3.7562661170959473\n",
      "Validation loss: 21.77441692352295 RMSE: 4.6663065\n",
      "Validation loss: 19.976567268371582 RMSE: 4.469515\n",
      "Validation loss: 24.344321250915527 RMSE: 4.933996\n",
      "96 14 1.847881555557251\n",
      "Validation loss: 23.341525077819824 RMSE: 4.831307\n",
      "Validation loss: 22.709717750549316 RMSE: 4.765472\n",
      "Validation loss: 22.975531578063965 RMSE: 4.7932796\n",
      "Validation loss: 16.507577896118164 RMSE: 4.062952\n",
      "100 0 6.230672359466553\n",
      "Validation loss: 14.66578483581543 RMSE: 3.8295932\n",
      "Validation loss: 16.33541202545166 RMSE: 4.0417094\n",
      "Validation loss: 17.39612865447998 RMSE: 4.1708665\n",
      "103 2 4.936060428619385\n",
      "Validation loss: 17.366943359375 RMSE: 4.1673665\n",
      "Validation loss: 19.192978858947754 RMSE: 4.3809795\n",
      "Validation loss: 14.69716739654541 RMSE: 3.8336883\n",
      "106 4 5.425404071807861\n",
      "Validation loss: 20.749217987060547 RMSE: 4.5551314\n",
      "Validation loss: 20.12975311279297 RMSE: 4.4866195\n",
      "Validation loss: 21.671691179275513 RMSE: 4.6552863\n",
      "109 6 2.52462100982666\n",
      "Validation loss: 18.79552459716797 RMSE: 4.3353806\n",
      "Validation loss: 28.22641944885254 RMSE: 5.3128543\n",
      "Validation loss: 16.431365966796875 RMSE: 4.053562\n",
      "112 8 2.6561243534088135\n",
      "Validation loss: 14.091446876525879 RMSE: 3.753858\n",
      "Validation loss: 25.953303337097168 RMSE: 5.094439\n",
      "Validation loss: 19.428300857543945 RMSE: 4.4077544\n",
      "115 10 2.2462971210479736\n",
      "Validation loss: 18.516266345977783 RMSE: 4.3030534\n",
      "Validation loss: 18.06612777709961 RMSE: 4.250427\n",
      "Validation loss: 17.979830741882324 RMSE: 4.240263\n",
      "118 12 1.5394470691680908\n",
      "Validation loss: 21.526031494140625 RMSE: 4.639616\n",
      "Validation loss: 19.389634609222412 RMSE: 4.4033666\n",
      "Validation loss: 22.85093402862549 RMSE: 4.7802653\n",
      "121 14 2.0899674892425537\n",
      "Validation loss: 20.11284065246582 RMSE: 4.4847345\n",
      "Validation loss: 14.865485191345215 RMSE: 3.8555784\n",
      "Validation loss: 18.567015647888184 RMSE: 4.3089457\n",
      "Validation loss: 15.741021156311035 RMSE: 3.9674954\n",
      "125 0 3.0861093997955322\n",
      "Validation loss: 19.66449761390686 RMSE: 4.4344673\n",
      "Validation loss: 22.616872310638428 RMSE: 4.7557197\n",
      "Validation loss: 14.939013957977295 RMSE: 3.8651013\n",
      "128 2 1.9850918054580688\n",
      "Validation loss: 27.30392551422119 RMSE: 5.225316\n",
      "Validation loss: 25.625940799713135 RMSE: 5.0622067\n",
      "Validation loss: 21.210022926330566 RMSE: 4.605434\n",
      "131 4 3.1408159732818604\n",
      "Validation loss: 22.451830863952637 RMSE: 4.7383366\n",
      "Validation loss: 17.584547996520996 RMSE: 4.1933937\n",
      "Validation loss: 26.305073738098145 RMSE: 5.1288466\n",
      "134 6 1.3577966690063477\n",
      "Validation loss: 31.072781562805176 RMSE: 5.5742965\n",
      "Validation loss: 24.87546730041504 RMSE: 4.9875317\n",
      "Validation loss: 19.046366214752197 RMSE: 4.3642144\n",
      "137 8 5.673667907714844\n",
      "Validation loss: 24.969229698181152 RMSE: 4.996922\n",
      "Validation loss: 16.03531837463379 RMSE: 4.004412\n",
      "Validation loss: 22.528632164001465 RMSE: 4.7464337\n",
      "140 10 1.2004228830337524\n",
      "Validation loss: 19.862756729125977 RMSE: 4.4567657\n",
      "Validation loss: 20.67252826690674 RMSE: 4.5467052\n",
      "Validation loss: 34.16777420043945 RMSE: 5.84532\n",
      "143 12 3.128316879272461\n",
      "Validation loss: 20.556789875030518 RMSE: 4.5339594\n",
      "Validation loss: 25.90773105621338 RMSE: 5.089964\n",
      "Validation loss: 26.61668586730957 RMSE: 5.159136\n",
      "146 14 2.776874542236328\n",
      "Validation loss: 29.40487003326416 RMSE: 5.422626\n",
      "Validation loss: 24.467494010925293 RMSE: 4.946463\n",
      "Validation loss: 29.171810150146484 RMSE: 5.401094\n",
      "Validation loss: 24.14041805267334 RMSE: 4.91329\n",
      "150 0 1.8072339296340942\n",
      "Validation loss: 21.031423568725586 RMSE: 4.586003\n",
      "Validation loss: 21.030479431152344 RMSE: 4.5859\n",
      "Validation loss: 25.586660385131836 RMSE: 5.0583253\n",
      "153 2 2.0969297885894775\n",
      "Validation loss: 18.323296546936035 RMSE: 4.2805724\n",
      "Validation loss: 20.528502941131592 RMSE: 4.530839\n",
      "Validation loss: 22.71070957183838 RMSE: 4.7655754\n",
      "156 4 1.315747857093811\n",
      "Validation loss: 24.973817825317383 RMSE: 4.9973807\n",
      "Validation loss: 26.84213924407959 RMSE: 5.18094\n",
      "Validation loss: 27.01047134399414 RMSE: 5.1971602\n",
      "159 6 1.4853343963623047\n",
      "Validation loss: 34.16631889343262 RMSE: 5.8451962\n",
      "Validation loss: 22.19453239440918 RMSE: 4.7111073\n",
      "Validation loss: 23.04523468017578 RMSE: 4.800545\n",
      "162 8 1.9914568662643433\n",
      "Validation loss: 22.57293462753296 RMSE: 4.751098\n",
      "Validation loss: 21.15269947052002 RMSE: 4.599206\n",
      "Validation loss: 17.60906410217285 RMSE: 4.196316\n",
      "165 10 1.9278732538223267\n",
      "Validation loss: 22.824939966201782 RMSE: 4.7775455\n",
      "Validation loss: 14.170923233032227 RMSE: 3.7644289\n",
      "Validation loss: 17.283658504486084 RMSE: 4.157362\n",
      "168 12 2.604344367980957\n",
      "Validation loss: 21.375743865966797 RMSE: 4.6233916\n",
      "Validation loss: 23.154646396636963 RMSE: 4.811928\n",
      "Validation loss: 16.03595542907715 RMSE: 4.004492\n",
      "171 14 1.7832214832305908\n",
      "Validation loss: 22.85395574569702 RMSE: 4.7805815\n",
      "Validation loss: 20.935093879699707 RMSE: 4.5754886\n",
      "Validation loss: 19.689337730407715 RMSE: 4.437267\n",
      "Validation loss: 18.315791130065918 RMSE: 4.2796955\n",
      "175 0 1.0294411182403564\n",
      "Validation loss: 13.65917682647705 RMSE: 3.6958323\n",
      "Validation loss: 23.70046091079712 RMSE: 4.868312\n",
      "Validation loss: 19.94440269470215 RMSE: 4.4659157\n",
      "178 2 2.1283674240112305\n",
      "Validation loss: 18.614900588989258 RMSE: 4.314499\n",
      "Validation loss: 16.975242614746094 RMSE: 4.120103\n",
      "Validation loss: 24.52565860748291 RMSE: 4.9523387\n",
      "181 4 2.9996306896209717\n",
      "Validation loss: 22.444412231445312 RMSE: 4.7375536\n",
      "Validation loss: 19.246710777282715 RMSE: 4.3871074\n",
      "Validation loss: 25.067633628845215 RMSE: 5.0067587\n",
      "184 6 1.345809817314148\n",
      "Validation loss: 16.362475872039795 RMSE: 4.045056\n",
      "Validation loss: 27.27935552597046 RMSE: 5.2229643\n",
      "Validation loss: 25.03608512878418 RMSE: 5.003607\n",
      "187 8 1.3634675741195679\n",
      "Validation loss: 18.091294288635254 RMSE: 4.253386\n",
      "Validation loss: 19.475730895996094 RMSE: 4.4131317\n",
      "Validation loss: 17.650280952453613 RMSE: 4.201224\n",
      "190 10 1.631072759628296\n",
      "Validation loss: 21.730725288391113 RMSE: 4.6616225\n",
      "Validation loss: 24.21208381652832 RMSE: 4.9205775\n",
      "Validation loss: 20.28615093231201 RMSE: 4.5040145\n",
      "193 12 1.1833100318908691\n",
      "Validation loss: 15.450906753540039 RMSE: 3.9307637\n",
      "Validation loss: 20.118074893951416 RMSE: 4.4853177\n",
      "Validation loss: 23.436467170715332 RMSE: 4.8411226\n",
      "196 14 2.6718924045562744\n",
      "Validation loss: 34.35565948486328 RMSE: 5.86137\n",
      "Validation loss: 22.859405517578125 RMSE: 4.7811513\n",
      "Validation loss: 24.026013374328613 RMSE: 4.9016333\n",
      "Validation loss: 18.677814245224 RMSE: 4.3217835\n",
      "Loaded trained model with success.\n",
      "Test loss: 14.046085005540114 Test RMSE: 3.7478106\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': 1e-06, 'gpu': 'cuda:1', 'task_name': 'freesolv', 'model_type': 'gin', 'model': {'num_layer': 3, 'emb_dim': 64, 'feat_dim': 64, 'drop_ratio': 0.5, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 780, 'task': 'regression', 'data_path': 'data/freesolv/freesolv.csv', 'target': ['expt']}}\n",
      "Running on: cuda:1\n",
      "641\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/641\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 14.562607765197754\n",
      "Validation loss: 68.12630271911621 RMSE: 8.253865\n",
      "Validation loss: 61.807437896728516 RMSE: 7.8617706\n",
      "Validation loss: 53.96753787994385 RMSE: 7.34626\n",
      "3 2 15.896940231323242\n",
      "Validation loss: 40.40351104736328 RMSE: 6.356375\n",
      "Validation loss: 21.036887168884277 RMSE: 4.586599\n",
      "Validation loss: 10.539148807525635 RMSE: 3.2464056\n",
      "6 4 7.140400409698486\n",
      "Validation loss: 11.649252891540527 RMSE: 3.4131\n",
      "Validation loss: 14.742493152618408 RMSE: 3.8395953\n",
      "Validation loss: 15.060463905334473 RMSE: 3.8807812\n",
      "9 6 13.1430082321167\n",
      "Validation loss: 11.757240772247314 RMSE: 3.4288836\n",
      "Validation loss: 13.29859733581543 RMSE: 3.6467242\n",
      "Validation loss: 12.931928634643555 RMSE: 3.5960991\n",
      "12 8 5.874906539916992\n",
      "Validation loss: 11.096921443939209 RMSE: 3.3312042\n",
      "Validation loss: 15.208452224731445 RMSE: 3.8998015\n",
      "Validation loss: 10.96873140335083 RMSE: 3.3119075\n",
      "15 10 6.7260003089904785\n",
      "Validation loss: 11.303415298461914 RMSE: 3.3620553\n",
      "Validation loss: 11.775634765625 RMSE: 3.4315646\n",
      "Validation loss: 12.447102546691895 RMSE: 3.528045\n",
      "18 12 3.5511891841888428\n",
      "Validation loss: 12.014890909194946 RMSE: 3.4662502\n",
      "Validation loss: 12.832720756530762 RMSE: 3.5822787\n",
      "Validation loss: 11.918006896972656 RMSE: 3.4522467\n",
      "21 14 5.498311519622803\n",
      "Validation loss: 16.622931480407715 RMSE: 4.077123\n",
      "Validation loss: 13.893766403198242 RMSE: 3.7274344\n",
      "Validation loss: 18.00231170654297 RMSE: 4.242913\n",
      "Validation loss: 16.912686347961426 RMSE: 4.1125035\n",
      "25 0 3.059638738632202\n",
      "Validation loss: 16.540541172027588 RMSE: 4.0670066\n",
      "Validation loss: 14.556778907775879 RMSE: 3.8153348\n",
      "Validation loss: 19.1513614654541 RMSE: 4.376227\n",
      "28 2 4.536535263061523\n",
      "Validation loss: 12.800012588500977 RMSE: 3.5777104\n",
      "Validation loss: 15.89291524887085 RMSE: 3.9865918\n",
      "Validation loss: 17.159637451171875 RMSE: 4.142419\n",
      "31 4 3.399416923522949\n",
      "Validation loss: 16.806842803955078 RMSE: 4.099615\n",
      "Validation loss: 18.719923973083496 RMSE: 4.326653\n",
      "Validation loss: 16.665616035461426 RMSE: 4.0823545\n",
      "34 6 2.2731754779815674\n",
      "Validation loss: 13.191338062286377 RMSE: 3.631988\n",
      "Validation loss: 17.770998001098633 RMSE: 4.215566\n",
      "Validation loss: 16.72844409942627 RMSE: 4.090042\n",
      "37 8 4.282464027404785\n",
      "Validation loss: 17.147958755493164 RMSE: 4.14101\n",
      "Validation loss: 14.390710353851318 RMSE: 3.793509\n",
      "Validation loss: 17.64693260192871 RMSE: 4.2008257\n",
      "40 10 4.544365406036377\n",
      "Validation loss: 15.63050651550293 RMSE: 3.9535434\n",
      "Validation loss: 20.648174285888672 RMSE: 4.5440264\n",
      "Validation loss: 12.044611930847168 RMSE: 3.4705346\n",
      "43 12 5.889005184173584\n",
      "Validation loss: 18.946694374084473 RMSE: 4.35278\n",
      "Validation loss: 18.36007308959961 RMSE: 4.284866\n",
      "Validation loss: 16.312684535980225 RMSE: 4.0388966\n",
      "46 14 2.7525668144226074\n",
      "Validation loss: 15.900195121765137 RMSE: 3.987505\n",
      "Validation loss: 16.606009483337402 RMSE: 4.075047\n",
      "Validation loss: 16.31428098678589 RMSE: 4.039094\n",
      "Validation loss: 15.008960723876953 RMSE: 3.87414\n",
      "50 0 5.56488037109375\n",
      "Validation loss: 13.529090881347656 RMSE: 3.6781912\n",
      "Validation loss: 14.506167888641357 RMSE: 3.8086965\n",
      "Validation loss: 16.109829902648926 RMSE: 4.0137053\n",
      "53 2 6.172870635986328\n",
      "Validation loss: 18.75911521911621 RMSE: 4.3311796\n",
      "Validation loss: 17.79923415184021 RMSE: 4.218915\n",
      "Validation loss: 17.011945247650146 RMSE: 4.1245537\n",
      "56 4 2.50166392326355\n",
      "Validation loss: 12.878318786621094 RMSE: 3.5886374\n",
      "Validation loss: 17.398340225219727 RMSE: 4.1711316\n",
      "Validation loss: 17.052547454833984 RMSE: 4.129473\n",
      "59 6 2.7257065773010254\n",
      "Validation loss: 18.601078987121582 RMSE: 4.3128967\n",
      "Validation loss: 13.813215732574463 RMSE: 3.7166138\n",
      "Validation loss: 12.135636329650879 RMSE: 3.483624\n",
      "62 8 2.4967799186706543\n",
      "Validation loss: 14.455482959747314 RMSE: 3.8020365\n",
      "Validation loss: 18.108399391174316 RMSE: 4.255397\n",
      "Validation loss: 15.880228519439697 RMSE: 3.9850004\n",
      "65 10 3.5467987060546875\n",
      "Validation loss: 16.015933513641357 RMSE: 4.0019913\n",
      "Validation loss: 12.660108089447021 RMSE: 3.5581043\n",
      "Validation loss: 12.042296409606934 RMSE: 3.470201\n",
      "68 12 1.7378156185150146\n",
      "Validation loss: 12.966121196746826 RMSE: 3.60085\n",
      "Validation loss: 14.607194900512695 RMSE: 3.8219361\n",
      "Validation loss: 15.116054058074951 RMSE: 3.887937\n",
      "71 14 3.4175124168395996\n",
      "Validation loss: 16.00574254989624 RMSE: 4.0007176\n",
      "Validation loss: 10.496276378631592 RMSE: 3.239796\n",
      "Validation loss: 11.982398509979248 RMSE: 3.4615602\n",
      "Validation loss: 15.92717981338501 RMSE: 3.9908872\n",
      "75 0 2.1735727787017822\n",
      "Validation loss: 12.132754802703857 RMSE: 3.4832106\n",
      "Validation loss: 16.533020496368408 RMSE: 4.066082\n",
      "Validation loss: 17.684101104736328 RMSE: 4.205247\n",
      "78 2 2.35919189453125\n",
      "Validation loss: 16.00678062438965 RMSE: 4.0008473\n",
      "Validation loss: 14.786136627197266 RMSE: 3.8452744\n",
      "Validation loss: 16.97727108001709 RMSE: 4.120349\n",
      "81 4 2.4885075092315674\n",
      "Validation loss: 14.973105430603027 RMSE: 3.8695097\n",
      "Validation loss: 14.231274604797363 RMSE: 3.7724361\n",
      "Validation loss: 15.777083396911621 RMSE: 3.9720378\n",
      "84 6 1.959735631942749\n",
      "Validation loss: 18.01381015777588 RMSE: 4.244268\n",
      "Validation loss: 18.41359281539917 RMSE: 4.291106\n",
      "Validation loss: 15.633351802825928 RMSE: 3.9539034\n",
      "87 8 4.067340850830078\n",
      "Validation loss: 17.521928071975708 RMSE: 4.1859202\n",
      "Validation loss: 16.509560585021973 RMSE: 4.063196\n",
      "Validation loss: 14.46471881866455 RMSE: 3.803251\n",
      "90 10 2.156822443008423\n",
      "Validation loss: 19.09910535812378 RMSE: 4.3702526\n",
      "Validation loss: 19.63764762878418 RMSE: 4.431439\n",
      "Validation loss: 15.982439994812012 RMSE: 3.9978044\n",
      "93 12 3.4883062839508057\n",
      "Validation loss: 14.627930164337158 RMSE: 3.8246477\n",
      "Validation loss: 18.820783615112305 RMSE: 4.338293\n",
      "Validation loss: 18.40407085418701 RMSE: 4.2899966\n",
      "96 14 1.228455662727356\n",
      "Validation loss: 16.45329189300537 RMSE: 4.056266\n",
      "Validation loss: 15.733660697937012 RMSE: 3.966568\n",
      "Validation loss: 14.446253299713135 RMSE: 3.8008227\n",
      "Validation loss: 17.046170234680176 RMSE: 4.1287003\n",
      "100 0 1.1653153896331787\n",
      "Validation loss: 25.912002563476562 RMSE: 5.0903835\n",
      "Validation loss: 21.530692100524902 RMSE: 4.6401176\n",
      "Validation loss: 18.312423706054688 RMSE: 4.2793016\n",
      "103 2 1.7141499519348145\n",
      "Validation loss: 14.195894479751587 RMSE: 3.767744\n",
      "Validation loss: 15.29227066040039 RMSE: 3.9105332\n",
      "Validation loss: 20.399755477905273 RMSE: 4.5166087\n",
      "106 4 1.4086111783981323\n",
      "Validation loss: 14.741106510162354 RMSE: 3.839415\n",
      "Validation loss: 14.11670207977295 RMSE: 3.7572198\n",
      "Validation loss: 18.98808765411377 RMSE: 4.3575325\n",
      "109 6 1.9240838289260864\n",
      "Validation loss: 16.792963981628418 RMSE: 4.097922\n",
      "Validation loss: 15.818567276000977 RMSE: 3.9772563\n",
      "Validation loss: 18.325566291809082 RMSE: 4.280837\n",
      "112 8 1.2349969148635864\n",
      "Validation loss: 12.958005905151367 RMSE: 3.5997229\n",
      "Validation loss: 14.129088401794434 RMSE: 3.7588677\n",
      "Validation loss: 17.30653142929077 RMSE: 4.160112\n",
      "115 10 1.9026482105255127\n",
      "Validation loss: 19.290757179260254 RMSE: 4.3921247\n",
      "Validation loss: 14.039270401000977 RMSE: 3.7469015\n",
      "Validation loss: 16.2386531829834 RMSE: 4.0297213\n",
      "118 12 1.3315678834915161\n",
      "Validation loss: 14.29675006866455 RMSE: 3.7811043\n",
      "Validation loss: 13.943179607391357 RMSE: 3.7340567\n",
      "Validation loss: 12.338485717773438 RMSE: 3.5126183\n",
      "121 14 1.8508871793746948\n",
      "Validation loss: 19.892653465270996 RMSE: 4.4601183\n",
      "Validation loss: 15.074155807495117 RMSE: 3.882545\n",
      "Validation loss: 19.291420936584473 RMSE: 4.3922\n",
      "Validation loss: 18.015395164489746 RMSE: 4.2444553\n",
      "125 0 2.261132001876831\n",
      "Validation loss: 15.152079582214355 RMSE: 3.8925674\n",
      "Validation loss: 16.34461545944214 RMSE: 4.042847\n",
      "Validation loss: 14.880990505218506 RMSE: 3.8575885\n",
      "128 2 2.0222768783569336\n",
      "Validation loss: 14.532243728637695 RMSE: 3.812118\n",
      "Validation loss: 17.09515380859375 RMSE: 4.134629\n",
      "Validation loss: 16.70985507965088 RMSE: 4.087769\n",
      "131 4 1.3767207860946655\n",
      "Validation loss: 16.282577991485596 RMSE: 4.035167\n",
      "Validation loss: 16.223758697509766 RMSE: 4.027873\n",
      "Validation loss: 16.62518334388733 RMSE: 4.0773993\n",
      "134 6 2.2076990604400635\n",
      "Validation loss: 17.66300678253174 RMSE: 4.2027383\n",
      "Validation loss: 17.99898624420166 RMSE: 4.2425213\n",
      "Validation loss: 15.37679672241211 RMSE: 3.9213257\n",
      "137 8 3.718284845352173\n",
      "Validation loss: 14.908737659454346 RMSE: 3.8611836\n",
      "Validation loss: 18.123631477355957 RMSE: 4.2571855\n",
      "Validation loss: 22.85028314590454 RMSE: 4.780197\n",
      "140 10 3.5142271518707275\n",
      "Validation loss: 21.910195350646973 RMSE: 4.680833\n",
      "Validation loss: 19.76735019683838 RMSE: 4.4460487\n",
      "Validation loss: 14.890974521636963 RMSE: 3.8588827\n",
      "143 12 1.119389295578003\n",
      "Validation loss: 16.93484592437744 RMSE: 4.1151967\n",
      "Validation loss: 16.012028217315674 RMSE: 4.001503\n",
      "Validation loss: 13.782469272613525 RMSE: 3.7124748\n",
      "146 14 0.9520121812820435\n",
      "Validation loss: 11.882235288619995 RMSE: 3.4470618\n",
      "Validation loss: 16.763617992401123 RMSE: 4.0943394\n",
      "Validation loss: 16.210123538970947 RMSE: 4.0261793\n",
      "Validation loss: 16.43376064300537 RMSE: 4.0538573\n",
      "150 0 1.2202750444412231\n",
      "Validation loss: 16.65023708343506 RMSE: 4.08047\n",
      "Validation loss: 22.76366424560547 RMSE: 4.771128\n",
      "Validation loss: 16.702980995178223 RMSE: 4.0869284\n",
      "153 2 1.487247347831726\n",
      "Validation loss: 13.060004234313965 RMSE: 3.613863\n",
      "Validation loss: 18.407867670059204 RMSE: 4.290439\n",
      "Validation loss: 23.673686027526855 RMSE: 4.8655605\n",
      "156 4 1.124345064163208\n",
      "Validation loss: 15.360841274261475 RMSE: 3.9192908\n",
      "Validation loss: 17.43095302581787 RMSE: 4.17504\n",
      "Validation loss: 15.852049350738525 RMSE: 3.9814634\n",
      "159 6 3.9698991775512695\n",
      "Validation loss: 18.716052532196045 RMSE: 4.3262053\n",
      "Validation loss: 21.239023208618164 RMSE: 4.608581\n",
      "Validation loss: 20.198692321777344 RMSE: 4.494295\n",
      "162 8 2.341061592102051\n",
      "Validation loss: 18.469573974609375 RMSE: 4.297624\n",
      "Validation loss: 16.51959753036499 RMSE: 4.064431\n",
      "Validation loss: 23.100557327270508 RMSE: 4.8063035\n",
      "165 10 1.1774851083755493\n",
      "Validation loss: 20.402193069458008 RMSE: 4.5168786\n",
      "Validation loss: 17.15799617767334 RMSE: 4.1422215\n",
      "Validation loss: 15.340478897094727 RMSE: 3.916692\n",
      "168 12 1.631188154220581\n",
      "Validation loss: 19.7711443901062 RMSE: 4.446475\n",
      "Validation loss: 13.566755771636963 RMSE: 3.6833076\n",
      "Validation loss: 16.110736846923828 RMSE: 4.013818\n",
      "171 14 3.564044952392578\n",
      "Validation loss: 14.403035402297974 RMSE: 3.7951329\n",
      "Validation loss: 19.31943988800049 RMSE: 4.3953886\n",
      "Validation loss: 15.261699199676514 RMSE: 3.906623\n",
      "Validation loss: 17.782875061035156 RMSE: 4.2169743\n",
      "175 0 2.8342034816741943\n",
      "Validation loss: 16.574556827545166 RMSE: 4.071186\n",
      "Validation loss: 19.88673973083496 RMSE: 4.4594555\n",
      "Validation loss: 22.79201316833496 RMSE: 4.7740974\n",
      "178 2 2.069714307785034\n",
      "Validation loss: 16.22410774230957 RMSE: 4.0279164\n",
      "Validation loss: 17.63986301422119 RMSE: 4.1999836\n",
      "Validation loss: 20.745614051818848 RMSE: 4.554735\n",
      "181 4 1.6226511001586914\n",
      "Validation loss: 23.72640371322632 RMSE: 4.8709755\n",
      "Validation loss: 20.480281829833984 RMSE: 4.5255146\n",
      "Validation loss: 28.132216453552246 RMSE: 5.303982\n",
      "184 6 0.929952085018158\n",
      "Validation loss: 22.276389122009277 RMSE: 4.719787\n",
      "Validation loss: 25.48781442642212 RMSE: 5.048546\n",
      "Validation loss: 18.622233390808105 RMSE: 4.3153486\n",
      "187 8 0.8047588467597961\n",
      "Validation loss: 16.247339725494385 RMSE: 4.030799\n",
      "Validation loss: 15.2778639793396 RMSE: 3.9086905\n",
      "Validation loss: 16.59747886657715 RMSE: 4.074\n",
      "190 10 1.941455364227295\n",
      "Validation loss: 20.83636474609375 RMSE: 4.5646863\n",
      "Validation loss: 25.55635643005371 RMSE: 5.0553293\n",
      "Validation loss: 22.059168815612793 RMSE: 4.6967187\n",
      "193 12 1.4188926219940186\n",
      "Validation loss: 19.177672386169434 RMSE: 4.379232\n",
      "Validation loss: 25.41195297241211 RMSE: 5.041027\n",
      "Validation loss: 19.013826370239258 RMSE: 4.3604846\n",
      "196 14 1.9913330078125\n",
      "Validation loss: 17.20439624786377 RMSE: 4.147818\n",
      "Validation loss: 22.944833755493164 RMSE: 4.790077\n",
      "Validation loss: 22.7376766204834 RMSE: 4.768404\n",
      "Validation loss: 19.694075107574463 RMSE: 4.437801\n",
      "Loaded trained model with success.\n",
      "Test loss: 12.810787112896259 Test RMSE: 3.579216\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': 1e-06, 'gpu': 'cuda:1', 'task_name': 'freesolv', 'model_type': 'gin', 'model': {'num_layer': 3, 'emb_dim': 64, 'feat_dim': 64, 'drop_ratio': 0.5, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 781, 'task': 'regression', 'data_path': 'data/freesolv/freesolv.csv', 'target': ['expt']}}\n",
      "Running on: cuda:1\n",
      "641\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/641\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 16.71216583251953\n",
      "Validation loss: 63.37727165222168 RMSE: 7.960984\n",
      "Validation loss: 57.7047004699707 RMSE: 7.596361\n",
      "Validation loss: 51.00370407104492 RMSE: 7.141688\n",
      "3 2 6.379263877868652\n",
      "Validation loss: 41.36331367492676 RMSE: 6.431432\n",
      "Validation loss: 30.001283645629883 RMSE: 5.4773426\n",
      "Validation loss: 22.91551113128662 RMSE: 4.787015\n",
      "6 4 4.232698917388916\n",
      "Validation loss: 20.055065155029297 RMSE: 4.478288\n",
      "Validation loss: 18.038005828857422 RMSE: 4.2471175\n",
      "Validation loss: 17.69803810119629 RMSE: 4.2069035\n",
      "9 6 4.886949062347412\n",
      "Validation loss: 19.206244468688965 RMSE: 4.382493\n",
      "Validation loss: 17.09207057952881 RMSE: 4.134256\n",
      "Validation loss: 15.84203815460205 RMSE: 3.980206\n",
      "12 8 6.2166852951049805\n",
      "Validation loss: 16.58912467956543 RMSE: 4.0729747\n",
      "Validation loss: 14.770381927490234 RMSE: 3.8432252\n",
      "Validation loss: 14.19704532623291 RMSE: 3.767897\n",
      "15 10 6.838376998901367\n",
      "Validation loss: 15.173460006713867 RMSE: 3.8953128\n",
      "Validation loss: 13.572500705718994 RMSE: 3.6840875\n",
      "Validation loss: 13.428872108459473 RMSE: 3.6645424\n",
      "18 12 3.6872854232788086\n",
      "Validation loss: 12.033084392547607 RMSE: 3.4688735\n",
      "Validation loss: 11.99126148223877 RMSE: 3.4628403\n",
      "Validation loss: 10.871314764022827 RMSE: 3.2971678\n",
      "21 14 5.2378249168396\n",
      "Validation loss: 10.570927143096924 RMSE: 3.251296\n",
      "Validation loss: 11.402017593383789 RMSE: 3.3766873\n",
      "Validation loss: 13.695703506469727 RMSE: 3.7007706\n",
      "Validation loss: 11.377112865447998 RMSE: 3.3729978\n",
      "25 0 4.7808732986450195\n",
      "Validation loss: 11.007075786590576 RMSE: 3.3176913\n",
      "Validation loss: 13.023237705230713 RMSE: 3.6087723\n",
      "Validation loss: 11.156742572784424 RMSE: 3.3401709\n",
      "28 2 4.874782562255859\n",
      "Validation loss: 12.106348037719727 RMSE: 3.4794178\n",
      "Validation loss: 14.978734970092773 RMSE: 3.870237\n",
      "Validation loss: 9.279426097869873 RMSE: 3.046215\n",
      "31 4 7.576962471008301\n",
      "Validation loss: 11.148151159286499 RMSE: 3.3388848\n",
      "Validation loss: 10.227685928344727 RMSE: 3.1980753\n",
      "Validation loss: 10.443565368652344 RMSE: 3.2316506\n",
      "34 6 2.9005229473114014\n",
      "Validation loss: 10.850083351135254 RMSE: 3.2939465\n",
      "Validation loss: 13.205536842346191 RMSE: 3.6339424\n",
      "Validation loss: 12.26990032196045 RMSE: 3.5028417\n",
      "37 8 4.97752571105957\n",
      "Validation loss: 10.75726318359375 RMSE: 3.2798266\n",
      "Validation loss: 13.138862133026123 RMSE: 3.6247566\n",
      "Validation loss: 11.599859237670898 RMSE: 3.4058566\n",
      "40 10 2.9690778255462646\n",
      "Validation loss: 8.28295350074768 RMSE: 2.878012\n",
      "Validation loss: 13.379887104034424 RMSE: 3.6578524\n",
      "Validation loss: 12.74193000793457 RMSE: 3.5695841\n",
      "43 12 5.007256507873535\n",
      "Validation loss: 14.773815631866455 RMSE: 3.843672\n",
      "Validation loss: 14.271430015563965 RMSE: 3.7777545\n",
      "Validation loss: 9.523961544036865 RMSE: 3.0860918\n",
      "46 14 4.6551833152771\n",
      "Validation loss: 11.714307308197021 RMSE: 3.4226172\n",
      "Validation loss: 15.37779426574707 RMSE: 3.921453\n",
      "Validation loss: 14.862354278564453 RMSE: 3.8551724\n",
      "Validation loss: 10.864418029785156 RMSE: 3.2961214\n",
      "50 0 3.638075590133667\n",
      "Validation loss: 11.683467149734497 RMSE: 3.4181087\n",
      "Validation loss: 13.084812641143799 RMSE: 3.617294\n",
      "Validation loss: 14.369874477386475 RMSE: 3.7907615\n",
      "53 2 4.057856559753418\n",
      "Validation loss: 11.741550922393799 RMSE: 3.4265945\n",
      "Validation loss: 8.978008031845093 RMSE: 2.9963324\n",
      "Validation loss: 9.263092041015625 RMSE: 3.0435328\n",
      "56 4 5.260028839111328\n",
      "Validation loss: 13.968019008636475 RMSE: 3.7373812\n",
      "Validation loss: 14.608131408691406 RMSE: 3.8220584\n",
      "Validation loss: 14.690013885498047 RMSE: 3.8327553\n",
      "59 6 3.9588048458099365\n",
      "Validation loss: 17.162102699279785 RMSE: 4.142717\n",
      "Validation loss: 13.258264541625977 RMSE: 3.6411898\n",
      "Validation loss: 15.818500518798828 RMSE: 3.9772482\n",
      "62 8 2.901028633117676\n",
      "Validation loss: 13.180344104766846 RMSE: 3.6304743\n",
      "Validation loss: 12.861985206604004 RMSE: 3.586361\n",
      "Validation loss: 11.936869621276855 RMSE: 3.4549773\n",
      "65 10 3.970813035964966\n",
      "Validation loss: 11.925945281982422 RMSE: 3.4533958\n",
      "Validation loss: 13.178497791290283 RMSE: 3.6302202\n",
      "Validation loss: 14.256604194641113 RMSE: 3.7757916\n",
      "68 12 1.704832911491394\n",
      "Validation loss: 11.03854751586914 RMSE: 3.3224308\n",
      "Validation loss: 13.151915073394775 RMSE: 3.6265569\n",
      "Validation loss: 11.645312428474426 RMSE: 3.412523\n",
      "71 14 2.149949312210083\n",
      "Validation loss: 13.404589176177979 RMSE: 3.6612277\n",
      "Validation loss: 15.512777328491211 RMSE: 3.9386263\n",
      "Validation loss: 9.361295700073242 RMSE: 3.0596237\n",
      "Validation loss: 14.022958278656006 RMSE: 3.744724\n",
      "75 0 2.1811814308166504\n",
      "Validation loss: 15.176259994506836 RMSE: 3.895672\n",
      "Validation loss: 12.293673992156982 RMSE: 3.5062337\n",
      "Validation loss: 14.019706726074219 RMSE: 3.74429\n",
      "78 2 2.2292375564575195\n",
      "Validation loss: 10.366517305374146 RMSE: 3.2197077\n",
      "Validation loss: 9.979620695114136 RMSE: 3.1590536\n",
      "Validation loss: 13.117711067199707 RMSE: 3.621838\n",
      "81 4 3.874061107635498\n",
      "Validation loss: 15.845928192138672 RMSE: 3.9806945\n",
      "Validation loss: 12.59426212310791 RMSE: 3.5488393\n",
      "Validation loss: 12.870191097259521 RMSE: 3.5875046\n",
      "84 6 3.11186146736145\n",
      "Validation loss: 13.074484825134277 RMSE: 3.6158655\n",
      "Validation loss: 15.014326095581055 RMSE: 3.8748326\n",
      "Validation loss: 14.576099395751953 RMSE: 3.8178656\n",
      "87 8 1.244343638420105\n",
      "Validation loss: 12.990678310394287 RMSE: 3.6042585\n",
      "Validation loss: 12.521560668945312 RMSE: 3.5385818\n",
      "Validation loss: 19.114357948303223 RMSE: 4.371997\n",
      "90 10 4.461533546447754\n",
      "Validation loss: 13.835264205932617 RMSE: 3.7195785\n",
      "Validation loss: 14.248002052307129 RMSE: 3.7746527\n",
      "Validation loss: 16.07122278213501 RMSE: 4.008893\n",
      "93 12 2.42769193649292\n",
      "Validation loss: 17.105658054351807 RMSE: 4.135899\n",
      "Validation loss: 19.940112113952637 RMSE: 4.4654355\n",
      "Validation loss: 13.015035152435303 RMSE: 3.6076357\n",
      "96 14 2.8196768760681152\n",
      "Validation loss: 12.68254041671753 RMSE: 3.5612552\n",
      "Validation loss: 12.736086368560791 RMSE: 3.5687654\n",
      "Validation loss: 15.926152229309082 RMSE: 3.9907584\n",
      "Validation loss: 19.0075101852417 RMSE: 4.3597608\n",
      "100 0 2.7108116149902344\n",
      "Validation loss: 16.002000331878662 RMSE: 4.00025\n",
      "Validation loss: 18.575164794921875 RMSE: 4.3098917\n",
      "Validation loss: 16.369924545288086 RMSE: 4.0459766\n",
      "103 2 1.9673631191253662\n",
      "Validation loss: 17.369678020477295 RMSE: 4.1676946\n",
      "Validation loss: 16.398592948913574 RMSE: 4.0495176\n",
      "Validation loss: 20.763648986816406 RMSE: 4.556714\n",
      "106 4 1.9426978826522827\n",
      "Validation loss: 16.360543251037598 RMSE: 4.0448165\n",
      "Validation loss: 16.82932949066162 RMSE: 4.1023564\n",
      "Validation loss: 16.740829467773438 RMSE: 4.0915556\n",
      "109 6 2.3032023906707764\n",
      "Validation loss: 18.014160633087158 RMSE: 4.244309\n",
      "Validation loss: 12.874222755432129 RMSE: 3.5880666\n",
      "Validation loss: 14.169208526611328 RMSE: 3.7642007\n",
      "112 8 2.2905328273773193\n",
      "Validation loss: 17.682223320007324 RMSE: 4.2050233\n",
      "Validation loss: 17.926817893981934 RMSE: 4.2340074\n",
      "Validation loss: 15.677872657775879 RMSE: 3.9595287\n",
      "115 10 3.4620473384857178\n",
      "Validation loss: 16.158183097839355 RMSE: 4.0197244\n",
      "Validation loss: 16.595244884490967 RMSE: 4.073726\n",
      "Validation loss: 16.336898803710938 RMSE: 4.041893\n",
      "118 12 2.6974005699157715\n",
      "Validation loss: 15.940712451934814 RMSE: 3.9925826\n",
      "Validation loss: 18.529097080230713 RMSE: 4.3045435\n",
      "Validation loss: 16.58894634246826 RMSE: 4.0729527\n",
      "121 14 1.2839967012405396\n",
      "Validation loss: 16.082965850830078 RMSE: 4.0103574\n",
      "Validation loss: 18.77379035949707 RMSE: 4.3328733\n",
      "Validation loss: 16.34394931793213 RMSE: 4.042765\n",
      "Validation loss: 15.125343799591064 RMSE: 3.8891315\n",
      "125 0 2.0013232231140137\n",
      "Validation loss: 21.356629371643066 RMSE: 4.6213236\n",
      "Validation loss: 17.29130268096924 RMSE: 4.1582813\n",
      "Validation loss: 16.536842346191406 RMSE: 4.0665517\n",
      "128 2 1.3745160102844238\n",
      "Validation loss: 16.66098976135254 RMSE: 4.0817876\n",
      "Validation loss: 19.70716094970703 RMSE: 4.439275\n",
      "Validation loss: 13.283231258392334 RMSE: 3.6446166\n",
      "131 4 2.176748752593994\n",
      "Validation loss: 18.204565048217773 RMSE: 4.2666807\n",
      "Validation loss: 16.007468223571777 RMSE: 4.0009336\n",
      "Validation loss: 24.355717658996582 RMSE: 4.935151\n",
      "134 6 2.2003419399261475\n",
      "Validation loss: 17.965917587280273 RMSE: 4.238622\n",
      "Validation loss: 13.927549362182617 RMSE: 3.7319632\n",
      "Validation loss: 21.86350154876709 RMSE: 4.6758423\n",
      "137 8 2.1301681995391846\n",
      "Validation loss: 17.65924644470215 RMSE: 4.202291\n",
      "Validation loss: 16.51876735687256 RMSE: 4.0643287\n",
      "Validation loss: 19.982266426086426 RMSE: 4.4701533\n",
      "140 10 1.8358904123306274\n",
      "Validation loss: 15.574097633361816 RMSE: 3.9464033\n",
      "Validation loss: 14.263481140136719 RMSE: 3.7767024\n",
      "Validation loss: 23.019722938537598 RMSE: 4.7978873\n",
      "143 12 1.5022279024124146\n",
      "Validation loss: 14.565841674804688 RMSE: 3.8165224\n",
      "Validation loss: 14.787570476531982 RMSE: 3.8454611\n",
      "Validation loss: 18.46473217010498 RMSE: 4.297061\n",
      "146 14 1.0163307189941406\n",
      "Validation loss: 17.885428428649902 RMSE: 4.2291164\n",
      "Validation loss: 19.849834442138672 RMSE: 4.455315\n",
      "Validation loss: 18.09535026550293 RMSE: 4.2538633\n",
      "Validation loss: 18.574424266815186 RMSE: 4.3098054\n",
      "150 0 1.4307876825332642\n",
      "Validation loss: 18.881269454956055 RMSE: 4.345258\n",
      "Validation loss: 14.970379829406738 RMSE: 3.8691576\n",
      "Validation loss: 20.21377182006836 RMSE: 4.495973\n",
      "153 2 2.0593857765197754\n",
      "Validation loss: 14.512868404388428 RMSE: 3.809576\n",
      "Validation loss: 14.839223384857178 RMSE: 3.8521714\n",
      "Validation loss: 17.473651885986328 RMSE: 4.1801496\n",
      "156 4 5.149200439453125\n",
      "Validation loss: 23.893901109695435 RMSE: 4.8881383\n",
      "Validation loss: 16.471117973327637 RMSE: 4.0584626\n",
      "Validation loss: 17.275737762451172 RMSE: 4.1564097\n",
      "159 6 2.552227020263672\n",
      "Validation loss: 17.200587272644043 RMSE: 4.1473594\n",
      "Validation loss: 20.967897415161133 RMSE: 4.5790715\n",
      "Validation loss: 18.456406593322754 RMSE: 4.296092\n",
      "162 8 3.0093703269958496\n",
      "Validation loss: 16.962695121765137 RMSE: 4.1185794\n",
      "Validation loss: 19.723924160003662 RMSE: 4.4411626\n",
      "Validation loss: 22.463364601135254 RMSE: 4.7395535\n",
      "165 10 1.7208755016326904\n",
      "Validation loss: 17.23763155937195 RMSE: 4.1518226\n",
      "Validation loss: 23.292207717895508 RMSE: 4.8262\n",
      "Validation loss: 22.49777889251709 RMSE: 4.7431827\n",
      "168 12 3.2216806411743164\n",
      "Validation loss: 21.973159790039062 RMSE: 4.687554\n",
      "Validation loss: 29.53378677368164 RMSE: 5.4344993\n",
      "Validation loss: 22.581395149230957 RMSE: 4.7519884\n",
      "171 14 1.892004370689392\n",
      "Validation loss: 17.009896278381348 RMSE: 4.1243057\n",
      "Validation loss: 14.03398084640503 RMSE: 3.7461956\n",
      "Validation loss: 21.075868606567383 RMSE: 4.5908465\n",
      "Validation loss: 15.695616245269775 RMSE: 3.961769\n",
      "175 0 1.133976936340332\n",
      "Validation loss: 17.370234489440918 RMSE: 4.1677613\n",
      "Validation loss: 19.651691913604736 RMSE: 4.433023\n",
      "Validation loss: 19.428950309753418 RMSE: 4.4078283\n",
      "178 2 2.0930020809173584\n",
      "Validation loss: 15.53022289276123 RMSE: 3.94084\n",
      "Validation loss: 15.054440975189209 RMSE: 3.8800054\n",
      "Validation loss: 15.143924951553345 RMSE: 3.8915198\n",
      "181 4 2.145183801651001\n",
      "Validation loss: 16.614473342895508 RMSE: 4.076085\n",
      "Validation loss: 17.336548805236816 RMSE: 4.1637187\n",
      "Validation loss: 13.99197244644165 RMSE: 3.7405846\n",
      "184 6 1.8601733446121216\n",
      "Validation loss: 14.876186847686768 RMSE: 3.856966\n",
      "Validation loss: 16.30474042892456 RMSE: 4.037913\n",
      "Validation loss: 19.149502754211426 RMSE: 4.376014\n",
      "187 8 2.0907483100891113\n",
      "Validation loss: 20.669273376464844 RMSE: 4.546347\n",
      "Validation loss: 22.479732513427734 RMSE: 4.7412796\n",
      "Validation loss: 19.09091854095459 RMSE: 4.3693156\n",
      "190 10 1.8230608701705933\n",
      "Validation loss: 22.721365928649902 RMSE: 4.766693\n",
      "Validation loss: 23.51935577392578 RMSE: 4.8496757\n",
      "Validation loss: 16.7485408782959 RMSE: 4.092498\n",
      "193 12 1.5527617931365967\n",
      "Validation loss: 19.639179229736328 RMSE: 4.431612\n",
      "Validation loss: 23.484917640686035 RMSE: 4.8461237\n",
      "Validation loss: 17.089125633239746 RMSE: 4.133899\n",
      "196 14 1.790539264678955\n",
      "Validation loss: 14.00709056854248 RMSE: 3.7426047\n",
      "Validation loss: 16.53573513031006 RMSE: 4.0664153\n",
      "Validation loss: 21.634594917297363 RMSE: 4.6513004\n",
      "Validation loss: 13.222249984741211 RMSE: 3.6362412\n",
      "Loaded trained model with success.\n",
      "Test loss: 16.18804320188669 Test RMSE: 4.023437\n"
     ]
    }
   ],
   "source": [
    "datasets = ['FreeSolv']\n",
    "seeds = [777, 778, 779, 780, 781]\n",
    "\n",
    "for dataset in datasets:\n",
    "    for seed in seeds:\n",
    "            if dataset == 'FreeSolv':\n",
    "            # FreeSolv 데이터셋에 대한 특정 옵션을 적용\n",
    "                !python finetuneReconNN.py \\\n",
    "                --task_name {dataset} \\\n",
    "                --seed {seed} \\\n",
    "                --dropout 0.5 \\\n",
    "                --num_layer 3 \\\n",
    "                --emb_dim 64 \\\n",
    "                --feat_dim 64 \\\n",
    "                --alpha 0.1 \\\n",
    "                --mask_edge 1 \\\n",
    "                --gpu cuda:1\n",
    "            \n",
    "            elif dataset == 'clintox':\n",
    "                !python finetuneReconNN.py \\\n",
    "                --task_name {dataset} \\\n",
    "                --seed {seed} \\\n",
    "                --dropout 0.3 \\\n",
    "                --num_layer 5 \\\n",
    "                --emb_dim 64 \\\n",
    "                --feat_dim 64 \\\n",
    "                --alpha 0.1 \\\n",
    "                --mask_edge 1 \\\n",
    "                --gpu cuda:0\n",
    "\n",
    "            else:\n",
    "                !python finetuneReconNN.py \\\n",
    "                --task_name {dataset} \\\n",
    "                --seed {seed} \\\n",
    "                --alpha 0.1 \\\n",
    "                --mask_edge 1 \\\n",
    "                --gpu cuda:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2a46ae-a6b6-4a66-befc-c4e38c83fdb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:0', 'task_name': 'freesolv', 'model_type': 'gin', 'model': {'num_layer': 5, 'emb_dim': 300, 'feat_dim': 300, 'drop_ratio': 0.3, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'random', 'seed': 777, 'task': 'regression', 'data_path': 'data/freesolv/freesolv.csv', 'target': ['expt']}}\n",
      "Running on: cuda:0\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 14.321107864379883\n",
      "Validation loss: 20.91863441467285 RMSE: 4.5736895\n",
      "Validation loss: 17.256500959396362 RMSE: 4.154094\n",
      "2 16 1.6740686893463135\n",
      "Validation loss: 9.5014009475708 RMSE: 3.0824342\n",
      "Validation loss: 18.261054039001465 RMSE: 4.2732954\n",
      "Validation loss: 4.354099988937378 RMSE: 2.0866482\n",
      "5 15 2.0978336334228516\n",
      "Validation loss: 2.7761409282684326 RMSE: 1.6661755\n",
      "Validation loss: 6.134705543518066 RMSE: 2.4768338\n",
      "Validation loss: 6.321134567260742 RMSE: 2.5141869\n",
      "8 14 3.4172286987304688\n",
      "Validation loss: 5.5209267139434814 RMSE: 2.3496654\n",
      "Validation loss: 13.93534231185913 RMSE: 3.7330072\n",
      "Validation loss: 5.228390216827393 RMSE: 2.2865674\n",
      "11 13 3.121549129486084\n",
      "Validation loss: 7.373233079910278 RMSE: 2.7153695\n",
      "Validation loss: 4.980787515640259 RMSE: 2.231768\n",
      "Validation loss: 4.149728178977966 RMSE: 2.0370882\n",
      "14 12 3.0783309936523438\n",
      "Validation loss: 5.393917560577393 RMSE: 2.322481\n",
      "Validation loss: 9.424787402153015 RMSE: 3.069982\n",
      "Validation loss: 3.304729461669922 RMSE: 1.8178914\n",
      "17 11 2.1237094402313232\n",
      "Validation loss: 13.586473822593689 RMSE: 3.6859837\n",
      "Validation loss: 2.705667734146118 RMSE: 1.6448911\n",
      "Validation loss: 4.860611200332642 RMSE: 2.204679\n",
      "20 10 2.180887460708618\n",
      "Validation loss: 9.186617851257324 RMSE: 3.0309432\n",
      "Validation loss: 6.59820419549942 RMSE: 2.5686972\n",
      "Validation loss: 5.45841646194458 RMSE: 2.3363254\n",
      "23 9 2.2084107398986816\n",
      "Validation loss: 10.025069236755371 RMSE: 3.1662388\n",
      "Validation loss: 2.3286338448524475 RMSE: 1.5259862\n",
      "Validation loss: 8.440638542175293 RMSE: 2.9052775\n",
      "26 8 1.3372224569320679\n",
      "Validation loss: 6.463868141174316 RMSE: 2.5424137\n",
      "Validation loss: 12.901103496551514 RMSE: 3.5918105\n",
      "Validation loss: 12.712461471557617 RMSE: 3.565454\n",
      "29 7 3.387467861175537\n",
      "Validation loss: 1.6997678875923157 RMSE: 1.3037512\n",
      "Validation loss: 8.209753513336182 RMSE: 2.865267\n",
      "Validation loss: 3.7261006832122803 RMSE: 1.9303108\n",
      "32 6 1.3978638648986816\n",
      "Validation loss: 3.634792923927307 RMSE: 1.9065133\n",
      "Validation loss: 23.70071315765381 RMSE: 4.8683376\n",
      "Validation loss: 2.1626139879226685 RMSE: 1.4705827\n",
      "35 5 2.1506173610687256\n",
      "Validation loss: 7.431595802307129 RMSE: 2.7260954\n",
      "Validation loss: 6.668334126472473 RMSE: 2.5823119\n",
      "Validation loss: 2.303868055343628 RMSE: 1.5178498\n",
      "38 4 5.720208168029785\n",
      "Validation loss: 10.526123046875 RMSE: 3.2443988\n",
      "Validation loss: 4.498493194580078 RMSE: 2.1209652\n",
      "Validation loss: 4.741388440132141 RMSE: 2.1774728\n",
      "41 3 1.1128249168395996\n",
      "Validation loss: 15.646967649459839 RMSE: 3.9556246\n",
      "Validation loss: 1.95924711227417 RMSE: 1.3997312\n",
      "Validation loss: 3.023181438446045 RMSE: 1.7387301\n",
      "44 2 1.488884449005127\n",
      "Validation loss: 8.31072986125946 RMSE: 2.8828337\n",
      "Validation loss: 3.8074792623519897 RMSE: 1.9512762\n",
      "Validation loss: 15.148390293121338 RMSE: 3.8920937\n",
      "47 1 1.3609793186187744\n",
      "Validation loss: 11.078186750411987 RMSE: 3.3283908\n",
      "Validation loss: 2.7154356241226196 RMSE: 1.6478577\n",
      "Validation loss: 9.06560730934143 RMSE: 3.0109143\n",
      "50 0 1.7610065937042236\n",
      "Validation loss: 5.494647741317749 RMSE: 2.3440666\n",
      "Validation loss: 4.9150495529174805 RMSE: 2.216991\n",
      "52 16 4.530332565307617\n",
      "Validation loss: 2.4536731243133545 RMSE: 1.5664204\n",
      "Validation loss: 7.778294801712036 RMSE: 2.7889593\n",
      "Validation loss: 11.89563512802124 RMSE: 3.4490044\n",
      "55 15 1.5943224430084229\n",
      "Validation loss: 8.33542251586914 RMSE: 2.887113\n",
      "Validation loss: 4.603169918060303 RMSE: 2.1454997\n",
      "Validation loss: 8.7169189453125 RMSE: 2.9524424\n",
      "58 14 6.113883972167969\n",
      "Validation loss: 3.524573802947998 RMSE: 1.8773848\n",
      "Validation loss: 3.1536667346954346 RMSE: 1.7758563\n",
      "Validation loss: 2.669476866722107 RMSE: 1.6338534\n",
      "61 13 3.593710422515869\n",
      "Validation loss: 2.729556918144226 RMSE: 1.6521369\n",
      "Validation loss: 1.4966734647750854 RMSE: 1.223386\n",
      "Validation loss: 10.484611511230469 RMSE: 3.2379944\n",
      "64 12 1.4421100616455078\n",
      "Validation loss: 15.941698551177979 RMSE: 3.9927056\n",
      "Validation loss: 2.03757643699646 RMSE: 1.4274371\n",
      "Validation loss: 4.891714930534363 RMSE: 2.2117226\n",
      "67 11 0.8361790180206299\n",
      "Validation loss: 1.9478195905685425 RMSE: 1.3956432\n",
      "Validation loss: 4.604289650917053 RMSE: 2.1457608\n",
      "Validation loss: 6.783350348472595 RMSE: 2.6044862\n",
      "70 10 2.0118415355682373\n",
      "Validation loss: 5.839918375015259 RMSE: 2.4165924\n",
      "Validation loss: 8.688260555267334 RMSE: 2.9475856\n",
      "Validation loss: 2.0918793082237244 RMSE: 1.446333\n",
      "73 9 0.996292233467102\n",
      "Validation loss: 2.770226240158081 RMSE: 1.6643996\n",
      "Validation loss: 3.364711880683899 RMSE: 1.8343152\n",
      "Validation loss: 2.3308213353157043 RMSE: 1.5267028\n",
      "76 8 1.6383395195007324\n",
      "Validation loss: 2.394361436367035 RMSE: 1.5473725\n",
      "Validation loss: 1.2451589703559875 RMSE: 1.1158668\n",
      "Validation loss: 9.45081090927124 RMSE: 3.0742173\n",
      "79 7 2.138673782348633\n",
      "Validation loss: 6.314155101776123 RMSE: 2.5127983\n",
      "Validation loss: 11.514304161071777 RMSE: 3.3932729\n",
      "Validation loss: 1.5443845093250275 RMSE: 1.2427326\n",
      "82 6 1.8259228467941284\n",
      "Validation loss: 2.806889295578003 RMSE: 1.6753772\n",
      "Validation loss: 2.180079460144043 RMSE: 1.4765092\n",
      "Validation loss: 13.70202922821045 RMSE: 3.7016258\n",
      "85 5 1.7427281141281128\n",
      "Validation loss: 2.6307966709136963 RMSE: 1.6219732\n",
      "Validation loss: 5.62000185251236 RMSE: 2.370654\n",
      "Validation loss: 2.6636844873428345 RMSE: 1.6320797\n",
      "88 4 0.8470762968063354\n",
      "Validation loss: 1.5560477375984192 RMSE: 1.2474165\n",
      "Validation loss: 6.000298738479614 RMSE: 2.4495509\n",
      "Validation loss: 3.4865431785583496 RMSE: 1.867229\n",
      "91 3 0.5117723345756531\n",
      "Validation loss: 1.3801007866859436 RMSE: 1.1747768\n",
      "Validation loss: 3.1592814922332764 RMSE: 1.7774367\n",
      "Validation loss: 2.77260684967041 RMSE: 1.6651146\n",
      "94 2 2.832580327987671\n",
      "Validation loss: 2.2825942039489746 RMSE: 1.5108255\n",
      "Validation loss: 1.2492355108261108 RMSE: 1.117692\n",
      "Validation loss: 4.164891719818115 RMSE: 2.0408063\n",
      "97 1 2.0177931785583496\n",
      "Validation loss: 3.6904574632644653 RMSE: 1.9210564\n",
      "Validation loss: 5.913467675447464 RMSE: 2.4317622\n",
      "Validation loss: 1.072115421295166 RMSE: 1.0354302\n",
      "100 0 1.43095862865448\n",
      "Validation loss: 2.359165281057358 RMSE: 1.5359572\n",
      "Validation loss: 3.742803692817688 RMSE: 1.9346328\n",
      "102 16 0.6668446660041809\n",
      "Validation loss: 2.897221565246582 RMSE: 1.7021226\n",
      "Validation loss: 1.9340905249118805 RMSE: 1.3907158\n",
      "Validation loss: 3.806449294090271 RMSE: 1.9510124\n",
      "105 15 2.747955322265625\n",
      "Validation loss: 2.2267810106277466 RMSE: 1.4922402\n",
      "Validation loss: 2.8945993185043335 RMSE: 1.7013521\n",
      "Validation loss: 2.2886324524879456 RMSE: 1.5128231\n",
      "108 14 1.856547236442566\n",
      "Validation loss: 5.9955174922943115 RMSE: 2.4485748\n",
      "Validation loss: 1.249184787273407 RMSE: 1.1176693\n",
      "Validation loss: 3.0336804389953613 RMSE: 1.7417461\n",
      "111 13 0.8801333904266357\n",
      "Validation loss: 1.4764221906661987 RMSE: 1.2150811\n",
      "Validation loss: 6.9158570766448975 RMSE: 2.6298018\n",
      "Validation loss: 1.338023066520691 RMSE: 1.1567296\n",
      "114 12 1.0879697799682617\n",
      "Validation loss: 2.7781455516815186 RMSE: 1.6667773\n",
      "Validation loss: 4.137959718704224 RMSE: 2.0341976\n",
      "Validation loss: 2.805605858564377 RMSE: 1.6749942\n",
      "117 11 1.7006199359893799\n",
      "Validation loss: 2.56577730178833 RMSE: 1.6018044\n",
      "Validation loss: 1.2324445247650146 RMSE: 1.1101552\n",
      "Validation loss: 1.9795788526535034 RMSE: 1.4069748\n",
      "120 10 1.2716553211212158\n",
      "Validation loss: 1.340436190366745 RMSE: 1.157772\n",
      "Validation loss: 1.4255233108997345 RMSE: 1.1939527\n",
      "Validation loss: 6.794361591339111 RMSE: 2.6065993\n",
      "123 9 1.2550132274627686\n",
      "Validation loss: 1.4356001019477844 RMSE: 1.1981653\n",
      "Validation loss: 5.750680923461914 RMSE: 2.3980577\n",
      "Validation loss: 1.3114582300186157 RMSE: 1.145189\n",
      "126 8 1.7500109672546387\n",
      "Validation loss: 4.8761069774627686 RMSE: 2.208191\n",
      "Validation loss: 1.45756334066391 RMSE: 1.2072957\n",
      "Validation loss: 1.1161895394325256 RMSE: 1.0564988\n",
      "129 7 0.6116553544998169\n",
      "Validation loss: 1.739623486995697 RMSE: 1.3189479\n",
      "Validation loss: 3.356816291809082 RMSE: 1.8321614\n",
      "Validation loss: 3.7187633514404297 RMSE: 1.9284096\n",
      "132 6 1.253115177154541\n",
      "Validation loss: 1.6520788371562958 RMSE: 1.2853323\n",
      "Validation loss: 1.0393221378326416 RMSE: 1.0194714\n",
      "Validation loss: 2.311925768852234 RMSE: 1.5205017\n",
      "135 5 0.5788388848304749\n",
      "Validation loss: 2.79392671585083 RMSE: 1.6715044\n",
      "Validation loss: 2.091659724712372 RMSE: 1.4462571\n",
      "Validation loss: 1.4883313179016113 RMSE: 1.2199719\n",
      "138 4 1.0223125219345093\n",
      "Validation loss: 2.0054184198379517 RMSE: 1.4161279\n",
      "Validation loss: 1.3556776642799377 RMSE: 1.1643357\n",
      "Validation loss: 1.5639651715755463 RMSE: 1.2505859\n",
      "141 3 1.521355152130127\n",
      "Validation loss: 0.9317711591720581 RMSE: 0.9652828\n",
      "Validation loss: 1.3833388686180115 RMSE: 1.1761543\n",
      "Validation loss: 1.6674079895019531 RMSE: 1.2912816\n",
      "144 2 1.359778642654419\n",
      "Validation loss: 6.275326251983643 RMSE: 2.5050602\n",
      "Validation loss: 2.056834638118744 RMSE: 1.4341669\n",
      "Validation loss: 1.3124879002571106 RMSE: 1.1456387\n",
      "147 1 1.4902451038360596\n",
      "Validation loss: 1.7479283809661865 RMSE: 1.3220924\n",
      "Validation loss: 3.0501595735549927 RMSE: 1.7464706\n",
      "Validation loss: 2.12426096200943 RMSE: 1.4574845\n",
      "150 0 0.7807320356369019\n",
      "Validation loss: 1.9108963012695312 RMSE: 1.3823519\n",
      "Validation loss: 1.4102342128753662 RMSE: 1.1875328\n",
      "152 16 0.3569975197315216\n",
      "Validation loss: 1.5386036038398743 RMSE: 1.2404047\n",
      "Validation loss: 0.9323343336582184 RMSE: 0.9655746\n",
      "Validation loss: 1.196922391653061 RMSE: 1.0940394\n",
      "155 15 1.0107722282409668\n",
      "Validation loss: 3.7611218690872192 RMSE: 1.9393616\n",
      "Validation loss: 3.757067382335663 RMSE: 1.9383152\n",
      "Validation loss: 5.740548729896545 RMSE: 2.395944\n",
      "158 14 2.9125609397888184\n",
      "Validation loss: 1.1788890063762665 RMSE: 1.0857666\n",
      "Validation loss: 1.6328120231628418 RMSE: 1.2778153\n",
      "Validation loss: 1.2574752569198608 RMSE: 1.1213721\n",
      "161 13 1.1761541366577148\n",
      "Validation loss: 4.028734087944031 RMSE: 2.0071704\n",
      "Validation loss: 0.9544443190097809 RMSE: 0.97695667\n",
      "Validation loss: 1.6894562244415283 RMSE: 1.2997906\n",
      "164 12 0.6288465261459351\n",
      "Validation loss: 1.0672178864479065 RMSE: 1.0330625\n",
      "Validation loss: 0.966910719871521 RMSE: 0.98331624\n",
      "Validation loss: 1.1631883382797241 RMSE: 1.0785121\n",
      "167 11 0.9410708546638489\n",
      "Validation loss: 1.181836485862732 RMSE: 1.087123\n",
      "Validation loss: 1.3985340595245361 RMSE: 1.1825962\n",
      "Validation loss: 3.3499534130096436 RMSE: 1.8302876\n",
      "170 10 1.0085904598236084\n",
      "Validation loss: 2.5982906818389893 RMSE: 1.6119213\n",
      "Validation loss: 1.89385986328125 RMSE: 1.3761758\n",
      "Validation loss: 1.6625536680221558 RMSE: 1.2894006\n",
      "173 9 0.9965384006500244\n",
      "Validation loss: 4.122945308685303 RMSE: 2.0305035\n",
      "Validation loss: 1.4736483097076416 RMSE: 1.2139392\n",
      "Validation loss: 0.7938440442085266 RMSE: 0.8909793\n",
      "176 8 1.9352898597717285\n",
      "Validation loss: 1.3663355112075806 RMSE: 1.1689036\n",
      "Validation loss: 1.0736965537071228 RMSE: 1.0361933\n",
      "Validation loss: 5.307312250137329 RMSE: 2.3037603\n",
      "179 7 1.3857712745666504\n",
      "Validation loss: 1.7182368636131287 RMSE: 1.3108155\n",
      "Validation loss: 1.4312275648117065 RMSE: 1.1963391\n",
      "Validation loss: 0.9997207522392273 RMSE: 0.9998604\n",
      "182 6 0.9685672521591187\n",
      "Validation loss: 1.9417122602462769 RMSE: 1.3934534\n",
      "Validation loss: 1.6379902362823486 RMSE: 1.27984\n",
      "Validation loss: 1.0898262858390808 RMSE: 1.0439473\n",
      "185 5 1.3943133354187012\n",
      "Validation loss: 1.1425244808197021 RMSE: 1.0688894\n",
      "Validation loss: 1.9647348523139954 RMSE: 1.4016898\n",
      "Validation loss: 1.6245195865631104 RMSE: 1.2745663\n",
      "188 4 0.5735785961151123\n",
      "Validation loss: 2.1779500246047974 RMSE: 1.4757879\n",
      "Validation loss: 1.380947470664978 RMSE: 1.1751372\n",
      "Validation loss: 1.5370616912841797 RMSE: 1.239783\n",
      "191 3 0.5352512001991272\n",
      "Validation loss: 2.5072742700576782 RMSE: 1.5834376\n",
      "Validation loss: 1.66990065574646 RMSE: 1.2922465\n",
      "Validation loss: 3.1005266904830933 RMSE: 1.7608316\n",
      "194 2 1.3924925327301025\n",
      "Validation loss: 4.185925126075745 RMSE: 2.0459528\n",
      "Validation loss: 0.7411659359931946 RMSE: 0.86090994\n",
      "Validation loss: 4.66570520401001 RMSE: 2.1600242\n",
      "197 1 1.9714056253433228\n",
      "Validation loss: 2.1266363859176636 RMSE: 1.4582992\n",
      "Validation loss: 0.8483651876449585 RMSE: 0.9210674\n",
      "Validation loss: 3.0864381194114685 RMSE: 1.7568266\n",
      "Loaded trained model with success.\n",
      "Test loss: 1.9685813188552856 Test RMSE: 1.4030615\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:0', 'task_name': 'freesolv', 'model_type': 'gin', 'model': {'num_layer': 5, 'emb_dim': 300, 'feat_dim': 300, 'drop_ratio': 0.3, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'random', 'seed': 778, 'task': 'regression', 'data_path': 'data/freesolv/freesolv.csv', 'target': ['expt']}}\n",
      "Running on: cuda:0\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 16.177167892456055\n",
      "Validation loss: 24.261737823486328 RMSE: 4.9256206\n",
      "Validation loss: 19.728859424591064 RMSE: 4.441718\n",
      "2 16 17.045284271240234\n",
      "Validation loss: 19.012720108032227 RMSE: 4.360358\n",
      "Validation loss: 6.400212526321411 RMSE: 2.5298643\n",
      "Validation loss: 17.22266435623169 RMSE: 4.1500196\n",
      "5 15 3.8309154510498047\n",
      "Validation loss: 5.235531568527222 RMSE: 2.2881284\n",
      "Validation loss: 27.575143814086914 RMSE: 5.251204\n",
      "Validation loss: 5.97870135307312 RMSE: 2.4451385\n",
      "8 14 3.077336311340332\n",
      "Validation loss: 6.119457244873047 RMSE: 2.4737537\n",
      "Validation loss: 5.8354332447052 RMSE: 2.4156642\n",
      "Validation loss: 7.723174333572388 RMSE: 2.7790601\n",
      "11 13 3.958364963531494\n",
      "Validation loss: 6.013454914093018 RMSE: 2.4522345\n",
      "Validation loss: 5.204346179962158 RMSE: 2.2813036\n",
      "Validation loss: 6.283418893814087 RMSE: 2.506675\n",
      "14 12 2.3135015964508057\n",
      "Validation loss: 4.100240707397461 RMSE: 2.0249052\n",
      "Validation loss: 9.616214275360107 RMSE: 3.1010022\n",
      "Validation loss: 7.279036045074463 RMSE: 2.6979687\n",
      "17 11 3.8547940254211426\n",
      "Validation loss: 9.974107682704926 RMSE: 3.1581812\n",
      "Validation loss: 3.30516254901886 RMSE: 1.8180106\n",
      "Validation loss: 2.5077884197235107 RMSE: 1.5835998\n",
      "20 10 2.145199775695801\n",
      "Validation loss: 3.2945258617401123 RMSE: 1.815083\n",
      "Validation loss: 7.0554587841033936 RMSE: 2.6562114\n",
      "Validation loss: 3.3896178603172302 RMSE: 1.8410914\n",
      "23 9 1.5921759605407715\n",
      "Validation loss: 5.975407600402832 RMSE: 2.4444647\n",
      "Validation loss: 3.8915568590164185 RMSE: 1.9727031\n",
      "Validation loss: 3.271517276763916 RMSE: 1.8087337\n",
      "26 8 1.3833270072937012\n",
      "Validation loss: 3.5469539165496826 RMSE: 1.8833357\n",
      "Validation loss: 3.483122944831848 RMSE: 1.8663127\n",
      "Validation loss: 6.065205097198486 RMSE: 2.4627638\n",
      "29 7 1.1739394664764404\n",
      "Validation loss: 6.60438871383667 RMSE: 2.5699005\n",
      "Validation loss: 3.480749785900116 RMSE: 1.8656768\n",
      "Validation loss: 2.2743804454803467 RMSE: 1.508105\n",
      "32 6 1.3863418102264404\n",
      "Validation loss: 8.4001704454422 RMSE: 2.8983047\n",
      "Validation loss: 3.368551254272461 RMSE: 1.8353615\n",
      "Validation loss: 7.511385917663574 RMSE: 2.7406907\n",
      "35 5 2.5965898036956787\n",
      "Validation loss: 3.83659827709198 RMSE: 1.9587238\n",
      "Validation loss: 3.257595717906952 RMSE: 1.804881\n",
      "Validation loss: 2.791750907897949 RMSE: 1.6708534\n",
      "38 4 1.881471037864685\n",
      "Validation loss: 9.365322828292847 RMSE: 3.0602813\n",
      "Validation loss: 11.13070011138916 RMSE: 3.3362706\n",
      "Validation loss: 6.849674940109253 RMSE: 2.6171882\n",
      "41 3 3.5577640533447266\n",
      "Validation loss: 4.5565197467803955 RMSE: 2.1346006\n",
      "Validation loss: 5.489298582077026 RMSE: 2.3429246\n",
      "Validation loss: 2.858259618282318 RMSE: 1.6906388\n",
      "44 2 2.3805320262908936\n",
      "Validation loss: 4.33515477180481 RMSE: 2.0821033\n",
      "Validation loss: 3.2206575870513916 RMSE: 1.7946192\n",
      "Validation loss: 5.4128406047821045 RMSE: 2.326551\n",
      "47 1 1.824760913848877\n",
      "Validation loss: 3.548068881034851 RMSE: 1.883632\n",
      "Validation loss: 4.326249122619629 RMSE: 2.0799637\n",
      "Validation loss: 3.7500126361846924 RMSE: 1.9364951\n",
      "50 0 2.920224189758301\n",
      "Validation loss: 2.8610774278640747 RMSE: 1.6914718\n",
      "Validation loss: 4.7691570520401 RMSE: 2.18384\n",
      "52 16 8.06419563293457\n",
      "Validation loss: 5.873327732086182 RMSE: 2.423495\n",
      "Validation loss: 4.766313314437866 RMSE: 2.1831892\n",
      "Validation loss: 5.336477994918823 RMSE: 2.310082\n",
      "55 15 0.8392414450645447\n",
      "Validation loss: 4.3221787214279175 RMSE: 2.0789852\n",
      "Validation loss: 4.687639951705933 RMSE: 2.1650958\n",
      "Validation loss: 5.042495012283325 RMSE: 2.2455502\n",
      "58 14 0.9096124768257141\n",
      "Validation loss: 3.4262540340423584 RMSE: 1.8510143\n",
      "Validation loss: 3.1890887022018433 RMSE: 1.7858019\n",
      "Validation loss: 8.866388320922852 RMSE: 2.9776487\n",
      "61 13 1.245629072189331\n",
      "Validation loss: 4.22921359539032 RMSE: 2.0565054\n",
      "Validation loss: 5.495846509933472 RMSE: 2.344322\n",
      "Validation loss: 3.5939204692840576 RMSE: 1.8957638\n",
      "64 12 0.957255482673645\n",
      "Validation loss: 3.9504001140594482 RMSE: 1.9875616\n",
      "Validation loss: 3.2795557975769043 RMSE: 1.8109546\n",
      "Validation loss: 6.611280679702759 RMSE: 2.5712411\n",
      "67 11 0.6362749338150024\n",
      "Validation loss: 3.502771496772766 RMSE: 1.8715692\n",
      "Validation loss: 4.1181793212890625 RMSE: 2.0293298\n",
      "Validation loss: 7.210947036743164 RMSE: 2.6853206\n",
      "70 10 2.4402623176574707\n",
      "Validation loss: 2.392658770084381 RMSE: 1.5468222\n",
      "Validation loss: 4.481552720069885 RMSE: 2.1169677\n",
      "Validation loss: 4.823304891586304 RMSE: 2.1962025\n",
      "73 9 0.842302680015564\n",
      "Validation loss: 7.994356632232666 RMSE: 2.8274293\n",
      "Validation loss: 2.0276458263397217 RMSE: 1.4239542\n",
      "Validation loss: 2.9081162214279175 RMSE: 1.70532\n",
      "76 8 1.0372307300567627\n",
      "Validation loss: 2.5272045135498047 RMSE: 1.5897183\n",
      "Validation loss: 3.901069402694702 RMSE: 1.9751126\n",
      "Validation loss: 1.6934426426887512 RMSE: 1.3013235\n",
      "79 7 5.582481861114502\n",
      "Validation loss: 4.278210818767548 RMSE: 2.0683837\n",
      "Validation loss: 2.150050461292267 RMSE: 1.4663051\n",
      "Validation loss: 4.639265656471252 RMSE: 2.1538954\n",
      "82 6 1.0610411167144775\n",
      "Validation loss: 3.7479405403137207 RMSE: 1.9359597\n",
      "Validation loss: 2.750980257987976 RMSE: 1.658608\n",
      "Validation loss: 7.1140453815460205 RMSE: 2.6672168\n",
      "85 5 2.820385456085205\n",
      "Validation loss: 2.742839813232422 RMSE: 1.6561521\n",
      "Validation loss: 6.140491247177124 RMSE: 2.4780014\n",
      "Validation loss: 4.836590051651001 RMSE: 2.199225\n",
      "88 4 1.0982080698013306\n",
      "Validation loss: 3.1317538022994995 RMSE: 1.7696763\n",
      "Validation loss: 6.137798070907593 RMSE: 2.477458\n",
      "Validation loss: 2.7881836891174316 RMSE: 1.6697855\n",
      "91 3 0.6676109433174133\n",
      "Validation loss: 3.0917364358901978 RMSE: 1.7583334\n",
      "Validation loss: 3.5074254274368286 RMSE: 1.8728125\n",
      "Validation loss: 3.1494717597961426 RMSE: 1.7746753\n",
      "94 2 1.818185567855835\n",
      "Validation loss: 3.318026840686798 RMSE: 1.8215451\n",
      "Validation loss: 3.2647088766098022 RMSE: 1.8068506\n",
      "Validation loss: 3.2623045444488525 RMSE: 1.806185\n",
      "97 1 3.451174736022949\n",
      "Validation loss: 4.204235553741455 RMSE: 2.0504236\n",
      "Validation loss: 2.0708935260772705 RMSE: 1.4390599\n",
      "Validation loss: 2.3481072187423706 RMSE: 1.5323536\n",
      "100 0 0.7608904838562012\n",
      "Validation loss: 1.9471333026885986 RMSE: 1.3953973\n",
      "Validation loss: 1.894763708114624 RMSE: 1.3765043\n",
      "102 16 0.40318411588668823\n",
      "Validation loss: 3.6346575021743774 RMSE: 1.906478\n",
      "Validation loss: 2.1790401935577393 RMSE: 1.4761574\n",
      "Validation loss: 2.420938491821289 RMSE: 1.5559366\n",
      "105 15 6.433834075927734\n",
      "Validation loss: 1.8929924964904785 RMSE: 1.3758606\n",
      "Validation loss: 2.8596739768981934 RMSE: 1.6910571\n",
      "Validation loss: 8.327637195587158 RMSE: 2.8857648\n",
      "108 14 0.6014808416366577\n",
      "Validation loss: 14.256386280059814 RMSE: 3.775763\n",
      "Validation loss: 15.315126657485962 RMSE: 3.9134548\n",
      "Validation loss: 2.776508629322052 RMSE: 1.666286\n",
      "111 13 1.560638666152954\n",
      "Validation loss: 4.6485559940338135 RMSE: 2.156051\n",
      "Validation loss: 2.097421407699585 RMSE: 1.4482478\n",
      "Validation loss: 2.6311044096946716 RMSE: 1.6220679\n",
      "114 12 1.1249645948410034\n",
      "Validation loss: 2.067474067211151 RMSE: 1.4378713\n",
      "Validation loss: 2.8440637588500977 RMSE: 1.6864352\n",
      "Validation loss: 10.749679565429688 RMSE: 3.2786703\n",
      "117 11 0.7704058885574341\n",
      "Validation loss: 2.4998421669006348 RMSE: 1.5810889\n",
      "Validation loss: 2.1385265588760376 RMSE: 1.4623704\n",
      "Validation loss: 2.2349127531051636 RMSE: 1.4949625\n",
      "120 10 2.030350923538208\n",
      "Validation loss: 2.8448140621185303 RMSE: 1.6866575\n",
      "Validation loss: 6.282335042953491 RMSE: 2.506459\n",
      "Validation loss: 3.8403666019439697 RMSE: 1.9596854\n",
      "123 9 0.34163355827331543\n",
      "Validation loss: 4.635653853416443 RMSE: 2.153057\n",
      "Validation loss: 3.7509816884994507 RMSE: 1.936745\n",
      "Validation loss: 4.0451420545578 RMSE: 2.011254\n",
      "126 8 0.39654847979545593\n",
      "Validation loss: 2.977458953857422 RMSE: 1.7255315\n",
      "Validation loss: 1.9943581819534302 RMSE: 1.4122175\n",
      "Validation loss: 9.022719502449036 RMSE: 3.0037842\n",
      "129 7 0.6537832021713257\n",
      "Validation loss: 2.475521445274353 RMSE: 1.573379\n",
      "Validation loss: 3.081158995628357 RMSE: 1.7553228\n",
      "Validation loss: 6.174708366394043 RMSE: 2.484896\n",
      "132 6 2.7577638626098633\n",
      "Validation loss: 6.466942071914673 RMSE: 2.5430183\n",
      "Validation loss: 2.8816038370132446 RMSE: 1.697529\n",
      "Validation loss: 2.4262691736221313 RMSE: 1.5576488\n",
      "135 5 1.3890268802642822\n",
      "Validation loss: 3.71411395072937 RMSE: 1.9272038\n",
      "Validation loss: 5.127269744873047 RMSE: 2.2643476\n",
      "Validation loss: 6.524946093559265 RMSE: 2.554398\n",
      "138 4 0.7051113247871399\n",
      "Validation loss: 2.26407253742218 RMSE: 1.5046835\n",
      "Validation loss: 5.4928752183914185 RMSE: 2.3436882\n",
      "Validation loss: 5.883825302124023 RMSE: 2.4256597\n",
      "141 3 3.058467149734497\n",
      "Validation loss: 2.166216492652893 RMSE: 1.4718072\n",
      "Validation loss: 2.9101377725601196 RMSE: 1.7059125\n",
      "Validation loss: 6.473652362823486 RMSE: 2.5443375\n",
      "144 2 1.248665452003479\n",
      "Validation loss: 3.6994166374206543 RMSE: 1.9233868\n",
      "Validation loss: 3.130277633666992 RMSE: 1.7692591\n",
      "Validation loss: 4.930642366409302 RMSE: 2.2205055\n",
      "147 1 1.0608155727386475\n",
      "Validation loss: 5.398394346237183 RMSE: 2.3234441\n",
      "Validation loss: 1.9192267656326294 RMSE: 1.3853614\n",
      "Validation loss: 2.368110775947571 RMSE: 1.5388666\n",
      "150 0 0.34353500604629517\n",
      "Validation loss: 3.953379511833191 RMSE: 1.9883107\n",
      "Validation loss: 2.4039742946624756 RMSE: 1.5504756\n",
      "152 16 8.636552810668945\n",
      "Validation loss: 2.7699525356292725 RMSE: 1.6643175\n",
      "Validation loss: 2.876649796962738 RMSE: 1.696069\n",
      "Validation loss: 6.884784698486328 RMSE: 2.6238875\n",
      "155 15 0.9985655546188354\n",
      "Validation loss: 1.9693673849105835 RMSE: 1.4033415\n",
      "Validation loss: 2.6073597073554993 RMSE: 1.6147323\n",
      "Validation loss: 2.4745023250579834 RMSE: 1.5730551\n",
      "158 14 1.2581570148468018\n",
      "Validation loss: 4.624174475669861 RMSE: 2.1503892\n",
      "Validation loss: 2.2568711042404175 RMSE: 1.5022886\n",
      "Validation loss: 3.3900887966156006 RMSE: 1.8412194\n",
      "161 13 0.8555150032043457\n",
      "Validation loss: 2.448672890663147 RMSE: 1.5648235\n",
      "Validation loss: 4.446964740753174 RMSE: 2.1087828\n",
      "Validation loss: 3.0658514499664307 RMSE: 1.7509575\n",
      "164 12 0.4280672073364258\n",
      "Validation loss: 2.7690131664276123 RMSE: 1.6640353\n",
      "Validation loss: 2.9017393589019775 RMSE: 1.7034489\n",
      "Validation loss: 2.7440223693847656 RMSE: 1.6565093\n",
      "167 11 4.516729354858398\n",
      "Validation loss: 16.870887279510498 RMSE: 4.1074195\n",
      "Validation loss: 4.072938561439514 RMSE: 2.0181525\n",
      "Validation loss: 3.5237984657287598 RMSE: 1.8771784\n",
      "170 10 3.036881923675537\n",
      "Validation loss: 3.036983370780945 RMSE: 1.7426944\n",
      "Validation loss: 2.44356369972229 RMSE: 1.5631903\n",
      "Validation loss: 3.3137908577919006 RMSE: 1.820382\n",
      "173 9 0.9306402206420898\n",
      "Validation loss: 3.2512301206588745 RMSE: 1.803117\n",
      "Validation loss: 2.7262980937957764 RMSE: 1.6511506\n",
      "Validation loss: 3.3111889362335205 RMSE: 1.819667\n",
      "176 8 1.9048376083374023\n",
      "Validation loss: 3.0413849353790283 RMSE: 1.7439568\n",
      "Validation loss: 2.903261661529541 RMSE: 1.703896\n",
      "Validation loss: 2.482684373855591 RMSE: 1.5756537\n",
      "179 7 1.1275672912597656\n",
      "Validation loss: 3.1045680046081543 RMSE: 1.7619786\n",
      "Validation loss: 2.8164854645729065 RMSE: 1.6782386\n",
      "Validation loss: 3.712785482406616 RMSE: 1.926859\n",
      "182 6 0.6586277484893799\n",
      "Validation loss: 10.226084232330322 RMSE: 3.197825\n",
      "Validation loss: 1.4486350417137146 RMSE: 1.2035925\n",
      "Validation loss: 2.521359086036682 RMSE: 1.5878788\n",
      "185 5 0.3939151465892792\n",
      "Validation loss: 2.095668315887451 RMSE: 1.4476424\n",
      "Validation loss: 2.631614923477173 RMSE: 1.6222253\n",
      "Validation loss: 2.4122458696365356 RMSE: 1.5531406\n",
      "188 4 0.7573904991149902\n",
      "Validation loss: 3.088388741016388 RMSE: 1.7573813\n",
      "Validation loss: 2.1823490858078003 RMSE: 1.4772775\n",
      "Validation loss: 2.7811315059661865 RMSE: 1.6676725\n",
      "191 3 0.6242269277572632\n",
      "Validation loss: 2.3720672130584717 RMSE: 1.5401515\n",
      "Validation loss: 1.6066965460777283 RMSE: 1.2675554\n",
      "Validation loss: 1.9161116480827332 RMSE: 1.3842369\n",
      "194 2 0.40893393754959106\n",
      "Validation loss: 2.829254627227783 RMSE: 1.682039\n",
      "Validation loss: 3.5460554361343384 RMSE: 1.883097\n",
      "Validation loss: 3.258698523044586 RMSE: 1.8051865\n",
      "197 1 0.43936824798583984\n",
      "Validation loss: 2.2613322734832764 RMSE: 1.5037729\n",
      "Validation loss: 3.0301384925842285 RMSE: 1.7407295\n",
      "Validation loss: 2.5140167474746704 RMSE: 1.5855651\n",
      "Loaded trained model with success.\n",
      "Test loss: 1.296982854604721 Test RMSE: 1.1388515\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:0', 'task_name': 'freesolv', 'model_type': 'gin', 'model': {'num_layer': 5, 'emb_dim': 300, 'feat_dim': 300, 'drop_ratio': 0.3, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'random', 'seed': 779, 'task': 'regression', 'data_path': 'data/freesolv/freesolv.csv', 'target': ['expt']}}\n",
      "Running on: cuda:0\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 25.34465789794922\n",
      "Validation loss: 12.407707452774048 RMSE: 3.5224576\n",
      "Validation loss: 10.950044393539429 RMSE: 3.3090851\n",
      "2 16 9.865896224975586\n",
      "Validation loss: 7.281047344207764 RMSE: 2.6983416\n",
      "Validation loss: 3.1205453872680664 RMSE: 1.7665067\n",
      "Validation loss: 8.189875364303589 RMSE: 2.8617957\n",
      "5 15 5.4235687255859375\n",
      "Validation loss: 11.925300598144531 RMSE: 3.4533033\n",
      "Validation loss: 11.280821323394775 RMSE: 3.3586934\n",
      "Validation loss: 9.048739194869995 RMSE: 3.0081117\n",
      "8 14 3.725132942199707\n",
      "Validation loss: 8.653762340545654 RMSE: 2.9417279\n",
      "Validation loss: 8.24027967453003 RMSE: 2.8705888\n",
      "Validation loss: 13.310264706611633 RMSE: 3.6483235\n",
      "11 13 3.842151165008545\n",
      "Validation loss: 13.993754386901855 RMSE: 3.7408228\n",
      "Validation loss: 40.17470169067383 RMSE: 6.3383517\n",
      "Validation loss: 5.9025468826293945 RMSE: 2.429516\n",
      "14 12 2.875326633453369\n",
      "Validation loss: 15.69568932056427 RMSE: 3.9617782\n",
      "Validation loss: 12.538475036621094 RMSE: 3.5409706\n",
      "Validation loss: 2.867928624153137 RMSE: 1.6934958\n",
      "17 11 1.5011425018310547\n",
      "Validation loss: 11.986506223678589 RMSE: 3.4621537\n",
      "Validation loss: 19.080206632614136 RMSE: 4.368089\n",
      "Validation loss: 9.991400957107544 RMSE: 3.1609185\n",
      "20 10 2.6338210105895996\n",
      "Validation loss: 7.560392141342163 RMSE: 2.7496164\n",
      "Validation loss: 6.972215056419373 RMSE: 2.640495\n",
      "Validation loss: 20.8851158618927 RMSE: 4.5700235\n",
      "23 9 2.8116395473480225\n",
      "Validation loss: 8.898826122283936 RMSE: 2.9830902\n",
      "Validation loss: 13.516381740570068 RMSE: 3.6764634\n",
      "Validation loss: 8.907856225967407 RMSE: 2.9846036\n",
      "26 8 3.271012306213379\n",
      "Validation loss: 4.919792026281357 RMSE: 2.21806\n",
      "Validation loss: 15.52579927444458 RMSE: 3.9402792\n",
      "Validation loss: 16.0887770652771 RMSE: 4.0110817\n",
      "29 7 2.6327834129333496\n",
      "Validation loss: 9.56667709350586 RMSE: 3.093005\n",
      "Validation loss: 18.736005783081055 RMSE: 4.328511\n",
      "Validation loss: 9.579350233078003 RMSE: 3.0950525\n",
      "32 6 7.236246109008789\n",
      "Validation loss: 13.506068885326385 RMSE: 3.6750605\n",
      "Validation loss: 7.609158158302307 RMSE: 2.7584705\n",
      "Validation loss: 9.251606613397598 RMSE: 3.0416455\n",
      "35 5 2.20535945892334\n",
      "Validation loss: 9.817100524902344 RMSE: 3.1332252\n",
      "Validation loss: 5.903929233551025 RMSE: 2.4298003\n",
      "Validation loss: 8.708036184310913 RMSE: 2.9509382\n",
      "38 4 2.9949190616607666\n",
      "Validation loss: 7.169894695281982 RMSE: 2.6776655\n",
      "Validation loss: 12.196779012680054 RMSE: 3.4923887\n",
      "Validation loss: 17.020306825637817 RMSE: 4.1255674\n",
      "41 3 0.8254646062850952\n",
      "Validation loss: 11.572498798370361 RMSE: 3.4018376\n",
      "Validation loss: 10.738515615463257 RMSE: 3.2769675\n",
      "Validation loss: 12.545060276985168 RMSE: 3.5419002\n",
      "44 2 1.2029037475585938\n",
      "Validation loss: 15.750840663909912 RMSE: 3.9687326\n",
      "Validation loss: 23.04930591583252 RMSE: 4.800969\n",
      "Validation loss: 5.263496160507202 RMSE: 2.2942312\n",
      "47 1 2.828524112701416\n",
      "Validation loss: 5.428640604019165 RMSE: 2.3299444\n",
      "Validation loss: 8.358381688594818 RMSE: 2.8910866\n",
      "Validation loss: 12.04343867301941 RMSE: 3.4703658\n",
      "50 0 3.5597305297851562\n",
      "Validation loss: 4.632461130619049 RMSE: 2.1523154\n",
      "Validation loss: 10.834916353225708 RMSE: 3.2916436\n",
      "52 16 0.014107247814536095\n",
      "Validation loss: 7.682161629199982 RMSE: 2.7716713\n",
      "Validation loss: 9.323116421699524 RMSE: 3.0533776\n",
      "Validation loss: 10.95346474647522 RMSE: 3.3096018\n",
      "55 15 1.3543438911437988\n",
      "Validation loss: 10.487406373023987 RMSE: 3.2384264\n",
      "Validation loss: 12.428297877311707 RMSE: 3.5253794\n",
      "Validation loss: 17.106172800064087 RMSE: 4.135961\n",
      "58 14 1.1939666271209717\n",
      "Validation loss: 3.819244623184204 RMSE: 1.9542888\n",
      "Validation loss: 8.40926730632782 RMSE: 2.8998737\n",
      "Validation loss: 4.636030197143555 RMSE: 2.1531446\n",
      "61 13 0.6500518321990967\n",
      "Validation loss: 11.045481145381927 RMSE: 3.3234744\n",
      "Validation loss: 12.095356941223145 RMSE: 3.477838\n",
      "Validation loss: 2.532334089279175 RMSE: 1.591331\n",
      "64 12 1.9962137937545776\n",
      "Validation loss: 12.296108186244965 RMSE: 3.5065806\n",
      "Validation loss: 9.195169925689697 RMSE: 3.0323539\n",
      "Validation loss: 7.6196160316467285 RMSE: 2.7603652\n",
      "67 11 0.8398916721343994\n",
      "Validation loss: 11.689944744110107 RMSE: 3.419056\n",
      "Validation loss: 11.393112003803253 RMSE: 3.3753684\n",
      "Validation loss: 8.976502358913422 RMSE: 2.996081\n",
      "70 10 0.8679982423782349\n",
      "Validation loss: 4.520162433385849 RMSE: 2.1260674\n",
      "Validation loss: 13.051826000213623 RMSE: 3.612731\n",
      "Validation loss: 5.0417174100875854 RMSE: 2.2453768\n",
      "73 9 3.008023262023926\n",
      "Validation loss: 13.149206697940826 RMSE: 3.6261835\n",
      "Validation loss: 6.895186543464661 RMSE: 2.6258693\n",
      "Validation loss: 10.904086887836456 RMSE: 3.3021333\n",
      "76 8 3.3582215309143066\n",
      "Validation loss: 6.922343492507935 RMSE: 2.6310346\n",
      "Validation loss: 8.562838077545166 RMSE: 2.9262328\n",
      "Validation loss: 9.650052189826965 RMSE: 3.1064532\n",
      "79 7 0.5592973232269287\n",
      "Validation loss: 5.833475112915039 RMSE: 2.415259\n",
      "Validation loss: 9.370941936969757 RMSE: 3.0611997\n",
      "Validation loss: 2.1663047075271606 RMSE: 1.471837\n",
      "82 6 2.181236505508423\n",
      "Validation loss: 8.572536408901215 RMSE: 2.9278898\n",
      "Validation loss: 14.920010566711426 RMSE: 3.862643\n",
      "Validation loss: 7.8420023918151855 RMSE: 2.800358\n",
      "85 5 1.2096376419067383\n",
      "Validation loss: 7.246490836143494 RMSE: 2.6919308\n",
      "Validation loss: 6.272541761398315 RMSE: 2.5045042\n",
      "Validation loss: 2.543134033679962 RMSE: 1.5947206\n",
      "88 4 1.7187529802322388\n",
      "Validation loss: 6.0916712284088135 RMSE: 2.4681315\n",
      "Validation loss: 9.425927877426147 RMSE: 3.0701678\n",
      "Validation loss: 11.839131355285645 RMSE: 3.4408035\n",
      "91 3 1.6918084621429443\n",
      "Validation loss: 4.942620515823364 RMSE: 2.2232006\n",
      "Validation loss: 8.239064931869507 RMSE: 2.870377\n",
      "Validation loss: 6.177694797515869 RMSE: 2.485497\n",
      "94 2 3.3657453060150146\n",
      "Validation loss: 5.081050276756287 RMSE: 2.254119\n",
      "Validation loss: 6.333524644374847 RMSE: 2.5166495\n",
      "Validation loss: 4.724359154701233 RMSE: 2.173559\n",
      "97 1 1.5374150276184082\n",
      "Validation loss: 10.146036028862 RMSE: 3.1852846\n",
      "Validation loss: 3.6952667236328125 RMSE: 1.9223077\n",
      "Validation loss: 4.303432643413544 RMSE: 2.0744717\n",
      "100 0 1.144290804862976\n",
      "Validation loss: 3.2733246088027954 RMSE: 1.8092335\n",
      "Validation loss: 6.557059973478317 RMSE: 2.5606756\n",
      "102 16 26.50275993347168\n",
      "Validation loss: 4.829246401786804 RMSE: 2.1975548\n",
      "Validation loss: 4.797077119350433 RMSE: 2.190223\n",
      "Validation loss: 1.6351289749145508 RMSE: 1.2787216\n",
      "105 15 1.4952147006988525\n",
      "Validation loss: 16.360883355140686 RMSE: 4.044859\n",
      "Validation loss: 13.22340178489685 RMSE: 3.6363995\n",
      "Validation loss: 2.176056385040283 RMSE: 1.4751463\n",
      "108 14 1.1397364139556885\n",
      "Validation loss: 2.25118088722229 RMSE: 1.5003936\n",
      "Validation loss: 2.4837217330932617 RMSE: 1.5759828\n",
      "Validation loss: 2.353823661804199 RMSE: 1.5342176\n",
      "111 13 1.7176434993743896\n",
      "Validation loss: 5.941526770591736 RMSE: 2.4375246\n",
      "Validation loss: 7.349184036254883 RMSE: 2.710938\n",
      "Validation loss: 5.214462041854858 RMSE: 2.2835197\n",
      "114 12 1.4763509035110474\n",
      "Validation loss: 3.8320605754852295 RMSE: 1.9575648\n",
      "Validation loss: 4.372048258781433 RMSE: 2.0909443\n",
      "Validation loss: 4.4103734493255615 RMSE: 2.100089\n",
      "117 11 1.1774027347564697\n",
      "Validation loss: 8.59807574748993 RMSE: 2.9322476\n",
      "Validation loss: 3.3164197206497192 RMSE: 1.8211039\n",
      "Validation loss: 7.782244324684143 RMSE: 2.7896674\n",
      "120 10 1.4214004278182983\n",
      "Validation loss: 2.061392664909363 RMSE: 1.4357553\n",
      "Validation loss: 3.192260265350342 RMSE: 1.7866898\n",
      "Validation loss: 2.5014848709106445 RMSE: 1.5816084\n",
      "123 9 1.549255132675171\n",
      "Validation loss: 7.7422440350055695 RMSE: 2.7824883\n",
      "Validation loss: 4.844755828380585 RMSE: 2.201081\n",
      "Validation loss: 2.65641325712204 RMSE: 1.6298505\n",
      "126 8 1.752204418182373\n",
      "Validation loss: 7.358728766441345 RMSE: 2.7126977\n",
      "Validation loss: 5.25514554977417 RMSE: 2.2924106\n",
      "Validation loss: 3.194562792778015 RMSE: 1.787334\n",
      "129 7 1.192499041557312\n",
      "Validation loss: 2.7923807501792908 RMSE: 1.6710418\n",
      "Validation loss: 13.555424094200134 RMSE: 3.6817691\n",
      "Validation loss: 3.234817624092102 RMSE: 1.7985598\n",
      "132 6 1.3933422565460205\n",
      "Validation loss: 3.969472259283066 RMSE: 1.9923533\n",
      "Validation loss: 4.381146192550659 RMSE: 2.093119\n",
      "Validation loss: 5.872615694999695 RMSE: 2.423348\n",
      "135 5 1.9760018587112427\n",
      "Validation loss: 3.626494348049164 RMSE: 1.9043357\n",
      "Validation loss: 3.935815393924713 RMSE: 1.9838895\n",
      "Validation loss: 8.364284634590149 RMSE: 2.8921072\n",
      "138 4 2.033379316329956\n",
      "Validation loss: 6.116712212562561 RMSE: 2.4731987\n",
      "Validation loss: 4.067933976650238 RMSE: 2.0169122\n",
      "Validation loss: 5.310504674911499 RMSE: 2.3044527\n",
      "141 3 1.7096061706542969\n",
      "Validation loss: 1.2207102179527283 RMSE: 1.1048576\n",
      "Validation loss: 4.561801373958588 RMSE: 2.135837\n",
      "Validation loss: 2.648259162902832 RMSE: 1.6273472\n",
      "144 2 0.7973589897155762\n",
      "Validation loss: 2.383908271789551 RMSE: 1.5439911\n",
      "Validation loss: 5.260348558425903 RMSE: 2.293545\n",
      "Validation loss: 4.5857884883880615 RMSE: 2.1414452\n",
      "147 1 0.9492155313491821\n",
      "Validation loss: 6.9498714208602905 RMSE: 2.6362607\n",
      "Validation loss: 3.611623227596283 RMSE: 1.9004271\n",
      "Validation loss: 2.5452182292938232 RMSE: 1.595374\n",
      "150 0 0.3967480957508087\n",
      "Validation loss: 3.5009982585906982 RMSE: 1.8710954\n",
      "Validation loss: 3.1697704792022705 RMSE: 1.7803854\n",
      "152 16 28.2015380859375\n",
      "Validation loss: 7.195996642112732 RMSE: 2.682536\n",
      "Validation loss: 2.9755430817604065 RMSE: 1.7249761\n",
      "Validation loss: 1.8819925785064697 RMSE: 1.3718574\n",
      "155 15 1.3871794939041138\n",
      "Validation loss: 9.145497441291809 RMSE: 3.0241523\n",
      "Validation loss: 2.7685552835464478 RMSE: 1.6638978\n",
      "Validation loss: 9.539920210838318 RMSE: 3.088676\n",
      "158 14 0.4960346817970276\n",
      "Validation loss: 1.184550404548645 RMSE: 1.0883704\n",
      "Validation loss: 4.768923878669739 RMSE: 2.1837866\n",
      "Validation loss: 4.139249265193939 RMSE: 2.0345144\n",
      "161 13 2.6640396118164062\n",
      "Validation loss: 3.756318509578705 RMSE: 1.9381224\n",
      "Validation loss: 2.733267068862915 RMSE: 1.6532598\n"
     ]
    }
   ],
   "source": [
    "seeds = list(range(777,782))\n",
    "# datasets = [\"bace\",  \"bbbp\", \"tox21\", \"toxcast\", \"sider\",  ]\n",
    "datasets = [\"freeSolv\",  \"lipo\", \"esol\", \"qm7\", \"bace\",  \"bbbp\", ]\n",
    "\n",
    "for dataset in datasets:\n",
    "    for seed in seeds: \n",
    "        !python finetuneRecon.py \\\n",
    "        --task_name {dataset} \\\n",
    "        --splitting scaffold \\\n",
    "        --seed {seed} \\\n",
    "        --alpha 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6764b07a-c44b-44d3-ab6b-161100c0d53b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:0', 'task_name': 'esol', 'model_type': 'gin', 'model': {'num_layer': 5, 'emb_dim': 300, 'feat_dim': 300, 'drop_ratio': 0.3, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 751, 'task': 'regression', 'data_path': 'data/esol/esol.csv', 'target': ['measured log solubility in mols per litre']}}\n",
      "Running on: cuda:0\n",
      "1127\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/1127\n",
      "Generating scaffold 1000/1127\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "tensor([[ 0.0806, -0.0410, -0.0872,  ..., -0.0789, -0.0887, -0.1107],\n",
      "        [ 0.0313,  0.0164,  0.1077,  ..., -0.0785,  0.1171,  0.1046],\n",
      "        [-0.0002, -0.0291, -0.0762,  ...,  0.0110,  0.0630,  0.0511],\n",
      "        ...,\n",
      "        [ 0.0313,  0.0164,  0.1077,  ..., -0.0785,  0.1171,  0.1046],\n",
      "        [-0.0175,  0.1190, -0.0096,  ...,  0.0821,  0.0621,  0.0488],\n",
      "        [ 0.0313,  0.0164,  0.1077,  ..., -0.0785,  0.1171,  0.1046]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "0 0 8.955981254577637\n",
      "tensor([[ 0.0312,  0.0165,  0.1078,  ..., -0.0786,  0.1172,  0.1047],\n",
      "        [ 0.0312,  0.0165,  0.1078,  ..., -0.0786,  0.1172,  0.1047],\n",
      "        [ 0.0312,  0.0165,  0.1078,  ..., -0.0786,  0.1172,  0.1047],\n",
      "        ...,\n",
      "        [ 0.0312,  0.0165,  0.1078,  ..., -0.0786,  0.1172,  0.1047],\n",
      "        [ 0.0312,  0.0165,  0.1078,  ..., -0.0786,  0.1172,  0.1047],\n",
      "        [ 0.0312,  0.0165,  0.1078,  ..., -0.0786,  0.1172,  0.1047]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0004, -0.0289, -0.0764,  ...,  0.0108,  0.0629,  0.0509],\n",
      "        [-0.0004, -0.0289, -0.0764,  ...,  0.0108,  0.0629,  0.0509],\n",
      "        [-0.0004, -0.0289, -0.0764,  ...,  0.0108,  0.0629,  0.0509],\n",
      "        ...,\n",
      "        [ 0.0311,  0.0166,  0.1079,  ..., -0.0787,  0.1173,  0.1048],\n",
      "        [ 0.0311,  0.0166,  0.1079,  ..., -0.0787,  0.1173,  0.1048],\n",
      "        [ 0.0311,  0.0166,  0.1079,  ..., -0.0787,  0.1173,  0.1048]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0004, -0.0288, -0.0764,  ...,  0.0108,  0.0629,  0.0509],\n",
      "        [-0.0004, -0.0288, -0.0764,  ...,  0.0108,  0.0629,  0.0509],\n",
      "        [ 0.0311,  0.0167,  0.1079,  ..., -0.0788,  0.1174,  0.1048],\n",
      "        ...,\n",
      "        [ 0.0187, -0.0968, -0.0305,  ...,  0.0381, -0.0655, -0.0279],\n",
      "        [ 0.0311,  0.0167,  0.1079,  ..., -0.0788,  0.1174,  0.1048],\n",
      "        [ 0.0311,  0.0167,  0.1079,  ..., -0.0788,  0.1174,  0.1048]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0310,  0.0168,  0.1079,  ..., -0.0789,  0.1174,  0.1049],\n",
      "        [-0.0003, -0.0287, -0.0764,  ...,  0.0107,  0.0629,  0.0509],\n",
      "        [-0.0003, -0.0287, -0.0764,  ...,  0.0107,  0.0629,  0.0509],\n",
      "        ...,\n",
      "        [ 0.0310,  0.0168,  0.1079,  ..., -0.0789,  0.1174,  0.1049],\n",
      "        [ 0.0310,  0.0168,  0.1079,  ..., -0.0789,  0.1174,  0.1049],\n",
      "        [ 0.0310,  0.0168,  0.1079,  ..., -0.0789,  0.1174,  0.1049]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0309,  0.0168,  0.1080,  ..., -0.0789,  0.1175,  0.1050],\n",
      "        [ 0.0309,  0.0168,  0.1080,  ..., -0.0789,  0.1175,  0.1050],\n",
      "        [-0.0961, -0.0527,  0.0569,  ...,  0.0218, -0.0656,  0.0982],\n",
      "        ...,\n",
      "        [ 0.0309,  0.0168,  0.1080,  ..., -0.0789,  0.1175,  0.1050],\n",
      "        [-0.0961, -0.0527,  0.0569,  ...,  0.0218, -0.0656,  0.0982],\n",
      "        [ 0.0065, -0.1107,  0.0619,  ...,  0.0173,  0.0921, -0.0094]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0065, -0.1107,  0.0619,  ...,  0.0174,  0.0921, -0.0095],\n",
      "        [ 0.0065, -0.1107,  0.0619,  ...,  0.0174,  0.0921, -0.0095],\n",
      "        [ 0.0309,  0.0169,  0.1080,  ..., -0.0790,  0.1176,  0.1050],\n",
      "        ...,\n",
      "        [-0.0961, -0.0528,  0.0569,  ...,  0.0219, -0.0655,  0.0981],\n",
      "        [ 0.0309,  0.0169,  0.1080,  ..., -0.0790,  0.1176,  0.1050],\n",
      "        [ 0.0065, -0.1107,  0.0619,  ...,  0.0174,  0.0921, -0.0095]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0002, -0.0285, -0.0763,  ...,  0.0108,  0.0630,  0.0507],\n",
      "        [-0.0002, -0.0285, -0.0763,  ...,  0.0108,  0.0630,  0.0507],\n",
      "        [ 0.0309,  0.0169,  0.1081,  ..., -0.0791,  0.1177,  0.1051],\n",
      "        ...,\n",
      "        [ 0.0309,  0.0169,  0.1081,  ..., -0.0791,  0.1177,  0.1051],\n",
      "        [ 0.0309,  0.0169,  0.1081,  ..., -0.0791,  0.1177,  0.1051],\n",
      "        [-0.0123,  0.1087, -0.0120,  ...,  0.0790,  0.0517, -0.0861]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0961, -0.0528,  0.0569,  ...,  0.0220, -0.0653,  0.0979],\n",
      "        [-0.0961, -0.0528,  0.0569,  ...,  0.0220, -0.0653,  0.0979],\n",
      "        [ 0.0804, -0.0412, -0.0873,  ..., -0.0789, -0.0891, -0.1105],\n",
      "        ...,\n",
      "        [ 0.0309,  0.0170,  0.1081,  ..., -0.0791,  0.1178,  0.1052],\n",
      "        [ 0.0309,  0.0170,  0.1081,  ..., -0.0791,  0.1178,  0.1052],\n",
      "        [-0.0961, -0.0528,  0.0569,  ...,  0.0220, -0.0653,  0.0979]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0002, -0.0284, -0.0764,  ...,  0.0108,  0.0631,  0.0506],\n",
      "        [-0.0002, -0.0284, -0.0764,  ...,  0.0108,  0.0631,  0.0506],\n",
      "        [ 0.0309,  0.0170,  0.1082,  ..., -0.0792,  0.1179,  0.1052],\n",
      "        ...,\n",
      "        [ 0.0309,  0.0170,  0.1082,  ..., -0.0792,  0.1179,  0.1052],\n",
      "        [ 0.0309,  0.0170,  0.1082,  ..., -0.0792,  0.1179,  0.1052],\n",
      "        [ 0.0309,  0.0170,  0.1082,  ..., -0.0792,  0.1179,  0.1052]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0310,  0.0170,  0.1083,  ..., -0.0792,  0.1179,  0.1053],\n",
      "        [-0.0002, -0.0284, -0.0765,  ...,  0.0108,  0.0631,  0.0505],\n",
      "        [-0.0002, -0.0284, -0.0765,  ...,  0.0108,  0.0631,  0.0505],\n",
      "        ...,\n",
      "        [ 0.0310,  0.0170,  0.1083,  ..., -0.0792,  0.1179,  0.1053],\n",
      "        [ 0.0310,  0.0170,  0.1083,  ..., -0.0792,  0.1179,  0.1053],\n",
      "        [ 0.0310,  0.0170,  0.1083,  ..., -0.0792,  0.1179,  0.1053]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0310,  0.0171,  0.1083,  ..., -0.0793,  0.1179,  0.1054],\n",
      "        [ 0.0070, -0.1110,  0.0617,  ...,  0.0178,  0.0925, -0.0099],\n",
      "        [ 0.0310,  0.0171,  0.1083,  ..., -0.0793,  0.1179,  0.1054],\n",
      "        ...,\n",
      "        [ 0.0310,  0.0171,  0.1083,  ..., -0.0793,  0.1179,  0.1054],\n",
      "        [ 0.0310,  0.0171,  0.1083,  ..., -0.0793,  0.1179,  0.1054],\n",
      "        [-0.0961, -0.0529,  0.0568,  ...,  0.0223, -0.0651,  0.0977]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0309,  0.0171,  0.1084,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [ 0.0309,  0.0171,  0.1084,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [-0.0961, -0.0529,  0.0568,  ...,  0.0223, -0.0650,  0.0976],\n",
      "        ...,\n",
      "        [ 0.0309,  0.0171,  0.1084,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [ 0.0309,  0.0171,  0.1084,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [ 0.0309,  0.0171,  0.1084,  ..., -0.0794,  0.1179,  0.1054]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0961, -0.0529,  0.0568,  ...,  0.0224, -0.0649,  0.0975],\n",
      "        [-0.0003, -0.0284, -0.0766,  ...,  0.0107,  0.0633,  0.0504],\n",
      "        [-0.0003, -0.0284, -0.0766,  ...,  0.0107,  0.0633,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0309,  0.0171,  0.1085,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [ 0.0071, -0.1112,  0.0617,  ...,  0.0179,  0.0926, -0.0100],\n",
      "        [ 0.0309,  0.0171,  0.1085,  ..., -0.0794,  0.1179,  0.1054]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0309,  0.0171,  0.1085,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [ 0.0309,  0.0171,  0.1085,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [ 0.0309,  0.0171,  0.1085,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        ...,\n",
      "        [ 0.0071, -0.1112,  0.0616,  ...,  0.0180,  0.0927, -0.0100],\n",
      "        [ 0.0309,  0.0171,  0.1085,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [ 0.0309,  0.0171,  0.1085,  ..., -0.0794,  0.1179,  0.1054]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0309,  0.0171,  0.1086,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [ 0.0309,  0.0171,  0.1086,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [ 0.0309,  0.0171,  0.1086,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        ...,\n",
      "        [ 0.0309,  0.0171,  0.1086,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [-0.0962, -0.0529,  0.0568,  ...,  0.0225, -0.0647,  0.0974],\n",
      "        [-0.0127,  0.1084, -0.0120,  ...,  0.0788,  0.0512, -0.0857]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0309,  0.0171,  0.1087,  ..., -0.0795,  0.1179,  0.1054],\n",
      "        [-0.0002, -0.0284, -0.0766,  ...,  0.0107,  0.0632,  0.0503],\n",
      "        [-0.0002, -0.0284, -0.0766,  ...,  0.0107,  0.0632,  0.0503],\n",
      "        ...,\n",
      "        [ 0.0309,  0.0171,  0.1087,  ..., -0.0795,  0.1179,  0.1054],\n",
      "        [ 0.0309,  0.0171,  0.1087,  ..., -0.0795,  0.1179,  0.1054],\n",
      "        [ 0.0309,  0.0171,  0.1087,  ..., -0.0795,  0.1179,  0.1054]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 3.0917e-02,  1.7134e-02,  1.0878e-01,  ..., -7.9471e-02,\n",
      "          1.1795e-01,  1.0532e-01],\n",
      "        [-1.1599e-04, -2.8463e-02, -7.6550e-02,  ...,  1.0742e-02,\n",
      "          6.3165e-02,  5.0226e-02],\n",
      "        [-1.1599e-04, -2.8463e-02, -7.6550e-02,  ...,  1.0742e-02,\n",
      "          6.3165e-02,  5.0226e-02],\n",
      "        ...,\n",
      "        [ 3.0917e-02,  1.7134e-02,  1.0878e-01,  ..., -7.9471e-02,\n",
      "          1.1795e-01,  1.0532e-01],\n",
      "        [ 3.0917e-02,  1.7134e-02,  1.0878e-01,  ..., -7.9471e-02,\n",
      "          1.1795e-01,  1.0532e-01],\n",
      "        [ 3.0917e-02,  1.7134e-02,  1.0878e-01,  ..., -7.9471e-02,\n",
      "          1.1795e-01,  1.0532e-01]], device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0309,  0.0172,  0.1088,  ..., -0.0795,  0.1180,  0.1053],\n",
      "        [ 0.0309,  0.0172,  0.1088,  ..., -0.0795,  0.1180,  0.1053],\n",
      "        [ 0.0309,  0.0172,  0.1088,  ..., -0.0795,  0.1180,  0.1053],\n",
      "        ...,\n",
      "        [ 0.0074, -0.1113,  0.0618,  ...,  0.0180,  0.0929, -0.0100],\n",
      "        [ 0.0309,  0.0172,  0.1088,  ..., -0.0795,  0.1180,  0.1053],\n",
      "        [-0.0964, -0.0527,  0.0568,  ...,  0.0224, -0.0647,  0.0975]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-9.6424e-02, -5.2667e-02,  5.6764e-02,  ...,  2.2343e-02,\n",
      "         -6.4659e-02,  9.7490e-02],\n",
      "        [ 3.0807e-02,  1.7243e-02,  1.0880e-01,  ..., -7.9564e-02,\n",
      "          1.1803e-01,  1.0522e-01],\n",
      "        [-1.7022e-05, -2.8596e-02, -7.6470e-02,  ...,  1.0681e-02,\n",
      "          6.3049e-02,  5.0292e-02],\n",
      "        ...,\n",
      "        [ 3.0807e-02,  1.7243e-02,  1.0880e-01,  ..., -7.9564e-02,\n",
      "          1.1803e-01,  1.0522e-01],\n",
      "        [-1.7267e-02,  1.1858e-01, -1.0125e-02,  ...,  8.2536e-02,\n",
      "          6.1876e-02,  4.8307e-02],\n",
      "        [ 3.0807e-02,  1.7243e-02,  1.0880e-01,  ..., -7.9564e-02,\n",
      "          1.1803e-01,  1.0522e-01]], device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 3.0753e-02,  1.7299e-02,  1.0883e-01,  ..., -7.9583e-02,\n",
      "          1.1807e-01,  1.0516e-01],\n",
      "        [ 3.0753e-02,  1.7299e-02,  1.0883e-01,  ..., -7.9583e-02,\n",
      "          1.1807e-01,  1.0516e-01],\n",
      "        [ 4.1295e-05, -2.8627e-02, -7.6411e-02,  ...,  1.0639e-02,\n",
      "          6.3006e-02,  5.0348e-02],\n",
      "        ...,\n",
      "        [ 3.0753e-02,  1.7299e-02,  1.0883e-01,  ..., -7.9583e-02,\n",
      "          1.1807e-01,  1.0516e-01],\n",
      "        [-1.2426e-02,  1.0806e-01, -1.2240e-02,  ...,  7.9107e-02,\n",
      "          5.1456e-02, -8.5975e-02],\n",
      "        [ 3.0753e-02,  1.7299e-02,  1.0883e-01,  ..., -7.9583e-02,\n",
      "          1.1807e-01,  1.0516e-01]], device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0307,  0.0173,  0.1089,  ..., -0.0796,  0.1181,  0.1051],\n",
      "        [ 0.0307,  0.0173,  0.1089,  ..., -0.0796,  0.1181,  0.1051],\n",
      "        [ 0.0307,  0.0173,  0.1089,  ..., -0.0796,  0.1181,  0.1051],\n",
      "        ...,\n",
      "        [ 0.0076, -0.1114,  0.0619,  ...,  0.0181,  0.0930, -0.0100],\n",
      "        [ 0.0307,  0.0173,  0.1089,  ..., -0.0796,  0.1181,  0.1051],\n",
      "        [ 0.0307,  0.0173,  0.1089,  ..., -0.0796,  0.1181,  0.1051]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0307,  0.0174,  0.1089,  ..., -0.0796,  0.1181,  0.1051],\n",
      "        [ 0.0808, -0.0411, -0.0881,  ..., -0.0790, -0.0893, -0.1108],\n",
      "        [ 0.0076, -0.1113,  0.0619,  ...,  0.0181,  0.0930, -0.0100],\n",
      "        ...,\n",
      "        [ 0.0307,  0.0174,  0.1089,  ..., -0.0796,  0.1181,  0.1051],\n",
      "        [-0.0966, -0.0525,  0.0567,  ...,  0.0222, -0.0646,  0.0975],\n",
      "        [-0.0966, -0.0525,  0.0567,  ...,  0.0222, -0.0646,  0.0975]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0001, -0.0288, -0.0764,  ...,  0.0106,  0.0630,  0.0504],\n",
      "        [ 0.0898,  0.0321,  0.0691,  ...,  0.0426, -0.0070,  0.0773],\n",
      "        [ 0.0001, -0.0288, -0.0764,  ...,  0.0106,  0.0630,  0.0504],\n",
      "        ...,\n",
      "        [-0.0967, -0.0524,  0.0567,  ...,  0.0222, -0.0646,  0.0975],\n",
      "        [ 0.0307,  0.0174,  0.1089,  ..., -0.0797,  0.1182,  0.1051],\n",
      "        [-0.0967, -0.0524,  0.0567,  ...,  0.0222, -0.0646,  0.0975]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0968, -0.0523,  0.0567,  ...,  0.0222, -0.0646,  0.0974],\n",
      "        [ 0.0001, -0.0288, -0.0764,  ...,  0.0106,  0.0630,  0.0504],\n",
      "        [ 0.0001, -0.0288, -0.0764,  ...,  0.0106,  0.0630,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0307,  0.0174,  0.1090,  ..., -0.0797,  0.1182,  0.1052],\n",
      "        [-0.0968, -0.0523,  0.0567,  ...,  0.0222, -0.0646,  0.0974],\n",
      "        [ 0.0307,  0.0174,  0.1090,  ..., -0.0797,  0.1182,  0.1052]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0306,  0.0174,  0.1090,  ..., -0.0798,  0.1182,  0.1052],\n",
      "        [ 0.0001, -0.0289, -0.0764,  ...,  0.0106,  0.0630,  0.0504],\n",
      "        [ 0.0001, -0.0289, -0.0764,  ...,  0.0106,  0.0630,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0306,  0.0174,  0.1090,  ..., -0.0798,  0.1182,  0.1052],\n",
      "        [-0.0968, -0.0523,  0.0566,  ...,  0.0222, -0.0646,  0.0974],\n",
      "        [-0.0122,  0.1078, -0.0123,  ...,  0.0793,  0.0515, -0.0860]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0122,  0.1078, -0.0123,  ...,  0.0793,  0.0516, -0.0860],\n",
      "        [ 0.0306,  0.0174,  0.1091,  ..., -0.0798,  0.1182,  0.1053],\n",
      "        [ 0.0306,  0.0174,  0.1091,  ..., -0.0798,  0.1182,  0.1053],\n",
      "        ...,\n",
      "        [ 0.0306,  0.0174,  0.1091,  ..., -0.0798,  0.1182,  0.1053],\n",
      "        [ 0.0306,  0.0174,  0.1091,  ..., -0.0798,  0.1182,  0.1053],\n",
      "        [ 0.0306,  0.0174,  0.1091,  ..., -0.0798,  0.1182,  0.1053]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0306,  0.0174,  0.1091,  ..., -0.0799,  0.1182,  0.1053],\n",
      "        [-0.0121,  0.1078, -0.0122,  ...,  0.0793,  0.0516, -0.0860],\n",
      "        [ 0.0306,  0.0174,  0.1091,  ..., -0.0799,  0.1182,  0.1053],\n",
      "        ...,\n",
      "        [ 0.0306,  0.0174,  0.1091,  ..., -0.0799,  0.1182,  0.1053],\n",
      "        [ 0.0306,  0.0174,  0.1091,  ..., -0.0799,  0.1182,  0.1053],\n",
      "        [-0.0170,  0.1189, -0.0102,  ...,  0.0827,  0.0622,  0.0487]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0121,  0.1077, -0.0122,  ...,  0.0793,  0.0516, -0.0860],\n",
      "        [ 0.0305,  0.0174,  0.1092,  ..., -0.0799,  0.1182,  0.1053],\n",
      "        [ 0.0305,  0.0174,  0.1092,  ..., -0.0799,  0.1182,  0.1053],\n",
      "        ...,\n",
      "        [ 0.0305,  0.0174,  0.1092,  ..., -0.0799,  0.1182,  0.1053],\n",
      "        [ 0.0001, -0.0291, -0.0765,  ...,  0.0107,  0.0630,  0.0504],\n",
      "        [ 0.0001, -0.0291, -0.0765,  ...,  0.0107,  0.0630,  0.0504]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Validation loss: 2.2496231530619935 RMSE: 1.4998744\n",
      "tensor([[ 0.0305,  0.0174,  0.1092,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [-0.0970, -0.0521,  0.0565,  ...,  0.0224, -0.0646,  0.0973],\n",
      "        [ 0.0001, -0.0292, -0.0766,  ...,  0.0107,  0.0630,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0305,  0.0174,  0.1092,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [ 0.0305,  0.0174,  0.1092,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [ 0.0305,  0.0174,  0.1092,  ..., -0.0800,  0.1182,  0.1054]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0002, -0.0292, -0.0766,  ...,  0.0108,  0.0630,  0.0504],\n",
      "        [ 0.0305,  0.0174,  0.1092,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [ 0.0305,  0.0174,  0.1092,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        ...,\n",
      "        [ 0.0305,  0.0174,  0.1092,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [-0.0971, -0.0521,  0.0565,  ...,  0.0224, -0.0646,  0.0972],\n",
      "        [ 0.0078, -0.1111,  0.0621,  ...,  0.0182,  0.0933, -0.0101]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0971, -0.0521,  0.0565,  ...,  0.0224, -0.0647,  0.0972],\n",
      "        [ 0.0002, -0.0292, -0.0766,  ...,  0.0108,  0.0630,  0.0504],\n",
      "        [ 0.0002, -0.0292, -0.0766,  ...,  0.0108,  0.0630,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0305,  0.0174,  0.1093,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [ 0.0305,  0.0174,  0.1093,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [ 0.0078, -0.1111,  0.0621,  ...,  0.0182,  0.0934, -0.0101]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0304,  0.0174,  0.1093,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [ 0.0304,  0.0174,  0.1093,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [ 0.0002, -0.0293, -0.0766,  ...,  0.0108,  0.0630,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0304,  0.0174,  0.1093,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [ 0.0304,  0.0174,  0.1093,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [ 0.0304,  0.0174,  0.1093,  ..., -0.0800,  0.1182,  0.1054]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [-0.0120,  0.1077, -0.0121,  ...,  0.0793,  0.0515, -0.0860],\n",
      "        ...,\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [ 0.0809, -0.0410, -0.0884,  ..., -0.0791, -0.0894, -0.1111],\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [ 0.0003, -0.0293, -0.0766,  ...,  0.0108,  0.0631,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [-0.0120,  0.1076, -0.0121,  ...,  0.0793,  0.0515, -0.0860]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        ...,\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0304,  0.0174,  0.1095,  ..., -0.0801,  0.1182,  0.1055],\n",
      "        [ 0.0304,  0.0174,  0.1095,  ..., -0.0801,  0.1182,  0.1055],\n",
      "        [ 0.0304,  0.0174,  0.1095,  ..., -0.0801,  0.1182,  0.1055],\n",
      "        ...,\n",
      "        [-0.0974, -0.0521,  0.0564,  ...,  0.0225, -0.0648,  0.0970],\n",
      "        [ 0.0233,  0.0582, -0.1101,  ...,  0.0704,  0.0737,  0.0701],\n",
      "        [-0.0974, -0.0521,  0.0564,  ...,  0.0225, -0.0648,  0.0970]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0303,  0.0174,  0.1095,  ..., -0.0801,  0.1182,  0.1055],\n",
      "        [ 0.0303,  0.0174,  0.1095,  ..., -0.0801,  0.1182,  0.1055],\n",
      "        [ 0.0003, -0.0294, -0.0767,  ...,  0.0108,  0.0631,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0303,  0.0174,  0.1095,  ..., -0.0801,  0.1182,  0.1055],\n",
      "        [ 0.0303,  0.0174,  0.1095,  ..., -0.0801,  0.1182,  0.1055],\n",
      "        [-0.0974, -0.0522,  0.0564,  ...,  0.0225, -0.0649,  0.0970]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0003, -0.0295, -0.0767,  ...,  0.0108,  0.0631,  0.0503],\n",
      "        [ 0.0003, -0.0295, -0.0767,  ...,  0.0108,  0.0631,  0.0503],\n",
      "        [ 0.0303,  0.0174,  0.1096,  ..., -0.0801,  0.1182,  0.1055],\n",
      "        ...,\n",
      "        [ 0.0303,  0.0174,  0.1096,  ..., -0.0801,  0.1182,  0.1055],\n",
      "        [ 0.0303,  0.0174,  0.1096,  ..., -0.0801,  0.1182,  0.1055],\n",
      "        [ 0.0303,  0.0174,  0.1096,  ..., -0.0801,  0.1182,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0303,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1055],\n",
      "        [ 0.0003, -0.0295, -0.0767,  ...,  0.0108,  0.0631,  0.0503],\n",
      "        [ 0.0003, -0.0295, -0.0767,  ...,  0.0108,  0.0631,  0.0503],\n",
      "        ...,\n",
      "        [-0.0167,  0.1191, -0.0102,  ...,  0.0827,  0.0622,  0.0491],\n",
      "        [-0.0167,  0.1191, -0.0102,  ...,  0.0827,  0.0622,  0.0491],\n",
      "        [ 0.0303,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0302,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1055],\n",
      "        [ 0.0004, -0.0296, -0.0768,  ...,  0.0108,  0.0631,  0.0503],\n",
      "        [ 0.0004, -0.0296, -0.0768,  ...,  0.0108,  0.0631,  0.0503],\n",
      "        ...,\n",
      "        [ 0.0302,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1055],\n",
      "        [ 0.0302,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1055],\n",
      "        [ 0.0302,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0079, -0.1110,  0.0623,  ...,  0.0182,  0.0935, -0.0101],\n",
      "        [-0.0976, -0.0523,  0.0563,  ...,  0.0225, -0.0650,  0.0970],\n",
      "        [ 0.0302,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0302,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0302,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0302,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0302,  0.0174,  0.1097,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0302,  0.0174,  0.1097,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0302,  0.0174,  0.1097,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        ...,\n",
      "        [-0.0977, -0.0523,  0.0563,  ...,  0.0225, -0.0649,  0.0970],\n",
      "        [ 0.0302,  0.0174,  0.1097,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0302,  0.0174,  0.1097,  ..., -0.0802,  0.1182,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [-0.0977, -0.0524,  0.0563,  ...,  0.0225, -0.0649,  0.0969],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0118,  0.1075, -0.0120,  ...,  0.0793,  0.0513, -0.0859],\n",
      "        [-0.0118,  0.1075, -0.0120,  ...,  0.0793,  0.0513, -0.0859],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0803,  0.1182,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0803,  0.1182,  0.1056],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0803,  0.1182,  0.1056],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0803,  0.1182,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0003, -0.0297, -0.0768,  ...,  0.0108,  0.0631,  0.0504],\n",
      "        [ 0.0003, -0.0297, -0.0768,  ...,  0.0108,  0.0631,  0.0504],\n",
      "        [ 0.0003, -0.0297, -0.0768,  ...,  0.0108,  0.0631,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0082, -0.1109,  0.0626,  ...,  0.0178,  0.0937, -0.0099],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0803,  0.1181,  0.1056],\n",
      "        [ 0.0813, -0.0407, -0.0882,  ..., -0.0792, -0.0893, -0.1111]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0300,  0.0174,  0.1096,  ..., -0.0804,  0.1181,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0804,  0.1181,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0804,  0.1181,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0804,  0.1181,  0.1056],\n",
      "        [ 0.0082, -0.1109,  0.0626,  ...,  0.0178,  0.0938, -0.0099],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0804,  0.1181,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0002, -0.0298, -0.0769,  ...,  0.0107,  0.0631,  0.0504],\n",
      "        [ 0.0002, -0.0298, -0.0769,  ...,  0.0107,  0.0631,  0.0504],\n",
      "        [ 0.0002, -0.0298, -0.0769,  ...,  0.0107,  0.0631,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0804,  0.1181,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0804,  0.1181,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0804,  0.1181,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0300,  0.0174,  0.1096,  ..., -0.0805,  0.1181,  0.1055],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0805,  0.1181,  0.1055],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0805,  0.1181,  0.1055],\n",
      "        ...,\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0805,  0.1181,  0.1055],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0805,  0.1181,  0.1055],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0805,  0.1181,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0174,  0.1097,  ..., -0.0805,  0.1181,  0.1055],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0805,  0.1181,  0.1055],\n",
      "        [ 0.0001, -0.0298, -0.0769,  ...,  0.0107,  0.0631,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0805,  0.1181,  0.1055],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0805,  0.1181,  0.1055],\n",
      "        [ 0.0900,  0.0317,  0.0689,  ...,  0.0427, -0.0067,  0.0771]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "1 21 1.6086289882659912\n",
      "tensor([[ 0.0299,  0.0174,  0.1097,  ..., -0.0806,  0.1181,  0.1055],\n",
      "        [ 0.0083, -0.1108,  0.0627,  ...,  0.0176,  0.0939, -0.0098],\n",
      "        [ 0.0083, -0.1108,  0.0627,  ...,  0.0176,  0.0939, -0.0098],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0806,  0.1181,  0.1055],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0806,  0.1181,  0.1055],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0806,  0.1181,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0174,  0.1097,  ..., -0.0806,  0.1181,  0.1056],\n",
      "        [-0.0983, -0.0523,  0.0559,  ...,  0.0224, -0.0645,  0.0968],\n",
      "        [-0.0983, -0.0523,  0.0559,  ...,  0.0224, -0.0645,  0.0968],\n",
      "        ...,\n",
      "        [ 0.0083, -0.1107,  0.0627,  ...,  0.0175,  0.0939, -0.0097],\n",
      "        [-0.0983, -0.0523,  0.0559,  ...,  0.0224, -0.0645,  0.0968],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0806,  0.1181,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0174,  0.1097,  ..., -0.0807,  0.1182,  0.1056],\n",
      "        [ 0.0002, -0.0300, -0.0769,  ...,  0.0107,  0.0631,  0.0505],\n",
      "        [ 0.0002, -0.0300, -0.0769,  ...,  0.0107,  0.0631,  0.0505],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0807,  0.1182,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0807,  0.1182,  0.1056],\n",
      "        [ 0.0901,  0.0316,  0.0689,  ...,  0.0427, -0.0068,  0.0770]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0174,  0.1097,  ..., -0.0807,  0.1182,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0807,  0.1182,  0.1056],\n",
      "        [ 0.0003, -0.0300, -0.0769,  ...,  0.0106,  0.0631,  0.0506],\n",
      "        ...,\n",
      "        [ 0.0901,  0.0316,  0.0689,  ...,  0.0427, -0.0069,  0.0771],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0807,  0.1182,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0807,  0.1182,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0174,  0.1097,  ..., -0.0808,  0.1182,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0808,  0.1182,  0.1056],\n",
      "        [-0.0116,  0.1073, -0.0120,  ...,  0.0794,  0.0513, -0.0860],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0808,  0.1182,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0808,  0.1182,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0808,  0.1182,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0986, -0.0523,  0.0560,  ...,  0.0223, -0.0644,  0.0966],\n",
      "        [ 0.0299,  0.0175,  0.1097,  ..., -0.0808,  0.1182,  0.1056],\n",
      "        [ 0.0299,  0.0175,  0.1097,  ..., -0.0808,  0.1182,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0175,  0.1097,  ..., -0.0808,  0.1182,  0.1056],\n",
      "        [ 0.0299,  0.0175,  0.1097,  ..., -0.0808,  0.1182,  0.1056],\n",
      "        [ 0.0299,  0.0175,  0.1097,  ..., -0.0808,  0.1182,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0082, -0.1106,  0.0628,  ...,  0.0174,  0.0939, -0.0098],\n",
      "        [-0.0987, -0.0523,  0.0560,  ...,  0.0223, -0.0644,  0.0966],\n",
      "        [ 0.0004, -0.0301, -0.0769,  ...,  0.0105,  0.0631,  0.0508],\n",
      "        ...,\n",
      "        [ 0.0004, -0.0301, -0.0769,  ...,  0.0105,  0.0631,  0.0508],\n",
      "        [ 0.0004, -0.0301, -0.0769,  ...,  0.0105,  0.0631,  0.0508],\n",
      "        [ 0.0004, -0.0301, -0.0769,  ...,  0.0105,  0.0631,  0.0508]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Validation loss: 17.010034324848547 RMSE: 4.1243224\n",
      "tensor([[ 0.0004, -0.0302, -0.0769,  ...,  0.0105,  0.0631,  0.0508],\n",
      "        [-0.0115,  0.1073, -0.0120,  ...,  0.0794,  0.0512, -0.0860],\n",
      "        [ 0.0299,  0.0175,  0.1096,  ..., -0.0809,  0.1182,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0082, -0.1106,  0.0628,  ...,  0.0174,  0.0939, -0.0097],\n",
      "        [ 0.0299,  0.0175,  0.1096,  ..., -0.0809,  0.1182,  0.1056],\n",
      "        [ 0.0299,  0.0175,  0.1096,  ..., -0.0809,  0.1182,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0175,  0.1096,  ..., -0.0809,  0.1181,  0.1056],\n",
      "        [ 0.0004, -0.0302, -0.0769,  ...,  0.0105,  0.0631,  0.0508],\n",
      "        [ 0.0004, -0.0302, -0.0769,  ...,  0.0105,  0.0631,  0.0508],\n",
      "        ...,\n",
      "        [-0.0989, -0.0523,  0.0560,  ...,  0.0224, -0.0644,  0.0966],\n",
      "        [ 0.0299,  0.0175,  0.1096,  ..., -0.0809,  0.1181,  0.1056],\n",
      "        [ 0.0299,  0.0175,  0.1096,  ..., -0.0809,  0.1181,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0175,  0.1096,  ..., -0.0810,  0.1181,  0.1056],\n",
      "        [ 0.0004, -0.0302, -0.0769,  ...,  0.0105,  0.0631,  0.0508],\n",
      "        [ 0.0004, -0.0302, -0.0769,  ...,  0.0105,  0.0631,  0.0508],\n",
      "        ...,\n",
      "        [-0.0989, -0.0523,  0.0560,  ...,  0.0225, -0.0644,  0.0966],\n",
      "        [ 0.0299,  0.0175,  0.1096,  ..., -0.0810,  0.1181,  0.1056],\n",
      "        [-0.0115,  0.1073, -0.0119,  ...,  0.0795,  0.0512, -0.0860]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0174,  0.1096,  ..., -0.0810,  0.1181,  0.1056],\n",
      "        [ 0.0004, -0.0303, -0.0770,  ...,  0.0104,  0.0630,  0.0508],\n",
      "        [ 0.0004, -0.0303, -0.0770,  ...,  0.0104,  0.0630,  0.0508],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0810,  0.1181,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0810,  0.1181,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0810,  0.1181,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056],\n",
      "        [-0.0115,  0.1073, -0.0118,  ...,  0.0795,  0.0512, -0.0859],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0300,  0.0174,  0.1096,  ..., -0.0811,  0.1179,  0.1056],\n",
      "        [-0.0993, -0.0524,  0.0558,  ...,  0.0228, -0.0645,  0.0966],\n",
      "        [ 0.0004, -0.0304, -0.0772,  ...,  0.0104,  0.0629,  0.0507],\n",
      "        ...,\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0811,  0.1179,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0811,  0.1179,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0811,  0.1179,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1179,  0.1056],\n",
      "        [-0.0115,  0.1073, -0.0117,  ...,  0.0795,  0.0512, -0.0859],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1179,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1179,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1179,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1179,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0994, -0.0524,  0.0558,  ...,  0.0229, -0.0646,  0.0966],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1179,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1179,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1179,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1179,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1179,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1178,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1178,  0.1056],\n",
      "        [ 0.0003, -0.0305, -0.0773,  ...,  0.0104,  0.0628,  0.0506],\n",
      "        ...,\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1178,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1178,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0995, -0.0524,  0.0558,  ...,  0.0229, -0.0646,  0.0966],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        ...,\n",
      "        [-0.0995, -0.0524,  0.0558,  ...,  0.0229, -0.0646,  0.0966],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0174,  0.1097,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0004, -0.0305, -0.0774,  ...,  0.0104,  0.0628,  0.0506],\n",
      "        [ 0.0004, -0.0305, -0.0774,  ...,  0.0104,  0.0628,  0.0506],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0165,  0.1185, -0.0105,  ...,  0.0828,  0.0629,  0.0487],\n",
      "        [ 0.0299,  0.0174,  0.1098,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0004, -0.0305, -0.0774,  ...,  0.0104,  0.0627,  0.0506],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0174,  0.1098,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1098,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1098,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0115,  0.1073, -0.0117,  ...,  0.0795,  0.0511, -0.0859],\n",
      "        [ 0.0299,  0.0174,  0.1098,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [-0.0997, -0.0524,  0.0558,  ...,  0.0230, -0.0647,  0.0966],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0174,  0.1098,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1098,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1098,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0298,  0.0175,  0.1099,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        ...,\n",
      "        [-0.0998, -0.0524,  0.0558,  ...,  0.0230, -0.0647,  0.0966],\n",
      "        [ 0.0234,  0.0589, -0.1091,  ...,  0.0714,  0.0753,  0.0714],\n",
      "        [ 0.0808, -0.0412, -0.0879,  ..., -0.0785, -0.0889, -0.1103]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0298,  0.0175,  0.1100,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1100,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0004, -0.0305, -0.0774,  ...,  0.0103,  0.0626,  0.0507],\n",
      "        ...,\n",
      "        [ 0.0298,  0.0175,  0.1100,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1100,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1100,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0298,  0.0175,  0.1100,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1100,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1100,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        ...,\n",
      "        [-0.0998, -0.0524,  0.0558,  ...,  0.0230, -0.0647,  0.0966],\n",
      "        [ 0.0298,  0.0175,  0.1100,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1100,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0005, -0.0305, -0.0774,  ...,  0.0103,  0.0626,  0.0507],\n",
      "        [ 0.0005, -0.0305, -0.0774,  ...,  0.0103,  0.0626,  0.0507],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0005, -0.0306, -0.0774,  ...,  0.0102,  0.0626,  0.0507],\n",
      "        [ 0.0005, -0.0306, -0.0774,  ...,  0.0102,  0.0626,  0.0507],\n",
      "        [ 0.0005, -0.0306, -0.0774,  ...,  0.0102,  0.0626,  0.0507],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.1000, -0.0523,  0.0557,  ...,  0.0230, -0.0646,  0.0964],\n",
      "        [ 0.0005, -0.0306, -0.0775,  ...,  0.0103,  0.0625,  0.0507],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0806, -0.0413, -0.0879,  ..., -0.0785, -0.0888, -0.1102],\n",
      "        [-0.1000, -0.0523,  0.0557,  ...,  0.0230, -0.0646,  0.0964],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0005, -0.0305, -0.0775,  ...,  0.0103,  0.0625,  0.0507],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0005, -0.0305, -0.0776,  ...,  0.0103,  0.0625,  0.0507],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [ 0.0005, -0.0305, -0.0776,  ...,  0.0103,  0.0625,  0.0507],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [-0.1001, -0.0524,  0.0557,  ...,  0.0231, -0.0646,  0.0963],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0112,  0.1071, -0.0116,  ...,  0.0795,  0.0511, -0.0860],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [ 0.0087, -0.1107,  0.0631,  ...,  0.0173,  0.0944, -0.0097],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [-0.1001, -0.0524,  0.0557,  ...,  0.0232, -0.0646,  0.0963],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.1001, -0.0524,  0.0556,  ...,  0.0232, -0.0645,  0.0963],\n",
      "        [-0.1001, -0.0524,  0.0556,  ...,  0.0232, -0.0645,  0.0963],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [ 0.0005, -0.0306, -0.0777,  ...,  0.0103,  0.0624,  0.0507],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [-0.1002, -0.0524,  0.0556,  ...,  0.0232, -0.0645,  0.0962],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1177,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.1002, -0.0524,  0.0556,  ...,  0.0233, -0.0645,  0.0962],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1177,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1176,  0.1056],\n",
      "        [ 0.0005, -0.0305, -0.0778,  ...,  0.0104,  0.0624,  0.0506],\n",
      "        [ 0.0089, -0.1107,  0.0631,  ...,  0.0172,  0.0945, -0.0097],\n",
      "        ...,\n",
      "        [-0.1002, -0.0524,  0.0556,  ...,  0.0233, -0.0645,  0.0962],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1176,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Validation loss: 3.044654910543324 RMSE: 1.744894\n",
      "tensor([[ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1176,  0.1056],\n",
      "        ...,\n",
      "        [-0.1002, -0.0523,  0.0556,  ...,  0.0233, -0.0645,  0.0962],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1176,  0.1056],\n",
      "        [ 0.0089, -0.1107,  0.0632,  ...,  0.0171,  0.0946, -0.0098]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0005, -0.0306, -0.0779,  ...,  0.0104,  0.0623,  0.0507],\n",
      "        [ 0.0005, -0.0306, -0.0779,  ...,  0.0104,  0.0623,  0.0507],\n",
      "        [ 0.0297,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0005, -0.0306, -0.0779,  ...,  0.0105,  0.0622,  0.0507],\n",
      "        [ 0.0005, -0.0306, -0.0779,  ...,  0.0105,  0.0622,  0.0507],\n",
      "        [ 0.0005, -0.0306, -0.0779,  ...,  0.0105,  0.0622,  0.0507],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0297,  0.0175,  0.1100,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0175,  0.1100,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0175,  0.1100,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0175,  0.1100,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        [-0.1002, -0.0523,  0.0556,  ...,  0.0233, -0.0645,  0.0962],\n",
      "        [ 0.0297,  0.0175,  0.1100,  ..., -0.0811,  0.1176,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0297,  0.0175,  0.1099,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        [-0.1002, -0.0523,  0.0556,  ...,  0.0233, -0.0645,  0.0962],\n",
      "        [ 0.0005, -0.0306, -0.0779,  ...,  0.0105,  0.0621,  0.0507],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0175,  0.1099,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0175,  0.1099,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0175,  0.1099,  ..., -0.0811,  0.1176,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0113,  0.1071, -0.0114,  ...,  0.0795,  0.0511, -0.0859],\n",
      "        [ 0.0297,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [ 0.0005, -0.0306, -0.0779,  ...,  0.0105,  0.0621,  0.0507],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0297,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [ 0.0005, -0.0306, -0.0779,  ...,  0.0105,  0.0621,  0.0507],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [-0.1003, -0.0524,  0.0556,  ...,  0.0234, -0.0646,  0.0962],\n",
      "        [ 0.0297,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0909,  0.0324,  0.0702,  ...,  0.0420, -0.0075,  0.0781],\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0298,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [-0.1003, -0.0525,  0.0556,  ...,  0.0234, -0.0645,  0.0961],\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0298,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1055],\n",
      "        ...,\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1055],\n",
      "        [-0.1004, -0.0525,  0.0556,  ...,  0.0234, -0.0645,  0.0961]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.1004, -0.0525,  0.0556,  ...,  0.0234, -0.0645,  0.0961],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1176,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1176,  0.1055],\n",
      "        ...,\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1176,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1176,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1176,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0005, -0.0305, -0.0779,  ...,  0.0106,  0.0620,  0.0506],\n",
      "        ...,\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        ...,\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [-0.1004, -0.0525,  0.0556,  ...,  0.0234, -0.0643,  0.0961],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        ...,\n",
      "        [ 0.0088, -0.1103,  0.0630,  ...,  0.0169,  0.0950, -0.0102],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "3 13 1.791797399520874\n",
      "tensor([[ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0239,  0.0583, -0.1088,  ...,  0.0713,  0.0755,  0.0716],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        ...,\n",
      "        [ 0.0089, -0.1103,  0.0630,  ...,  0.0169,  0.0951, -0.0102],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0910,  0.0325,  0.0701,  ...,  0.0420, -0.0072,  0.0780],\n",
      "        [ 0.0005, -0.0306, -0.0780,  ...,  0.0108,  0.0618,  0.0505],\n",
      "        [ 0.0005, -0.0306, -0.0780,  ...,  0.0108,  0.0618,  0.0505],\n",
      "        ...,\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1054],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1054],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1054]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0111,  0.1072, -0.0113,  ...,  0.0795,  0.0512, -0.0859],\n",
      "        [-0.1005, -0.0524,  0.0556,  ...,  0.0235, -0.0642,  0.0960],\n",
      "        [ 0.0298,  0.0176,  0.1101,  ..., -0.0810,  0.1174,  0.1054],\n",
      "        ...,\n",
      "        [ 0.0298,  0.0176,  0.1101,  ..., -0.0810,  0.1174,  0.1054],\n",
      "        [ 0.0298,  0.0176,  0.1101,  ..., -0.0810,  0.1174,  0.1054],\n",
      "        [ 0.0298,  0.0176,  0.1101,  ..., -0.0810,  0.1174,  0.1054]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1174,  0.1054],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1174,  0.1054],\n",
      "        [ 0.0005, -0.0306, -0.0780,  ...,  0.0108,  0.0617,  0.0505],\n",
      "        ...,\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1174,  0.1054],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1174,  0.1054],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1174,  0.1054]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n"
     ]
    }
   ],
   "source": [
    "seeds = [range(777,782)]\n",
    "for seed in seeds:\n",
    "    !python finetuneRecon.py \\\n",
    "    --task_name bbbp \\\n",
    "    --splitting scaffold \\\n",
    "    --seed {seed}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ce4ecd8-eaee-4c19-8a8d-195f56a1af16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 1, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': 1e-06, 'gpu': 'cuda:1', 'task_name': 'freesolv', 'model_type': 'gin', 'model': {'num_layer': 5, 'emb_dim': 300, 'feat_dim': 300, 'drop_ratio': 0.3, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 750, 'task': 'regression', 'data_path': 'data/freesolv/freesolv.csv', 'target': ['expt']}}\n",
      "Running on: cuda:1\n",
      "641\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/641\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 39.390193939208984\n",
      "Validation loss: 47.715105056762695 RMSE: 6.9076123\n",
      "Loaded trained model with success.\n",
      "Test loss: 23.89341037456806 Test RMSE: 4.8880887\n"
     ]
    }
   ],
   "source": [
    "!python finetuneReconNN.py \\\n",
    "--task_name freesolv \\\n",
    "--splitting scaffold \\\n",
    "--seed 750 \\\n",
    "--random_masking 1 \\\n",
    "--alpha 0.1 \\\n",
    "--mask_rate 0.2 \\\n",
    "--mask_edge 1 \\\n",
    "--gpu cuda:1 \\\n",
    "--epochs 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c350a18-74eb-42bb-ac32-ea54a4810977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tf/MolCLR-master - copy2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1537fe0-447a-43d8-90e0-6fa2e27ae65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "483786aa-1d33-4c99-b6a1-dea4f8c5cedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 10\n",
    "num_classes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f834183-27af-4128-9b00-975829c3dc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_node = torch.randn(num_nodes, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62d0161b-718c-43a9-b1fb-9331cfee6ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_node_label = torch.randint(0,num_classes, (num_nodes, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3730f1a7-27ac-4dc5-8e9d-3fdddae73954",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, prediced_classes = torch.max(pred_node, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99403bc3-e586-45e0-9721-4e71da0a9c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, prediced_classes = torch.max(pred_node, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89a58134-3106-420b-b4ce-a7f39642f13b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4224,  2.0152, -0.8221],\n",
       "        [-0.6797,  0.7852,  0.8896],\n",
       "        [ 1.0098, -0.3531,  0.5847],\n",
       "        [ 0.5771,  0.8337, -0.1788],\n",
       "        [-0.8700,  0.0082,  0.2619],\n",
       "        [-1.1317, -1.1674, -0.2489],\n",
       "        [ 1.7766, -0.0508, -0.0692],\n",
       "        [-0.8634, -1.4741,  1.3934],\n",
       "        [-0.4561,  0.9929, -0.2693],\n",
       "        [ 0.5695, -0.1895, -0.2701]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77c1959b-1126-4984-8209-2c985d3ab49c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 0, 1, 2, 2, 0, 2, 1, 0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediced_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79a78ae8-8ab8-4ef8-8c7b-2d6ae5c5542d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_node_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9da51ee-4134-4539-aa70-0fc2226f73cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_predictions = (prediced_classes == mask_node_label[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6df2d293-0fe0-4b73-b414-c6cdd567e3f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False,  True, False,  True, False,  True, False, False])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb8e2d38-0355-4198-9035-664ca3a6bda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = correct_predictions.float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afa8427a-2f8a-466d-a394-1bc7fa1ae53f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3000)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "335403f7-30fe-4eb3-99ba-d357a825e77a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30000001192092896"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414449ac-09e8-4507-8b7f-69ea4ddc818d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
