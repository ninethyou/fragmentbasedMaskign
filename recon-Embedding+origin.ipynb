{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cbc95c-8c82-4b39-9acb-eac3d7dea845",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:1', 'task_name': 'freesolv', 'model_type': 'gin', 'model': {'num_layer': 3, 'emb_dim': 64, 'feat_dim': 64, 'drop_ratio': 0.5, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 777, 'task': 'regression', 'data_path': 'data/freesolv/freesolv.csv', 'target': ['expt']}}\n",
      "Running on: cuda:1\n",
      "641\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/641\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 22.517520904541016\n",
      "Validation loss: 67.6897964477539 RMSE: 8.227381\n",
      "Validation loss: 62.99831008911133 RMSE: 7.937147\n",
      "Validation loss: 58.434492111206055 RMSE: 7.644245\n",
      "3 2 12.36631965637207\n",
      "Validation loss: 53.32700538635254 RMSE: 7.302534\n",
      "Validation loss: 47.09413528442383 RMSE: 6.862517\n",
      "Validation loss: 45.28282928466797 RMSE: 6.729252\n",
      "6 4 4.963541030883789\n",
      "Validation loss: 44.07360076904297 RMSE: 6.638795\n",
      "Validation loss: 44.03787040710449 RMSE: 6.6361036\n",
      "Validation loss: 40.91326904296875 RMSE: 6.396348\n",
      "9 6 4.740598201751709\n",
      "Validation loss: 42.07425498962402 RMSE: 6.4864664\n",
      "Validation loss: 42.71102237701416 RMSE: 6.535367\n",
      "Validation loss: 43.666263580322266 RMSE: 6.6080456\n",
      "12 8 4.857743740081787\n",
      "Validation loss: 38.93981170654297 RMSE: 6.2401776\n",
      "Validation loss: 41.96563529968262 RMSE: 6.478089\n",
      "Validation loss: 41.72791862487793 RMSE: 6.4597154\n",
      "15 10 3.9370925426483154\n",
      "Validation loss: 41.019060134887695 RMSE: 6.404612\n",
      "Validation loss: 41.650962829589844 RMSE: 6.453756\n",
      "Validation loss: 42.381072998046875 RMSE: 6.5100746\n",
      "18 12 4.971634864807129\n",
      "Validation loss: 43.911550521850586 RMSE: 6.626579\n",
      "Validation loss: 41.44223594665527 RMSE: 6.4375644\n",
      "Validation loss: 39.74073600769043 RMSE: 6.3040257\n",
      "21 14 3.907604455947876\n",
      "Validation loss: 42.03591728210449 RMSE: 6.4835114\n",
      "Validation loss: 40.75300979614258 RMSE: 6.383808\n",
      "Validation loss: 42.94853591918945 RMSE: 6.5535135\n",
      "Validation loss: 40.96748733520508 RMSE: 6.400585\n",
      "25 0 4.702131748199463\n",
      "Validation loss: 42.62892150878906 RMSE: 6.529083\n",
      "Validation loss: 41.30075454711914 RMSE: 6.4265666\n",
      "Validation loss: 40.786468505859375 RMSE: 6.3864284\n",
      "28 2 3.5064945220947266\n",
      "Validation loss: 38.320966720581055 RMSE: 6.1903934\n",
      "Validation loss: 41.29376792907715 RMSE: 6.426023\n",
      "Validation loss: 40.44414520263672 RMSE: 6.359571\n",
      "31 4 3.267131805419922\n",
      "Validation loss: 41.678401947021484 RMSE: 6.4558816\n",
      "Validation loss: 44.3173713684082 RMSE: 6.65713\n",
      "Validation loss: 44.29593849182129 RMSE: 6.6555195\n",
      "34 6 6.158547878265381\n",
      "Validation loss: 44.05500030517578 RMSE: 6.637394\n",
      "Validation loss: 42.528385162353516 RMSE: 6.5213795\n",
      "Validation loss: 41.13255310058594 RMSE: 6.4134665\n",
      "37 8 3.6092147827148438\n",
      "Validation loss: 39.99377250671387 RMSE: 6.3240633\n",
      "Validation loss: 42.282432556152344 RMSE: 6.5024943\n",
      "Validation loss: 46.02728080749512 RMSE: 6.784341\n",
      "40 10 5.027864933013916\n",
      "Validation loss: 43.81078910827637 RMSE: 6.618972\n",
      "Validation loss: 37.31850814819336 RMSE: 6.108888\n",
      "Validation loss: 37.64428520202637 RMSE: 6.1354938\n",
      "43 12 2.564863681793213\n",
      "Validation loss: 39.80248737335205 RMSE: 6.3089213\n",
      "Validation loss: 36.64357376098633 RMSE: 6.0533934\n",
      "Validation loss: 37.4778413772583 RMSE: 6.121915\n",
      "46 14 2.1042442321777344\n",
      "Validation loss: 37.488054275512695 RMSE: 6.122749\n",
      "Validation loss: 39.06891441345215 RMSE: 6.250513\n",
      "Validation loss: 36.2240104675293 RMSE: 6.018639\n",
      "Validation loss: 36.79567909240723 RMSE: 6.065944\n",
      "50 0 4.268093585968018\n",
      "Validation loss: 36.56354522705078 RMSE: 6.0467796\n",
      "Validation loss: 33.582319259643555 RMSE: 5.7950253\n",
      "Validation loss: 35.265708923339844 RMSE: 5.9384937\n",
      "53 2 1.808908462524414\n",
      "Validation loss: 35.843862533569336 RMSE: 5.9869747\n",
      "Validation loss: 34.52088165283203 RMSE: 5.8754473\n",
      "Validation loss: 32.86706829071045 RMSE: 5.7329807\n",
      "56 4 2.2608044147491455\n",
      "Validation loss: 37.69666767120361 RMSE: 6.139761\n",
      "Validation loss: 39.899139404296875 RMSE: 6.3165765\n",
      "Validation loss: 29.77731418609619 RMSE: 5.456859\n",
      "59 6 3.724238872528076\n",
      "Validation loss: 35.20554256439209 RMSE: 5.933426\n",
      "Validation loss: 30.498008728027344 RMSE: 5.5225\n",
      "Validation loss: 34.91404056549072 RMSE: 5.9088106\n",
      "62 8 2.183962821960449\n",
      "Validation loss: 33.42426109313965 RMSE: 5.781372\n",
      "Validation loss: 32.102630615234375 RMSE: 5.665919\n",
      "Validation loss: 32.59455490112305 RMSE: 5.7091637\n",
      "65 10 2.2544326782226562\n",
      "Validation loss: 30.21949863433838 RMSE: 5.4972267\n",
      "Validation loss: 32.1859655380249 RMSE: 5.673268\n",
      "Validation loss: 31.782309532165527 RMSE: 5.63758\n",
      "68 12 1.8147188425064087\n",
      "Validation loss: 29.230031967163086 RMSE: 5.406481\n",
      "Validation loss: 31.854278564453125 RMSE: 5.643959\n",
      "Validation loss: 31.76901912689209 RMSE: 5.636401\n",
      "71 14 2.1483540534973145\n",
      "Validation loss: 30.92927837371826 RMSE: 5.5614095\n",
      "Validation loss: 29.946084022521973 RMSE: 5.4723015\n",
      "Validation loss: 32.112985610961914 RMSE: 5.666832\n",
      "Validation loss: 33.905866622924805 RMSE: 5.8228745\n",
      "75 0 1.6426599025726318\n",
      "Validation loss: 32.18739891052246 RMSE: 5.6733937\n",
      "Validation loss: 30.318763732910156 RMSE: 5.5062475\n",
      "Validation loss: 33.527164459228516 RMSE: 5.790264\n",
      "78 2 1.8003655672073364\n",
      "Validation loss: 33.232316970825195 RMSE: 5.764748\n",
      "Validation loss: 32.77721977233887 RMSE: 5.7251396\n",
      "Validation loss: 35.59745979309082 RMSE: 5.9663606\n",
      "81 4 1.5514918565750122\n",
      "Validation loss: 32.916316986083984 RMSE: 5.7372746\n",
      "Validation loss: 32.44145393371582 RMSE: 5.69574\n",
      "Validation loss: 31.683046340942383 RMSE: 5.6287694\n",
      "84 6 2.2982025146484375\n",
      "Validation loss: 33.736287117004395 RMSE: 5.808295\n",
      "Validation loss: 35.01490592956543 RMSE: 5.9173393\n",
      "Validation loss: 32.1301155090332 RMSE: 5.6683435\n",
      "87 8 1.7273391485214233\n",
      "Validation loss: 35.47060585021973 RMSE: 5.9557204\n",
      "Validation loss: 33.26999378204346 RMSE: 5.7680144\n",
      "Validation loss: 33.01045513153076 RMSE: 5.7454724\n",
      "90 10 2.659710168838501\n",
      "Validation loss: 31.130850791931152 RMSE: 5.579503\n",
      "Validation loss: 30.827726364135742 RMSE: 5.5522723\n",
      "Validation loss: 32.67983055114746 RMSE: 5.7166276\n",
      "93 12 2.4574778079986572\n",
      "Validation loss: 35.322381019592285 RMSE: 5.9432635\n",
      "Validation loss: 34.65311813354492 RMSE: 5.88669\n",
      "Validation loss: 34.61931228637695 RMSE: 5.8838177\n",
      "96 14 2.303760051727295\n",
      "Validation loss: 33.41807842254639 RMSE: 5.780837\n",
      "Validation loss: 34.318970680236816 RMSE: 5.8582397\n",
      "Validation loss: 34.1534309387207 RMSE: 5.844094\n",
      "Validation loss: 36.352975845336914 RMSE: 6.0293427\n",
      "100 0 2.060392379760742\n",
      "Validation loss: 36.262779235839844 RMSE: 6.021858\n",
      "Validation loss: 35.916940689086914 RMSE: 5.9930744\n",
      "Validation loss: 38.28656768798828 RMSE: 6.187614\n",
      "103 2 2.2391843795776367\n",
      "Validation loss: 37.88906669616699 RMSE: 6.1554093\n",
      "Validation loss: 37.309173583984375 RMSE: 6.108124\n",
      "Validation loss: 36.158413887023926 RMSE: 6.0131865\n",
      "106 4 6.605300426483154\n",
      "Validation loss: 35.19002056121826 RMSE: 5.9321175\n",
      "Validation loss: 33.598647117614746 RMSE: 5.796434\n",
      "Validation loss: 33.98930263519287 RMSE: 5.8300347\n",
      "109 6 1.7048654556274414\n",
      "Validation loss: 33.44238090515137 RMSE: 5.782939\n",
      "Validation loss: 33.93866157531738 RMSE: 5.82569\n",
      "Validation loss: 30.763598442077637 RMSE: 5.5464945\n",
      "112 8 1.742095947265625\n",
      "Validation loss: 33.71331977844238 RMSE: 5.806317\n",
      "Validation loss: 34.70590782165527 RMSE: 5.8911724\n",
      "Validation loss: 37.28280067443848 RMSE: 6.1059647\n",
      "115 10 1.5128124952316284\n",
      "Validation loss: 30.58695697784424 RMSE: 5.5305476\n",
      "Validation loss: 33.68646812438965 RMSE: 5.804004\n",
      "Validation loss: 32.42065620422363 RMSE: 5.693914\n",
      "118 12 2.0318877696990967\n",
      "Validation loss: 29.882454872131348 RMSE: 5.4664845\n",
      "Validation loss: 32.402588844299316 RMSE: 5.692327\n",
      "Validation loss: 32.3351411819458 RMSE: 5.6864\n",
      "121 14 5.072284698486328\n",
      "Validation loss: 36.67506790161133 RMSE: 6.055994\n",
      "Validation loss: 31.77626609802246 RMSE: 5.6370444\n",
      "Validation loss: 33.88170051574707 RMSE: 5.820799\n",
      "Validation loss: 31.7407808303833 RMSE: 5.633896\n",
      "125 0 2.808894395828247\n",
      "Validation loss: 34.0937557220459 RMSE: 5.838986\n",
      "Validation loss: 33.37001419067383 RMSE: 5.776679\n",
      "Validation loss: 32.67762565612793 RMSE: 5.716435\n",
      "128 2 2.285155773162842\n",
      "Validation loss: 33.67819786071777 RMSE: 5.8032923\n",
      "Validation loss: 33.42409038543701 RMSE: 5.7813573\n",
      "Validation loss: 29.824331283569336 RMSE: 5.461166\n",
      "131 4 1.88601815700531\n",
      "Validation loss: 31.669511795043945 RMSE: 5.627567\n",
      "Validation loss: 30.152960777282715 RMSE: 5.4911714\n",
      "Validation loss: 33.69230651855469 RMSE: 5.8045073\n",
      "134 6 1.1823641061782837\n",
      "Validation loss: 32.14667892456055 RMSE: 5.6698036\n",
      "Validation loss: 34.207401275634766 RMSE: 5.8487096\n",
      "Validation loss: 27.96206760406494 RMSE: 5.287917\n",
      "137 8 1.2916407585144043\n",
      "Validation loss: 30.31433868408203 RMSE: 5.505846\n",
      "Validation loss: 29.29681968688965 RMSE: 5.412654\n",
      "Validation loss: 30.539259910583496 RMSE: 5.5262337\n",
      "140 10 1.770758867263794\n",
      "Validation loss: 33.410966873168945 RMSE: 5.780222\n",
      "Validation loss: 31.21791362762451 RMSE: 5.5872993\n",
      "Validation loss: 32.14370346069336 RMSE: 5.669542\n",
      "143 12 2.4943416118621826\n",
      "Validation loss: 30.668798446655273 RMSE: 5.5379415\n",
      "Validation loss: 31.464661598205566 RMSE: 5.609337\n",
      "Validation loss: 29.851356506347656 RMSE: 5.4636393\n",
      "146 14 1.5385279655456543\n",
      "Validation loss: 32.637091636657715 RMSE: 5.7128882\n",
      "Validation loss: 31.239301681518555 RMSE: 5.589213\n",
      "Validation loss: 29.480711936950684 RMSE: 5.4296145\n",
      "Validation loss: 31.06826877593994 RMSE: 5.5738916\n",
      "150 0 1.7343800067901611\n",
      "Validation loss: 31.97916603088379 RMSE: 5.6550126\n",
      "Validation loss: 33.089385986328125 RMSE: 5.7523375\n",
      "Validation loss: 30.564523696899414 RMSE: 5.528519\n",
      "153 2 2.3309149742126465\n",
      "Validation loss: 30.17767333984375 RMSE: 5.493421\n",
      "Validation loss: 33.554187297821045 RMSE: 5.7925973\n",
      "Validation loss: 36.02268981933594 RMSE: 6.0018907\n",
      "156 4 1.6464234590530396\n",
      "Validation loss: 33.5965633392334 RMSE: 5.7962546\n",
      "Validation loss: 33.93682670593262 RMSE: 5.825532\n",
      "Validation loss: 32.594632148742676 RMSE: 5.709171\n",
      "159 6 1.6739563941955566\n",
      "Validation loss: 37.023969650268555 RMSE: 6.0847325\n",
      "Validation loss: 30.264010429382324 RMSE: 5.5012736\n",
      "Validation loss: 31.916380882263184 RMSE: 5.6494584\n",
      "162 8 2.2337300777435303\n",
      "Validation loss: 31.751850128173828 RMSE: 5.634878\n",
      "Validation loss: 30.453381538391113 RMSE: 5.518458\n",
      "Validation loss: 33.45866012573242 RMSE: 5.784346\n",
      "165 10 2.328310966491699\n",
      "Validation loss: 33.55327224731445 RMSE: 5.7925186\n",
      "Validation loss: 34.24159622192383 RMSE: 5.851632\n",
      "Validation loss: 30.77933120727539 RMSE: 5.547912\n",
      "168 12 1.9195585250854492\n",
      "Validation loss: 32.68964385986328 RMSE: 5.717486\n",
      "Validation loss: 33.664825439453125 RMSE: 5.8021398\n",
      "Validation loss: 34.94729232788086 RMSE: 5.911623\n",
      "171 14 2.2032322883605957\n",
      "Validation loss: 33.09105110168457 RMSE: 5.752482\n",
      "Validation loss: 35.25489330291748 RMSE: 5.937583\n",
      "Validation loss: 34.899513244628906 RMSE: 5.907581\n",
      "Validation loss: 33.20511054992676 RMSE: 5.7623873\n",
      "175 0 0.9691895246505737\n",
      "Validation loss: 35.25061798095703 RMSE: 5.937223\n",
      "Validation loss: 34.58998489379883 RMSE: 5.8813252\n",
      "Validation loss: 31.019287109375 RMSE: 5.569496\n",
      "178 2 0.8810778260231018\n",
      "Validation loss: 33.89538860321045 RMSE: 5.8219743\n",
      "Validation loss: 33.08974266052246 RMSE: 5.7523685\n",
      "Validation loss: 33.554073333740234 RMSE: 5.7925878\n",
      "181 4 1.6661972999572754\n",
      "Validation loss: 33.920400619506836 RMSE: 5.8241224\n",
      "Validation loss: 32.905056953430176 RMSE: 5.7362933\n",
      "Validation loss: 33.41718864440918 RMSE: 5.7807603\n",
      "184 6 2.167415142059326\n",
      "Validation loss: 31.00658416748047 RMSE: 5.5683556\n",
      "Validation loss: 31.817235946655273 RMSE: 5.6406765\n",
      "Validation loss: 31.04801368713379 RMSE: 5.572074\n",
      "187 8 1.8729958534240723\n",
      "Validation loss: 34.387168884277344 RMSE: 5.8640575\n",
      "Validation loss: 34.154794692993164 RMSE: 5.8442106\n",
      "Validation loss: 31.887187957763672 RMSE: 5.646874\n",
      "190 10 2.2986867427825928\n",
      "Validation loss: 32.62649917602539 RMSE: 5.7119613\n",
      "Validation loss: 34.03814888000488 RMSE: 5.8342223\n",
      "Validation loss: 34.904778480529785 RMSE: 5.9080267\n",
      "193 12 2.28934645652771\n",
      "Validation loss: 34.404672622680664 RMSE: 5.8655496\n",
      "Validation loss: 34.67310333251953 RMSE: 5.888387\n",
      "Validation loss: 33.217050552368164 RMSE: 5.7634234\n",
      "196 14 1.451532244682312\n",
      "Validation loss: 34.65690803527832 RMSE: 5.8870115\n",
      "Validation loss: 37.31004047393799 RMSE: 6.108195\n",
      "Validation loss: 33.82940673828125 RMSE: 5.816305\n",
      "Validation loss: 34.766937255859375 RMSE: 5.8963494\n",
      "Loaded trained model with success.\n",
      "Test loss: 15.483920522836538 Test RMSE: 3.9349613\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:1', 'task_name': 'freesolv', 'model_type': 'gin', 'model': {'num_layer': 3, 'emb_dim': 64, 'feat_dim': 64, 'drop_ratio': 0.5, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 778, 'task': 'regression', 'data_path': 'data/freesolv/freesolv.csv', 'target': ['expt']}}\n",
      "Running on: cuda:1\n",
      "641\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/641\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 18.716367721557617\n",
      "Validation loss: 63.277305603027344 RMSE: 7.9547033\n",
      "Validation loss: 58.835933685302734 RMSE: 7.670459\n",
      "Validation loss: 54.08535957336426 RMSE: 7.3542747\n",
      "3 2 12.699549674987793\n",
      "Validation loss: 48.258094787597656 RMSE: 6.9468045\n",
      "Validation loss: 43.969247817993164 RMSE: 6.6309314\n",
      "Validation loss: 36.20577812194824 RMSE: 6.0171237\n",
      "6 4 11.222251892089844\n",
      "Validation loss: 34.91397285461426 RMSE: 5.9088044\n",
      "Validation loss: 36.284555435180664 RMSE: 6.0236664\n",
      "Validation loss: 35.33664512634277 RMSE: 5.9444637\n",
      "9 6 5.213518142700195\n",
      "Validation loss: 34.80180358886719 RMSE: 5.899305\n",
      "Validation loss: 31.61503028869629 RMSE: 5.6227245\n",
      "Validation loss: 32.29976749420166 RMSE: 5.6832886\n",
      "12 8 7.989064693450928\n",
      "Validation loss: 31.866320610046387 RMSE: 5.645026\n",
      "Validation loss: 30.124740600585938 RMSE: 5.4886007\n",
      "Validation loss: 28.79953384399414 RMSE: 5.36652\n",
      "15 10 3.2333085536956787\n",
      "Validation loss: 29.677857398986816 RMSE: 5.4477386\n",
      "Validation loss: 28.0389404296875 RMSE: 5.295181\n",
      "Validation loss: 28.17284393310547 RMSE: 5.30781\n",
      "18 12 6.683070659637451\n",
      "Validation loss: 30.00753688812256 RMSE: 5.477914\n",
      "Validation loss: 29.628402709960938 RMSE: 5.4431977\n",
      "Validation loss: 27.237869262695312 RMSE: 5.2189913\n",
      "21 14 3.607194423675537\n",
      "Validation loss: 27.092132568359375 RMSE: 5.2050104\n",
      "Validation loss: 27.209253311157227 RMSE: 5.216249\n",
      "Validation loss: 30.301313400268555 RMSE: 5.504663\n",
      "Validation loss: 27.643786430358887 RMSE: 5.2577357\n",
      "25 0 2.98185658454895\n",
      "Validation loss: 27.501399993896484 RMSE: 5.244178\n",
      "Validation loss: 27.25447940826416 RMSE: 5.2205825\n",
      "Validation loss: 28.697192192077637 RMSE: 5.3569756\n",
      "28 2 2.7549631595611572\n",
      "Validation loss: 25.903803825378418 RMSE: 5.0895777\n",
      "Validation loss: 26.564443588256836 RMSE: 5.1540704\n",
      "Validation loss: 26.343900680541992 RMSE: 5.132631\n",
      "31 4 4.830543518066406\n",
      "Validation loss: 27.698126792907715 RMSE: 5.2629013\n",
      "Validation loss: 25.235309600830078 RMSE: 5.0234756\n",
      "Validation loss: 25.889613151550293 RMSE: 5.088184\n",
      "34 6 2.856372833251953\n",
      "Validation loss: 26.485161781311035 RMSE: 5.1463737\n",
      "Validation loss: 25.01426410675049 RMSE: 5.001426\n",
      "Validation loss: 24.628308296203613 RMSE: 4.9626913\n",
      "37 8 2.8670461177825928\n",
      "Validation loss: 23.839879035949707 RMSE: 4.88261\n",
      "Validation loss: 27.025636672973633 RMSE: 5.198619\n",
      "Validation loss: 25.514690399169922 RMSE: 5.0512066\n",
      "40 10 1.8867089748382568\n",
      "Validation loss: 26.069199562072754 RMSE: 5.1058006\n",
      "Validation loss: 26.8231840133667 RMSE: 5.1791105\n",
      "Validation loss: 27.690027236938477 RMSE: 5.262131\n",
      "43 12 2.8598012924194336\n",
      "Validation loss: 26.09719181060791 RMSE: 5.108541\n",
      "Validation loss: 23.965444564819336 RMSE: 4.8954515\n",
      "Validation loss: 24.935436248779297 RMSE: 4.9935393\n",
      "46 14 2.1172068119049072\n",
      "Validation loss: 22.405622482299805 RMSE: 4.733458\n",
      "Validation loss: 27.20899486541748 RMSE: 5.216224\n",
      "Validation loss: 23.183414459228516 RMSE: 4.814916\n",
      "Validation loss: 23.492915153503418 RMSE: 4.846949\n",
      "50 0 2.453411817550659\n",
      "Validation loss: 24.717525482177734 RMSE: 4.971672\n",
      "Validation loss: 23.13249683380127 RMSE: 4.809625\n",
      "Validation loss: 21.777034759521484 RMSE: 4.666587\n",
      "53 2 2.514697790145874\n",
      "Validation loss: 25.396153450012207 RMSE: 5.0394597\n",
      "Validation loss: 22.32650852203369 RMSE: 4.7250934\n",
      "Validation loss: 21.144437789916992 RMSE: 4.598308\n",
      "56 4 3.4919626712799072\n",
      "Validation loss: 20.36082172393799 RMSE: 4.5122967\n",
      "Validation loss: 23.19534206390381 RMSE: 4.8161545\n",
      "Validation loss: 19.62752914428711 RMSE: 4.430297\n",
      "59 6 1.448129415512085\n",
      "Validation loss: 21.863219261169434 RMSE: 4.6758122\n",
      "Validation loss: 19.59846258163452 RMSE: 4.4270153\n",
      "Validation loss: 22.14093017578125 RMSE: 4.7054152\n",
      "62 8 2.3002512454986572\n",
      "Validation loss: 21.885150909423828 RMSE: 4.678157\n",
      "Validation loss: 19.496931076049805 RMSE: 4.415533\n",
      "Validation loss: 23.43725872039795 RMSE: 4.841204\n",
      "65 10 3.836017608642578\n",
      "Validation loss: 20.39921808242798 RMSE: 4.5165496\n",
      "Validation loss: 20.011863708496094 RMSE: 4.473462\n",
      "Validation loss: 23.702247619628906 RMSE: 4.8684955\n",
      "68 12 1.9498037099838257\n",
      "Validation loss: 18.32614231109619 RMSE: 4.2809043\n",
      "Validation loss: 19.05473518371582 RMSE: 4.365173\n",
      "Validation loss: 22.740757942199707 RMSE: 4.768727\n",
      "71 14 2.540466785430908\n",
      "Validation loss: 20.11326789855957 RMSE: 4.4847817\n",
      "Validation loss: 21.38657569885254 RMSE: 4.6245623\n",
      "Validation loss: 21.25256061553955 RMSE: 4.6100497\n",
      "Validation loss: 23.176616668701172 RMSE: 4.81421\n",
      "75 0 1.7718794345855713\n",
      "Validation loss: 21.111804008483887 RMSE: 4.594758\n",
      "Validation loss: 22.619778633117676 RMSE: 4.7560253\n",
      "Validation loss: 23.462810516357422 RMSE: 4.8438425\n",
      "78 2 1.750899076461792\n",
      "Validation loss: 25.018245697021484 RMSE: 5.0018244\n",
      "Validation loss: 20.468286514282227 RMSE: 4.524189\n",
      "Validation loss: 26.15828800201416 RMSE: 5.114517\n",
      "81 4 4.656157970428467\n",
      "Validation loss: 20.509709358215332 RMSE: 4.5287647\n",
      "Validation loss: 26.382445335388184 RMSE: 5.1363845\n",
      "Validation loss: 27.394655227661133 RMSE: 5.23399\n",
      "84 6 2.156944751739502\n",
      "Validation loss: 24.66908550262451 RMSE: 4.966798\n",
      "Validation loss: 24.394314765930176 RMSE: 4.93906\n",
      "Validation loss: 25.998722076416016 RMSE: 5.0988936\n",
      "87 8 2.749310255050659\n",
      "Validation loss: 23.348718643188477 RMSE: 4.8320513\n",
      "Validation loss: 22.05211067199707 RMSE: 4.6959677\n",
      "Validation loss: 20.248737335205078 RMSE: 4.49986\n",
      "90 10 2.5469329357147217\n",
      "Validation loss: 29.699069023132324 RMSE: 5.449685\n",
      "Validation loss: 22.99576473236084 RMSE: 4.7953897\n",
      "Validation loss: 23.22276782989502 RMSE: 4.819001\n",
      "93 12 2.8347232341766357\n",
      "Validation loss: 24.22213363647461 RMSE: 4.921599\n",
      "Validation loss: 25.62997341156006 RMSE: 5.0626054\n",
      "Validation loss: 24.384212493896484 RMSE: 4.9380374\n",
      "96 14 2.0410878658294678\n",
      "Validation loss: 24.909385681152344 RMSE: 4.9909306\n",
      "Validation loss: 21.924057006835938 RMSE: 4.6823134\n",
      "Validation loss: 24.49188804626465 RMSE: 4.948928\n",
      "Validation loss: 19.920883178710938 RMSE: 4.4632816\n",
      "100 0 1.725693941116333\n",
      "Validation loss: 21.478723526000977 RMSE: 4.6345143\n",
      "Validation loss: 22.77028465270996 RMSE: 4.771822\n",
      "Validation loss: 23.134584426879883 RMSE: 4.8098426\n",
      "103 2 0.9475757479667664\n",
      "Validation loss: 28.03034782409668 RMSE: 5.2943697\n",
      "Validation loss: 25.391392707824707 RMSE: 5.0389876\n",
      "Validation loss: 25.231224060058594 RMSE: 5.0230694\n",
      "106 4 4.3329267501831055\n",
      "Validation loss: 20.217631340026855 RMSE: 4.4964023\n",
      "Validation loss: 25.295973777770996 RMSE: 5.0295105\n",
      "Validation loss: 23.580854415893555 RMSE: 4.856012\n",
      "109 6 1.3921282291412354\n",
      "Validation loss: 23.477745056152344 RMSE: 4.8453836\n",
      "Validation loss: 23.6514835357666 RMSE: 4.863279\n",
      "Validation loss: 25.797149658203125 RMSE: 5.079089\n",
      "112 8 2.3412790298461914\n",
      "Validation loss: 26.18291473388672 RMSE: 5.1169243\n",
      "Validation loss: 24.364978313446045 RMSE: 4.9360895\n",
      "Validation loss: 23.686643600463867 RMSE: 4.866893\n",
      "115 10 1.703139305114746\n",
      "Validation loss: 18.363496780395508 RMSE: 4.285265\n",
      "Validation loss: 27.097747802734375 RMSE: 5.2055497\n",
      "Validation loss: 25.139049530029297 RMSE: 5.0138855\n",
      "118 12 2.400721788406372\n",
      "Validation loss: 20.332551956176758 RMSE: 4.5091634\n",
      "Validation loss: 22.235045433044434 RMSE: 4.715405\n",
      "Validation loss: 21.549144744873047 RMSE: 4.6421056\n",
      "121 14 2.0229992866516113\n",
      "Validation loss: 24.58582878112793 RMSE: 4.9584103\n",
      "Validation loss: 23.21030330657959 RMSE: 4.8177075\n",
      "Validation loss: 21.945229530334473 RMSE: 4.6845737\n",
      "Validation loss: 19.16240692138672 RMSE: 4.3774886\n",
      "125 0 1.1820573806762695\n",
      "Validation loss: 21.57455348968506 RMSE: 4.6448417\n",
      "Validation loss: 27.31467628479004 RMSE: 5.2263446\n",
      "Validation loss: 24.233631134033203 RMSE: 4.9227667\n",
      "128 2 2.322349786758423\n",
      "Validation loss: 24.292116165161133 RMSE: 4.9287033\n",
      "Validation loss: 24.212998390197754 RMSE: 4.920671\n",
      "Validation loss: 21.994871616363525 RMSE: 4.6898694\n",
      "131 4 1.7714084386825562\n",
      "Validation loss: 20.20587158203125 RMSE: 4.4950943\n",
      "Validation loss: 17.3968825340271 RMSE: 4.170957\n",
      "Validation loss: 21.72276544570923 RMSE: 4.6607685\n",
      "134 6 2.5158863067626953\n",
      "Validation loss: 26.071492195129395 RMSE: 5.106025\n",
      "Validation loss: 20.597719192504883 RMSE: 4.5384707\n",
      "Validation loss: 21.989055633544922 RMSE: 4.689249\n",
      "137 8 2.5338375568389893\n",
      "Validation loss: 22.579564094543457 RMSE: 4.751796\n",
      "Validation loss: 24.230255126953125 RMSE: 4.922424\n",
      "Validation loss: 21.71061134338379 RMSE: 4.6594644\n",
      "140 10 2.0002365112304688\n",
      "Validation loss: 23.207810401916504 RMSE: 4.8174486\n",
      "Validation loss: 24.056438446044922 RMSE: 4.9047365\n",
      "Validation loss: 18.251700401306152 RMSE: 4.272201\n",
      "143 12 2.516857624053955\n",
      "Validation loss: 23.437840461730957 RMSE: 4.8412642\n",
      "Validation loss: 24.203039169311523 RMSE: 4.919658\n",
      "Validation loss: 23.54214859008789 RMSE: 4.852025\n",
      "146 14 1.447129726409912\n",
      "Validation loss: 24.601350784301758 RMSE: 4.959975\n",
      "Validation loss: 26.65486431121826 RMSE: 5.162835\n",
      "Validation loss: 23.01491355895996 RMSE: 4.797386\n",
      "Validation loss: 20.983640670776367 RMSE: 4.58079\n",
      "150 0 1.6528584957122803\n",
      "Validation loss: 25.629318237304688 RMSE: 5.0625405\n",
      "Validation loss: 21.177050590515137 RMSE: 4.601853\n",
      "Validation loss: 26.22878074645996 RMSE: 5.121404\n",
      "153 2 3.810819625854492\n",
      "Validation loss: 22.373727798461914 RMSE: 4.7300878\n",
      "Validation loss: 21.03537940979004 RMSE: 4.586434\n",
      "Validation loss: 19.90785789489746 RMSE: 4.461822\n",
      "156 4 1.5422104597091675\n",
      "Validation loss: 18.697731018066406 RMSE: 4.324087\n",
      "Validation loss: 22.518402099609375 RMSE: 4.745356\n",
      "Validation loss: 25.24973201751709 RMSE: 5.024911\n",
      "159 6 1.9365100860595703\n",
      "Validation loss: 28.941574096679688 RMSE: 5.3797374\n",
      "Validation loss: 18.4370756149292 RMSE: 4.293842\n",
      "Validation loss: 19.121849060058594 RMSE: 4.3728538\n",
      "162 8 2.1697280406951904\n",
      "Validation loss: 19.496407508850098 RMSE: 4.415474\n",
      "Validation loss: 23.13479232788086 RMSE: 4.809864\n",
      "Validation loss: 20.987096786499023 RMSE: 4.5811677\n",
      "165 10 1.5929163694381714\n",
      "Validation loss: 26.68737030029297 RMSE: 5.1659822\n",
      "Validation loss: 22.523674964904785 RMSE: 4.745911\n",
      "Validation loss: 20.833126068115234 RMSE: 4.564332\n",
      "168 12 1.5469657182693481\n",
      "Validation loss: 25.687679290771484 RMSE: 5.0683017\n",
      "Validation loss: 24.02969741821289 RMSE: 4.9020095\n",
      "Validation loss: 18.586000442504883 RMSE: 4.311148\n",
      "171 14 2.4922595024108887\n",
      "Validation loss: 24.633429527282715 RMSE: 4.9632077\n",
      "Validation loss: 18.1751651763916 RMSE: 4.2632337\n",
      "Validation loss: 19.930075645446777 RMSE: 4.4643116\n",
      "Validation loss: 22.36476230621338 RMSE: 4.72914\n",
      "175 0 2.911381483078003\n",
      "Validation loss: 25.504185676574707 RMSE: 5.0501666\n",
      "Validation loss: 22.306105613708496 RMSE: 4.7229342\n",
      "Validation loss: 24.718212127685547 RMSE: 4.9717417\n",
      "178 2 2.8701367378234863\n",
      "Validation loss: 26.788414001464844 RMSE: 5.1757526\n",
      "Validation loss: 21.322853088378906 RMSE: 4.6176677\n",
      "Validation loss: 23.22637176513672 RMSE: 4.8193746\n",
      "181 4 1.0405350923538208\n",
      "Validation loss: 22.914570808410645 RMSE: 4.7869167\n",
      "Validation loss: 19.504048347473145 RMSE: 4.416339\n",
      "Validation loss: 26.11927604675293 RMSE: 5.110702\n",
      "184 6 2.1138017177581787\n",
      "Validation loss: 22.230223655700684 RMSE: 4.714894\n",
      "Validation loss: 27.145740509033203 RMSE: 5.2101574\n",
      "Validation loss: 20.276395797729492 RMSE: 4.5029316\n",
      "187 8 1.535146713256836\n",
      "Validation loss: 24.791049003601074 RMSE: 4.979061\n",
      "Validation loss: 21.001405239105225 RMSE: 4.5827293\n",
      "Validation loss: 21.11941909790039 RMSE: 4.595587\n",
      "190 10 1.7150230407714844\n",
      "Validation loss: 26.38070774078369 RMSE: 5.136215\n",
      "Validation loss: 18.89774227142334 RMSE: 4.3471537\n",
      "Validation loss: 22.894235610961914 RMSE: 4.784792\n",
      "193 12 1.7748868465423584\n",
      "Validation loss: 19.524446487426758 RMSE: 4.418648\n",
      "Validation loss: 20.4485502243042 RMSE: 4.5220075\n",
      "Validation loss: 22.48849391937256 RMSE: 4.7422037\n",
      "196 14 1.9694575071334839\n",
      "Validation loss: 22.05725383758545 RMSE: 4.6965146\n",
      "Validation loss: 22.324666023254395 RMSE: 4.7248983\n",
      "Validation loss: 24.16433620452881 RMSE: 4.9157233\n",
      "Validation loss: 27.026294708251953 RMSE: 5.1986823\n",
      "Loaded trained model with success.\n",
      "Test loss: 12.377163960383488 Test RMSE: 3.5181193\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:1', 'task_name': 'freesolv', 'model_type': 'gin', 'model': {'num_layer': 3, 'emb_dim': 64, 'feat_dim': 64, 'drop_ratio': 0.5, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 779, 'task': 'regression', 'data_path': 'data/freesolv/freesolv.csv', 'target': ['expt']}}\n",
      "Running on: cuda:1\n",
      "641\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/641\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 25.582077026367188\n",
      "Validation loss: 72.11811828613281 RMSE: 8.492239\n",
      "Validation loss: 67.18330001831055 RMSE: 8.196542\n",
      "Validation loss: 62.28834915161133 RMSE: 7.892297\n",
      "3 2 13.805245399475098\n",
      "Validation loss: 57.36180877685547 RMSE: 7.5737586\n",
      "Validation loss: 52.0710563659668 RMSE: 7.216028\n",
      "Validation loss: 48.61292839050293 RMSE: 6.972297\n",
      "6 4 7.309428691864014\n",
      "Validation loss: 48.737810134887695 RMSE: 6.9812474\n",
      "Validation loss: 49.24930000305176 RMSE: 7.0177846\n",
      "Validation loss: 47.0731086730957 RMSE: 6.8609843\n",
      "9 6 5.374399662017822\n",
      "Validation loss: 45.34278106689453 RMSE: 6.733705\n",
      "Validation loss: 45.651206970214844 RMSE: 6.7565675\n",
      "Validation loss: 44.189178466796875 RMSE: 6.6474934\n",
      "12 8 5.664373874664307\n",
      "Validation loss: 44.56321430206299 RMSE: 6.6755686\n",
      "Validation loss: 43.88575077056885 RMSE: 6.624632\n",
      "Validation loss: 42.92026138305664 RMSE: 6.5513554\n",
      "15 10 3.2569057941436768\n",
      "Validation loss: 43.102163314819336 RMSE: 6.5652237\n",
      "Validation loss: 42.66645431518555 RMSE: 6.531956\n",
      "Validation loss: 42.47841262817383 RMSE: 6.5175467\n",
      "18 12 4.802433013916016\n",
      "Validation loss: 41.57305717468262 RMSE: 6.447717\n",
      "Validation loss: 39.62414741516113 RMSE: 6.294771\n",
      "Validation loss: 39.43382263183594 RMSE: 6.2796354\n",
      "21 14 3.613490104675293\n",
      "Validation loss: 38.210044860839844 RMSE: 6.1814275\n",
      "Validation loss: 42.053443908691406 RMSE: 6.484863\n",
      "Validation loss: 42.38523864746094 RMSE: 6.5103946\n",
      "Validation loss: 41.84724235534668 RMSE: 6.4689445\n",
      "25 0 4.022406101226807\n",
      "Validation loss: 41.22712326049805 RMSE: 6.420835\n",
      "Validation loss: 40.62903594970703 RMSE: 6.374091\n",
      "Validation loss: 38.98940181732178 RMSE: 6.2441497\n",
      "28 2 3.2289133071899414\n",
      "Validation loss: 39.227365493774414 RMSE: 6.263175\n",
      "Validation loss: 41.7194766998291 RMSE: 6.4590616\n",
      "Validation loss: 41.12780952453613 RMSE: 6.4130964\n",
      "31 4 1.963655948638916\n",
      "Validation loss: 41.000112533569336 RMSE: 6.4031334\n",
      "Validation loss: 42.569509506225586 RMSE: 6.5245314\n",
      "Validation loss: 42.201377868652344 RMSE: 6.4962587\n",
      "34 6 2.858720064163208\n",
      "Validation loss: 41.797019958496094 RMSE: 6.4650617\n",
      "Validation loss: 40.08950614929199 RMSE: 6.3316274\n",
      "Validation loss: 40.779985427856445 RMSE: 6.3859205\n",
      "37 8 2.524729013442993\n",
      "Validation loss: 42.18357849121094 RMSE: 6.494889\n",
      "Validation loss: 43.00699806213379 RMSE: 6.557972\n",
      "Validation loss: 40.68077278137207 RMSE: 6.378148\n",
      "40 10 3.1906614303588867\n",
      "Validation loss: 40.55667495727539 RMSE: 6.3684125\n",
      "Validation loss: 42.8476676940918 RMSE: 6.545813\n",
      "Validation loss: 43.516353607177734 RMSE: 6.5966926\n",
      "43 12 5.083442687988281\n",
      "Validation loss: 41.101585388183594 RMSE: 6.4110518\n",
      "Validation loss: 41.07603454589844 RMSE: 6.4090586\n",
      "Validation loss: 44.20884323120117 RMSE: 6.648973\n",
      "46 14 3.240668535232544\n",
      "Validation loss: 42.310808181762695 RMSE: 6.504676\n",
      "Validation loss: 40.80450630187988 RMSE: 6.3878403\n",
      "Validation loss: 39.975690841674805 RMSE: 6.3226333\n",
      "Validation loss: 39.28796195983887 RMSE: 6.268011\n",
      "50 0 3.6638975143432617\n",
      "Validation loss: 36.98013687133789 RMSE: 6.0811296\n",
      "Validation loss: 40.45973205566406 RMSE: 6.3607965\n",
      "Validation loss: 37.77707290649414 RMSE: 6.1463056\n",
      "53 2 1.7902350425720215\n",
      "Validation loss: 39.270660400390625 RMSE: 6.26663\n",
      "Validation loss: 37.82330513000488 RMSE: 6.1500654\n",
      "Validation loss: 40.34914207458496 RMSE: 6.3520975\n",
      "56 4 3.4084274768829346\n",
      "Validation loss: 37.057308197021484 RMSE: 6.0874715\n",
      "Validation loss: 36.887006759643555 RMSE: 6.0734677\n",
      "Validation loss: 36.54507827758789 RMSE: 6.045253\n",
      "59 6 2.6582908630371094\n",
      "Validation loss: 35.82644844055176 RMSE: 5.98552\n",
      "Validation loss: 36.215115547180176 RMSE: 6.0178995\n",
      "Validation loss: 34.20604705810547 RMSE: 5.8485937\n",
      "62 8 3.8923397064208984\n",
      "Validation loss: 36.967641830444336 RMSE: 6.0801024\n",
      "Validation loss: 34.435569763183594 RMSE: 5.8681827\n",
      "Validation loss: 34.682549476623535 RMSE: 5.8891892\n",
      "65 10 2.424184560775757\n",
      "Validation loss: 34.9763879776001 RMSE: 5.914084\n",
      "Validation loss: 30.43889045715332 RMSE: 5.517145\n",
      "Validation loss: 34.911542892456055 RMSE: 5.9085994\n",
      "68 12 2.8328821659088135\n",
      "Validation loss: 33.955814361572266 RMSE: 5.827162\n",
      "Validation loss: 36.10583686828613 RMSE: 6.008813\n",
      "Validation loss: 36.73734474182129 RMSE: 6.061134\n",
      "71 14 3.2723140716552734\n",
      "Validation loss: 34.34342384338379 RMSE: 5.8603263\n",
      "Validation loss: 33.914628982543945 RMSE: 5.8236265\n",
      "Validation loss: 32.333412170410156 RMSE: 5.686248\n",
      "Validation loss: 32.7735481262207 RMSE: 5.7248187\n",
      "75 0 2.1805174350738525\n",
      "Validation loss: 31.948734283447266 RMSE: 5.6523213\n",
      "Validation loss: 36.375038146972656 RMSE: 6.0311723\n",
      "Validation loss: 32.21944999694824 RMSE: 5.6762176\n",
      "78 2 3.8665244579315186\n",
      "Validation loss: 30.885854721069336 RMSE: 5.557504\n",
      "Validation loss: 36.08731460571289 RMSE: 6.007272\n",
      "Validation loss: 32.340237617492676 RMSE: 5.686848\n",
      "81 4 3.2004647254943848\n",
      "Validation loss: 33.65080261230469 RMSE: 5.8009315\n",
      "Validation loss: 32.78098392486572 RMSE: 5.725468\n",
      "Validation loss: 29.794692993164062 RMSE: 5.4584517\n",
      "84 6 2.53735089302063\n",
      "Validation loss: 35.67525863647461 RMSE: 5.972877\n",
      "Validation loss: 35.20600700378418 RMSE: 5.933465\n",
      "Validation loss: 32.05281734466553 RMSE: 5.661521\n",
      "87 8 2.5141873359680176\n",
      "Validation loss: 34.06642246246338 RMSE: 5.8366446\n",
      "Validation loss: 36.83122444152832 RMSE: 6.0688734\n",
      "Validation loss: 35.33915710449219 RMSE: 5.9446745\n",
      "90 10 1.5945583581924438\n",
      "Validation loss: 34.261091232299805 RMSE: 5.8532977\n",
      "Validation loss: 32.48593521118164 RMSE: 5.6996436\n",
      "Validation loss: 33.4731559753418 RMSE: 5.785599\n",
      "93 12 3.964296340942383\n",
      "Validation loss: 33.88392448425293 RMSE: 5.8209896\n",
      "Validation loss: 33.591219902038574 RMSE: 5.7957935\n",
      "Validation loss: 34.07863426208496 RMSE: 5.837691\n",
      "96 14 1.6620866060256958\n",
      "Validation loss: 32.37583827972412 RMSE: 5.6899767\n",
      "Validation loss: 35.88773155212402 RMSE: 5.9906373\n",
      "Validation loss: 34.34603691101074 RMSE: 5.8605494\n",
      "Validation loss: 33.810686111450195 RMSE: 5.8146954\n",
      "100 0 6.5704145431518555\n",
      "Validation loss: 33.70625305175781 RMSE: 5.805709\n",
      "Validation loss: 34.31781578063965 RMSE: 5.858141\n",
      "Validation loss: 33.749122619628906 RMSE: 5.8093996\n",
      "103 2 4.277045726776123\n",
      "Validation loss: 36.25417900085449 RMSE: 6.0211444\n",
      "Validation loss: 33.08319282531738 RMSE: 5.751799\n",
      "Validation loss: 37.60159111022949 RMSE: 6.132014\n",
      "106 4 5.7221527099609375\n",
      "Validation loss: 36.11451530456543 RMSE: 6.0095353\n",
      "Validation loss: 33.87155342102051 RMSE: 5.819927\n",
      "Validation loss: 34.79102897644043 RMSE: 5.898392\n",
      "109 6 2.6323046684265137\n",
      "Validation loss: 33.69890594482422 RMSE: 5.8050756\n",
      "Validation loss: 30.232030868530273 RMSE: 5.4983664\n",
      "Validation loss: 34.74890422821045 RMSE: 5.8948197\n",
      "112 8 2.159341812133789\n",
      "Validation loss: 33.72028827667236 RMSE: 5.806917\n",
      "Validation loss: 29.166229248046875 RMSE: 5.4005766\n",
      "Validation loss: 32.612613677978516 RMSE: 5.7107453\n",
      "115 10 1.5933520793914795\n",
      "Validation loss: 32.45482635498047 RMSE: 5.6969132\n",
      "Validation loss: 31.738107681274414 RMSE: 5.6336584\n",
      "Validation loss: 34.1567440032959 RMSE: 5.8443775\n",
      "118 12 1.5921536684036255\n",
      "Validation loss: 33.426055908203125 RMSE: 5.781527\n",
      "Validation loss: 32.440890312194824 RMSE: 5.6956906\n",
      "Validation loss: 36.633758544921875 RMSE: 6.0525827\n",
      "121 14 2.1948366165161133\n",
      "Validation loss: 36.72230529785156 RMSE: 6.059893\n",
      "Validation loss: 32.61085891723633 RMSE: 5.710592\n",
      "Validation loss: 32.6787052154541 RMSE: 5.716529\n",
      "Validation loss: 34.908315658569336 RMSE: 5.908326\n",
      "125 0 3.036158561706543\n",
      "Validation loss: 34.29647254943848 RMSE: 5.8563194\n",
      "Validation loss: 32.28854274749756 RMSE: 5.682301\n",
      "Validation loss: 34.13209915161133 RMSE: 5.842268\n",
      "128 2 2.0268454551696777\n",
      "Validation loss: 35.136444091796875 RMSE: 5.9276004\n",
      "Validation loss: 31.77052879333496 RMSE: 5.636535\n",
      "Validation loss: 34.52671146392822 RMSE: 5.875943\n",
      "131 4 3.1886425018310547\n",
      "Validation loss: 35.924838066101074 RMSE: 5.993733\n",
      "Validation loss: 35.16155242919922 RMSE: 5.9297175\n",
      "Validation loss: 30.766446113586426 RMSE: 5.546751\n",
      "134 6 1.1997207403182983\n",
      "Validation loss: 30.473639488220215 RMSE: 5.5202937\n",
      "Validation loss: 32.29352569580078 RMSE: 5.6827393\n",
      "Validation loss: 36.87366485595703 RMSE: 6.072369\n",
      "137 8 5.598361492156982\n",
      "Validation loss: 34.817291259765625 RMSE: 5.900618\n",
      "Validation loss: 34.66653633117676 RMSE: 5.8878293\n",
      "Validation loss: 35.18781661987305 RMSE: 5.931932\n",
      "140 10 0.8694682121276855\n",
      "Validation loss: 34.823426246643066 RMSE: 5.901138\n",
      "Validation loss: 33.044087409973145 RMSE: 5.748399\n",
      "Validation loss: 30.442015647888184 RMSE: 5.517428\n",
      "143 12 3.1406972408294678\n",
      "Validation loss: 31.87586212158203 RMSE: 5.6458707\n",
      "Validation loss: 31.70346164703369 RMSE: 5.630583\n",
      "Validation loss: 30.68373394012451 RMSE: 5.53929\n",
      "146 14 3.1879167556762695\n",
      "Validation loss: 32.01607894897461 RMSE: 5.6582756\n",
      "Validation loss: 33.08761787414551 RMSE: 5.7521834\n",
      "Validation loss: 29.27114200592041 RMSE: 5.410281\n",
      "Validation loss: 29.509370803833008 RMSE: 5.432253\n",
      "150 0 1.7239093780517578\n",
      "Validation loss: 30.233427047729492 RMSE: 5.498493\n",
      "Validation loss: 30.51403045654297 RMSE: 5.5239506\n",
      "Validation loss: 29.35807704925537 RMSE: 5.4183097\n",
      "153 2 1.7637330293655396\n",
      "Validation loss: 32.026662826538086 RMSE: 5.6592107\n",
      "Validation loss: 31.802913665771484 RMSE: 5.639407\n",
      "Validation loss: 31.900724411010742 RMSE: 5.6480722\n",
      "156 4 1.3020416498184204\n",
      "Validation loss: 29.30235481262207 RMSE: 5.413165\n",
      "Validation loss: 30.763700485229492 RMSE: 5.5465035\n",
      "Validation loss: 30.344975471496582 RMSE: 5.5086274\n",
      "159 6 1.658664345741272\n",
      "Validation loss: 28.045632362365723 RMSE: 5.2958126\n",
      "Validation loss: 30.140836715698242 RMSE: 5.490067\n",
      "Validation loss: 28.4386043548584 RMSE: 5.3327856\n",
      "162 8 1.8635447025299072\n",
      "Validation loss: 29.322189331054688 RMSE: 5.4149966\n",
      "Validation loss: 29.463760375976562 RMSE: 5.4280534\n",
      "Validation loss: 30.939594268798828 RMSE: 5.562337\n",
      "165 10 2.101947069168091\n",
      "Validation loss: 30.568400382995605 RMSE: 5.5288696\n",
      "Validation loss: 30.17091941833496 RMSE: 5.492806\n",
      "Validation loss: 30.52445697784424 RMSE: 5.524894\n",
      "168 12 2.804550886154175\n",
      "Validation loss: 31.038633346557617 RMSE: 5.5712323\n",
      "Validation loss: 30.73607349395752 RMSE: 5.5440125\n",
      "Validation loss: 30.362360954284668 RMSE: 5.5102053\n",
      "171 14 1.6220067739486694\n",
      "Validation loss: 29.062841415405273 RMSE: 5.3909965\n",
      "Validation loss: 30.969558715820312 RMSE: 5.56503\n",
      "Validation loss: 28.79371929168701 RMSE: 5.3659782\n",
      "Validation loss: 27.186999320983887 RMSE: 5.2141156\n",
      "175 0 0.7292212247848511\n",
      "Validation loss: 30.84977912902832 RMSE: 5.554258\n",
      "Validation loss: 28.170588493347168 RMSE: 5.307597\n",
      "Validation loss: 29.67289638519287 RMSE: 5.4472833\n",
      "178 2 2.0309290885925293\n",
      "Validation loss: 28.519634246826172 RMSE: 5.340378\n",
      "Validation loss: 29.100473403930664 RMSE: 5.3944855\n",
      "Validation loss: 31.052812576293945 RMSE: 5.572505\n",
      "181 4 2.6037991046905518\n",
      "Validation loss: 29.048158645629883 RMSE: 5.389634\n",
      "Validation loss: 29.8161563873291 RMSE: 5.4604173\n",
      "Validation loss: 31.144350051879883 RMSE: 5.5807123\n",
      "184 6 1.487065076828003\n",
      "Validation loss: 33.837392807006836 RMSE: 5.816992\n",
      "Validation loss: 29.397655487060547 RMSE: 5.4219604\n",
      "Validation loss: 29.477952003479004 RMSE: 5.4293604\n",
      "187 8 1.3449959754943848\n",
      "Validation loss: 29.558021545410156 RMSE: 5.436729\n",
      "Validation loss: 31.30422019958496 RMSE: 5.5950174\n",
      "Validation loss: 29.231550216674805 RMSE: 5.406621\n",
      "190 10 1.8140654563903809\n",
      "Validation loss: 29.79665470123291 RMSE: 5.4586315\n",
      "Validation loss: 28.681397438049316 RMSE: 5.3555017\n",
      "Validation loss: 32.17192268371582 RMSE: 5.6720295\n",
      "193 12 1.1305207014083862\n",
      "Validation loss: 30.738901138305664 RMSE: 5.544267\n",
      "Validation loss: 30.68537712097168 RMSE: 5.5394382\n",
      "Validation loss: 31.323698043823242 RMSE: 5.596758\n",
      "196 14 3.194962978363037\n",
      "Validation loss: 29.935053825378418 RMSE: 5.471294\n",
      "Validation loss: 29.095529556274414 RMSE: 5.394027\n",
      "Validation loss: 28.906452178955078 RMSE: 5.376472\n",
      "Validation loss: 30.126963138580322 RMSE: 5.488804\n",
      "Loaded trained model with success.\n",
      "Test loss: 14.555535419170672 Test RMSE: 3.815172\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:1', 'task_name': 'freesolv', 'model_type': 'gin', 'model': {'num_layer': 3, 'emb_dim': 64, 'feat_dim': 64, 'drop_ratio': 0.5, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 780, 'task': 'regression', 'data_path': 'data/freesolv/freesolv.csv', 'target': ['expt']}}\n",
      "Running on: cuda:1\n",
      "641\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/641\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 14.42834758758545\n",
      "Validation loss: 68.00545501708984 RMSE: 8.246542\n",
      "Validation loss: 62.35257530212402 RMSE: 7.8963647\n",
      "Validation loss: 56.19856929779053 RMSE: 7.4965706\n",
      "3 2 15.586517333984375\n",
      "Validation loss: 47.34821128845215 RMSE: 6.881004\n",
      "Validation loss: 33.355133056640625 RMSE: 5.7753906\n",
      "Validation loss: 27.156414031982422 RMSE: 5.2111816\n",
      "6 4 6.872641563415527\n",
      "Validation loss: 28.404644012451172 RMSE: 5.329601\n",
      "Validation loss: 28.58120346069336 RMSE: 5.346139\n",
      "Validation loss: 28.544490814208984 RMSE: 5.3427043\n",
      "9 6 12.46845817565918\n",
      "Validation loss: 26.89996910095215 RMSE: 5.186518\n",
      "Validation loss: 26.527917861938477 RMSE: 5.150526\n",
      "Validation loss: 24.30063819885254 RMSE: 4.9295673\n",
      "12 8 5.581258773803711\n",
      "Validation loss: 23.314751625061035 RMSE: 4.828535\n",
      "Validation loss: 25.26613426208496 RMSE: 5.0265427\n",
      "Validation loss: 23.106693267822266 RMSE: 4.806942\n",
      "15 10 5.391284465789795\n",
      "Validation loss: 23.411200523376465 RMSE: 4.838512\n",
      "Validation loss: 24.951144218444824 RMSE: 4.995112\n",
      "Validation loss: 23.319442749023438 RMSE: 4.8290205\n",
      "18 12 3.268540620803833\n",
      "Validation loss: 23.338868618011475 RMSE: 4.831032\n",
      "Validation loss: 24.224563598632812 RMSE: 4.9218454\n",
      "Validation loss: 28.48394775390625 RMSE: 5.3370357\n",
      "21 14 4.6617913246154785\n",
      "Validation loss: 25.1221981048584 RMSE: 5.0122046\n",
      "Validation loss: 26.106348991394043 RMSE: 5.1094375\n",
      "Validation loss: 27.517621994018555 RMSE: 5.245724\n",
      "Validation loss: 27.671802520751953 RMSE: 5.2603993\n",
      "25 0 2.3197879791259766\n",
      "Validation loss: 29.058125495910645 RMSE: 5.390559\n",
      "Validation loss: 27.716047286987305 RMSE: 5.2646036\n",
      "Validation loss: 30.92473030090332 RMSE: 5.561001\n",
      "28 2 5.213890075683594\n",
      "Validation loss: 26.747238159179688 RMSE: 5.1717734\n",
      "Validation loss: 30.841922760009766 RMSE: 5.5535502\n",
      "Validation loss: 31.87443733215332 RMSE: 5.6457453\n",
      "31 4 3.345924139022827\n",
      "Validation loss: 33.14068412780762 RMSE: 5.756795\n",
      "Validation loss: 29.649243354797363 RMSE: 5.445112\n",
      "Validation loss: 31.4739990234375 RMSE: 5.610169\n",
      "34 6 2.102825403213501\n",
      "Validation loss: 28.103178024291992 RMSE: 5.3012433\n",
      "Validation loss: 30.00215721130371 RMSE: 5.4774227\n",
      "Validation loss: 30.38472557067871 RMSE: 5.512234\n",
      "37 8 3.38864803314209\n",
      "Validation loss: 33.99037170410156 RMSE: 5.8301263\n",
      "Validation loss: 29.485669136047363 RMSE: 5.430071\n",
      "Validation loss: 30.478896141052246 RMSE: 5.5207696\n",
      "40 10 3.822343587875366\n",
      "Validation loss: 32.32192420959473 RMSE: 5.6852374\n",
      "Validation loss: 33.67229080200195 RMSE: 5.802783\n",
      "Validation loss: 30.086697578430176 RMSE: 5.485134\n",
      "43 12 4.939002990722656\n",
      "Validation loss: 34.64940643310547 RMSE: 5.8863745\n",
      "Validation loss: 31.539247512817383 RMSE: 5.6159816\n",
      "Validation loss: 37.26972007751465 RMSE: 6.104893\n",
      "46 14 2.6882758140563965\n",
      "Validation loss: 34.96965980529785 RMSE: 5.9135146\n",
      "Validation loss: 35.039002418518066 RMSE: 5.919375\n",
      "Validation loss: 33.42383575439453 RMSE: 5.7813354\n",
      "Validation loss: 35.42129898071289 RMSE: 5.9515796\n",
      "50 0 5.168140888214111\n",
      "Validation loss: 37.46860885620117 RMSE: 6.1211605\n",
      "Validation loss: 35.928059577941895 RMSE: 5.994002\n",
      "Validation loss: 34.369558334350586 RMSE: 5.862556\n",
      "53 2 5.736649513244629\n",
      "Validation loss: 37.54051971435547 RMSE: 6.127032\n",
      "Validation loss: 36.39393615722656 RMSE: 6.0327387\n",
      "Validation loss: 33.19295787811279 RMSE: 5.761333\n",
      "56 4 2.425039529800415\n",
      "Validation loss: 36.13176155090332 RMSE: 6.01097\n",
      "Validation loss: 37.448787689208984 RMSE: 6.1195416\n",
      "Validation loss: 36.40237808227539 RMSE: 6.033438\n",
      "59 6 2.0242631435394287\n",
      "Validation loss: 41.1163330078125 RMSE: 6.412202\n",
      "Validation loss: 40.83832550048828 RMSE: 6.390487\n",
      "Validation loss: 33.46901607513428 RMSE: 5.785241\n",
      "62 8 2.5339791774749756\n",
      "Validation loss: 36.858070373535156 RMSE: 6.071085\n",
      "Validation loss: 34.99136543273926 RMSE: 5.91535\n",
      "Validation loss: 34.60382843017578 RMSE: 5.882502\n",
      "65 10 3.853076934814453\n",
      "Validation loss: 34.52565383911133 RMSE: 5.8758535\n",
      "Validation loss: 37.687631607055664 RMSE: 6.1390257\n",
      "Validation loss: 38.680166244506836 RMSE: 6.219338\n",
      "68 12 1.2589033842086792\n",
      "Validation loss: 35.594160079956055 RMSE: 5.9660845\n",
      "Validation loss: 33.694098472595215 RMSE: 5.8046618\n",
      "Validation loss: 35.692129135131836 RMSE: 5.974289\n",
      "71 14 3.0580949783325195\n",
      "Validation loss: 36.965341567993164 RMSE: 6.0799127\n",
      "Validation loss: 31.768445014953613 RMSE: 5.63635\n",
      "Validation loss: 33.60445213317871 RMSE: 5.7969346\n",
      "Validation loss: 36.206871032714844 RMSE: 6.017215\n",
      "75 0 1.9364168643951416\n",
      "Validation loss: 34.34533977508545 RMSE: 5.86049\n",
      "Validation loss: 33.889774322509766 RMSE: 5.8214927\n",
      "Validation loss: 31.55371856689453 RMSE: 5.61727\n",
      "78 2 1.9328168630599976\n",
      "Validation loss: 36.37190628051758 RMSE: 6.0309124\n",
      "Validation loss: 32.55842971801758 RMSE: 5.7059994\n",
      "Validation loss: 36.648054122924805 RMSE: 6.0537634\n",
      "81 4 2.415823459625244\n",
      "Validation loss: 33.21685981750488 RMSE: 5.7634068\n",
      "Validation loss: 33.36754131317139 RMSE: 5.776464\n",
      "Validation loss: 33.85160160064697 RMSE: 5.818213\n",
      "84 6 1.9729610681533813\n",
      "Validation loss: 32.46811389923096 RMSE: 5.69808\n",
      "Validation loss: 32.19503211975098 RMSE: 5.6740665\n",
      "Validation loss: 33.57490539550781 RMSE: 5.794386\n",
      "87 8 4.1058268547058105\n",
      "Validation loss: 36.038269996643066 RMSE: 6.003188\n",
      "Validation loss: 37.25346565246582 RMSE: 6.103562\n",
      "Validation loss: 36.013614654541016 RMSE: 6.001135\n",
      "90 10 1.8994642496109009\n",
      "Validation loss: 36.41363334655762 RMSE: 6.0343714\n",
      "Validation loss: 37.527015686035156 RMSE: 6.12593\n",
      "Validation loss: 34.825613021850586 RMSE: 5.9013233\n",
      "93 12 3.2062928676605225\n",
      "Validation loss: 35.46812629699707 RMSE: 5.9555125\n",
      "Validation loss: 35.808021545410156 RMSE: 5.98398\n",
      "Validation loss: 34.22805404663086 RMSE: 5.8504744\n",
      "96 14 0.932175874710083\n",
      "Validation loss: 35.37033271789551 RMSE: 5.9472966\n",
      "Validation loss: 36.05618667602539 RMSE: 6.0046806\n",
      "Validation loss: 32.92278575897217 RMSE: 5.7378383\n",
      "Validation loss: 30.608531951904297 RMSE: 5.5324974\n",
      "100 0 1.007515549659729\n",
      "Validation loss: 38.767560958862305 RMSE: 6.2263603\n",
      "Validation loss: 36.02096748352051 RMSE: 6.001747\n",
      "Validation loss: 35.249911308288574 RMSE: 5.937164\n",
      "103 2 1.4323261976242065\n",
      "Validation loss: 36.4102668762207 RMSE: 6.034092\n",
      "Validation loss: 35.62252426147461 RMSE: 5.9684606\n",
      "Validation loss: 36.062482833862305 RMSE: 6.0052047\n",
      "106 4 1.1254626512527466\n",
      "Validation loss: 37.143117904663086 RMSE: 6.0945153\n",
      "Validation loss: 35.243896484375 RMSE: 5.936657\n",
      "Validation loss: 40.34088897705078 RMSE: 6.3514476\n",
      "109 6 2.174605369567871\n",
      "Validation loss: 35.71368980407715 RMSE: 5.9760933\n",
      "Validation loss: 34.75592803955078 RMSE: 5.895416\n",
      "Validation loss: 38.93907165527344 RMSE: 6.240118\n",
      "112 8 1.5910735130310059\n",
      "Validation loss: 37.01384735107422 RMSE: 6.0839005\n",
      "Validation loss: 37.61433219909668 RMSE: 6.133053\n",
      "Validation loss: 35.090810775756836 RMSE: 5.9237494\n",
      "115 10 1.6318598985671997\n",
      "Validation loss: 40.11822319030762 RMSE: 6.3338947\n",
      "Validation loss: 40.33652114868164 RMSE: 6.351104\n",
      "Validation loss: 41.178762435913086 RMSE: 6.417068\n",
      "118 12 1.3300377130508423\n",
      "Validation loss: 37.996103286743164 RMSE: 6.1640983\n",
      "Validation loss: 38.02569103240967 RMSE: 6.166497\n",
      "Validation loss: 39.099748611450195 RMSE: 6.252979\n",
      "121 14 1.7247748374938965\n",
      "Validation loss: 43.741790771484375 RMSE: 6.6137576\n",
      "Validation loss: 35.590415954589844 RMSE: 5.9657702\n",
      "Validation loss: 40.133548736572266 RMSE: 6.335105\n",
      "Validation loss: 37.59553909301758 RMSE: 6.1315203\n",
      "125 0 1.413415789604187\n",
      "Validation loss: 38.025630950927734 RMSE: 6.1664925\n",
      "Validation loss: 35.09243202209473 RMSE: 5.923887\n",
      "Validation loss: 35.3167781829834 RMSE: 5.9427924\n",
      "128 2 1.910952091217041\n",
      "Validation loss: 35.15497016906738 RMSE: 5.9291625\n",
      "Validation loss: 33.88900852203369 RMSE: 5.8214264\n",
      "Validation loss: 37.8299674987793 RMSE: 6.1506076\n",
      "131 4 1.219904899597168\n",
      "Validation loss: 37.97022247314453 RMSE: 6.1619983\n",
      "Validation loss: 39.29457664489746 RMSE: 6.2685385\n",
      "Validation loss: 38.12807083129883 RMSE: 6.1747937\n",
      "134 6 2.19211745262146\n",
      "Validation loss: 40.70358657836914 RMSE: 6.3799357\n",
      "Validation loss: 39.152578353881836 RMSE: 6.257202\n",
      "Validation loss: 35.870174407958984 RMSE: 5.989171\n",
      "137 8 3.840635299682617\n",
      "Validation loss: 39.80621147155762 RMSE: 6.3092165\n",
      "Validation loss: 39.244468688964844 RMSE: 6.2645407\n",
      "Validation loss: 39.446170806884766 RMSE: 6.280619\n",
      "140 10 2.9410483837127686\n",
      "Validation loss: 40.65286064147949 RMSE: 6.3759594\n",
      "Validation loss: 38.74440383911133 RMSE: 6.2245007\n",
      "Validation loss: 41.61616516113281 RMSE: 6.4510593\n",
      "143 12 1.502678632736206\n",
      "Validation loss: 39.015567779541016 RMSE: 6.2462444\n",
      "Validation loss: 39.24355888366699 RMSE: 6.2644677\n",
      "Validation loss: 35.31193542480469 RMSE: 5.942385\n",
      "146 14 1.0716145038604736\n",
      "Validation loss: 35.60136032104492 RMSE: 5.966687\n",
      "Validation loss: 42.09688186645508 RMSE: 6.488211\n",
      "Validation loss: 40.81041717529297 RMSE: 6.388303\n",
      "Validation loss: 37.75300598144531 RMSE: 6.1443477\n",
      "150 0 1.3127795457839966\n",
      "Validation loss: 39.856895446777344 RMSE: 6.313232\n",
      "Validation loss: 37.368295669555664 RMSE: 6.1129613\n",
      "Validation loss: 37.85651206970215 RMSE: 6.152765\n",
      "153 2 1.5233964920043945\n",
      "Validation loss: 37.62320327758789 RMSE: 6.1337757\n",
      "Validation loss: 41.3281364440918 RMSE: 6.428696\n",
      "Validation loss: 40.13734245300293 RMSE: 6.335404\n",
      "156 4 1.7023297548294067\n",
      "Validation loss: 37.58559989929199 RMSE: 6.1307096\n",
      "Validation loss: 39.986907958984375 RMSE: 6.32352\n",
      "Validation loss: 38.4644718170166 RMSE: 6.201973\n",
      "159 6 3.596280336380005\n",
      "Validation loss: 37.37885093688965 RMSE: 6.1138244\n",
      "Validation loss: 43.64230728149414 RMSE: 6.606232\n",
      "Validation loss: 38.1899356842041 RMSE: 6.1798005\n",
      "162 8 2.184682607650757\n",
      "Validation loss: 37.59258842468262 RMSE: 6.1312795\n",
      "Validation loss: 38.498870849609375 RMSE: 6.204746\n",
      "Validation loss: 39.16831588745117 RMSE: 6.25846\n",
      "165 10 1.2944377660751343\n",
      "Validation loss: 36.68457508087158 RMSE: 6.0567794\n",
      "Validation loss: 36.429368019104004 RMSE: 6.0356746\n",
      "Validation loss: 34.67997074127197 RMSE: 5.8889704\n",
      "168 12 1.6603871583938599\n",
      "Validation loss: 41.39047050476074 RMSE: 6.4335427\n",
      "Validation loss: 39.63279724121094 RMSE: 6.2954583\n",
      "Validation loss: 41.16619873046875 RMSE: 6.416089\n",
      "171 14 3.5034403800964355\n",
      "Validation loss: 37.34455680847168 RMSE: 6.1110196\n",
      "Validation loss: 38.00239372253418 RMSE: 6.1646085\n",
      "Validation loss: 38.701053619384766 RMSE: 6.2210174\n",
      "Validation loss: 38.63448143005371 RMSE: 6.2156644\n",
      "175 0 2.952906847000122\n",
      "Validation loss: 34.91649055480957 RMSE: 5.9090176\n",
      "Validation loss: 36.49572944641113 RMSE: 6.0411696\n",
      "Validation loss: 39.61130714416504 RMSE: 6.2937517\n",
      "178 2 1.8961012363433838\n",
      "Validation loss: 34.69124889373779 RMSE: 5.889928\n",
      "Validation loss: 35.38509750366211 RMSE: 5.9485373\n",
      "Validation loss: 37.079145431518555 RMSE: 6.0892644\n",
      "181 4 1.294865369796753\n",
      "Validation loss: 35.81622886657715 RMSE: 5.9846663\n",
      "Validation loss: 37.89000701904297 RMSE: 6.155486\n",
      "Validation loss: 36.72414588928223 RMSE: 6.060045\n",
      "184 6 1.2556859254837036\n",
      "Validation loss: 35.23647117614746 RMSE: 5.936032\n",
      "Validation loss: 38.45241165161133 RMSE: 6.2010007\n",
      "Validation loss: 34.29823303222656 RMSE: 5.856469\n",
      "187 8 0.9318453073501587\n",
      "Validation loss: 34.60996627807617 RMSE: 5.8830237\n",
      "Validation loss: 35.38453483581543 RMSE: 5.94849\n",
      "Validation loss: 33.76820087432861 RMSE: 5.8110414\n",
      "190 10 1.7124768495559692\n",
      "Validation loss: 37.169729232788086 RMSE: 6.0966983\n",
      "Validation loss: 35.31636428833008 RMSE: 5.9427576\n",
      "Validation loss: 39.68901443481445 RMSE: 6.2999215\n",
      "193 12 1.2030088901519775\n",
      "Validation loss: 37.78424835205078 RMSE: 6.146889\n",
      "Validation loss: 42.123220443725586 RMSE: 6.4902406\n",
      "Validation loss: 40.02555847167969 RMSE: 6.3265758\n",
      "196 14 1.7464768886566162\n",
      "Validation loss: 39.16500186920166 RMSE: 6.258195\n",
      "Validation loss: 41.1082706451416 RMSE: 6.411573\n",
      "Validation loss: 38.683298110961914 RMSE: 6.2195897\n",
      "Validation loss: 35.113365173339844 RMSE: 5.925653\n",
      "Loaded trained model with success.\n",
      "Test loss: 8.752708728496845 Test RMSE: 2.9584978\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:1', 'task_name': 'freesolv', 'model_type': 'gin', 'model': {'num_layer': 3, 'emb_dim': 64, 'feat_dim': 64, 'drop_ratio': 0.5, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 781, 'task': 'regression', 'data_path': 'data/freesolv/freesolv.csv', 'target': ['expt']}}\n",
      "Running on: cuda:1\n",
      "641\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/641\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 16.523387908935547\n",
      "Validation loss: 63.394229888916016 RMSE: 7.9620495\n",
      "Validation loss: 58.06632995605469 RMSE: 7.6201262\n",
      "Validation loss: 52.03696823120117 RMSE: 7.2136655\n",
      "3 2 6.0417022705078125\n",
      "Validation loss: 43.68758773803711 RMSE: 6.6096587\n",
      "Validation loss: 34.4345817565918 RMSE: 5.8680987\n",
      "Validation loss: 28.644527435302734 RMSE: 5.3520584\n",
      "6 4 4.237954139709473\n",
      "Validation loss: 29.08510112762451 RMSE: 5.3930607\n",
      "Validation loss: 28.096332550048828 RMSE: 5.300597\n",
      "Validation loss: 25.83794403076172 RMSE: 5.0831037\n",
      "9 6 4.876662731170654\n",
      "Validation loss: 27.746384620666504 RMSE: 5.2674837\n",
      "Validation loss: 27.462337493896484 RMSE: 5.2404523\n",
      "Validation loss: 26.771790504455566 RMSE: 5.1741467\n",
      "12 8 6.22295618057251\n",
      "Validation loss: 25.852773666381836 RMSE: 5.0845623\n",
      "Validation loss: 24.539494514465332 RMSE: 4.9537354\n",
      "Validation loss: 25.665465354919434 RMSE: 5.066109\n",
      "15 10 6.625997066497803\n",
      "Validation loss: 28.77774429321289 RMSE: 5.364489\n",
      "Validation loss: 25.863306999206543 RMSE: 5.085598\n",
      "Validation loss: 28.033021926879883 RMSE: 5.294622\n",
      "18 12 3.150982141494751\n",
      "Validation loss: 25.220404148101807 RMSE: 5.0219917\n",
      "Validation loss: 28.566494941711426 RMSE: 5.3447638\n",
      "Validation loss: 29.341078758239746 RMSE: 5.4167404\n",
      "21 14 4.377978801727295\n",
      "Validation loss: 26.840457439422607 RMSE: 5.1807775\n",
      "Validation loss: 28.08487892150879 RMSE: 5.299517\n",
      "Validation loss: 31.42120933532715 RMSE: 5.605462\n",
      "Validation loss: 25.83457088470459 RMSE: 5.0827723\n",
      "25 0 4.269043922424316\n",
      "Validation loss: 28.386367797851562 RMSE: 5.3278856\n",
      "Validation loss: 29.245261192321777 RMSE: 5.407889\n",
      "Validation loss: 26.881299018859863 RMSE: 5.184718\n",
      "28 2 4.459248065948486\n",
      "Validation loss: 28.17354393005371 RMSE: 5.3078756\n",
      "Validation loss: 28.722936630249023 RMSE: 5.3593783\n",
      "Validation loss: 23.80001926422119 RMSE: 4.878526\n",
      "31 4 7.125214099884033\n",
      "Validation loss: 27.53670883178711 RMSE: 5.2475433\n",
      "Validation loss: 27.61318588256836 RMSE: 5.254825\n",
      "Validation loss: 26.73657512664795 RMSE: 5.1707425\n",
      "34 6 2.3079590797424316\n",
      "Validation loss: 25.87998867034912 RMSE: 5.087238\n",
      "Validation loss: 27.752403259277344 RMSE: 5.268055\n",
      "Validation loss: 27.83940887451172 RMSE: 5.276306\n",
      "37 8 4.602085590362549\n",
      "Validation loss: 27.761104583740234 RMSE: 5.268881\n",
      "Validation loss: 24.836901664733887 RMSE: 4.9836636\n",
      "Validation loss: 28.968503952026367 RMSE: 5.38224\n",
      "40 10 3.213404417037964\n",
      "Validation loss: 27.659652709960938 RMSE: 5.2592444\n",
      "Validation loss: 29.40495491027832 RMSE: 5.4226336\n",
      "Validation loss: 28.584999084472656 RMSE: 5.346494\n",
      "43 12 4.309898853302002\n",
      "Validation loss: 26.2781400680542 RMSE: 5.1262207\n",
      "Validation loss: 31.134425163269043 RMSE: 5.579823\n",
      "Validation loss: 26.589046478271484 RMSE: 5.156457\n",
      "46 14 6.035799026489258\n",
      "Validation loss: 27.80014419555664 RMSE: 5.2725844\n",
      "Validation loss: 27.231525421142578 RMSE: 5.218384\n",
      "Validation loss: 28.4106502532959 RMSE: 5.330164\n",
      "Validation loss: 31.40319061279297 RMSE: 5.603855\n",
      "50 0 3.3780221939086914\n",
      "Validation loss: 26.878554344177246 RMSE: 5.184453\n",
      "Validation loss: 29.0292911529541 RMSE: 5.387884\n",
      "Validation loss: 28.393680572509766 RMSE: 5.3285723\n",
      "53 2 4.118660926818848\n",
      "Validation loss: 29.1596736907959 RMSE: 5.3999696\n",
      "Validation loss: 31.185436248779297 RMSE: 5.584392\n",
      "Validation loss: 28.5452938079834 RMSE: 5.3427796\n",
      "56 4 4.978497505187988\n",
      "Validation loss: 29.062299728393555 RMSE: 5.390946\n",
      "Validation loss: 31.13595962524414 RMSE: 5.579961\n",
      "Validation loss: 28.557836532592773 RMSE: 5.3439536\n",
      "59 6 3.494065046310425\n",
      "Validation loss: 28.820570945739746 RMSE: 5.3684793\n",
      "Validation loss: 29.316575050354004 RMSE: 5.4144783\n",
      "Validation loss: 29.73137378692627 RMSE: 5.452648\n",
      "62 8 2.6342403888702393\n",
      "Validation loss: 28.21394443511963 RMSE: 5.3116803\n",
      "Validation loss: 29.686439514160156 RMSE: 5.4485264\n",
      "Validation loss: 29.38062858581543 RMSE: 5.42039\n",
      "65 10 3.956110954284668\n",
      "Validation loss: 29.579830169677734 RMSE: 5.438734\n",
      "Validation loss: 34.304402351379395 RMSE: 5.856996\n",
      "Validation loss: 32.346134185791016 RMSE: 5.687366\n",
      "68 12 1.6315033435821533\n",
      "Validation loss: 31.228443145751953 RMSE: 5.5882416\n",
      "Validation loss: 30.498409271240234 RMSE: 5.5225363\n",
      "Validation loss: 28.03573989868164 RMSE: 5.2948785\n",
      "71 14 2.121981382369995\n",
      "Validation loss: 32.128570556640625 RMSE: 5.668207\n",
      "Validation loss: 31.200355529785156 RMSE: 5.5857277\n",
      "Validation loss: 32.863224029541016 RMSE: 5.732645\n",
      "Validation loss: 30.93748664855957 RMSE: 5.5621476\n",
      "75 0 2.1122283935546875\n",
      "Validation loss: 31.09050178527832 RMSE: 5.575886\n",
      "Validation loss: 32.6230525970459 RMSE: 5.7116594\n",
      "Validation loss: 33.50584602355957 RMSE: 5.788423\n",
      "78 2 2.2112784385681152\n",
      "Validation loss: 31.969209671020508 RMSE: 5.654132\n",
      "Validation loss: 33.29822063446045 RMSE: 5.770461\n",
      "Validation loss: 34.664798736572266 RMSE: 5.887682\n",
      "81 4 4.129698753356934\n",
      "Validation loss: 30.02055549621582 RMSE: 5.4791017\n",
      "Validation loss: 34.04106903076172 RMSE: 5.8344727\n",
      "Validation loss: 32.168782234191895 RMSE: 5.671753\n",
      "84 6 3.421142101287842\n",
      "Validation loss: 34.52199172973633 RMSE: 5.8755417\n",
      "Validation loss: 32.69951057434082 RMSE: 5.7183485\n",
      "Validation loss: 32.80032253265381 RMSE: 5.7271566\n",
      "87 8 1.2786906957626343\n",
      "Validation loss: 30.903045654296875 RMSE: 5.559051\n",
      "Validation loss: 31.678643226623535 RMSE: 5.6283784\n",
      "Validation loss: 28.616287231445312 RMSE: 5.349419\n",
      "90 10 4.381117343902588\n",
      "Validation loss: 30.606298446655273 RMSE: 5.532296\n",
      "Validation loss: 32.15883922576904 RMSE: 5.670876\n",
      "Validation loss: 30.779842376708984 RMSE: 5.5479584\n",
      "93 12 2.3047680854797363\n",
      "Validation loss: 30.70551300048828 RMSE: 5.5412555\n",
      "Validation loss: 30.69658374786377 RMSE: 5.5404496\n",
      "Validation loss: 31.425427436828613 RMSE: 5.605839\n",
      "96 14 2.6845908164978027\n",
      "Validation loss: 31.08387279510498 RMSE: 5.575291\n",
      "Validation loss: 32.208356857299805 RMSE: 5.675241\n",
      "Validation loss: 29.513997077941895 RMSE: 5.4326787\n",
      "Validation loss: 27.914769172668457 RMSE: 5.283443\n",
      "100 0 2.9125752449035645\n",
      "Validation loss: 30.568519592285156 RMSE: 5.52888\n",
      "Validation loss: 28.600974082946777 RMSE: 5.347988\n",
      "Validation loss: 31.55156898498535 RMSE: 5.6170783\n",
      "103 2 2.2894062995910645\n",
      "Validation loss: 30.7898006439209 RMSE: 5.548856\n",
      "Validation loss: 33.32672309875488 RMSE: 5.77293\n",
      "Validation loss: 31.605905532836914 RMSE: 5.621913\n",
      "106 4 1.715071439743042\n",
      "Validation loss: 30.62223243713379 RMSE: 5.5337358\n",
      "Validation loss: 30.089487075805664 RMSE: 5.4853883\n",
      "Validation loss: 32.94895553588867 RMSE: 5.740118\n",
      "109 6 1.8516759872436523\n",
      "Validation loss: 30.57455825805664 RMSE: 5.5294266\n",
      "Validation loss: 30.041942596435547 RMSE: 5.481053\n",
      "Validation loss: 26.77567195892334 RMSE: 5.1745214\n",
      "112 8 2.334528684616089\n",
      "Validation loss: 29.33730983734131 RMSE: 5.4163923\n",
      "Validation loss: 32.524314880371094 RMSE: 5.703009\n",
      "Validation loss: 29.994585037231445 RMSE: 5.4767313\n",
      "115 10 3.039458751678467\n",
      "Validation loss: 28.9416561126709 RMSE: 5.379745\n",
      "Validation loss: 29.54641342163086 RMSE: 5.4356613\n",
      "Validation loss: 28.68010711669922 RMSE: 5.355381\n",
      "118 12 2.789044141769409\n",
      "Validation loss: 31.889227867126465 RMSE: 5.647055\n",
      "Validation loss: 30.44603157043457 RMSE: 5.517792\n",
      "Validation loss: 28.847993850708008 RMSE: 5.3710327\n",
      "121 14 1.6198608875274658\n",
      "Validation loss: 31.151952743530273 RMSE: 5.5813932\n",
      "Validation loss: 29.1491756439209 RMSE: 5.398998\n",
      "Validation loss: 29.899008750915527 RMSE: 5.4679985\n",
      "Validation loss: 30.971633911132812 RMSE: 5.5652165\n",
      "125 0 1.7916799783706665\n",
      "Validation loss: 30.566079139709473 RMSE: 5.52866\n",
      "Validation loss: 31.6857967376709 RMSE: 5.6290135\n",
      "Validation loss: 29.968355178833008 RMSE: 5.474336\n",
      "128 2 1.171280860900879\n",
      "Validation loss: 29.992812156677246 RMSE: 5.4765697\n",
      "Validation loss: 30.91180992126465 RMSE: 5.559839\n",
      "Validation loss: 34.55319595336914 RMSE: 5.8781962\n",
      "131 4 1.9946043491363525\n",
      "Validation loss: 29.531408309936523 RMSE: 5.434281\n",
      "Validation loss: 30.795419692993164 RMSE: 5.549362\n",
      "Validation loss: 31.839017868041992 RMSE: 5.642607\n",
      "134 6 2.0647330284118652\n",
      "Validation loss: 31.036531448364258 RMSE: 5.5710444\n",
      "Validation loss: 35.28982162475586 RMSE: 5.940523\n",
      "Validation loss: 31.973142623901367 RMSE: 5.6544795\n",
      "137 8 2.0255730152130127\n",
      "Validation loss: 30.98449993133545 RMSE: 5.566372\n",
      "Validation loss: 32.59352111816406 RMSE: 5.709073\n",
      "Validation loss: 32.65499496459961 RMSE: 5.714455\n",
      "140 10 1.551040530204773\n",
      "Validation loss: 31.330671310424805 RMSE: 5.5973806\n",
      "Validation loss: 31.26662254333496 RMSE: 5.5916567\n",
      "Validation loss: 30.933135986328125 RMSE: 5.5617566\n",
      "143 12 1.2710700035095215\n",
      "Validation loss: 30.79243564605713 RMSE: 5.5490932\n",
      "Validation loss: 33.348154067993164 RMSE: 5.774786\n",
      "Validation loss: 31.273412704467773 RMSE: 5.5922637\n",
      "146 14 0.7989996671676636\n",
      "Validation loss: 33.1313533782959 RMSE: 5.755984\n",
      "Validation loss: 31.047565460205078 RMSE: 5.5720344\n",
      "Validation loss: 30.826765060424805 RMSE: 5.5521855\n",
      "Validation loss: 32.82498741149902 RMSE: 5.7293096\n",
      "150 0 1.5584659576416016\n",
      "Validation loss: 32.722177505493164 RMSE: 5.72033\n",
      "Validation loss: 32.107038497924805 RMSE: 5.6663074\n",
      "Validation loss: 33.78059768676758 RMSE: 5.8121076\n",
      "153 2 1.6920543909072876\n",
      "Validation loss: 33.51551818847656 RMSE: 5.789259\n",
      "Validation loss: 32.76156997680664 RMSE: 5.7237725\n",
      "Validation loss: 31.81650733947754 RMSE: 5.6406126\n",
      "156 4 5.311330318450928\n",
      "Validation loss: 32.269859313964844 RMSE: 5.6806564\n",
      "Validation loss: 32.64381980895996 RMSE: 5.713477\n",
      "Validation loss: 34.27542495727539 RMSE: 5.854522\n",
      "159 6 2.1299500465393066\n",
      "Validation loss: 34.761122703552246 RMSE: 5.8958564\n",
      "Validation loss: 32.485267639160156 RMSE: 5.699585\n",
      "Validation loss: 31.53192138671875 RMSE: 5.6153293\n",
      "162 8 3.3467190265655518\n",
      "Validation loss: 36.437124252319336 RMSE: 6.0363173\n",
      "Validation loss: 33.53940486907959 RMSE: 5.7913218\n",
      "Validation loss: 33.435224533081055 RMSE: 5.78232\n",
      "165 10 1.8409185409545898\n",
      "Validation loss: 34.21082592010498 RMSE: 5.8490024\n",
      "Validation loss: 33.438602447509766 RMSE: 5.782612\n",
      "Validation loss: 32.3013482093811 RMSE: 5.6834273\n",
      "168 12 2.9279003143310547\n",
      "Validation loss: 33.44705772399902 RMSE: 5.783343\n",
      "Validation loss: 31.623912811279297 RMSE: 5.623514\n",
      "Validation loss: 33.370121002197266 RMSE: 5.7766876\n",
      "171 14 1.743818759918213\n",
      "Validation loss: 35.29155158996582 RMSE: 5.9406695\n",
      "Validation loss: 34.922346115112305 RMSE: 5.9095125\n",
      "Validation loss: 32.896820068359375 RMSE: 5.7355747\n",
      "Validation loss: 33.330519676208496 RMSE: 5.773259\n",
      "175 0 1.3227096796035767\n",
      "Validation loss: 32.190975189208984 RMSE: 5.673709\n",
      "Validation loss: 33.81038856506348 RMSE: 5.81467\n",
      "Validation loss: 31.24042797088623 RMSE: 5.5893135\n",
      "178 2 1.631088376045227\n",
      "Validation loss: 33.78623962402344 RMSE: 5.812593\n",
      "Validation loss: 33.83011245727539 RMSE: 5.8163657\n",
      "Validation loss: 32.70102787017822 RMSE: 5.718481\n",
      "181 4 2.068770170211792\n",
      "Validation loss: 31.615453720092773 RMSE: 5.622762\n",
      "Validation loss: 31.482471466064453 RMSE: 5.6109242\n",
      "Validation loss: 33.012939453125 RMSE: 5.745689\n",
      "184 6 1.9528706073760986\n",
      "Validation loss: 34.37668228149414 RMSE: 5.863163\n",
      "Validation loss: 33.48280715942383 RMSE: 5.786433\n",
      "Validation loss: 31.554526329040527 RMSE: 5.6173415\n",
      "187 8 1.8986525535583496\n",
      "Validation loss: 33.87863254547119 RMSE: 5.820535\n",
      "Validation loss: 31.63733673095703 RMSE: 5.6247077\n",
      "Validation loss: 35.08826446533203 RMSE: 5.923535\n",
      "190 10 1.7892379760742188\n",
      "Validation loss: 32.50274848937988 RMSE: 5.701118\n",
      "Validation loss: 34.8873176574707 RMSE: 5.9065485\n",
      "Validation loss: 34.57171440124512 RMSE: 5.8797717\n",
      "193 12 1.2510106563568115\n",
      "Validation loss: 32.64871120452881 RMSE: 5.7139053\n",
      "Validation loss: 34.03385543823242 RMSE: 5.833854\n",
      "Validation loss: 36.02242660522461 RMSE: 6.0018687\n",
      "196 14 2.1372523307800293\n",
      "Validation loss: 33.66938018798828 RMSE: 5.802532\n",
      "Validation loss: 33.18193435668945 RMSE: 5.7603765\n",
      "Validation loss: 32.9973201751709 RMSE: 5.7443295\n",
      "Validation loss: 34.683671951293945 RMSE: 5.889284\n",
      "Loaded trained model with success.\n",
      "Test loss: 14.433881466205303 Test RMSE: 3.7991948\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:1', 'task_name': 'esol', 'model_type': 'gin', 'model': {'num_layer': 5, 'emb_dim': 300, 'feat_dim': 300, 'drop_ratio': 0.3, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 777, 'task': 'regression', 'data_path': 'data/esol/esol.csv', 'target': ['measured log solubility in mols per litre']}}\n",
      "Running on: cuda:1\n",
      "1127\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/1127\n",
      "Generating scaffold 1000/1127\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 7.851883888244629\n",
      "Validation loss: 6.85554759270322 RMSE: 2.61831\n",
      "1 21 1.7157374620437622\n",
      "Validation loss: 11.211888676196073 RMSE: 3.3484159\n",
      "Validation loss: 12.50973973232033 RMSE: 3.536911\n",
      "3 13 1.3205991983413696\n",
      "Validation loss: 12.758448921473681 RMSE: 3.571897\n",
      "Validation loss: 12.934867099323103 RMSE: 3.5965075\n",
      "5 5 1.3380266427993774\n",
      "Validation loss: 16.117310177963393 RMSE: 4.014637\n",
      "6 26 1.3561956882476807\n",
      "Validation loss: 17.933626183366354 RMSE: 4.2348113\n",
      "Validation loss: 13.48358769543403 RMSE: 3.6720006\n",
      "8 18 1.3074119091033936\n",
      "Validation loss: 12.85865072233487 RMSE: 3.5858963\n",
      "Validation loss: 9.354589673270166 RMSE: 3.0585275\n",
      "10 10 1.5158765316009521\n",
      "Validation loss: 12.195014624469048 RMSE: 3.492136\n",
      "Validation loss: 9.391357160247532 RMSE: 3.0645323\n",
      "12 2 0.6952447295188904\n",
      "Validation loss: 11.864252714984184 RMSE: 3.4444525\n",
      "13 23 0.9483497738838196\n",
      "Validation loss: 10.436314456230772 RMSE: 3.2305286\n",
      "Validation loss: 14.039023821332814 RMSE: 3.7468684\n",
      "15 15 0.5923830270767212\n",
      "Validation loss: 16.283281942384434 RMSE: 4.035255\n",
      "Validation loss: 12.023502046028069 RMSE: 3.4674923\n",
      "17 7 1.4316670894622803\n",
      "Validation loss: 12.05238895078676 RMSE: 3.4716551\n",
      "18 28 0.9391363263130188\n",
      "Validation loss: 9.971331765166425 RMSE: 3.1577413\n",
      "Validation loss: 11.103786974881602 RMSE: 3.3322344\n",
      "20 20 1.682701587677002\n",
      "Validation loss: 12.505771130587148 RMSE: 3.53635\n",
      "Validation loss: 11.685538941780022 RMSE: 3.4184117\n",
      "22 12 0.6966823935508728\n",
      "Validation loss: 8.258667566080009 RMSE: 2.8737898\n",
      "Validation loss: 16.140460360366685 RMSE: 4.0175195\n",
      "24 4 0.9107686281204224\n",
      "Validation loss: 15.20499965363899 RMSE: 3.8993587\n",
      "25 25 0.4539617598056793\n",
      "Validation loss: 16.656917310394018 RMSE: 4.081289\n",
      "Validation loss: 8.05114880283322 RMSE: 2.8374546\n",
      "27 17 0.6838868856430054\n",
      "Validation loss: 14.996922265111873 RMSE: 3.872586\n",
      "Validation loss: 12.78756907977889 RMSE: 3.5759711\n",
      "29 9 0.8032304048538208\n",
      "Validation loss: 12.771366929585955 RMSE: 3.5737052\n",
      "Validation loss: 6.623540055435316 RMSE: 2.573624\n",
      "31 1 0.928033709526062\n",
      "Validation loss: 8.741174744293753 RMSE: 2.9565477\n",
      "32 22 0.6291850805282593\n",
      "Validation loss: 12.230930716590544 RMSE: 3.4972746\n",
      "Validation loss: 11.245021718793211 RMSE: 3.3533597\n",
      "34 14 1.0748546123504639\n",
      "Validation loss: 10.520141618441691 RMSE: 3.2434766\n",
      "Validation loss: 9.774739915290764 RMSE: 3.126458\n",
      "36 6 0.7324355840682983\n",
      "Validation loss: 11.683615456640194 RMSE: 3.4181304\n",
      "37 27 1.1241371631622314\n",
      "Validation loss: 14.228287688398783 RMSE: 3.7720404\n",
      "Validation loss: 14.610667583161751 RMSE: 3.8223903\n",
      "39 19 0.8244720697402954\n",
      "Validation loss: 16.225948013035598 RMSE: 4.0281444\n",
      "Validation loss: 12.88055542296013 RMSE: 3.588949\n",
      "41 11 1.0944470167160034\n",
      "Validation loss: 15.118348045686705 RMSE: 3.888232\n",
      "Validation loss: 12.838204375410502 RMSE: 3.5830438\n",
      "43 3 0.6477464437484741\n",
      "Validation loss: 11.290302795646465 RMSE: 3.3601046\n",
      "44 24 1.0958693027496338\n",
      "Validation loss: 12.426377904098645 RMSE: 3.5251067\n",
      "Validation loss: 7.270476619754217 RMSE: 2.696382\n",
      "46 16 0.5965009927749634\n",
      "Validation loss: 9.548931704158276 RMSE: 3.0901346\n",
      "Validation loss: 12.063454028779427 RMSE: 3.4732482\n",
      "48 8 0.5772160887718201\n",
      "Validation loss: 13.304314402352393 RMSE: 3.647508\n",
      "Validation loss: 10.0894215339053 RMSE: 3.176385\n",
      "50 0 0.4843212366104126\n",
      "Validation loss: 9.05869349758182 RMSE: 3.0097663\n",
      "51 21 0.6449159979820251\n",
      "Validation loss: 13.745269471565178 RMSE: 3.7074614\n",
      "Validation loss: 14.635731958709988 RMSE: 3.8256676\n",
      "53 13 0.5997264385223389\n",
      "Validation loss: 12.793798623886783 RMSE: 3.5768418\n",
      "Validation loss: 11.612982066331712 RMSE: 3.4077823\n",
      "55 5 0.9217814803123474\n",
      "Validation loss: 10.105802392537615 RMSE: 3.1789622\n",
      "56 26 1.4700300693511963\n",
      "Validation loss: 12.581988368414144 RMSE: 3.5471098\n",
      "Validation loss: 5.9479922995103145 RMSE: 2.4388506\n",
      "58 18 0.9036045670509338\n",
      "Validation loss: 8.261387073888187 RMSE: 2.8742628\n",
      "Validation loss: 14.22142712415847 RMSE: 3.7711308\n",
      "60 10 1.0780426263809204\n",
      "Validation loss: 14.786252604121655 RMSE: 3.8452897\n",
      "Validation loss: 13.629447033974976 RMSE: 3.691808\n",
      "62 2 0.6278347373008728\n",
      "Validation loss: 11.796354808638581 RMSE: 3.434582\n",
      "63 23 0.3166837990283966\n",
      "Validation loss: 11.992306582695615 RMSE: 3.462991\n",
      "Validation loss: 11.179667447520568 RMSE: 3.3436007\n",
      "65 15 0.35058650374412537\n",
      "Validation loss: 12.466020136807872 RMSE: 3.530725\n",
      "Validation loss: 14.123298028929044 RMSE: 3.7580976\n",
      "67 7 0.5342824459075928\n",
      "Validation loss: 9.490140315705696 RMSE: 3.0806074\n",
      "68 28 3.6838738918304443\n",
      "Validation loss: 7.854579630151259 RMSE: 2.8026023\n",
      "Validation loss: 10.197267667382164 RMSE: 3.193316\n",
      "70 20 0.7545217871665955\n",
      "Validation loss: 10.203108116588762 RMSE: 3.1942303\n",
      "Validation loss: 10.902794441290661 RMSE: 3.3019378\n",
      "72 12 0.9520332217216492\n",
      "Validation loss: 8.75479933645873 RMSE: 2.958851\n",
      "Validation loss: 10.53142259395228 RMSE: 3.2452154\n",
      "74 4 0.8487349152565002\n",
      "Validation loss: 9.72024858525369 RMSE: 3.1177313\n",
      "75 25 0.565328061580658\n",
      "Validation loss: 10.169559445001383 RMSE: 3.1889746\n",
      "Validation loss: 13.345029771855447 RMSE: 3.6530848\n",
      "77 17 0.5232581496238708\n",
      "Validation loss: 11.344752796983297 RMSE: 3.3681972\n",
      "Validation loss: 9.021390560453971 RMSE: 3.003563\n",
      "79 9 0.8081820011138916\n",
      "Validation loss: 7.78500290254576 RMSE: 2.7901616\n",
      "Validation loss: 11.748184972104774 RMSE: 3.4275625\n",
      "81 1 0.8616446852684021\n",
      "Validation loss: 7.010080489437137 RMSE: 2.6476557\n",
      "82 22 0.6655190587043762\n",
      "Validation loss: 11.006932689025339 RMSE: 3.3176699\n",
      "Validation loss: 12.52895661581934 RMSE: 3.5396266\n",
      "84 14 0.8345679640769958\n",
      "Validation loss: 15.203388872399794 RMSE: 3.8991523\n",
      "Validation loss: 12.80103756052203 RMSE: 3.5778537\n",
      "86 6 0.8512534499168396\n",
      "Validation loss: 9.440593896713931 RMSE: 3.0725548\n",
      "87 27 0.952470600605011\n",
      "Validation loss: 13.610628431877204 RMSE: 3.6892586\n",
      "Validation loss: 9.67648870755086 RMSE: 3.1107056\n",
      "89 19 0.3573872447013855\n",
      "Validation loss: 12.168876445398922 RMSE: 3.4883916\n",
      "Validation loss: 11.62775273449653 RMSE: 3.4099488\n",
      "91 11 0.4264490306377411\n",
      "Validation loss: 11.45619545151702 RMSE: 3.3847003\n",
      "Validation loss: 10.823622956740119 RMSE: 3.2899277\n",
      "93 3 0.47942864894866943\n",
      "Validation loss: 8.159861142656444 RMSE: 2.856547\n",
      "94 24 0.48840126395225525\n",
      "Validation loss: 8.52772936778786 RMSE: 2.9202278\n",
      "Validation loss: 13.308524342764795 RMSE: 3.648085\n",
      "96 16 0.5378037095069885\n",
      "Validation loss: 9.624207420686705 RMSE: 3.1022906\n",
      "Validation loss: 6.1853543003048514 RMSE: 2.4870374\n",
      "98 8 0.811040997505188\n",
      "Validation loss: 4.646230047782965 RMSE: 2.1555114\n",
      "Validation loss: 9.854027250171763 RMSE: 3.1391125\n",
      "100 0 0.533471405506134\n",
      "Validation loss: 9.321285492551011 RMSE: 3.0530782\n",
      "101 21 0.39688172936439514\n",
      "Validation loss: 7.880642080728987 RMSE: 2.807248\n",
      "Validation loss: 11.994480977016213 RMSE: 3.4633048\n",
      "103 13 0.44060441851615906\n",
      "Validation loss: 12.248515669223481 RMSE: 3.499788\n",
      "Validation loss: 9.698639228280667 RMSE: 3.1142638\n",
      "105 5 0.9228436946868896\n",
      "Validation loss: 14.522572331723914 RMSE: 3.8108494\n",
      "106 26 0.5602243542671204\n",
      "Validation loss: 12.75134300974618 RMSE: 3.5709023\n",
      "Validation loss: 9.755941956444124 RMSE: 3.1234503\n",
      "108 18 0.3841656744480133\n",
      "Validation loss: 5.66619255690448 RMSE: 2.3803763\n",
      "Validation loss: 13.03035239835756 RMSE: 3.6097581\n",
      "110 10 1.0495355129241943\n",
      "Validation loss: 10.920843014674904 RMSE: 3.3046699\n",
      "Validation loss: 11.592803583735913 RMSE: 3.4048207\n",
      "112 2 0.7844844460487366\n",
      "Validation loss: 12.3618696516594 RMSE: 3.5159447\n",
      "113 23 0.5773066878318787\n",
      "Validation loss: 13.650670389158536 RMSE: 3.6946816\n",
      "Validation loss: 8.851616627347154 RMSE: 2.9751668\n",
      "115 15 0.8010344505310059\n",
      "Validation loss: 10.76434314356441 RMSE: 3.2809057\n",
      "Validation loss: 12.858733540087675 RMSE: 3.5859077\n",
      "117 7 0.5937286019325256\n",
      "Validation loss: 10.190467741637104 RMSE: 3.1922512\n",
      "118 28 0.6792827844619751\n",
      "Validation loss: 11.222328135397582 RMSE: 3.3499744\n",
      "Validation loss: 14.045059567004179 RMSE: 3.747674\n",
      "120 20 0.3381027579307556\n",
      "Validation loss: 12.868261354159465 RMSE: 3.587236\n",
      "Validation loss: 11.18673407292999 RMSE: 3.3446577\n",
      "122 12 0.8021899461746216\n",
      "Validation loss: 13.238083476513888 RMSE: 3.638418\n",
      "Validation loss: 13.95831133412049 RMSE: 3.7360823\n",
      "124 4 0.4610236585140228\n",
      "Validation loss: 10.658423263414772 RMSE: 3.264724\n",
      "125 25 0.41129523515701294\n",
      "Validation loss: 12.244565558644522 RMSE: 3.4992237\n",
      "Validation loss: 10.700845422997938 RMSE: 3.2712147\n",
      "127 17 0.3491915166378021\n",
      "Validation loss: 11.995623065307077 RMSE: 3.4634697\n",
      "Validation loss: 8.399187640806215 RMSE: 2.8981352\n",
      "129 9 0.8243268132209778\n",
      "Validation loss: 11.146541629217367 RMSE: 3.3386438\n",
      "Validation loss: 14.579275747316073 RMSE: 3.8182817\n",
      "131 1 0.728831946849823\n",
      "Validation loss: 8.263930168827024 RMSE: 2.8747053\n",
      "132 22 0.4385426938533783\n",
      "Validation loss: 11.411060713033761 RMSE: 3.378026\n",
      "Validation loss: 11.084161927214765 RMSE: 3.3292885\n",
      "134 14 0.4791804850101471\n",
      "Validation loss: 9.724531545048267 RMSE: 3.118418\n",
      "Validation loss: 10.91951817537831 RMSE: 3.3044696\n",
      "136 6 0.788788914680481\n",
      "Validation loss: 13.417365150114076 RMSE: 3.6629722\n",
      "137 27 0.2765147089958191\n",
      "Validation loss: 12.966065052336296 RMSE: 3.6008422\n",
      "Validation loss: 11.706233741962805 RMSE: 3.4214375\n",
      "139 19 0.4894562065601349\n",
      "Validation loss: 11.357722940698133 RMSE: 3.3701222\n",
      "Validation loss: 6.137473865947892 RMSE: 2.4773927\n",
      "141 11 0.29435622692108154\n",
      "Validation loss: 10.7884732474268 RMSE: 3.2845812\n",
      "Validation loss: 16.235096526356926 RMSE: 4.0292797\n",
      "143 3 0.462202787399292\n",
      "Validation loss: 8.239149650641247 RMSE: 2.870392\n",
      "144 24 0.6777210235595703\n",
      "Validation loss: 7.197820460901851 RMSE: 2.6828754\n",
      "Validation loss: 9.033546397116332 RMSE: 3.0055861\n",
      "146 16 0.6943234801292419\n",
      "Validation loss: 9.752858668302013 RMSE: 3.1229568\n",
      "Validation loss: 7.704913130903666 RMSE: 2.7757726\n",
      "148 8 0.9145590662956238\n",
      "Validation loss: 8.496139982105356 RMSE: 2.914814\n",
      "Validation loss: 7.704596215644769 RMSE: 2.7757154\n",
      "150 0 0.4511207938194275\n",
      "Validation loss: 7.8724729740514165 RMSE: 2.8057926\n",
      "151 21 0.46563807129859924\n",
      "Validation loss: 12.5070629204269 RMSE: 3.5365326\n",
      "Validation loss: 11.565605796543897 RMSE: 3.4008243\n",
      "153 13 0.40366417169570923\n",
      "Validation loss: 4.0601208484278315 RMSE: 2.014974\n",
      "Validation loss: 6.567800724400883 RMSE: 2.562772\n",
      "155 5 0.391464501619339\n",
      "Validation loss: 5.492001217023462 RMSE: 2.3435018\n",
      "156 26 0.6606211066246033\n",
      "Validation loss: 4.8119823658360845 RMSE: 2.193623\n",
      "Validation loss: 11.278952615450969 RMSE: 3.3584151\n",
      "158 18 0.8395015597343445\n",
      "Validation loss: 16.781641825110512 RMSE: 4.09654\n",
      "Validation loss: 12.762332072300193 RMSE: 3.5724406\n",
      "160 10 0.5297560095787048\n",
      "Validation loss: 10.31184109544332 RMSE: 3.2112055\n",
      "Validation loss: 7.377887987457545 RMSE: 2.7162268\n",
      "162 2 0.24610885977745056\n",
      "Validation loss: 12.051006570326544 RMSE: 3.471456\n",
      "163 23 0.2796728014945984\n",
      "Validation loss: 9.598234231493114 RMSE: 3.0981016\n",
      "Validation loss: 7.312635472390504 RMSE: 2.7041886\n",
      "165 15 0.3791183829307556\n",
      "Validation loss: 7.256935799016362 RMSE: 2.69387\n",
      "Validation loss: 11.808153523808032 RMSE: 3.4362996\n",
      "167 7 0.30479010939598083\n",
      "Validation loss: 10.254779368375255 RMSE: 3.2023084\n",
      "168 28 0.34071823954582214\n",
      "Validation loss: 9.519844970871917 RMSE: 3.0854244\n",
      "Validation loss: 10.165368156095521 RMSE: 3.1883173\n",
      "170 20 0.359676718711853\n",
      "Validation loss: 6.612918389582001 RMSE: 2.5715594\n",
      "Validation loss: 10.309889367196412 RMSE: 3.2109015\n",
      "172 12 0.3113783895969391\n",
      "Validation loss: 7.7610075621478325 RMSE: 2.7858584\n",
      "Validation loss: 7.106152597781831 RMSE: 2.6657367\n",
      "174 4 0.267326682806015\n",
      "Validation loss: 4.315192492662278 RMSE: 2.0773041\n",
      "175 25 0.7545861005783081\n",
      "Validation loss: 9.076507024005451 RMSE: 3.0127242\n",
      "Validation loss: 11.287547651645356 RMSE: 3.3596945\n",
      "177 17 0.6298184394836426\n",
      "Validation loss: 9.583353405505155 RMSE: 3.095699\n",
      "Validation loss: 9.930739170682115 RMSE: 3.1513073\n",
      "179 9 0.34080174565315247\n",
      "Validation loss: 3.723462227171501 RMSE: 1.9296274\n",
      "Validation loss: 7.539222675087178 RMSE: 2.7457645\n",
      "181 1 0.16457334160804749\n",
      "Validation loss: 7.369168547402441 RMSE: 2.7146213\n",
      "182 22 0.6179825663566589\n",
      "Validation loss: 4.478630622931286 RMSE: 2.1162775\n",
      "Validation loss: 10.520817334673046 RMSE: 3.2435808\n",
      "184 14 0.4347164034843445\n",
      "Validation loss: 11.538059445609033 RMSE: 3.396772\n",
      "Validation loss: 9.709641313130877 RMSE: 3.11603\n",
      "186 6 0.2514690160751343\n",
      "Validation loss: 10.230549913592043 RMSE: 3.198523\n",
      "187 27 0.5444774031639099\n",
      "Validation loss: 6.268380874026138 RMSE: 2.5036733\n",
      "Validation loss: 7.03320553028478 RMSE: 2.6520193\n",
      "189 19 0.262998104095459\n",
      "Validation loss: 7.933676804061484 RMSE: 2.8166783\n",
      "Validation loss: 11.279294520352794 RMSE: 3.3584661\n",
      "191 11 0.3573428988456726\n",
      "Validation loss: 10.453192297336274 RMSE: 3.2331398\n",
      "Validation loss: 10.776962668494841 RMSE: 3.2828286\n",
      "193 3 0.2936883270740509\n",
      "Validation loss: 10.77924370554696 RMSE: 3.2831757\n",
      "194 24 0.4117911756038666\n",
      "Validation loss: 8.156527240719415 RMSE: 2.8559635\n",
      "Validation loss: 7.6579105832935435 RMSE: 2.767293\n",
      "196 16 0.2649359703063965\n",
      "Validation loss: 8.685642839533038 RMSE: 2.9471416\n",
      "Validation loss: 7.305885774899373 RMSE: 2.7029402\n",
      "198 8 0.9520086646080017\n",
      "Validation loss: 7.351841715584814 RMSE: 2.711428\n",
      "Validation loss: 6.111906534802597 RMSE: 2.472227\n",
      "Loaded trained model with success.\n",
      "Test loss: 4.425674628367466 Test RMSE: 2.1037288\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:1', 'task_name': 'esol', 'model_type': 'gin', 'model': {'num_layer': 5, 'emb_dim': 300, 'feat_dim': 300, 'drop_ratio': 0.3, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 778, 'task': 'regression', 'data_path': 'data/esol/esol.csv', 'target': ['measured log solubility in mols per litre']}}\n",
      "Running on: cuda:1\n",
      "1127\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/1127\n",
      "Generating scaffold 1000/1127\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 16.999868392944336\n",
      "Validation loss: 4.869643363277469 RMSE: 2.206727\n",
      "1 21 2.1396021842956543\n",
      "Validation loss: 6.018993497949786 RMSE: 2.453364\n",
      "Validation loss: 9.173561914832192 RMSE: 3.0287888\n",
      "3 13 2.0849246978759766\n",
      "Validation loss: 6.667433873742028 RMSE: 2.5821376\n",
      "Validation loss: 3.7251414071142146 RMSE: 1.9300624\n",
      "5 5 1.5190540552139282\n",
      "Validation loss: 8.16130684540335 RMSE: 2.8568\n",
      "6 26 1.3643821477890015\n",
      "Validation loss: 5.378982332955419 RMSE: 2.3192635\n",
      "Validation loss: 6.537251611726474 RMSE: 2.5568051\n",
      "8 18 0.8741591572761536\n",
      "Validation loss: 7.093019595188377 RMSE: 2.6632724\n",
      "Validation loss: 8.092873400291511 RMSE: 2.8447974\n",
      "10 10 0.9888883829116821\n",
      "Validation loss: 6.4176945053370655 RMSE: 2.5333169\n",
      "Validation loss: 5.2912175634266 RMSE: 2.3002648\n",
      "12 2 0.5842853784561157\n",
      "Validation loss: 6.3166681433145975 RMSE: 2.5132983\n",
      "13 23 0.74289470911026\n",
      "Validation loss: 4.40152793014999 RMSE: 2.097982\n",
      "Validation loss: 7.248205227134502 RMSE: 2.6922493\n",
      "15 15 1.079642653465271\n",
      "Validation loss: 8.439165220851391 RMSE: 2.905024\n",
      "Validation loss: 5.140404494462815 RMSE: 2.267246\n",
      "17 7 2.0257720947265625\n",
      "Validation loss: 3.970997152075303 RMSE: 1.992736\n",
      "18 28 1.0815411806106567\n",
      "Validation loss: 4.688760305927918 RMSE: 2.1653547\n",
      "Validation loss: 6.992837610497939 RMSE: 2.6443975\n",
      "20 20 0.5514382123947144\n",
      "Validation loss: 7.272358227620082 RMSE: 2.696731\n",
      "Validation loss: 5.1978490626917475 RMSE: 2.2798793\n",
      "22 12 2.3707828521728516\n",
      "Validation loss: 7.006004413672253 RMSE: 2.6468859\n",
      "Validation loss: 8.79960113289082 RMSE: 2.966412\n",
      "24 4 0.6820238828659058\n",
      "Validation loss: 9.06674499005343 RMSE: 3.0111036\n",
      "25 25 1.30973219871521\n",
      "Validation loss: 9.294975643664335 RMSE: 3.0487661\n",
      "Validation loss: 7.855365255237681 RMSE: 2.8027422\n",
      "27 17 0.7564168572425842\n",
      "Validation loss: 6.8774095636553465 RMSE: 2.6224816\n",
      "Validation loss: 8.770490637922709 RMSE: 2.9615014\n",
      "29 9 0.6743860840797424\n",
      "Validation loss: 7.186445202447672 RMSE: 2.6807547\n",
      "Validation loss: 12.920021664779798 RMSE: 3.594443\n",
      "31 1 0.7702495455741882\n",
      "Validation loss: 6.8475915562790055 RMSE: 2.6167903\n",
      "32 22 0.866300106048584\n",
      "Validation loss: 6.929088373099808 RMSE: 2.632316\n",
      "Validation loss: 7.808794397168455 RMSE: 2.794422\n",
      "34 14 0.545567512512207\n",
      "Validation loss: 8.416417894110216 RMSE: 2.9011064\n",
      "Validation loss: 8.687540974237223 RMSE: 2.9474635\n",
      "36 6 1.0117006301879883\n",
      "Validation loss: 8.446391468554472 RMSE: 2.9062676\n",
      "37 27 0.7925666570663452\n",
      "Validation loss: 6.0891116361702435 RMSE: 2.4676125\n",
      "Validation loss: 8.581692079527189 RMSE: 2.9294524\n",
      "39 19 0.6085801124572754\n",
      "Validation loss: 5.510136688705039 RMSE: 2.347368\n",
      "Validation loss: 4.888315432894546 RMSE: 2.2109535\n",
      "41 11 1.4328980445861816\n",
      "Validation loss: 7.062374317540532 RMSE: 2.6575127\n",
      "Validation loss: 3.945202781035837 RMSE: 1.9862535\n",
      "43 3 1.0867931842803955\n",
      "Validation loss: 3.9941596288596632 RMSE: 1.9985393\n",
      "44 24 0.6298247575759888\n",
      "Validation loss: 6.520365639070494 RMSE: 2.5535007\n",
      "Validation loss: 6.02223068000996 RMSE: 2.4540234\n",
      "46 16 0.5750909447669983\n",
      "Validation loss: 9.30085001582593 RMSE: 3.0497296\n",
      "Validation loss: 7.987785900588584 RMSE: 2.8262672\n",
      "48 8 0.6860097646713257\n",
      "Validation loss: 6.294470516981277 RMSE: 2.5088782\n",
      "Validation loss: 6.512235907326757 RMSE: 2.5519083\n",
      "50 0 1.357529640197754\n",
      "Validation loss: 6.820916116765115 RMSE: 2.6116884\n",
      "51 21 0.3687387704849243\n",
      "Validation loss: 5.197341501185324 RMSE: 2.2797678\n",
      "Validation loss: 10.127790527006166 RMSE: 3.182419\n",
      "53 13 0.3188130855560303\n",
      "Validation loss: 4.967715208509327 RMSE: 2.2288373\n",
      "Validation loss: 9.398657140478624 RMSE: 3.065723\n",
      "55 5 1.0530610084533691\n",
      "Validation loss: 7.114228484904872 RMSE: 2.667251\n",
      "56 26 1.716856598854065\n",
      "Validation loss: 8.722151941957726 RMSE: 2.9533288\n",
      "Validation loss: 7.01221736342506 RMSE: 2.6480591\n",
      "58 18 0.4832964837551117\n",
      "Validation loss: 3.415802936638351 RMSE: 1.848189\n",
      "Validation loss: 4.1049789538425685 RMSE: 2.0260746\n",
      "60 10 0.8454976677894592\n",
      "Validation loss: 4.33023677674015 RMSE: 2.0809221\n",
      "Validation loss: 7.855379062416279 RMSE: 2.8027449\n",
      "62 2 0.5770760178565979\n",
      "Validation loss: 6.8066255265632565 RMSE: 2.6089509\n",
      "63 23 0.5343185663223267\n",
      "Validation loss: 8.294362545013428 RMSE: 2.8799937\n",
      "Validation loss: 6.034942652271912 RMSE: 2.456612\n",
      "65 15 0.8028672933578491\n",
      "Validation loss: 6.369010511752778 RMSE: 2.5236897\n",
      "Validation loss: 10.97818455653908 RMSE: 3.3133345\n",
      "67 7 0.7207891345024109\n",
      "Validation loss: 6.658828954781051 RMSE: 2.5804708\n",
      "68 28 2.7628962993621826\n",
      "Validation loss: 7.778864911172242 RMSE: 2.7890615\n",
      "Validation loss: 6.539260054056624 RMSE: 2.5571978\n",
      "70 20 0.8532069325447083\n",
      "Validation loss: 6.447388091973499 RMSE: 2.5391707\n",
      "Validation loss: 6.42418209008411 RMSE: 2.5345972\n",
      "72 12 0.6536756753921509\n",
      "Validation loss: 7.702101234841136 RMSE: 2.775266\n",
      "Validation loss: 6.7289930613695 RMSE: 2.5940301\n",
      "74 4 0.8257204294204712\n",
      "Validation loss: 7.35949243697445 RMSE: 2.7128384\n",
      "75 25 1.143429160118103\n",
      "Validation loss: 5.928839345948886 RMSE: 2.4349208\n",
      "Validation loss: 5.211789346374242 RMSE: 2.2829344\n",
      "77 17 0.5568298697471619\n",
      "Validation loss: 5.871681664897277 RMSE: 2.4231553\n",
      "Validation loss: 6.651741622823529 RMSE: 2.579097\n",
      "79 9 0.3647274672985077\n",
      "Validation loss: 6.236318301310582 RMSE: 2.4972622\n",
      "Validation loss: 5.062907818144402 RMSE: 2.2500906\n",
      "81 1 0.5081608295440674\n",
      "Validation loss: 4.922628339412993 RMSE: 2.2187\n",
      "82 22 0.6689730882644653\n",
      "Validation loss: 5.747882387279409 RMSE: 2.397474\n",
      "Validation loss: 6.5387682408358145 RMSE: 2.5571015\n",
      "84 14 0.5330458283424377\n",
      "Validation loss: 6.824412240391284 RMSE: 2.6123576\n",
      "Validation loss: 5.491619295778528 RMSE: 2.3434205\n",
      "86 6 0.7232590317726135\n",
      "Validation loss: 5.6191395151931625 RMSE: 2.3704724\n",
      "87 27 0.7271350026130676\n",
      "Validation loss: 6.33915637024736 RMSE: 2.5177681\n",
      "Validation loss: 7.7138430586958355 RMSE: 2.7773807\n",
      "89 19 0.3695008158683777\n",
      "Validation loss: 9.299978990470414 RMSE: 3.0495868\n",
      "Validation loss: 9.01794052967983 RMSE: 3.0029886\n",
      "91 11 0.8114627599716187\n",
      "Validation loss: 8.130309193535188 RMSE: 2.8513696\n",
      "Validation loss: 4.805365845165421 RMSE: 2.1921146\n",
      "93 3 0.7021039128303528\n",
      "Validation loss: 8.275904170179789 RMSE: 2.876787\n",
      "94 24 0.6206879615783691\n",
      "Validation loss: 7.7532117809869545 RMSE: 2.784459\n",
      "Validation loss: 8.728049421732404 RMSE: 2.954327\n",
      "96 16 0.465099573135376\n",
      "Validation loss: 9.829810978037067 RMSE: 3.1352527\n",
      "Validation loss: 7.306442345138144 RMSE: 2.7030432\n",
      "98 8 0.6388593316078186\n",
      "Validation loss: 8.335504696432467 RMSE: 2.8871274\n",
      "Validation loss: 10.495572866591733 RMSE: 3.2396872\n",
      "100 0 0.6021225452423096\n",
      "Validation loss: 6.6731710602751875 RMSE: 2.5832481\n",
      "101 21 0.35739076137542725\n",
      "Validation loss: 8.200457155177023 RMSE: 2.863644\n",
      "Validation loss: 10.276312439842561 RMSE: 3.2056687\n",
      "103 13 0.6095173358917236\n",
      "Validation loss: 7.942629649575832 RMSE: 2.818267\n",
      "Validation loss: 7.588897916068018 RMSE: 2.7547953\n",
      "105 5 0.8260229825973511\n",
      "Validation loss: 6.329337639091289 RMSE: 2.5158174\n",
      "106 26 1.4462778568267822\n",
      "Validation loss: 8.799125477276018 RMSE: 2.966332\n",
      "Validation loss: 8.284437280840578 RMSE: 2.87827\n",
      "108 18 0.2404918521642685\n",
      "Validation loss: 9.648162605488196 RMSE: 3.1061492\n",
      "Validation loss: 7.763235248295607 RMSE: 2.7862585\n",
      "110 10 0.4606810212135315\n",
      "Validation loss: 7.878604872036824 RMSE: 2.8068852\n",
      "Validation loss: 7.765301273987356 RMSE: 2.786629\n",
      "112 2 0.49216458201408386\n",
      "Validation loss: 7.191233453497422 RMSE: 2.6816475\n",
      "113 23 0.6482134461402893\n",
      "Validation loss: 10.742758995663804 RMSE: 3.2776146\n",
      "Validation loss: 7.699785688282114 RMSE: 2.7748487\n",
      "115 15 0.7251396179199219\n",
      "Validation loss: 9.056660711237814 RMSE: 3.0094285\n",
      "Validation loss: 5.9386466076943725 RMSE: 2.4369338\n",
      "117 7 0.7918412089347839\n",
      "Validation loss: 9.905913867781647 RMSE: 3.1473663\n",
      "118 28 0.527033805847168\n",
      "Validation loss: 10.181376157608707 RMSE: 3.190827\n",
      "Validation loss: 6.495739299639136 RMSE: 2.548674\n",
      "120 20 0.45105674862861633\n",
      "Validation loss: 9.739747393447741 RMSE: 3.1208568\n",
      "Validation loss: 8.866716418646078 RMSE: 2.977703\n",
      "122 12 0.34816211462020874\n",
      "Validation loss: 9.969732917515577 RMSE: 3.1574883\n",
      "Validation loss: 7.230279276856279 RMSE: 2.6889179\n",
      "124 4 0.7143517136573792\n",
      "Validation loss: 9.676953906506563 RMSE: 3.1107802\n",
      "125 25 0.8555803894996643\n",
      "Validation loss: 9.592775825905589 RMSE: 3.0972207\n",
      "Validation loss: 12.962626305301633 RMSE: 3.600365\n",
      "127 17 0.30722448229789734\n",
      "Validation loss: 10.403582007484099 RMSE: 3.2254586\n",
      "Validation loss: 14.028228607852903 RMSE: 3.7454278\n",
      "129 9 0.8646687865257263\n",
      "Validation loss: 8.434371003007467 RMSE: 2.9041986\n",
      "Validation loss: 9.695898798714698 RMSE: 3.1138237\n",
      "131 1 1.3385783433914185\n",
      "Validation loss: 9.758440574713513 RMSE: 3.12385\n",
      "132 22 0.9711655974388123\n",
      "Validation loss: 8.639071658649275 RMSE: 2.93923\n",
      "Validation loss: 8.92160062874313 RMSE: 2.9869049\n",
      "134 14 0.9366722702980042\n",
      "Validation loss: 6.3957958010445655 RMSE: 2.5289912\n",
      "Validation loss: 8.244480293408959 RMSE: 2.87132\n",
      "136 6 1.293732762336731\n",
      "Validation loss: 8.117617691512656 RMSE: 2.8491433\n",
      "137 27 0.7338529229164124\n",
      "Validation loss: 8.521448852741614 RMSE: 2.919152\n",
      "Validation loss: 10.302451597905792 RMSE: 3.2097433\n",
      "139 19 0.3574230670928955\n",
      "Validation loss: 8.960725653488025 RMSE: 2.993447\n",
      "Validation loss: 7.822995228050029 RMSE: 2.7969618\n",
      "141 11 0.34813088178634644\n",
      "Validation loss: 10.226366659181307 RMSE: 3.197869\n",
      "Validation loss: 11.104376539719842 RMSE: 3.332323\n",
      "143 3 0.271486759185791\n",
      "Validation loss: 10.199738510942037 RMSE: 3.193703\n",
      "144 24 0.5788171887397766\n",
      "Validation loss: 9.532775693235145 RMSE: 3.0875192\n",
      "Validation loss: 12.091622715502714 RMSE: 3.4773011\n",
      "146 16 0.3692777752876282\n",
      "Validation loss: 10.726554068843875 RMSE: 3.2751417\n",
      "Validation loss: 9.431564318395294 RMSE: 3.0710855\n",
      "148 8 0.3329448997974396\n",
      "Validation loss: 10.929496765136719 RMSE: 3.305979\n",
      "Validation loss: 12.853654785493834 RMSE: 3.5851994\n",
      "150 0 0.385306715965271\n",
      "Validation loss: 10.979165701739555 RMSE: 3.3134825\n",
      "151 21 0.5269835591316223\n",
      "Validation loss: 7.893221462722373 RMSE: 2.8094876\n",
      "Validation loss: 7.594356026269693 RMSE: 2.755786\n",
      "153 13 0.3124167025089264\n",
      "Validation loss: 8.394975721308615 RMSE: 2.8974085\n",
      "Validation loss: 11.193797972349994 RMSE: 3.3457134\n",
      "155 5 0.4856356382369995\n",
      "Validation loss: 8.518575870885257 RMSE: 2.91866\n",
      "156 26 0.669085681438446\n",
      "Validation loss: 6.585288473990111 RMSE: 2.566182\n",
      "Validation loss: 7.438668251037598 RMSE: 2.7273922\n",
      "158 18 0.5355960726737976\n",
      "Validation loss: 8.322611091411218 RMSE: 2.8848937\n",
      "Validation loss: 7.254462377159997 RMSE: 2.6934109\n",
      "160 10 0.32302990555763245\n",
      "Validation loss: 10.574285532520936 RMSE: 3.2518127\n",
      "Validation loss: 11.977715036510366 RMSE: 3.4608839\n",
      "162 2 1.1809971332550049\n",
      "Validation loss: 9.604877699792912 RMSE: 3.0991738\n",
      "163 23 0.9966172575950623\n",
      "Validation loss: 12.342706764693808 RMSE: 3.5132189\n",
      "Validation loss: 12.583097449446146 RMSE: 3.5472662\n",
      "165 15 0.6742182970046997\n",
      "Validation loss: 9.88453097048059 RMSE: 3.1439674\n",
      "Validation loss: 3.6899414189093935 RMSE: 1.920922\n",
      "167 7 0.6858372092247009\n",
      "Validation loss: 4.5491454791178745 RMSE: 2.1328726\n",
      "168 28 0.4496960937976837\n",
      "Validation loss: 6.096656972328119 RMSE: 2.469141\n",
      "Validation loss: 4.205622194087611 RMSE: 2.0507615\n",
      "170 20 0.6141719222068787\n",
      "Validation loss: 4.411441203767219 RMSE: 2.1003432\n",
      "Validation loss: 5.9998221861577665 RMSE: 2.4494534\n",
      "172 12 0.6160867810249329\n",
      "Validation loss: 5.3254594549668575 RMSE: 2.3076956\n",
      "Validation loss: 7.24045625197149 RMSE: 2.6908097\n",
      "174 4 0.36450982093811035\n",
      "Validation loss: 8.072091284051405 RMSE: 2.8411424\n",
      "175 25 0.35301473736763\n",
      "Validation loss: 11.840277173877817 RMSE: 3.4409704\n",
      "Validation loss: 10.139848261807872 RMSE: 3.184313\n",
      "177 17 0.3682026267051697\n",
      "Validation loss: 3.8970689056193932 RMSE: 1.9740994\n",
      "Validation loss: 4.691069062832183 RMSE: 2.1658876\n",
      "179 9 0.31968992948532104\n",
      "Validation loss: 7.8930662079195 RMSE: 2.8094602\n",
      "Validation loss: 7.672930810303814 RMSE: 2.7700055\n",
      "181 1 0.48700085282325745\n",
      "Validation loss: 8.680458005550689 RMSE: 2.9462616\n",
      "182 22 0.9409634470939636\n",
      "Validation loss: 6.8813050700499945 RMSE: 2.6232243\n",
      "Validation loss: 5.434911301705689 RMSE: 2.3312895\n",
      "184 14 0.48493462800979614\n",
      "Validation loss: 6.609684108632855 RMSE: 2.5709305\n",
      "Validation loss: 9.560759173030347 RMSE: 3.0920477\n",
      "186 6 0.30477750301361084\n",
      "Validation loss: 6.047181914337968 RMSE: 2.4591017\n",
      "187 27 0.31584832072257996\n",
      "Validation loss: 10.040906534785718 RMSE: 3.168739\n",
      "Validation loss: 7.726438800845526 RMSE: 2.7796474\n",
      "189 19 0.49526506662368774\n",
      "Validation loss: 6.643477022120383 RMSE: 2.5774944\n",
      "Validation loss: 6.736945954044308 RMSE: 2.5955627\n",
      "191 11 0.35692349076271057\n",
      "Validation loss: 7.321954879085575 RMSE: 2.7059112\n",
      "Validation loss: 6.698832115240856 RMSE: 2.58821\n",
      "193 3 0.2086467444896698\n",
      "Validation loss: 6.607276182259079 RMSE: 2.5704622\n",
      "194 24 0.6561264991760254\n",
      "Validation loss: 6.559783775194556 RMSE: 2.5612075\n",
      "Validation loss: 8.406993916604371 RMSE: 2.8994815\n",
      "196 16 0.4626307189464569\n",
      "Validation loss: 6.165143751465114 RMSE: 2.4829707\n",
      "Validation loss: 9.552525102564719 RMSE: 3.0907161\n",
      "198 8 0.621049702167511\n",
      "Validation loss: 9.756414358594776 RMSE: 3.123526\n",
      "Validation loss: 10.281653750259265 RMSE: 3.206502\n",
      "Loaded trained model with success.\n",
      "Test loss: 3.5903758981586558 Test RMSE: 1.8948287\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:1', 'task_name': 'esol', 'model_type': 'gin', 'model': {'num_layer': 5, 'emb_dim': 300, 'feat_dim': 300, 'drop_ratio': 0.3, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 779, 'task': 'regression', 'data_path': 'data/esol/esol.csv', 'target': ['measured log solubility in mols per litre']}}\n",
      "Running on: cuda:1\n",
      "1127\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/1127\n",
      "Generating scaffold 1000/1127\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 14.859318733215332\n",
      "Validation loss: 5.290507751228535 RMSE: 2.3001106\n",
      "1 21 1.5843379497528076\n",
      "Validation loss: 7.244304496630103 RMSE: 2.6915245\n",
      "Validation loss: 10.997861035102236 RMSE: 3.3163023\n",
      "3 13 1.5800657272338867\n",
      "Validation loss: 9.670228207005863 RMSE: 3.1096992\n",
      "Validation loss: 4.762428224614236 RMSE: 2.182299\n",
      "5 5 1.6419633626937866\n",
      "Validation loss: 10.848473067832204 RMSE: 3.293702\n",
      "6 26 1.5099849700927734\n",
      "Validation loss: 6.504508520649598 RMSE: 2.5503938\n",
      "Validation loss: 8.43786814360492 RMSE: 2.904801\n",
      "8 18 1.3484305143356323\n",
      "Validation loss: 8.76217956036593 RMSE: 2.960098\n",
      "Validation loss: 4.712796726058015 RMSE: 2.1708975\n",
      "10 10 0.9958405494689941\n",
      "Validation loss: 7.92330203858097 RMSE: 2.814836\n",
      "Validation loss: 8.869390496110494 RMSE: 2.9781523\n",
      "12 2 1.3851033449172974\n",
      "Validation loss: 8.35030541377785 RMSE: 2.8896894\n",
      "13 23 1.1949909925460815\n",
      "Validation loss: 9.335562621597695 RMSE: 3.0554152\n",
      "Validation loss: 5.68200302967983 RMSE: 2.3836954\n",
      "15 15 1.7138789892196655\n",
      "Validation loss: 5.4307775180951685 RMSE: 2.3304026\n",
      "Validation loss: 6.966365079964157 RMSE: 2.6393874\n",
      "17 7 0.9117274284362793\n",
      "Validation loss: 5.379649179171672 RMSE: 2.319407\n",
      "18 28 1.7759065628051758\n",
      "Validation loss: 6.990543112290644 RMSE: 2.6439636\n",
      "Validation loss: 6.782771576822332 RMSE: 2.6043756\n",
      "20 20 0.7965306639671326\n",
      "Validation loss: 5.3931837208503115 RMSE: 2.3223228\n",
      "Validation loss: 8.69616381467971 RMSE: 2.948926\n",
      "22 12 1.624018907546997\n",
      "Validation loss: 5.096229388650539 RMSE: 2.257483\n",
      "Validation loss: 4.787156750670577 RMSE: 2.1879573\n",
      "24 4 2.08901309967041\n",
      "Validation loss: 5.577246505602271 RMSE: 2.3616195\n",
      "25 25 0.7149198651313782\n",
      "Validation loss: 7.57785210567238 RMSE: 2.75279\n",
      "Validation loss: 7.5909393150194555 RMSE: 2.755166\n",
      "27 17 0.5583021640777588\n",
      "Validation loss: 7.596374047540985 RMSE: 2.756152\n",
      "Validation loss: 8.465620619005861 RMSE: 2.9095738\n",
      "29 9 0.9942357540130615\n",
      "Validation loss: 8.24525536900073 RMSE: 2.8714554\n",
      "Validation loss: 7.899318205571808 RMSE: 2.8105726\n",
      "31 1 1.1647443771362305\n",
      "Validation loss: 6.958203092085577 RMSE: 2.6378405\n",
      "32 22 0.5909401774406433\n",
      "Validation loss: 5.404655494521149 RMSE: 2.3247914\n",
      "Validation loss: 6.375488614614031 RMSE: 2.524973\n",
      "34 14 1.4010586738586426\n",
      "Validation loss: 6.102599527983539 RMSE: 2.4703438\n",
      "Validation loss: 8.278393163090259 RMSE: 2.8772197\n",
      "36 6 1.0574766397476196\n",
      "Validation loss: 8.28873975298046 RMSE: 2.879017\n",
      "37 27 0.512266993522644\n",
      "Validation loss: 5.116457871631184 RMSE: 2.2619588\n",
      "Validation loss: 6.38562591518976 RMSE: 2.5269797\n",
      "39 19 1.324735164642334\n",
      "Validation loss: 6.9537942852594155 RMSE: 2.6370049\n",
      "Validation loss: 7.023939563109812 RMSE: 2.6502717\n",
      "41 11 0.6797666549682617\n",
      "Validation loss: 5.806625041286503 RMSE: 2.409694\n",
      "Validation loss: 6.129118092292178 RMSE: 2.4757056\n",
      "43 3 0.704048752784729\n",
      "Validation loss: 7.7060608906028545 RMSE: 2.7759793\n",
      "44 24 0.41009214520454407\n",
      "Validation loss: 5.723237354143531 RMSE: 2.3923287\n",
      "Validation loss: 6.082742163565307 RMSE: 2.4663217\n",
      "46 16 0.9677668809890747\n",
      "Validation loss: 7.218205945681682 RMSE: 2.686672\n",
      "Validation loss: 4.483390006343876 RMSE: 2.1174016\n",
      "48 8 0.7680137753486633\n",
      "Validation loss: 4.388483731092605 RMSE: 2.0948708\n",
      "Validation loss: 4.881439820855065 RMSE: 2.209398\n",
      "50 0 0.5430092215538025\n",
      "Validation loss: 4.771406190585246 RMSE: 2.1843548\n",
      "51 21 0.7943031191825867\n",
      "Validation loss: 6.623072902713202 RMSE: 2.573533\n",
      "Validation loss: 6.5040945120617355 RMSE: 2.5503128\n",
      "53 13 0.7252018451690674\n",
      "Validation loss: 3.463275673115148 RMSE: 1.8609878\n",
      "Validation loss: 4.227726754889024 RMSE: 2.0561438\n",
      "55 5 0.4576196074485779\n",
      "Validation loss: 4.852524166613554 RMSE: 2.2028446\n",
      "56 26 1.3343671560287476\n",
      "Validation loss: 3.767681216771624 RMSE: 1.9410516\n",
      "Validation loss: 5.724060337100409 RMSE: 2.3925009\n",
      "58 18 0.5430741310119629\n",
      "Validation loss: 4.624362903358662 RMSE: 2.150433\n",
      "Validation loss: 4.654215762045531 RMSE: 2.1573632\n",
      "60 10 0.9419904351234436\n",
      "Validation loss: 5.149103265948 RMSE: 2.2691636\n",
      "Validation loss: 5.119434500162581 RMSE: 2.2626169\n",
      "62 2 0.4579159617424011\n",
      "Validation loss: 7.585028192638296 RMSE: 2.7540932\n",
      "63 23 0.32709455490112305\n",
      "Validation loss: 6.0488421262893 RMSE: 2.4594393\n",
      "Validation loss: 7.248364224898077 RMSE: 2.6922786\n",
      "65 15 1.513568639755249\n",
      "Validation loss: 6.3614862197268325 RMSE: 2.5221987\n",
      "Validation loss: 5.65427338760511 RMSE: 2.3778715\n",
      "67 7 0.443502813577652\n",
      "Validation loss: 5.327776904654714 RMSE: 2.3081977\n",
      "68 28 1.97505521774292\n",
      "Validation loss: 5.872897000439399 RMSE: 2.4234061\n",
      "Validation loss: 5.868099254844463 RMSE: 2.422416\n",
      "70 20 0.6881715059280396\n",
      "Validation loss: 6.143940368584827 RMSE: 2.4786973\n",
      "Validation loss: 7.339994388344014 RMSE: 2.7092423\n",
      "72 12 0.6230199933052063\n",
      "Validation loss: 7.212356419689887 RMSE: 2.685583\n",
      "Validation loss: 4.629403698760851 RMSE: 2.151605\n",
      "74 4 0.5445103645324707\n",
      "Validation loss: 6.001925557060579 RMSE: 2.4498827\n",
      "75 25 0.7052897810935974\n",
      "Validation loss: 6.2811734655262095 RMSE: 2.506227\n",
      "Validation loss: 8.04427776083482 RMSE: 2.8362434\n",
      "77 17 1.0408395528793335\n",
      "Validation loss: 6.508088289108952 RMSE: 2.5510955\n",
      "Validation loss: 5.969984037686238 RMSE: 2.443355\n",
      "79 9 0.6403407454490662\n",
      "Validation loss: 6.185742006892651 RMSE: 2.4871151\n",
      "Validation loss: 5.2233134497583436 RMSE: 2.285457\n",
      "81 1 0.47747084498405457\n",
      "Validation loss: 6.6504047064654594 RMSE: 2.5788376\n",
      "82 22 0.7324788570404053\n",
      "Validation loss: 6.44494137721779 RMSE: 2.538689\n",
      "Validation loss: 8.651575164457338 RMSE: 2.941356\n",
      "84 14 0.6945792436599731\n",
      "Validation loss: 7.301945074469642 RMSE: 2.7022111\n",
      "Validation loss: 6.046111423357398 RMSE: 2.458884\n",
      "86 6 0.847162663936615\n",
      "Validation loss: 6.250353087366155 RMSE: 2.5000706\n",
      "87 27 0.587326169013977\n",
      "Validation loss: 6.758950993023087 RMSE: 2.5997982\n",
      "Validation loss: 8.021981754134186 RMSE: 2.8323104\n",
      "89 19 0.6858534812927246\n",
      "Validation loss: 9.69656315947001 RMSE: 3.1139305\n",
      "Validation loss: 7.697066775465434 RMSE: 2.7743587\n",
      "91 11 0.6118598580360413\n",
      "Validation loss: 7.154262019469675 RMSE: 2.6747453\n",
      "Validation loss: 8.19643491559324 RMSE: 2.8629417\n",
      "93 3 0.4987504482269287\n",
      "Validation loss: 8.554196762827646 RMSE: 2.9247558\n",
      "94 24 0.5056208968162537\n",
      "Validation loss: 8.367317613247222 RMSE: 2.8926318\n",
      "Validation loss: 10.56876733661753 RMSE: 3.2509642\n",
      "96 16 0.3718172311782837\n",
      "Validation loss: 8.548313681003266 RMSE: 2.92375\n",
      "Validation loss: 8.324842292650612 RMSE: 2.8852804\n",
      "98 8 0.5237540006637573\n",
      "Validation loss: 6.815301679931911 RMSE: 2.6106133\n",
      "Validation loss: 7.347288148593059 RMSE: 2.710588\n",
      "100 0 0.592934250831604\n",
      "Validation loss: 6.9656879922984976 RMSE: 2.6392589\n",
      "101 21 1.1153229475021362\n",
      "Validation loss: 7.306393074778329 RMSE: 2.703034\n",
      "Validation loss: 6.931514938320734 RMSE: 2.632777\n",
      "103 13 0.5732256174087524\n",
      "Validation loss: 7.482662175608947 RMSE: 2.7354455\n",
      "Validation loss: 8.2327979476051 RMSE: 2.869285\n",
      "105 5 0.5318537354469299\n",
      "Validation loss: 7.598015177566393 RMSE: 2.7564497\n",
      "106 26 0.6349183917045593\n",
      "Validation loss: 6.5311128312507565 RMSE: 2.5556042\n",
      "Validation loss: 7.609122508395035 RMSE: 2.7584639\n",
      "108 18 0.2806830406188965\n",
      "Validation loss: 7.9816437864725565 RMSE: 2.8251803\n",
      "Validation loss: 6.6491347308707445 RMSE: 2.5785916\n",
      "110 10 0.3975004553794861\n",
      "Validation loss: 8.91611785382296 RMSE: 2.985987\n",
      "Validation loss: 7.818205837654856 RMSE: 2.7961056\n",
      "112 2 0.2940850257873535\n",
      "Validation loss: 9.15185219418686 RMSE: 3.0252025\n",
      "113 23 0.6622948050498962\n",
      "Validation loss: 7.144047019755946 RMSE: 2.672835\n",
      "Validation loss: 7.968415732932302 RMSE: 2.8228383\n",
      "115 15 0.7644814252853394\n",
      "Validation loss: 7.135132473126977 RMSE: 2.671167\n",
      "Validation loss: 7.16994281785678 RMSE: 2.6776748\n",
      "117 7 1.209947943687439\n",
      "Validation loss: 8.292795712968944 RMSE: 2.8797214\n",
      "118 28 0.8492715358734131\n",
      "Validation loss: 6.7238274000387275 RMSE: 2.5930345\n",
      "Validation loss: 6.839829470204041 RMSE: 2.6153069\n",
      "120 20 0.47173038125038147\n",
      "Validation loss: 7.806400269533681 RMSE: 2.7939937\n",
      "Validation loss: 9.264493461203786 RMSE: 3.043763\n",
      "122 12 0.5210695862770081\n",
      "Validation loss: 7.830553215161889 RMSE: 2.7983127\n",
      "Validation loss: 9.190321685993567 RMSE: 3.0315545\n",
      "124 4 1.1216567754745483\n",
      "Validation loss: 8.863138949976559 RMSE: 2.9771023\n",
      "125 25 0.43598926067352295\n",
      "Validation loss: 10.1355092884165 RMSE: 3.1836314\n",
      "Validation loss: 7.473841034205614 RMSE: 2.7338326\n",
      "127 17 1.174058198928833\n",
      "Validation loss: 9.090095410304787 RMSE: 3.0149784\n",
      "Validation loss: 9.333705935857992 RMSE: 3.0551116\n",
      "129 9 0.4754366874694824\n",
      "Validation loss: 11.225695027714282 RMSE: 3.3504767\n",
      "Validation loss: 7.7721563398310565 RMSE: 2.7878587\n",
      "131 1 0.5170980095863342\n",
      "Validation loss: 8.183580373240783 RMSE: 2.8606958\n",
      "132 22 0.3299658000469208\n",
      "Validation loss: 7.3710957755029725 RMSE: 2.7149763\n",
      "Validation loss: 8.048480603547223 RMSE: 2.8369844\n",
      "134 14 0.8347235321998596\n",
      "Validation loss: 10.05567192820321 RMSE: 3.171068\n",
      "Validation loss: 7.560825972430474 RMSE: 2.7496958\n",
      "136 6 0.4364447593688965\n",
      "Validation loss: 6.839095824587662 RMSE: 2.6151664\n",
      "137 27 0.2905818819999695\n",
      "Validation loss: 6.089679388873345 RMSE: 2.4677277\n",
      "Validation loss: 8.226644828256253 RMSE: 2.8682127\n",
      "139 19 1.677253246307373\n",
      "Validation loss: 6.638031297025427 RMSE: 2.5764377\n",
      "Validation loss: 7.217563753634428 RMSE: 2.6865523\n",
      "141 11 0.7296745181083679\n",
      "Validation loss: 5.824502835231545 RMSE: 2.4134007\n",
      "Validation loss: 7.067253703564669 RMSE: 2.6584306\n",
      "143 3 0.21615517139434814\n",
      "Validation loss: 10.04188068778114 RMSE: 3.1688926\n",
      "144 24 0.6992720365524292\n",
      "Validation loss: 9.097742831812496 RMSE: 3.0162466\n",
      "Validation loss: 6.5075211904745185 RMSE: 2.5509841\n",
      "146 16 1.2919297218322754\n",
      "Validation loss: 8.91820368302607 RMSE: 2.986336\n",
      "Validation loss: 8.50491835163758 RMSE: 2.9163194\n",
      "148 8 0.48706233501434326\n",
      "Validation loss: 8.272468305267063 RMSE: 2.8761897\n",
      "Validation loss: 8.264807274911256 RMSE: 2.8748577\n",
      "150 0 0.6269466876983643\n",
      "Validation loss: 9.3733247402495 RMSE: 3.0615885\n",
      "151 21 0.5396132469177246\n",
      "Validation loss: 6.984685729035234 RMSE: 2.6428556\n",
      "Validation loss: 7.955430098339519 RMSE: 2.820537\n",
      "153 13 0.4225524663925171\n",
      "Validation loss: 6.742341185038069 RMSE: 2.596602\n",
      "Validation loss: 7.6126332325218 RMSE: 2.7591\n",
      "155 5 0.3622215986251831\n",
      "Validation loss: 6.346668808861116 RMSE: 2.5192595\n",
      "156 26 0.6319241523742676\n",
      "Validation loss: 7.164735768748596 RMSE: 2.6767025\n",
      "Validation loss: 8.228995053114089 RMSE: 2.8686223\n",
      "158 18 0.3615560233592987\n",
      "Validation loss: 5.644410247296358 RMSE: 2.3757968\n",
      "Validation loss: 6.47783475217566 RMSE: 2.545159\n",
      "160 10 0.6046377420425415\n",
      "Validation loss: 7.561733870379693 RMSE: 2.7498608\n",
      "Validation loss: 11.348785505885571 RMSE: 3.3687959\n",
      "162 2 0.5220156908035278\n",
      "Validation loss: 7.955020229373358 RMSE: 2.8204646\n",
      "163 23 0.2762291729450226\n",
      "Validation loss: 7.632907512968621 RMSE: 2.7627716\n",
      "Validation loss: 9.723192400636925 RMSE: 3.1182034\n",
      "165 15 0.3768180310726166\n",
      "Validation loss: 8.932557983736022 RMSE: 2.9887385\n",
      "Validation loss: 8.85081905601299 RMSE: 2.9750328\n",
      "167 7 0.35031723976135254\n",
      "Validation loss: 10.287658362262016 RMSE: 3.207438\n",
      "168 28 2.4359824657440186\n",
      "Validation loss: 7.888096049823592 RMSE: 2.8085754\n",
      "Validation loss: 8.316090204019462 RMSE: 2.8837633\n",
      "170 20 0.21722625195980072\n",
      "Validation loss: 10.468255578944113 RMSE: 3.2354681\n",
      "Validation loss: 12.733216724564544 RMSE: 3.5683634\n",
      "172 12 0.6583374738693237\n",
      "Validation loss: 9.493302180703763 RMSE: 3.0811203\n",
      "Validation loss: 8.575082150180783 RMSE: 2.9283242\n",
      "174 4 0.44668346643447876\n",
      "Validation loss: 12.266921364100634 RMSE: 3.5024164\n",
      "175 25 0.3270341753959656\n",
      "Validation loss: 10.449783983483778 RMSE: 3.2326124\n",
      "Validation loss: 7.541373767683991 RMSE: 2.7461562\n",
      "177 17 0.35697823762893677\n",
      "Validation loss: 8.704007701536195 RMSE: 2.9502554\n",
      "Validation loss: 8.85424424061733 RMSE: 2.975608\n",
      "179 9 0.6057987213134766\n",
      "Validation loss: 10.926627690813183 RMSE: 3.3055449\n",
      "Validation loss: 8.741717119132524 RMSE: 2.9566395\n",
      "181 1 0.3644540011882782\n",
      "Validation loss: 8.263081010464019 RMSE: 2.8745575\n",
      "182 22 0.39816009998321533\n",
      "Validation loss: 7.665573879680802 RMSE: 2.7686772\n",
      "Validation loss: 9.072855755291155 RMSE: 3.012118\n",
      "184 14 0.2820088267326355\n",
      "Validation loss: 6.7184987194770205 RMSE: 2.5920067\n",
      "Validation loss: 10.509585532466922 RMSE: 3.2418492\n",
      "186 6 0.7164676785469055\n",
      "Validation loss: 11.754146854434394 RMSE: 3.4284322\n",
      "187 27 0.25786176323890686\n",
      "Validation loss: 10.58160037488009 RMSE: 3.2529373\n",
      "Validation loss: 9.593600935640588 RMSE: 3.097354\n",
      "189 19 0.2808120548725128\n",
      "Validation loss: 11.804393337891165 RMSE: 3.4357524\n",
      "Validation loss: 11.340389758084727 RMSE: 3.3675494\n",
      "191 11 0.3931506872177124\n",
      "Validation loss: 10.917033288331158 RMSE: 3.3040936\n",
      "Validation loss: 10.102500442909983 RMSE: 3.178443\n",
      "193 3 0.7067859172821045\n",
      "Validation loss: 10.112346758884666 RMSE: 3.1799917\n",
      "194 24 0.48285630345344543\n",
      "Validation loss: 11.43507821581005 RMSE: 3.3815792\n",
      "Validation loss: 10.442892555641917 RMSE: 3.2315466\n",
      "196 16 0.1683848649263382\n",
      "Validation loss: 7.937983723868311 RMSE: 2.817443\n",
      "Validation loss: 12.301531724170246 RMSE: 3.507354\n",
      "198 8 0.857833206653595\n",
      "Validation loss: 11.289667213912558 RMSE: 3.36001\n",
      "Validation loss: 10.678619722349454 RMSE: 3.2678156\n",
      "Loaded trained model with success.\n",
      "Test loss: 3.8250550548587223 Test RMSE: 1.9557748\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:1', 'task_name': 'esol', 'model_type': 'gin', 'model': {'num_layer': 5, 'emb_dim': 300, 'feat_dim': 300, 'drop_ratio': 0.3, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 780, 'task': 'regression', 'data_path': 'data/esol/esol.csv', 'target': ['measured log solubility in mols per litre']}}\n",
      "Running on: cuda:1\n",
      "1127\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/1127\n",
      "Generating scaffold 1000/1127\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 17.167451858520508\n",
      "Validation loss: 4.281754793319027 RMSE: 2.06924\n",
      "1 21 4.161821365356445\n",
      "Validation loss: 5.861413065311128 RMSE: 2.4210355\n",
      "Validation loss: 4.406317736195252 RMSE: 2.099123\n",
      "3 13 2.0753438472747803\n",
      "Validation loss: 3.4435943206854627 RMSE: 1.8556923\n",
      "Validation loss: 3.8047082466361797 RMSE: 1.9505662\n",
      "5 5 1.206053614616394\n",
      "Validation loss: 4.56437258593804 RMSE: 2.1364393\n",
      "6 26 1.4283664226531982\n",
      "Validation loss: 5.981674886382787 RMSE: 2.4457462\n",
      "Validation loss: 3.661668039001195 RMSE: 1.9135485\n",
      "8 18 1.280663251876831\n",
      "Validation loss: 4.308677785164487 RMSE: 2.0757356\n",
      "Validation loss: 3.691550575526415 RMSE: 1.9213407\n",
      "10 10 0.9989776611328125\n",
      "Validation loss: 3.2211975207370993 RMSE: 1.7947695\n",
      "Validation loss: 3.914446130263067 RMSE: 1.9784958\n",
      "12 2 0.8823509812355042\n",
      "Validation loss: 3.1425043692631003 RMSE: 1.772711\n",
      "13 23 1.6500911712646484\n",
      "Validation loss: 3.083649306170708 RMSE: 1.7560322\n",
      "Validation loss: 2.7463446401916776 RMSE: 1.6572099\n",
      "15 15 1.5486057996749878\n",
      "Validation loss: 3.615423700450796 RMSE: 1.9014268\n",
      "Validation loss: 2.824512308677741 RMSE: 1.6806285\n",
      "17 7 0.8488145470619202\n",
      "Validation loss: 2.8200030854317992 RMSE: 1.6792865\n",
      "18 28 5.202521800994873\n",
      "Validation loss: 4.793280917986304 RMSE: 2.189356\n",
      "Validation loss: 3.5273234126842126 RMSE: 1.878117\n",
      "20 20 0.670099139213562\n",
      "Validation loss: 3.2305223456526226 RMSE: 1.7973653\n",
      "Validation loss: 3.181700512371232 RMSE: 1.783732\n",
      "22 12 0.702747642993927\n",
      "Validation loss: 3.679489026027443 RMSE: 1.9181995\n",
      "Validation loss: 2.638860102248403 RMSE: 1.6244568\n",
      "24 4 0.967053234577179\n",
      "Validation loss: 2.5749317407608032 RMSE: 1.6046593\n",
      "25 25 0.7166351675987244\n",
      "Validation loss: 2.9612226486206055 RMSE: 1.7208203\n",
      "Validation loss: 2.373163979665368 RMSE: 1.5405076\n",
      "27 17 0.9770793914794922\n",
      "Validation loss: 2.608095434914648 RMSE: 1.6149598\n",
      "Validation loss: 2.709050920157306 RMSE: 1.6459194\n",
      "29 9 0.5214827656745911\n",
      "Validation loss: 2.8104754148331366 RMSE: 1.6764472\n",
      "Validation loss: 3.518442850197311 RMSE: 1.8757513\n",
      "31 1 0.9139285683631897\n",
      "Validation loss: 2.840465596291871 RMSE: 1.6853681\n",
      "32 22 0.4866226315498352\n",
      "Validation loss: 2.946714663927534 RMSE: 1.7165997\n",
      "Validation loss: 3.680844638199933 RMSE: 1.9185528\n",
      "34 14 0.49867698550224304\n",
      "Validation loss: 3.3043669219565603 RMSE: 1.8177917\n",
      "Validation loss: 3.5630563676884743 RMSE: 1.8876059\n",
      "36 6 1.0666182041168213\n",
      "Validation loss: 4.016889859089809 RMSE: 2.004218\n",
      "37 27 1.4136958122253418\n",
      "Validation loss: 3.027607370266872 RMSE: 1.7400022\n",
      "Validation loss: 3.7305757978321177 RMSE: 1.9314698\n",
      "39 19 1.5963613986968994\n",
      "Validation loss: 3.1572179435628707 RMSE: 1.7768562\n",
      "Validation loss: 2.769309681073754 RMSE: 1.6641244\n",
      "41 11 0.6120967864990234\n",
      "Validation loss: 2.693385786714807 RMSE: 1.6411537\n",
      "Validation loss: 3.572650557070707 RMSE: 1.8901457\n",
      "43 3 0.39607277512550354\n",
      "Validation loss: 2.964460216792284 RMSE: 1.7217607\n",
      "44 24 0.505603551864624\n",
      "Validation loss: 2.678254221392944 RMSE: 1.6365374\n",
      "Validation loss: 3.151871588377826 RMSE: 1.7753512\n",
      "46 16 1.039329171180725\n",
      "Validation loss: 3.358308609608 RMSE: 1.8325689\n",
      "Validation loss: 3.573071007180003 RMSE: 1.890257\n",
      "48 8 0.8503376245498657\n",
      "Validation loss: 3.0125161673115417 RMSE: 1.7356602\n",
      "Validation loss: 4.492589216316696 RMSE: 2.1195729\n",
      "50 0 0.3928810954093933\n",
      "Validation loss: 3.107198181405532 RMSE: 1.7627246\n",
      "51 21 0.7915236353874207\n",
      "Validation loss: 3.5574287904047335 RMSE: 1.8861147\n",
      "Validation loss: 4.950066081190531 RMSE: 2.2248745\n",
      "53 13 0.7093260288238525\n",
      "Validation loss: 3.086377365399251 RMSE: 1.7568089\n",
      "Validation loss: 2.7721845829381353 RMSE: 1.6649879\n",
      "55 5 0.8653964400291443\n",
      "Validation loss: 3.1258920061904774 RMSE: 1.7680192\n",
      "56 26 0.32744792103767395\n",
      "Validation loss: 2.571691650204954 RMSE: 1.6036495\n",
      "Validation loss: 3.812535857732317 RMSE: 1.9525716\n",
      "58 18 0.6238411664962769\n",
      "Validation loss: 3.5841145431045938 RMSE: 1.8931758\n",
      "Validation loss: 5.2580272623922975 RMSE: 2.2930388\n",
      "60 10 0.6886093616485596\n",
      "Validation loss: 3.3184047188379067 RMSE: 1.8216488\n",
      "Validation loss: 3.1723100548296905 RMSE: 1.7810979\n",
      "62 2 0.4937784969806671\n",
      "Validation loss: 3.1719071211013117 RMSE: 1.7809849\n",
      "63 23 0.9239220023155212\n",
      "Validation loss: 2.842914408287116 RMSE: 1.6860945\n",
      "Validation loss: 3.273662873074017 RMSE: 1.8093266\n",
      "65 15 0.36024537682533264\n",
      "Validation loss: 3.947907426715952 RMSE: 1.9869342\n",
      "Validation loss: 5.664388226196829 RMSE: 2.3799975\n",
      "67 7 0.39707937836647034\n",
      "Validation loss: 3.183413539312582 RMSE: 1.7842124\n",
      "68 28 2.3986282348632812\n",
      "Validation loss: 3.422444480710325 RMSE: 1.849985\n",
      "Validation loss: 3.575912880686532 RMSE: 1.8910085\n",
      "70 20 1.1414155960083008\n",
      "Validation loss: 5.715003384953052 RMSE: 2.3906074\n",
      "Validation loss: 3.9120463523189577 RMSE: 1.9778894\n",
      "72 12 0.5556352734565735\n",
      "Validation loss: 3.4425411203266245 RMSE: 1.8554087\n",
      "Validation loss: 3.9531355241758632 RMSE: 1.9882493\n",
      "74 4 1.224000096321106\n",
      "Validation loss: 3.854092891237377 RMSE: 1.9631845\n",
      "75 25 0.41479599475860596\n",
      "Validation loss: 3.3364856306430513 RMSE: 1.826605\n",
      "Validation loss: 3.450778948522247 RMSE: 1.8576272\n",
      "77 17 0.8783134818077087\n",
      "Validation loss: 5.214460718948229 RMSE: 2.2835195\n",
      "Validation loss: 5.065089993772253 RMSE: 2.2505755\n",
      "79 9 0.8306350708007812\n",
      "Validation loss: 3.8743537400676087 RMSE: 1.9683378\n",
      "Validation loss: 2.514502571747366 RMSE: 1.5857183\n",
      "81 1 0.6795676946640015\n",
      "Validation loss: 3.079184808562287 RMSE: 1.7547607\n",
      "82 22 0.38549625873565674\n",
      "Validation loss: 3.2224790733472437 RMSE: 1.7951264\n",
      "Validation loss: 3.661963091487378 RMSE: 1.9136256\n",
      "84 14 0.24598678946495056\n",
      "Validation loss: 3.887447732739744 RMSE: 1.9716611\n",
      "Validation loss: 3.8696277458055883 RMSE: 1.9671369\n",
      "86 6 0.5415493845939636\n",
      "Validation loss: 3.3894565896650333 RMSE: 1.8410478\n",
      "87 27 0.4852505922317505\n",
      "Validation loss: 5.126621980582718 RMSE: 2.2642047\n",
      "Validation loss: 5.472806584518568 RMSE: 2.339403\n",
      "89 19 0.6889928579330444\n",
      "Validation loss: 5.029148848710862 RMSE: 2.2425766\n",
      "Validation loss: 4.859991601083131 RMSE: 2.204539\n",
      "91 11 0.7508680820465088\n",
      "Validation loss: 4.035313737075941 RMSE: 2.008809\n",
      "Validation loss: 5.858364594721161 RMSE: 2.4204059\n",
      "93 3 0.7858079075813293\n",
      "Validation loss: 4.175902948970288 RMSE: 2.0435026\n",
      "94 24 0.6316835284233093\n",
      "Validation loss: 5.25434037858406 RMSE: 2.292235\n",
      "Validation loss: 3.8533073357776204 RMSE: 1.9629844\n",
      "96 16 0.8539261221885681\n",
      "Validation loss: 3.951624135000516 RMSE: 1.9878691\n",
      "Validation loss: 3.0371841076201043 RMSE: 1.742752\n",
      "98 8 0.7196143269538879\n",
      "Validation loss: 4.155824545210442 RMSE: 2.0385838\n",
      "Validation loss: 3.51736242159278 RMSE: 1.8754632\n",
      "100 0 0.2815394103527069\n",
      "Validation loss: 3.879380156508589 RMSE: 1.9696141\n",
      "101 21 0.9152989387512207\n",
      "Validation loss: 4.628399460716585 RMSE: 2.1513715\n",
      "Validation loss: 3.881883211895428 RMSE: 1.9702495\n",
      "103 13 0.4413074851036072\n",
      "Validation loss: 4.201559130069429 RMSE: 2.0497706\n",
      "Validation loss: 4.352426910822371 RMSE: 2.0862472\n",
      "105 5 0.49293068051338196\n",
      "Validation loss: 2.7508562159749257 RMSE: 1.6585704\n",
      "106 26 0.5117525458335876\n",
      "Validation loss: 3.9802685079321396 RMSE: 1.995061\n",
      "Validation loss: 3.222010317101943 RMSE: 1.794996\n",
      "108 18 0.6589958071708679\n",
      "Validation loss: 5.374467803313669 RMSE: 2.3182898\n",
      "Validation loss: 5.249743102926068 RMSE: 2.2912319\n",
      "110 10 0.8541086316108704\n",
      "Validation loss: 4.580093077853718 RMSE: 2.1401153\n",
      "Validation loss: 4.06828826296646 RMSE: 2.0169997\n",
      "112 2 0.5489547252655029\n",
      "Validation loss: 4.290862231127984 RMSE: 2.0714395\n",
      "113 23 0.45768582820892334\n",
      "Validation loss: 3.8953028442585365 RMSE: 1.9736521\n",
      "Validation loss: 4.567793170962713 RMSE: 2.1372397\n",
      "115 15 1.078604817390442\n",
      "Validation loss: 5.171321084014083 RMSE: 2.274054\n",
      "Validation loss: 3.7644825910044983 RMSE: 1.9402275\n",
      "117 7 0.5868000984191895\n",
      "Validation loss: 5.78140390024776 RMSE: 2.4044552\n",
      "118 28 0.5457209348678589\n",
      "Validation loss: 4.383112472770488 RMSE: 2.0935884\n",
      "Validation loss: 3.845507701941296 RMSE: 1.9609966\n",
      "120 20 0.2635229527950287\n",
      "Validation loss: 4.275790092164436 RMSE: 2.0677984\n",
      "Validation loss: 3.8063026318507913 RMSE: 1.9509747\n",
      "122 12 0.6599692106246948\n",
      "Validation loss: 5.012555004221149 RMSE: 2.2388735\n",
      "Validation loss: 4.14323980829357 RMSE: 2.0354948\n",
      "124 4 1.7682056427001953\n",
      "Validation loss: 4.960465047211773 RMSE: 2.22721\n",
      "125 25 0.43558716773986816\n",
      "Validation loss: 5.369289191423264 RMSE: 2.3171728\n",
      "Validation loss: 6.2091219446300405 RMSE: 2.491811\n",
      "127 17 0.6562110185623169\n",
      "Validation loss: 6.010039494100925 RMSE: 2.451538\n",
      "Validation loss: 4.705648076217787 RMSE: 2.1692505\n",
      "129 9 0.7631973624229431\n",
      "Validation loss: 6.062243014310313 RMSE: 2.4621623\n",
      "Validation loss: 5.1811400599184285 RMSE: 2.2762117\n",
      "131 1 0.43805932998657227\n",
      "Validation loss: 3.8563278755255506 RMSE: 1.9637536\n",
      "132 22 0.4960957169532776\n",
      "Validation loss: 4.636722644873425 RMSE: 2.153305\n",
      "Validation loss: 4.241267868902831 RMSE: 2.059434\n",
      "134 14 0.6067417860031128\n",
      "Validation loss: 4.936431652676743 RMSE: 2.2218084\n",
      "Validation loss: 4.961485698159817 RMSE: 2.2274394\n",
      "136 6 0.4425186216831207\n",
      "Validation loss: 3.6544857531522226 RMSE: 1.9116709\n",
      "137 27 0.5819421410560608\n",
      "Validation loss: 4.186264143580884 RMSE: 2.0460362\n",
      "Validation loss: 4.761615938844934 RMSE: 2.1821127\n",
      "139 19 0.7546023726463318\n",
      "Validation loss: 3.3335863299074426 RMSE: 1.825811\n",
      "Validation loss: 4.249636903273321 RMSE: 2.0614648\n",
      "141 11 0.572131872177124\n",
      "Validation loss: 4.733063807529685 RMSE: 2.1755607\n",
      "Validation loss: 4.105891042051062 RMSE: 2.0263\n",
      "143 3 0.5588791966438293\n",
      "Validation loss: 3.8783559356115562 RMSE: 1.9693543\n",
      "144 24 0.26427966356277466\n",
      "Validation loss: 3.649326370880667 RMSE: 1.910321\n",
      "Validation loss: 3.3939677930511203 RMSE: 1.8422725\n",
      "146 16 0.2777431011199951\n",
      "Validation loss: 4.243214944822598 RMSE: 2.0599065\n",
      "Validation loss: 4.502798913854413 RMSE: 2.12198\n",
      "148 8 0.716346263885498\n",
      "Validation loss: 4.047352377292329 RMSE: 2.0118032\n",
      "Validation loss: 3.9538546579074016 RMSE: 1.9884303\n",
      "150 0 0.3887746334075928\n",
      "Validation loss: 4.838327699002966 RMSE: 2.1996198\n",
      "151 21 1.7166751623153687\n",
      "Validation loss: 3.4274221774751106 RMSE: 1.8513299\n",
      "Validation loss: 3.493097744156829 RMSE: 1.8689833\n",
      "153 13 0.5244569182395935\n",
      "Validation loss: 2.934577880707462 RMSE: 1.713061\n",
      "Validation loss: 3.160693641257497 RMSE: 1.7778339\n",
      "155 5 0.48247891664505005\n",
      "Validation loss: 4.2551478580036 RMSE: 2.062801\n",
      "156 26 0.44798198342323303\n",
      "Validation loss: 3.5161324328025887 RMSE: 1.8751352\n",
      "Validation loss: 5.352613491294658 RMSE: 2.3135715\n",
      "158 18 0.23584963381290436\n",
      "Validation loss: 3.4685338260853187 RMSE: 1.8624\n",
      "Validation loss: 3.7092558134973577 RMSE: 1.9259429\n",
      "160 10 0.9880300760269165\n",
      "Validation loss: 5.378242252147303 RMSE: 2.3191037\n",
      "Validation loss: 6.129058723956083 RMSE: 2.4756937\n",
      "162 2 0.9083418250083923\n",
      "Validation loss: 4.539977670770831 RMSE: 2.1307225\n",
      "163 23 0.605213463306427\n",
      "Validation loss: 5.5041270129448545 RMSE: 2.3460877\n",
      "Validation loss: 4.357656240463257 RMSE: 2.0874999\n",
      "165 15 0.2959573268890381\n",
      "Validation loss: 6.223016530011607 RMSE: 2.4945977\n",
      "Validation loss: 5.141975571623946 RMSE: 2.2675924\n",
      "167 7 0.44884905219078064\n",
      "Validation loss: 4.855875146072523 RMSE: 2.2036052\n",
      "168 28 0.2163962423801422\n",
      "Validation loss: 3.650628638478507 RMSE: 1.9106617\n",
      "Validation loss: 3.8984865720293165 RMSE: 1.9744585\n",
      "170 20 0.41739794611930847\n",
      "Validation loss: 2.9657243416372654 RMSE: 1.7221279\n",
      "Validation loss: 2.782707119410017 RMSE: 1.6681447\n",
      "172 12 0.5171223282814026\n",
      "Validation loss: 2.7064373872976386 RMSE: 1.6451253\n",
      "Validation loss: 3.370553434422586 RMSE: 1.8359066\n",
      "174 4 0.3951898515224457\n",
      "Validation loss: 3.721471069133387 RMSE: 1.9291114\n",
      "175 25 0.8545404076576233\n",
      "Validation loss: 4.0939622706016605 RMSE: 2.0233543\n",
      "Validation loss: 4.569161191450811 RMSE: 2.1375597\n",
      "177 17 0.48232242465019226\n",
      "Validation loss: 3.5655173052728704 RMSE: 1.8882577\n",
      "Validation loss: 3.8939908932795566 RMSE: 1.9733198\n",
      "179 9 0.32079726457595825\n",
      "Validation loss: 5.344559374108779 RMSE: 2.3118303\n",
      "Validation loss: 4.989147500654237 RMSE: 2.23364\n",
      "181 1 0.465209424495697\n",
      "Validation loss: 5.192108339967981 RMSE: 2.27862\n",
      "182 22 0.2261449545621872\n",
      "Validation loss: 4.894988146503414 RMSE: 2.212462\n",
      "Validation loss: 3.2318792838965895 RMSE: 1.7977427\n",
      "184 14 0.6827210783958435\n",
      "Validation loss: 2.4189941672097266 RMSE: 1.5553116\n",
      "Validation loss: 4.55848140210177 RMSE: 2.13506\n",
      "186 6 0.28454864025115967\n",
      "Validation loss: 3.6536302102350557 RMSE: 1.911447\n",
      "187 27 0.678223192691803\n",
      "Validation loss: 2.994250289106791 RMSE: 1.7303902\n",
      "Validation loss: 3.655564063418228 RMSE: 1.911953\n",
      "189 19 0.40256956219673157\n",
      "Validation loss: 3.272052914695402 RMSE: 1.8088816\n",
      "Validation loss: 3.66323705690097 RMSE: 1.9139584\n",
      "191 11 0.5532144904136658\n",
      "Validation loss: 4.0324292140724385 RMSE: 2.008091\n",
      "Validation loss: 3.9725260713459116 RMSE: 1.9931197\n",
      "193 3 0.2400341033935547\n",
      "Validation loss: 4.281878095812502 RMSE: 2.06927\n",
      "194 24 0.5079219937324524\n",
      "Validation loss: 3.9664480369702906 RMSE: 1.9915944\n",
      "Validation loss: 4.044706753924885 RMSE: 2.0111458\n",
      "196 16 0.7652536034584045\n",
      "Validation loss: 4.356722947770515 RMSE: 2.0872765\n",
      "Validation loss: 3.4568114470591587 RMSE: 1.8592503\n",
      "198 8 0.5698404908180237\n",
      "Validation loss: 3.652537696129453 RMSE: 1.9111613\n",
      "Validation loss: 4.005382151730292 RMSE: 2.0013452\n",
      "Loaded trained model with success.\n",
      "Test loss: 2.9330979343009207 Test RMSE: 1.712629\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:1', 'task_name': 'esol', 'model_type': 'gin', 'model': {'num_layer': 5, 'emb_dim': 300, 'feat_dim': 300, 'drop_ratio': 0.3, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 781, 'task': 'regression', 'data_path': 'data/esol/esol.csv', 'target': ['measured log solubility in mols per litre']}}\n",
      "Running on: cuda:1\n",
      "1127\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/1127\n",
      "Generating scaffold 1000/1127\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 14.954634666442871\n",
      "Validation loss: 5.221119053595889 RMSE: 2.2849767\n",
      "1 21 2.6866238117218018\n",
      "Validation loss: 6.859172578406545 RMSE: 2.619002\n",
      "Validation loss: 12.589950941305245 RMSE: 3.548232\n",
      "3 13 1.5169029235839844\n",
      "Validation loss: 6.931179603644177 RMSE: 2.6327133\n",
      "Validation loss: 9.547450512911366 RMSE: 3.089895\n",
      "5 5 2.1244382858276367\n",
      "Validation loss: 8.087781534785718 RMSE: 2.8439026\n",
      "6 26 0.989162802696228\n",
      "Validation loss: 12.327109041467176 RMSE: 3.5109982\n",
      "Validation loss: 12.998275841231894 RMSE: 3.605312\n",
      "8 18 1.2719475030899048\n",
      "Validation loss: 8.214086195008944 RMSE: 2.8660226\n",
      "Validation loss: 7.6285902884154195 RMSE: 2.7619903\n",
      "10 10 2.183471441268921\n",
      "Validation loss: 7.368399582077972 RMSE: 2.7144797\n",
      "Validation loss: 8.1047018186181 RMSE: 2.846876\n",
      "12 2 2.049171209335327\n",
      "Validation loss: 8.01557382651135 RMSE: 2.831179\n",
      "13 23 1.497039794921875\n",
      "Validation loss: 7.750940622481624 RMSE: 2.7840512\n",
      "Validation loss: 6.158830727096152 RMSE: 2.481699\n",
      "15 15 1.0936291217803955\n",
      "Validation loss: 4.898217509278154 RMSE: 2.2131917\n",
      "Validation loss: 5.089116231530114 RMSE: 2.2559068\n",
      "17 7 1.2237941026687622\n",
      "Validation loss: 6.0196304785466825 RMSE: 2.4534936\n",
      "18 28 1.873227596282959\n",
      "Validation loss: 6.603154904019516 RMSE: 2.5696604\n",
      "Validation loss: 4.703284689810424 RMSE: 2.1687057\n",
      "20 20 0.8391215205192566\n",
      "Validation loss: 7.67565231618628 RMSE: 2.7704966\n",
      "Validation loss: 9.22921446572363 RMSE: 3.0379622\n",
      "22 12 0.9073582291603088\n",
      "Validation loss: 8.35374156563683 RMSE: 2.890284\n",
      "Validation loss: 9.109803284164023 RMSE: 3.018245\n",
      "24 4 0.6881308555603027\n",
      "Validation loss: 6.757307858593696 RMSE: 2.5994823\n",
      "25 25 0.7009782791137695\n",
      "Validation loss: 7.158612293479717 RMSE: 2.6755583\n",
      "Validation loss: 10.522031497111362 RMSE: 3.2437682\n",
      "27 17 0.4904899001121521\n",
      "Validation loss: 6.792298067987493 RMSE: 2.6062038\n",
      "Validation loss: 6.828202260279022 RMSE: 2.613083\n",
      "29 9 0.9674676060676575\n",
      "Validation loss: 11.036296565975762 RMSE: 3.322092\n",
      "Validation loss: 6.923467509514462 RMSE: 2.6312482\n",
      "31 1 0.7538946866989136\n",
      "Validation loss: 9.057951724634762 RMSE: 3.009643\n",
      "32 22 1.1494725942611694\n",
      "Validation loss: 8.147825270627452 RMSE: 2.8544395\n",
      "Validation loss: 5.568134172827796 RMSE: 2.3596895\n",
      "34 14 1.2231521606445312\n",
      "Validation loss: 9.598391963317331 RMSE: 3.098127\n",
      "Validation loss: 11.176306631712787 RMSE: 3.3430984\n",
      "36 6 0.8253027200698853\n",
      "Validation loss: 11.315041466096861 RMSE: 3.3637836\n",
      "37 27 0.6295577883720398\n",
      "Validation loss: 8.452999098111043 RMSE: 2.9074044\n",
      "Validation loss: 7.258008847194436 RMSE: 2.6940691\n",
      "39 19 1.0572121143341064\n",
      "Validation loss: 7.363708711303441 RMSE: 2.7136157\n",
      "Validation loss: 9.142600253619978 RMSE: 3.0236735\n",
      "41 11 0.5644378662109375\n",
      "Validation loss: 8.294197188014477 RMSE: 2.8799648\n",
      "Validation loss: 6.3675054955271495 RMSE: 2.5233915\n",
      "43 3 0.6994228363037109\n",
      "Validation loss: 7.883636229861099 RMSE: 2.8077812\n",
      "44 24 0.7338024377822876\n",
      "Validation loss: 5.544186431749732 RMSE: 2.3546095\n",
      "Validation loss: 6.569625478930178 RMSE: 2.5631282\n",
      "46 16 0.9690487384796143\n",
      "Validation loss: 6.528578876394086 RMSE: 2.5551085\n",
      "Validation loss: 6.577127009366466 RMSE: 2.564591\n",
      "48 8 0.8621518611907959\n",
      "Validation loss: 3.729089722169184 RMSE: 1.9310851\n",
      "Validation loss: 7.918303936983632 RMSE: 2.8139482\n",
      "50 0 0.6183596849441528\n",
      "Validation loss: 4.028992446122971 RMSE: 2.007235\n",
      "51 21 0.44286707043647766\n",
      "Validation loss: 6.017656697636157 RMSE: 2.4530911\n",
      "Validation loss: 5.272926014081567 RMSE: 2.2962852\n",
      "53 13 0.4993699789047241\n",
      "Validation loss: 5.844349295692107 RMSE: 2.4175088\n",
      "Validation loss: 7.125503810106125 RMSE: 2.669364\n",
      "55 5 0.40274283289909363\n",
      "Validation loss: 5.454834849433562 RMSE: 2.3355587\n",
      "56 26 0.5133029818534851\n",
      "Validation loss: 3.791344834639963 RMSE: 1.9471376\n",
      "Validation loss: 4.644779312927111 RMSE: 2.155175\n",
      "58 18 0.42561694979667664\n",
      "Validation loss: 5.3006840047583115 RMSE: 2.3023214\n",
      "Validation loss: 5.3727268413104845 RMSE: 2.3179142\n",
      "60 10 1.671037197113037\n",
      "Validation loss: 4.284415869586236 RMSE: 2.0698829\n",
      "Validation loss: 5.222660275687159 RMSE: 2.2853138\n",
      "62 2 0.7326644659042358\n",
      "Validation loss: 5.122159282718084 RMSE: 2.2632189\n",
      "63 23 0.561468243598938\n",
      "Validation loss: 5.073481610390992 RMSE: 2.252439\n",
      "Validation loss: 4.384605122878488 RMSE: 2.0939448\n",
      "65 15 0.7836865782737732\n",
      "Validation loss: 5.946065591499869 RMSE: 2.4384556\n",
      "Validation loss: 4.613356653567964 RMSE: 2.1478724\n",
      "67 7 0.734329342842102\n",
      "Validation loss: 6.856871613359029 RMSE: 2.6185627\n",
      "68 28 1.964393138885498\n",
      "Validation loss: 5.409147621256061 RMSE: 2.3257575\n",
      "Validation loss: 5.748942860459859 RMSE: 2.3976953\n",
      "70 20 0.9655018448829651\n",
      "Validation loss: 5.099397304838738 RMSE: 2.2581847\n",
      "Validation loss: 5.618643579229844 RMSE: 2.3703678\n",
      "72 12 0.9422823786735535\n",
      "Validation loss: 6.144269766005794 RMSE: 2.4787636\n",
      "Validation loss: 5.339106234828983 RMSE: 2.3106506\n",
      "74 4 0.8581402897834778\n",
      "Validation loss: 5.593561615564127 RMSE: 2.365071\n",
      "75 25 0.765677273273468\n",
      "Validation loss: 5.744478816479708 RMSE: 2.3967643\n",
      "Validation loss: 6.038043925192504 RMSE: 2.4572432\n",
      "77 17 0.9527244567871094\n",
      "Validation loss: 8.137364075247165 RMSE: 2.8526065\n",
      "Validation loss: 6.379812751196127 RMSE: 2.525829\n",
      "79 9 0.5049774646759033\n",
      "Validation loss: 6.62839738035624 RMSE: 2.5745673\n",
      "Validation loss: 5.751678070135876 RMSE: 2.3982658\n",
      "81 1 0.5150740146636963\n",
      "Validation loss: 4.692101558752819 RMSE: 2.166126\n",
      "82 22 0.34190186858177185\n",
      "Validation loss: 6.239553000019715 RMSE: 2.4979098\n",
      "Validation loss: 4.7441400342283 RMSE: 2.1781049\n",
      "84 14 0.4491831660270691\n",
      "Validation loss: 4.923935257228075 RMSE: 2.2189941\n",
      "Validation loss: 8.573173750818302 RMSE: 2.9279983\n",
      "86 6 1.0076889991760254\n",
      "Validation loss: 4.849737682173737 RMSE: 2.2022119\n",
      "87 27 0.52374267578125\n",
      "Validation loss: 5.451354950930165 RMSE: 2.3348136\n",
      "Validation loss: 6.209654516878381 RMSE: 2.4919178\n",
      "89 19 0.5735173225402832\n",
      "Validation loss: 5.062408126560988 RMSE: 2.2499797\n",
      "Validation loss: 8.709965840905113 RMSE: 2.951265\n",
      "91 11 0.8512949347496033\n",
      "Validation loss: 4.883338919783061 RMSE: 2.209828\n",
      "Validation loss: 5.720071556293859 RMSE: 2.3916671\n",
      "93 3 0.5523948669433594\n",
      "Validation loss: 6.33025302718171 RMSE: 2.5159993\n",
      "94 24 0.5162192583084106\n",
      "Validation loss: 5.1460698937947775 RMSE: 2.268495\n",
      "Validation loss: 7.541147046384558 RMSE: 2.7461147\n",
      "96 16 0.8798633217811584\n",
      "Validation loss: 7.376156722549844 RMSE: 2.715908\n",
      "Validation loss: 6.710420152782339 RMSE: 2.590448\n",
      "98 8 0.7623525261878967\n",
      "Validation loss: 6.895191563969165 RMSE: 2.6258695\n",
      "Validation loss: 6.042253257954015 RMSE: 2.4580996\n",
      "100 0 0.5794631242752075\n",
      "Validation loss: 6.417131959864523 RMSE: 2.5332057\n",
      "101 21 0.6514908075332642\n",
      "Validation loss: 5.144408913840235 RMSE: 2.2681289\n",
      "Validation loss: 6.077030956217673 RMSE: 2.4651635\n",
      "103 13 0.43764379620552063\n",
      "Validation loss: 5.564192366811026 RMSE: 2.358854\n",
      "Validation loss: 5.806592413809447 RMSE: 2.4096873\n",
      "105 5 0.7245019674301147\n",
      "Validation loss: 5.413807181130468 RMSE: 2.3267589\n",
      "106 26 0.45360082387924194\n",
      "Validation loss: 6.463850586815218 RMSE: 2.5424104\n",
      "Validation loss: 5.301041328801518 RMSE: 2.302399\n",
      "108 18 0.5389549136161804\n",
      "Validation loss: 8.10080852339753 RMSE: 2.846192\n",
      "Validation loss: 7.317644081284515 RMSE: 2.7051144\n",
      "110 10 0.6274809241294861\n",
      "Validation loss: 6.820267415679662 RMSE: 2.6115642\n",
      "Validation loss: 4.905416969704417 RMSE: 2.2148175\n",
      "112 2 0.559817910194397\n",
      "Validation loss: 6.168573257142463 RMSE: 2.4836612\n",
      "113 23 0.37803399562835693\n",
      "Validation loss: 4.493678227990075 RMSE: 2.1198297\n",
      "Validation loss: 6.245226657496089 RMSE: 2.4990451\n",
      "115 15 0.3637901246547699\n",
      "Validation loss: 7.177425696786526 RMSE: 2.6790717\n",
      "Validation loss: 6.075493593131546 RMSE: 2.4648516\n",
      "117 7 0.690475583076477\n",
      "Validation loss: 6.93111187588852 RMSE: 2.6327004\n",
      "118 28 0.1358950138092041\n",
      "Validation loss: 7.582831361652476 RMSE: 2.753694\n",
      "Validation loss: 6.599632617646614 RMSE: 2.568975\n",
      "120 20 0.3639879822731018\n",
      "Validation loss: 5.735659000092903 RMSE: 2.3949237\n",
      "Validation loss: 5.350216861319753 RMSE: 2.3130534\n",
      "122 12 0.7714498043060303\n",
      "Validation loss: 5.704420904142666 RMSE: 2.388393\n",
      "Validation loss: 6.754217489630775 RMSE: 2.5988877\n",
      "124 4 0.7507511377334595\n",
      "Validation loss: 5.806085316480789 RMSE: 2.4095821\n",
      "125 25 0.6127725839614868\n",
      "Validation loss: 7.374544557216948 RMSE: 2.7156112\n",
      "Validation loss: 4.913524433574845 RMSE: 2.2166471\n",
      "127 17 1.1298820972442627\n",
      "Validation loss: 5.970851471993775 RMSE: 2.4435327\n",
      "Validation loss: 7.1668110779956375 RMSE: 2.6770902\n",
      "129 9 0.41817009449005127\n",
      "Validation loss: 5.807262614764999 RMSE: 2.4098263\n",
      "Validation loss: 5.254087317306383 RMSE: 2.2921798\n",
      "131 1 0.3262696862220764\n",
      "Validation loss: 7.282607399256883 RMSE: 2.6986306\n",
      "132 22 0.5097096562385559\n",
      "Validation loss: 8.338004914005246 RMSE: 2.8875604\n",
      "Validation loss: 6.933396630582556 RMSE: 2.6331346\n",
      "134 14 0.31651660799980164\n",
      "Validation loss: 7.430532295092017 RMSE: 2.7259004\n",
      "Validation loss: 7.260665458915508 RMSE: 2.6945622\n",
      "136 6 0.7247886657714844\n",
      "Validation loss: 8.501486769819682 RMSE: 2.915731\n",
      "137 27 0.5730281472206116\n",
      "Validation loss: 6.972612207969733 RMSE: 2.6405704\n",
      "Validation loss: 5.972150701337156 RMSE: 2.4437985\n",
      "139 19 0.5746873617172241\n",
      "Validation loss: 7.108993171590619 RMSE: 2.6662695\n",
      "Validation loss: 6.955899128871682 RMSE: 2.637404\n",
      "141 11 0.5122170448303223\n",
      "Validation loss: 6.010963735327256 RMSE: 2.4517267\n",
      "Validation loss: 8.858212006830536 RMSE: 2.976275\n",
      "143 3 0.3968214988708496\n",
      "Validation loss: 9.748208029080281 RMSE: 3.122212\n",
      "144 24 0.29172834753990173\n",
      "Validation loss: 6.484744468621448 RMSE: 2.5465162\n",
      "Validation loss: 6.151453668037347 RMSE: 2.4802125\n",
      "146 16 0.3863305449485779\n",
      "Validation loss: 9.43112562398995 RMSE: 3.0710137\n",
      "Validation loss: 7.500537053673669 RMSE: 2.7387106\n",
      "148 8 0.25666770339012146\n",
      "Validation loss: 8.455621622305001 RMSE: 2.9078553\n",
      "Validation loss: 7.111519678504066 RMSE: 2.6667433\n",
      "150 0 0.7490163445472717\n",
      "Validation loss: 7.585331587664849 RMSE: 2.754148\n",
      "151 21 0.4779753088951111\n",
      "Validation loss: 6.543371403111821 RMSE: 2.5580015\n",
      "Validation loss: 7.713867942843817 RMSE: 2.777385\n",
      "153 13 0.7854545712471008\n",
      "Validation loss: 6.530290780869206 RMSE: 2.5554433\n",
      "Validation loss: 6.948151419648027 RMSE: 2.6359348\n",
      "155 5 0.5879135131835938\n",
      "Validation loss: 5.928272196676879 RMSE: 2.4348044\n",
      "156 26 0.5099608302116394\n",
      "Validation loss: 6.3228114980511965 RMSE: 2.5145202\n",
      "Validation loss: 9.055842914412507 RMSE: 3.0092926\n",
      "158 18 0.6540390849113464\n",
      "Validation loss: 7.892063934191138 RMSE: 2.8092818\n",
      "Validation loss: 6.344180334985784 RMSE: 2.5187657\n",
      "160 10 0.5844787359237671\n",
      "Validation loss: 6.3974796776222975 RMSE: 2.5293238\n",
      "Validation loss: 6.9411688483921825 RMSE: 2.63461\n",
      "162 2 0.6748944520950317\n",
      "Validation loss: 9.728380072433337 RMSE: 3.1190352\n",
      "163 23 0.7764503955841064\n",
      "Validation loss: 6.953225313034733 RMSE: 2.6368968\n",
      "Validation loss: 6.352753229900799 RMSE: 2.5204668\n",
      "165 15 0.5366800427436829\n",
      "Validation loss: 9.447056306146942 RMSE: 3.0736063\n",
      "Validation loss: 9.185087351672417 RMSE: 3.0306911\n",
      "167 7 0.63832688331604\n",
      "Validation loss: 6.9651636191174 RMSE: 2.6391597\n",
      "168 28 0.46067729592323303\n",
      "Validation loss: 7.7304034950458895 RMSE: 2.7803602\n",
      "Validation loss: 8.435764802240692 RMSE: 2.9044387\n",
      "170 20 0.7935093641281128\n",
      "Validation loss: 9.409042598926916 RMSE: 3.0674162\n",
      "Validation loss: 8.05790739565824 RMSE: 2.8386452\n",
      "172 12 0.48560193181037903\n",
      "Validation loss: 7.860437857366241 RMSE: 2.8036473\n",
      "Validation loss: 5.642360383430414 RMSE: 2.3753655\n",
      "174 4 0.47568678855895996\n",
      "Validation loss: 5.65844061944337 RMSE: 2.3787477\n",
      "175 25 0.1761334091424942\n",
      "Validation loss: 5.809834476065847 RMSE: 2.4103599\n",
      "Validation loss: 6.725246064430844 RMSE: 2.5933077\n",
      "177 17 0.956554651260376\n",
      "Validation loss: 5.817917612801611 RMSE: 2.412036\n",
      "Validation loss: 6.935281521451157 RMSE: 2.6334922\n",
      "179 9 0.34089112281799316\n",
      "Validation loss: 5.918735883932198 RMSE: 2.432845\n",
      "Validation loss: 6.061070129934666 RMSE: 2.4619238\n",
      "181 1 0.3461250960826874\n",
      "Validation loss: 5.8028426887714755 RMSE: 2.408909\n",
      "182 22 0.3052597939968109\n",
      "Validation loss: 9.13892392352619 RMSE: 3.0230653\n",
      "Validation loss: 8.861865625972241 RMSE: 2.9768887\n",
      "184 14 0.7142882943153381\n",
      "Validation loss: 7.685957081549991 RMSE: 2.772356\n",
      "Validation loss: 11.058023959134532 RMSE: 3.3253605\n",
      "186 6 0.5153173804283142\n",
      "Validation loss: 8.273991542579854 RMSE: 2.8764546\n",
      "187 27 0.3758127689361572\n",
      "Validation loss: 9.207734610127137 RMSE: 3.0344248\n",
      "Validation loss: 7.565120207524933 RMSE: 2.7504764\n",
      "189 19 0.578527569770813\n",
      "Validation loss: 8.178368880685452 RMSE: 2.8597846\n",
      "Validation loss: 7.224104045766644 RMSE: 2.6877694\n",
      "191 11 0.5979828834533691\n",
      "Validation loss: 6.8151800126101065 RMSE: 2.61059\n",
      "Validation loss: 8.26206131319029 RMSE: 2.8743804\n",
      "193 3 0.46962401270866394\n",
      "Validation loss: 6.639767018039669 RMSE: 2.5767744\n",
      "194 24 0.44251033663749695\n",
      "Validation loss: 6.294675181397294 RMSE: 2.508919\n",
      "Validation loss: 8.984008793282298 RMSE: 2.9973338\n",
      "196 16 0.5659497976303101\n",
      "Validation loss: 9.026935990932769 RMSE: 3.0044858\n",
      "Validation loss: 8.98293311195036 RMSE: 2.9971542\n",
      "198 8 0.41357526183128357\n",
      "Validation loss: 8.337527300404236 RMSE: 2.8874774\n",
      "Validation loss: 8.088769912719727 RMSE: 2.8440764\n",
      "Loaded trained model with success.\n",
      "Test loss: 4.188890431834533 Test RMSE: 2.0466778\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:1', 'task_name': 'lipo', 'model_type': 'gin', 'model': {'num_layer': 5, 'emb_dim': 300, 'feat_dim': 300, 'drop_ratio': 0.3, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 777, 'task': 'regression', 'data_path': 'data/lipophilicity/Lipophilicity.csv', 'target': ['exp']}}\n",
      "Running on: cuda:1\n",
      "4199\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/4199\n",
      "Generating scaffold 1000/4199\n",
      "Generating scaffold 2000/4199\n",
      "Generating scaffold 3000/4199\n",
      "Generating scaffold 4000/4199\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 7.530498027801514\n",
      "0 50 1.0618488788604736\n",
      "0 100 1.082703948020935\n",
      "Validation loss: 1.4666264352344331 RMSE: 1.2110436\n",
      "1 45 1.5237313508987427\n",
      "1 95 1.0373947620391846\n",
      "Validation loss: 1.45978540068581 RMSE: 1.2082157\n",
      "2 40 0.7502725720405579\n",
      "2 90 0.6735138297080994\n",
      "Validation loss: 1.8246300913038709 RMSE: 1.3507887\n",
      "3 35 0.6597764492034912\n",
      "3 85 0.8405156135559082\n",
      "Validation loss: 1.4115867194675265 RMSE: 1.1881021\n",
      "4 30 0.5574991703033447\n",
      "4 80 0.6664561629295349\n",
      "Validation loss: 1.6923226311093285 RMSE: 1.300893\n",
      "5 25 0.7649093270301819\n",
      "5 75 0.7251167297363281\n",
      "Validation loss: 1.3928666864122663 RMSE: 1.1801977\n",
      "6 20 0.6782136559486389\n",
      "6 70 0.7807782888412476\n",
      "Validation loss: 2.0045032342274984 RMSE: 1.4158047\n",
      "7 15 0.6690433025360107\n",
      "7 65 0.500325620174408\n",
      "Validation loss: 1.7211373896825881 RMSE: 1.3119214\n",
      "8 10 0.9145680069923401\n",
      "8 60 1.2021076679229736\n",
      "Validation loss: 1.3362619502203805 RMSE: 1.155968\n",
      "9 5 0.969879686832428\n",
      "9 55 0.5298208594322205\n",
      "Validation loss: 1.2282127550670079 RMSE: 1.1082476\n",
      "10 0 0.7346919178962708\n",
      "10 50 0.5738659501075745\n",
      "10 100 0.7098032832145691\n",
      "Validation loss: 1.5303710086005076 RMSE: 1.2370816\n",
      "11 45 0.6338128447532654\n",
      "11 95 0.7812811136245728\n",
      "Validation loss: 1.3755884647369385 RMSE: 1.1728548\n",
      "12 40 0.5571459531784058\n",
      "12 90 0.7366545796394348\n",
      "Validation loss: 1.9243442569460187 RMSE: 1.3872074\n",
      "13 35 0.5553578734397888\n",
      "13 85 0.9153976440429688\n",
      "Validation loss: 1.2230859007154191 RMSE: 1.1059322\n",
      "14 30 0.45280721783638\n",
      "14 80 0.8590405583381653\n",
      "Validation loss: 1.5281021765300207 RMSE: 1.2361643\n",
      "15 25 0.6245337724685669\n",
      "15 75 0.46008917689323425\n",
      "Validation loss: 1.4453051078887214 RMSE: 1.2022084\n",
      "16 20 0.6658086180686951\n",
      "16 70 0.679137647151947\n",
      "Validation loss: 1.9729884726660591 RMSE: 1.4046311\n",
      "17 15 0.999481201171875\n",
      "17 65 0.5490443706512451\n",
      "Validation loss: 2.5118308748517717 RMSE: 1.5848757\n",
      "18 10 1.0469095706939697\n",
      "18 60 0.4818442165851593\n",
      "Validation loss: 1.7399963742210751 RMSE: 1.3190892\n",
      "19 5 0.7602757215499878\n",
      "19 55 0.29711270332336426\n",
      "Validation loss: 1.5383144015357608 RMSE: 1.240288\n",
      "20 0 0.49945172667503357\n",
      "20 50 0.7399861216545105\n",
      "20 100 0.6583786010742188\n",
      "Validation loss: 1.5532596043178013 RMSE: 1.2462983\n",
      "21 45 1.1930221319198608\n",
      "21 95 0.5048491358757019\n",
      "Validation loss: 1.2395502703530448 RMSE: 1.113351\n",
      "22 40 0.716295599937439\n",
      "22 90 0.3948219120502472\n",
      "Validation loss: 1.2914537827173869 RMSE: 1.1364214\n",
      "23 35 0.9290645718574524\n",
      "23 85 0.4335811734199524\n",
      "Validation loss: 2.9936105115073066 RMSE: 1.7302053\n",
      "24 30 0.561024010181427\n",
      "24 80 0.42041724920272827\n",
      "Validation loss: 1.4302599940981184 RMSE: 1.1959348\n",
      "25 25 0.35696375370025635\n",
      "25 75 0.9766605496406555\n",
      "Validation loss: 1.2270800902729944 RMSE: 1.1077365\n",
      "26 20 0.5472314953804016\n",
      "26 70 0.5880845189094543\n",
      "Validation loss: 2.449864977882022 RMSE: 1.5652045\n",
      "27 15 0.7171410918235779\n",
      "27 65 0.6518374085426331\n",
      "Validation loss: 1.6803109645843506 RMSE: 1.2962681\n",
      "28 10 0.4095211327075958\n",
      "28 60 0.5737969875335693\n",
      "Validation loss: 2.415792208626157 RMSE: 1.554282\n",
      "29 5 0.5718290209770203\n",
      "29 55 0.6297799348831177\n",
      "Validation loss: 1.500363252276466 RMSE: 1.2248932\n",
      "30 0 0.5245912075042725\n",
      "30 50 0.8436487913131714\n",
      "30 100 0.3880216181278229\n",
      "Validation loss: 1.6310036670593988 RMSE: 1.2771076\n",
      "31 45 0.8282755613327026\n",
      "31 95 0.4514789283275604\n",
      "Validation loss: 1.4069275186175392 RMSE: 1.1861397\n",
      "32 40 0.4583219587802887\n",
      "32 90 0.40853580832481384\n",
      "Validation loss: 2.2840664522988456 RMSE: 1.5113128\n",
      "33 35 0.6276333332061768\n",
      "33 85 0.6740924715995789\n",
      "Validation loss: 1.3372007778712682 RMSE: 1.156374\n",
      "34 30 0.31334877014160156\n",
      "34 80 0.29653990268707275\n",
      "Validation loss: 2.63812442052932 RMSE: 1.6242304\n",
      "35 25 0.42731693387031555\n",
      "35 75 0.4476318657398224\n",
      "Validation loss: 2.604674388113476 RMSE: 1.6139003\n",
      "36 20 0.29120033979415894\n",
      "36 70 0.6003204584121704\n",
      "Validation loss: 2.8345447744641987 RMSE: 1.6836106\n",
      "37 15 0.5434817671775818\n",
      "37 65 0.5443452000617981\n",
      "Validation loss: 1.3967116866792952 RMSE: 1.1818256\n",
      "38 10 0.8951453566551208\n",
      "38 60 0.6861191987991333\n",
      "Validation loss: 2.15579986118135 RMSE: 1.4682642\n",
      "39 5 0.3426206409931183\n",
      "39 55 0.9649809002876282\n",
      "Validation loss: 2.1430282887958345 RMSE: 1.4639086\n",
      "40 0 0.44429776072502136\n",
      "40 50 0.40967437624931335\n",
      "40 100 0.41706982254981995\n",
      "Validation loss: 2.3153539793831963 RMSE: 1.5216287\n",
      "41 45 0.40742769837379456\n",
      "41 95 0.5732471942901611\n",
      "Validation loss: 2.5874914033072334 RMSE: 1.6085681\n",
      "42 40 0.3420458734035492\n",
      "42 90 0.3071953058242798\n",
      "Validation loss: 1.6394350653602963 RMSE: 1.2804042\n",
      "43 35 0.4078909754753113\n",
      "43 85 0.48843154311180115\n",
      "Validation loss: 1.9911072708311535 RMSE: 1.4110659\n",
      "44 30 0.5058605074882507\n",
      "44 80 0.5112900733947754\n",
      "Validation loss: 2.3313625335693358 RMSE: 1.5268799\n",
      "45 25 0.2653736174106598\n",
      "45 75 0.4096115231513977\n",
      "Validation loss: 1.2363872119358608 RMSE: 1.1119295\n",
      "46 20 0.35664328932762146\n",
      "46 70 0.38953763246536255\n",
      "Validation loss: 1.3879800103959583 RMSE: 1.1781256\n",
      "47 15 0.557304322719574\n",
      "47 65 0.2780478000640869\n",
      "Validation loss: 2.8903110390617734 RMSE: 1.7000915\n",
      "48 10 0.3945557475090027\n",
      "48 60 0.30909305810928345\n",
      "Validation loss: 1.1222946144285657 RMSE: 1.0593841\n",
      "49 5 0.6207468509674072\n",
      "49 55 0.3201623260974884\n",
      "Validation loss: 1.5829967135474796 RMSE: 1.258172\n",
      "50 0 0.8201839923858643\n",
      "50 50 0.46472781896591187\n",
      "50 100 0.23070423305034637\n",
      "Validation loss: 1.1977262020111084 RMSE: 1.0944068\n",
      "51 45 0.779940128326416\n",
      "51 95 0.6942011117935181\n",
      "Validation loss: 4.55790829431443 RMSE: 2.1349258\n",
      "52 40 0.35654547810554504\n",
      "52 90 0.3361632823944092\n",
      "Validation loss: 1.6153433527265275 RMSE: 1.2709615\n",
      "53 35 0.4689566195011139\n",
      "53 85 0.4191320836544037\n",
      "Validation loss: 1.8797292073567708 RMSE: 1.3710322\n",
      "54 30 0.2276814579963684\n",
      "54 80 0.2680928707122803\n",
      "Validation loss: 1.2057402605102177 RMSE: 1.098062\n",
      "55 25 0.19014158844947815\n",
      "55 75 0.5033488869667053\n",
      "Validation loss: 1.2442800612676712 RMSE: 1.115473\n",
      "56 20 0.3680473268032074\n",
      "56 70 0.4221731126308441\n",
      "Validation loss: 1.9930822099958148 RMSE: 1.4117657\n",
      "57 15 0.33682897686958313\n",
      "57 65 0.45081260800361633\n",
      "Validation loss: 1.2174207710084461 RMSE: 1.1033679\n",
      "58 10 0.47434189915657043\n",
      "58 60 0.3548153340816498\n",
      "Validation loss: 1.2556089037940616 RMSE: 1.1205395\n",
      "59 5 0.4867526888847351\n",
      "59 55 0.3911750912666321\n",
      "Validation loss: 1.3705514362880162 RMSE: 1.1707056\n",
      "60 0 0.30540475249290466\n",
      "60 50 0.5358557105064392\n",
      "60 100 0.3649788796901703\n",
      "Validation loss: 1.478003912880307 RMSE: 1.2157319\n",
      "61 45 0.2525556981563568\n",
      "61 95 0.5038607716560364\n",
      "Validation loss: 1.9863026323772612 RMSE: 1.4093626\n",
      "62 40 0.6927452683448792\n",
      "62 90 0.42184311151504517\n",
      "Validation loss: 1.4755275964736938 RMSE: 1.214713\n",
      "63 35 0.3658069968223572\n",
      "63 85 0.32851704955101013\n",
      "Validation loss: 1.2304811125709898 RMSE: 1.1092706\n",
      "64 30 0.5793275833129883\n",
      "64 80 0.30130869150161743\n",
      "Validation loss: 1.210471621013823 RMSE: 1.1002144\n",
      "65 25 0.2985294461250305\n",
      "65 75 0.1769508719444275\n",
      "Validation loss: 1.1504116989317394 RMSE: 1.0725725\n",
      "66 20 0.8590019941329956\n",
      "66 70 0.5131503343582153\n",
      "Validation loss: 1.6564853140286038 RMSE: 1.2870452\n",
      "67 15 0.4075163006782532\n",
      "67 65 0.27215829491615295\n",
      "Validation loss: 1.127026633350622 RMSE: 1.0616151\n",
      "68 10 0.4499626159667969\n",
      "68 60 0.3530416190624237\n",
      "Validation loss: 1.9642340137844994 RMSE: 1.4015113\n",
      "69 5 0.32110223174095154\n",
      "69 55 0.29772427678108215\n",
      "Validation loss: 2.0408871434983755 RMSE: 1.4285963\n",
      "70 0 0.342119425535202\n",
      "70 50 0.2485029399394989\n",
      "70 100 0.7178699374198914\n",
      "Validation loss: 2.620027596609933 RMSE: 1.61865\n",
      "71 45 0.2305402159690857\n",
      "71 95 0.32713091373443604\n",
      "Validation loss: 1.1663409853265398 RMSE: 1.0799727\n",
      "72 40 0.39938387274742126\n",
      "72 90 0.4003713130950928\n",
      "Validation loss: 1.2275828304744902 RMSE: 1.1079634\n",
      "73 35 0.42309796810150146\n",
      "73 85 0.4156644940376282\n",
      "Validation loss: 1.4222265220823742 RMSE: 1.1925714\n",
      "74 30 0.22351737320423126\n",
      "74 80 0.33820515871047974\n",
      "Validation loss: 2.479804942721412 RMSE: 1.5747397\n",
      "75 25 0.32514527440071106\n",
      "75 75 0.702448844909668\n",
      "Validation loss: 1.6019888696216402 RMSE: 1.265697\n",
      "76 20 0.2646087408065796\n",
      "76 70 0.3228922188282013\n",
      "Validation loss: 1.2097326573871432 RMSE: 1.0998784\n",
      "77 15 0.34667450189590454\n",
      "77 65 0.5400950312614441\n",
      "Validation loss: 1.3000197512762888 RMSE: 1.140184\n",
      "78 10 0.5673620700836182\n",
      "78 60 0.4185596704483032\n",
      "Validation loss: 2.0801435288928802 RMSE: 1.4422703\n",
      "79 5 0.4946790635585785\n",
      "79 55 0.2650032043457031\n",
      "Validation loss: 2.2924612979094188 RMSE: 1.5140877\n",
      "80 0 0.2800561487674713\n",
      "80 50 0.27271193265914917\n",
      "80 100 0.40415430068969727\n",
      "Validation loss: 2.4641511712755477 RMSE: 1.5697615\n",
      "81 45 0.2113608717918396\n",
      "81 95 0.29375723004341125\n",
      "Validation loss: 1.6393629709879558 RMSE: 1.2803762\n",
      "82 40 0.26250407099723816\n",
      "82 90 0.2463545799255371\n",
      "Validation loss: 1.9256387267793929 RMSE: 1.3876739\n",
      "83 35 0.31871497631073\n",
      "83 85 0.15383881330490112\n",
      "Validation loss: 1.7464839299519856 RMSE: 1.3215461\n",
      "84 30 0.32267290353775024\n",
      "84 80 0.323625385761261\n",
      "Validation loss: 1.845716599055699 RMSE: 1.3585715\n",
      "85 25 0.3384705185890198\n",
      "85 75 0.4050394296646118\n",
      "Validation loss: 1.382681978316534 RMSE: 1.175875\n",
      "86 20 0.2810496985912323\n",
      "86 70 0.19842760264873505\n",
      "Validation loss: 1.2524836313156855 RMSE: 1.1191442\n",
      "87 15 0.26871344447135925\n",
      "87 65 0.23840796947479248\n",
      "Validation loss: 1.5830163149606613 RMSE: 1.2581798\n",
      "88 10 0.202255517244339\n",
      "88 60 0.2369333952665329\n",
      "Validation loss: 1.8317808412370227 RMSE: 1.3534329\n",
      "89 5 0.29819515347480774\n",
      "89 55 0.32088223099708557\n",
      "Validation loss: 2.144712976046971 RMSE: 1.4644839\n",
      "90 0 0.43752479553222656\n",
      "90 50 0.4225388765335083\n",
      "90 100 0.4571601152420044\n",
      "Validation loss: 1.7643476508912586 RMSE: 1.3282875\n",
      "91 45 0.34017041325569153\n",
      "91 95 0.2942178547382355\n",
      "Validation loss: 1.925285695848011 RMSE: 1.3875467\n",
      "92 40 0.3443569839000702\n",
      "92 90 0.2285638302564621\n",
      "Validation loss: 2.766193607875279 RMSE: 1.6631879\n",
      "93 35 0.2589184045791626\n",
      "93 85 0.26692602038383484\n",
      "Validation loss: 1.6093519676299322 RMSE: 1.2686024\n",
      "94 30 0.35210511088371277\n",
      "94 80 0.5186865925788879\n",
      "Validation loss: 2.394164382843744 RMSE: 1.5473088\n",
      "95 25 0.16980421543121338\n",
      "95 75 0.2589563727378845\n",
      "Validation loss: 1.6859332675025576 RMSE: 1.2984349\n",
      "96 20 0.5051889419555664\n",
      "96 70 0.24049755930900574\n",
      "Validation loss: 1.452668629373823 RMSE: 1.2052671\n",
      "97 15 0.26342397928237915\n",
      "97 65 0.2201225310564041\n",
      "Validation loss: 1.9072337195986793 RMSE: 1.3810263\n",
      "98 10 0.4555269777774811\n",
      "98 60 0.20319148898124695\n",
      "Validation loss: 1.561693228994097 RMSE: 1.2496772\n",
      "99 5 0.5291463732719421\n",
      "99 55 0.4317707419395447\n",
      "Validation loss: 1.210451298668271 RMSE: 1.1002051\n",
      "100 0 0.25483229756355286\n",
      "100 50 0.3379615247249603\n",
      "100 100 0.2543458938598633\n",
      "Validation loss: 1.4279574292046684 RMSE: 1.1949717\n",
      "101 45 0.38798391819000244\n",
      "101 95 0.30706506967544556\n",
      "Validation loss: 1.551972040108272 RMSE: 1.2457817\n",
      "102 40 0.40312668681144714\n",
      "102 90 0.31393155455589294\n",
      "Validation loss: 1.4584005878085182 RMSE: 1.2076427\n",
      "103 35 0.22283457219600677\n",
      "103 85 0.28058600425720215\n",
      "Validation loss: 1.2271390404020037 RMSE: 1.1077632\n",
      "104 30 0.1270993947982788\n",
      "104 80 0.3888976275920868\n",
      "Validation loss: 1.2978825989223661 RMSE: 1.1392466\n",
      "105 25 0.2017805427312851\n",
      "105 75 0.20109866559505463\n",
      "Validation loss: 1.2920155150549752 RMSE: 1.1366686\n",
      "106 20 0.34732314944267273\n",
      "106 70 0.2609196901321411\n",
      "Validation loss: 1.3948056652432397 RMSE: 1.181019\n",
      "107 15 0.39631274342536926\n",
      "107 65 0.3021750748157501\n",
      "Validation loss: 1.290280757063911 RMSE: 1.1359053\n",
      "108 10 0.1371244192123413\n",
      "108 60 0.3099214732646942\n",
      "Validation loss: 1.2031023559116183 RMSE: 1.0968602\n",
      "109 5 0.4308595657348633\n",
      "109 55 0.28190359473228455\n",
      "Validation loss: 1.2061871483212425 RMSE: 1.0982655\n",
      "110 0 0.3904215395450592\n",
      "110 50 0.3033067286014557\n",
      "110 100 0.2954629361629486\n",
      "Validation loss: 1.2281818668047586 RMSE: 1.1082337\n",
      "111 45 0.1998860239982605\n",
      "111 95 0.25572746992111206\n",
      "Validation loss: 1.4761160237448556 RMSE: 1.2149551\n",
      "112 40 0.2127443253993988\n",
      "112 90 0.15076349675655365\n",
      "Validation loss: 1.3982484545026506 RMSE: 1.1824756\n",
      "113 35 0.30036765336990356\n",
      "113 85 0.27161547541618347\n",
      "Validation loss: 1.3359469073159354 RMSE: 1.1558317\n",
      "114 30 0.2584356963634491\n",
      "114 80 0.38181817531585693\n",
      "Validation loss: 1.221683322815668 RMSE: 1.1052978\n",
      "115 25 0.17050589621067047\n",
      "115 75 0.3066369295120239\n",
      "Validation loss: 1.8798573902675084 RMSE: 1.371079\n",
      "116 20 0.3802720308303833\n",
      "116 70 0.4925239086151123\n",
      "Validation loss: 1.4678562255132765 RMSE: 1.2115512\n",
      "117 15 0.2523195743560791\n",
      "117 65 0.2227669060230255\n",
      "Validation loss: 1.2449077129364015 RMSE: 1.1157542\n",
      "118 10 0.2684933841228485\n",
      "118 60 0.21853801608085632\n",
      "Validation loss: 1.1959328719547817 RMSE: 1.0935872\n",
      "119 5 0.14451496303081512\n",
      "119 55 0.1953774243593216\n",
      "Validation loss: 1.313984677905128 RMSE: 1.1462917\n",
      "120 0 0.10146994143724442\n",
      "120 50 0.307759553194046\n",
      "120 100 0.2886635363101959\n",
      "Validation loss: 1.2060115450904483 RMSE: 1.0981855\n",
      "121 45 0.19940640032291412\n",
      "121 95 0.25097373127937317\n",
      "Validation loss: 1.5796846912020728 RMSE: 1.256855\n",
      "122 40 0.31258803606033325\n",
      "122 90 0.21898698806762695\n",
      "Validation loss: 1.33274500086194 RMSE: 1.1544458\n",
      "123 35 0.1858653426170349\n",
      "123 85 0.16056737303733826\n",
      "Validation loss: 1.778949997538612 RMSE: 1.3337728\n",
      "124 30 0.31068605184555054\n",
      "124 80 0.5090997219085693\n",
      "Validation loss: 1.2792201842580522 RMSE: 1.1310261\n",
      "125 25 0.15662561357021332\n",
      "125 75 0.252336710691452\n",
      "Validation loss: 1.4226286984625316 RMSE: 1.19274\n",
      "126 20 0.21269062161445618\n",
      "126 70 0.211775004863739\n",
      "Validation loss: 1.2826773620787122 RMSE: 1.1325535\n",
      "127 15 0.2302086055278778\n",
      "127 65 0.28504306077957153\n",
      "Validation loss: 1.2422031504767281 RMSE: 1.1145418\n",
      "128 10 0.31896722316741943\n",
      "128 60 0.15010598301887512\n",
      "Validation loss: 1.3965021303721836 RMSE: 1.181737\n",
      "129 5 0.18860545754432678\n",
      "129 55 0.3543299436569214\n",
      "Validation loss: 1.2806971748669942 RMSE: 1.1316789\n",
      "130 0 0.24115729331970215\n",
      "130 50 0.3262183368206024\n",
      "130 100 0.32322958111763\n",
      "Validation loss: 1.2351229463304791 RMSE: 1.1113608\n",
      "131 45 0.39546382427215576\n",
      "131 95 0.2693380117416382\n",
      "Validation loss: 1.4311736616350355 RMSE: 1.1963167\n",
      "132 40 0.3074268698692322\n",
      "132 90 0.23555471003055573\n",
      "Validation loss: 1.5516194479806082 RMSE: 1.2456402\n",
      "133 35 0.2549819350242615\n",
      "133 85 0.25078216195106506\n",
      "Validation loss: 1.4261619916984014 RMSE: 1.1942203\n",
      "134 30 0.1600365936756134\n",
      "134 80 0.18331679701805115\n",
      "Validation loss: 1.4883303528740293 RMSE: 1.2199714\n",
      "135 25 0.2180982530117035\n",
      "135 75 0.3417022228240967\n",
      "Validation loss: 1.2265376647313435 RMSE: 1.1074917\n",
      "136 20 0.19489161670207977\n",
      "136 70 0.2654765844345093\n",
      "Validation loss: 1.4470604510534377 RMSE: 1.2029383\n",
      "137 15 0.19667865335941315\n",
      "137 65 0.3292151093482971\n",
      "Validation loss: 1.2221283765066238 RMSE: 1.1054991\n",
      "138 10 0.2116200476884842\n",
      "138 60 0.24535267055034637\n",
      "Validation loss: 1.2900656552541823 RMSE: 1.1358106\n",
      "139 5 0.12111490964889526\n",
      "139 55 0.29677051305770874\n",
      "Validation loss: 1.2201934451148624 RMSE: 1.1046237\n",
      "140 0 0.16690827906131744\n",
      "140 50 0.2697853147983551\n",
      "140 100 0.14183583855628967\n",
      "Validation loss: 1.1824873288472493 RMSE: 1.0874223\n",
      "141 45 0.4137905240058899\n",
      "141 95 0.2170691192150116\n",
      "Validation loss: 1.244691482612065 RMSE: 1.1156574\n",
      "142 40 0.2108863741159439\n",
      "142 90 0.16637058556079865\n",
      "Validation loss: 1.158717399551755 RMSE: 1.0764374\n",
      "143 35 0.21074844896793365\n",
      "143 85 0.17615969479084015\n",
      "Validation loss: 1.2188216198058355 RMSE: 1.1040026\n",
      "144 30 0.26910409331321716\n",
      "144 80 0.14486928284168243\n",
      "Validation loss: 1.5083768651598977 RMSE: 1.22816\n",
      "145 25 0.24757885932922363\n",
      "145 75 0.19558504223823547\n",
      "Validation loss: 1.2663898808615548 RMSE: 1.1253399\n",
      "146 20 0.26794883608818054\n",
      "146 70 0.24135121703147888\n",
      "Validation loss: 1.5294785647165208 RMSE: 1.2367209\n",
      "147 15 0.15647627413272858\n",
      "147 65 0.23780764639377594\n",
      "Validation loss: 1.2801697691281637 RMSE: 1.1314459\n",
      "148 10 0.24152106046676636\n",
      "148 60 0.23435047268867493\n",
      "Validation loss: 1.487701822462536 RMSE: 1.2197138\n",
      "149 5 0.15160416066646576\n",
      "149 55 0.3105419874191284\n",
      "Validation loss: 1.2869155157180059 RMSE: 1.134423\n",
      "150 0 0.22708669304847717\n",
      "150 50 0.2554929852485657\n",
      "150 100 0.39445555210113525\n",
      "Validation loss: 2.102308482215518 RMSE: 1.449934\n",
      "151 45 0.2191218137741089\n",
      "151 95 0.2105095088481903\n",
      "Validation loss: 1.4363025733402797 RMSE: 1.1984584\n",
      "152 40 0.22264285385608673\n",
      "152 90 0.27811193466186523\n",
      "Validation loss: 1.2304181598481678 RMSE: 1.1092421\n",
      "153 35 0.14061041176319122\n",
      "153 85 0.26167306303977966\n",
      "Validation loss: 1.3888123126257033 RMSE: 1.1784788\n",
      "154 30 0.3412407636642456\n",
      "154 80 0.17189565300941467\n",
      "Validation loss: 1.5834196902456739 RMSE: 1.2583401\n",
      "155 25 0.2118152529001236\n",
      "155 75 0.2771909236907959\n",
      "Validation loss: 1.4740394149507796 RMSE: 1.2141002\n",
      "156 20 0.18816877901554108\n",
      "156 70 0.18696117401123047\n",
      "Validation loss: 1.2308807327633813 RMSE: 1.1094507\n",
      "157 15 0.1945594698190689\n",
      "157 65 0.2540554404258728\n",
      "Validation loss: 1.2540505931490944 RMSE: 1.119844\n",
      "158 10 0.23420773446559906\n",
      "158 60 0.3355346620082855\n",
      "Validation loss: 1.2333107675824846 RMSE: 1.1105453\n",
      "159 5 0.21495331823825836\n",
      "159 55 0.13170528411865234\n",
      "Validation loss: 1.2445438759667533 RMSE: 1.1155913\n",
      "160 0 0.1707041710615158\n",
      "160 50 0.3843476176261902\n",
      "160 100 0.1926233172416687\n",
      "Validation loss: 1.281333505539667 RMSE: 1.13196\n",
      "161 45 0.15481409430503845\n",
      "161 95 0.16488111019134521\n",
      "Validation loss: 1.6254053433736166 RMSE: 1.2749138\n",
      "162 40 0.20759718120098114\n",
      "162 90 0.17085474729537964\n",
      "Validation loss: 1.3319527762276786 RMSE: 1.1541026\n",
      "163 35 0.3283293843269348\n",
      "163 85 0.2236211597919464\n",
      "Validation loss: 1.6992003963107154 RMSE: 1.3035338\n",
      "164 30 0.20330119132995605\n",
      "164 80 0.24779941141605377\n",
      "Validation loss: 1.283293322722117 RMSE: 1.1328254\n",
      "165 25 0.1692034900188446\n",
      "165 75 0.2596134543418884\n",
      "Validation loss: 1.2838501901853652 RMSE: 1.1330711\n",
      "166 20 0.13890425860881805\n",
      "166 70 0.14437748491764069\n",
      "Validation loss: 1.3936671960921514 RMSE: 1.1805369\n",
      "167 15 0.18438777327537537\n",
      "167 65 0.18651223182678223\n",
      "Validation loss: 1.3532018627439226 RMSE: 1.163272\n",
      "168 10 0.1700633019208908\n",
      "168 60 0.17315556108951569\n",
      "Validation loss: 1.4189628237769718 RMSE: 1.1912023\n",
      "169 5 0.20697450637817383\n",
      "169 55 0.153150275349617\n",
      "Validation loss: 1.3970001334235782 RMSE: 1.1819475\n",
      "170 0 0.15757232904434204\n",
      "170 50 0.23168928921222687\n",
      "170 100 0.12229655683040619\n",
      "Validation loss: 1.7543985275995164 RMSE: 1.324537\n",
      "171 45 0.15189751982688904\n",
      "171 95 0.21720613539218903\n",
      "Validation loss: 1.3762366204034715 RMSE: 1.1731311\n",
      "172 40 0.2150575816631317\n",
      "172 90 0.1871851086616516\n",
      "Validation loss: 1.3134546041488648 RMSE: 1.1460605\n",
      "173 35 0.15338927507400513\n",
      "173 85 0.21183694899082184\n",
      "Validation loss: 1.5389720525060382 RMSE: 1.2405531\n",
      "174 30 0.2325361967086792\n",
      "174 80 0.33967041969299316\n",
      "Validation loss: 1.2182375862484887 RMSE: 1.1037381\n",
      "175 25 0.2259463667869568\n",
      "175 75 0.5490257143974304\n",
      "Validation loss: 1.2835939952305384 RMSE: 1.132958\n",
      "176 20 0.1817680299282074\n",
      "176 70 0.18770597875118256\n",
      "Validation loss: 1.3128752254304432 RMSE: 1.1458077\n",
      "177 15 0.14434558153152466\n",
      "177 65 0.21420300006866455\n",
      "Validation loss: 1.359668055034819 RMSE: 1.166048\n",
      "178 10 0.14773574471473694\n",
      "178 60 0.11003416776657104\n",
      "Validation loss: 1.463975511278425 RMSE: 1.2099485\n",
      "179 5 0.26116421818733215\n",
      "179 55 0.1478734165430069\n",
      "Validation loss: 1.3399733293624152 RMSE: 1.1575722\n",
      "180 0 0.22298239171504974\n",
      "180 50 0.23248060047626495\n",
      "180 100 0.21538016200065613\n",
      "Validation loss: 1.377356937953404 RMSE: 1.1736085\n",
      "181 45 0.1792692095041275\n",
      "181 95 0.2650883197784424\n",
      "Validation loss: 1.2746952329363141 RMSE: 1.129024\n",
      "182 40 0.17978064715862274\n",
      "182 90 0.15991660952568054\n",
      "Validation loss: 1.293204119091942 RMSE: 1.1371914\n",
      "183 35 0.16993677616119385\n",
      "183 85 0.20248542726039886\n",
      "Validation loss: 1.3803365585349856 RMSE: 1.1748773\n",
      "184 30 0.20226845145225525\n",
      "184 80 0.11729379743337631\n",
      "Validation loss: 1.3543060209069933 RMSE: 1.1637465\n",
      "185 25 0.1767687201499939\n",
      "185 75 0.1818646490573883\n",
      "Validation loss: 1.3228194384347824 RMSE: 1.150139\n",
      "186 20 0.17612284421920776\n",
      "186 70 0.13817918300628662\n",
      "Validation loss: 1.2837827647016162 RMSE: 1.1330414\n",
      "187 15 0.2551453709602356\n",
      "187 65 0.21289777755737305\n",
      "Validation loss: 1.3017857330186027 RMSE: 1.1409582\n",
      "188 10 0.20810672640800476\n",
      "188 60 0.2069464921951294\n",
      "Validation loss: 1.335516881942749 RMSE: 1.1556457\n",
      "189 5 0.12579844892024994\n",
      "189 55 0.17114682495594025\n",
      "Validation loss: 1.3373126915522984 RMSE: 1.1564224\n",
      "190 0 0.2149873524904251\n",
      "190 50 0.2815474271774292\n",
      "190 100 0.33948493003845215\n",
      "Validation loss: 1.323580203169868 RMSE: 1.1504695\n",
      "191 45 0.15889808535575867\n",
      "191 95 0.2604518532752991\n",
      "Validation loss: 1.2936248336519514 RMSE: 1.1373763\n",
      "192 40 0.16813009977340698\n",
      "192 90 0.1943453848361969\n",
      "Validation loss: 1.2105935445853642 RMSE: 1.1002698\n",
      "193 35 0.20887595415115356\n",
      "193 85 0.10363065451383591\n",
      "Validation loss: 1.3354868383634657 RMSE: 1.1556327\n",
      "194 30 0.18314313888549805\n",
      "194 80 0.14142917096614838\n",
      "Validation loss: 1.2635707781428382 RMSE: 1.1240866\n",
      "195 25 0.1832873523235321\n",
      "195 75 0.11309516429901123\n",
      "Validation loss: 1.241947136606489 RMSE: 1.1144269\n",
      "196 20 0.2601635158061981\n",
      "196 70 0.3123842775821686\n",
      "Validation loss: 1.2379639687992277 RMSE: 1.1126382\n",
      "197 15 0.2215619534254074\n",
      "197 65 0.14374160766601562\n",
      "Validation loss: 1.330435721505256 RMSE: 1.1534451\n",
      "198 10 0.2225276678800583\n",
      "198 60 0.0736684650182724\n",
      "Validation loss: 1.309082139106024 RMSE: 1.1441513\n",
      "199 5 0.13513149321079254\n",
      "199 55 0.09681829810142517\n",
      "Validation loss: 1.246013255346389 RMSE: 1.1162497\n",
      "Loaded trained model with success.\n",
      "Test loss: 1.0096567437762305 Test RMSE: 1.0048168\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:1', 'task_name': 'lipo', 'model_type': 'gin', 'model': {'num_layer': 5, 'emb_dim': 300, 'feat_dim': 300, 'drop_ratio': 0.3, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 778, 'task': 'regression', 'data_path': 'data/lipophilicity/Lipophilicity.csv', 'target': ['exp']}}\n",
      "Running on: cuda:1\n",
      "4199\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/4199\n",
      "Generating scaffold 1000/4199\n",
      "Generating scaffold 2000/4199\n",
      "Generating scaffold 3000/4199\n",
      "Generating scaffold 4000/4199\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 7.733251094818115\n",
      "0 50 1.9957005977630615\n",
      "0 100 1.3693028688430786\n",
      "Validation loss: 1.9869682993207658 RMSE: 1.4095987\n",
      "1 45 0.8646578788757324\n",
      "1 95 0.8060728907585144\n",
      "Validation loss: 7.0682445253644675 RMSE: 2.658617\n",
      "2 40 0.9280081987380981\n",
      "2 90 0.8979592323303223\n",
      "Validation loss: 5.923870318276542 RMSE: 2.4339004\n",
      "3 35 1.1106665134429932\n",
      "3 85 1.2499945163726807\n",
      "Validation loss: 4.687315341404506 RMSE: 2.165021\n",
      "4 30 0.8029743432998657\n",
      "4 80 1.2126433849334717\n",
      "Validation loss: 6.751528403872535 RMSE: 2.5983703\n",
      "5 25 0.7540413737297058\n",
      "5 75 0.6863986253738403\n",
      "Validation loss: 5.186337834312802 RMSE: 2.2773533\n",
      "6 20 0.7239918112754822\n",
      "6 70 0.5513954162597656\n",
      "Validation loss: 4.323060294560023 RMSE: 2.079197\n",
      "7 15 0.7367071509361267\n",
      "7 65 0.8861978650093079\n",
      "Validation loss: 7.370042964390346 RMSE: 2.7147822\n",
      "8 10 0.8875871300697327\n",
      "8 60 0.6475743055343628\n",
      "Validation loss: 4.732820845785595 RMSE: 2.1755047\n",
      "9 5 1.2918264865875244\n",
      "9 55 0.45671898126602173\n",
      "Validation loss: 4.272115284488315 RMSE: 2.0669096\n",
      "10 0 1.1401731967926025\n",
      "10 50 0.6708844900131226\n",
      "10 100 0.5148539543151855\n",
      "Validation loss: 7.073980467660086 RMSE: 2.6596956\n"
     ]
    }
   ],
   "source": [
    "datasets = ['FreeSolv', 'ESOL', 'Lipo', 'qm7', \"bace\",  \"bbbp\",  'tox21', 'clintox', 'sider',]\n",
    "seeds = [777, 778, 779, 780, 781]\n",
    "\n",
    "for dataset in datasets:\n",
    "    for seed in seeds:\n",
    "            if dataset == 'FreeSolv':\n",
    "            # FreeSolv 데이터셋에 대한 특정 옵션을 적용\n",
    "                !python finetuneRecon1.py \\\n",
    "                --task_name {dataset} \\\n",
    "                --seed {seed} \\\n",
    "                --dropout 0.5 \\\n",
    "                --num_layer 3 \\\n",
    "                --emb_dim 64 \\\n",
    "                --feat_dim 64 \\\n",
    "                --alpha 0.1 \\\n",
    "                --gpu cuda:1 \\\n",
    "                \n",
    "            else:\n",
    "                !python finetuneRecon1.py \\\n",
    "                --task_name {dataset} \\\n",
    "                --seed {seed} \\\n",
    "                --alpha 0.1 \\\n",
    "                --gpu cuda:1 \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2a46ae-a6b6-4a66-befc-c4e38c83fdb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:0', 'task_name': 'freesolv', 'model_type': 'gin', 'model': {'num_layer': 5, 'emb_dim': 300, 'feat_dim': 300, 'drop_ratio': 0.3, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'random', 'seed': 777, 'task': 'regression', 'data_path': 'data/freesolv/freesolv.csv', 'target': ['expt']}}\n",
      "Running on: cuda:0\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 14.321107864379883\n",
      "Validation loss: 20.91863441467285 RMSE: 4.5736895\n",
      "Validation loss: 17.256500959396362 RMSE: 4.154094\n",
      "2 16 1.6740686893463135\n",
      "Validation loss: 9.5014009475708 RMSE: 3.0824342\n",
      "Validation loss: 18.261054039001465 RMSE: 4.2732954\n",
      "Validation loss: 4.354099988937378 RMSE: 2.0866482\n",
      "5 15 2.0978336334228516\n",
      "Validation loss: 2.7761409282684326 RMSE: 1.6661755\n",
      "Validation loss: 6.134705543518066 RMSE: 2.4768338\n",
      "Validation loss: 6.321134567260742 RMSE: 2.5141869\n",
      "8 14 3.4172286987304688\n",
      "Validation loss: 5.5209267139434814 RMSE: 2.3496654\n",
      "Validation loss: 13.93534231185913 RMSE: 3.7330072\n",
      "Validation loss: 5.228390216827393 RMSE: 2.2865674\n",
      "11 13 3.121549129486084\n",
      "Validation loss: 7.373233079910278 RMSE: 2.7153695\n",
      "Validation loss: 4.980787515640259 RMSE: 2.231768\n",
      "Validation loss: 4.149728178977966 RMSE: 2.0370882\n",
      "14 12 3.0783309936523438\n",
      "Validation loss: 5.393917560577393 RMSE: 2.322481\n",
      "Validation loss: 9.424787402153015 RMSE: 3.069982\n",
      "Validation loss: 3.304729461669922 RMSE: 1.8178914\n",
      "17 11 2.1237094402313232\n",
      "Validation loss: 13.586473822593689 RMSE: 3.6859837\n",
      "Validation loss: 2.705667734146118 RMSE: 1.6448911\n",
      "Validation loss: 4.860611200332642 RMSE: 2.204679\n",
      "20 10 2.180887460708618\n",
      "Validation loss: 9.186617851257324 RMSE: 3.0309432\n",
      "Validation loss: 6.59820419549942 RMSE: 2.5686972\n",
      "Validation loss: 5.45841646194458 RMSE: 2.3363254\n",
      "23 9 2.2084107398986816\n",
      "Validation loss: 10.025069236755371 RMSE: 3.1662388\n",
      "Validation loss: 2.3286338448524475 RMSE: 1.5259862\n",
      "Validation loss: 8.440638542175293 RMSE: 2.9052775\n",
      "26 8 1.3372224569320679\n",
      "Validation loss: 6.463868141174316 RMSE: 2.5424137\n",
      "Validation loss: 12.901103496551514 RMSE: 3.5918105\n",
      "Validation loss: 12.712461471557617 RMSE: 3.565454\n",
      "29 7 3.387467861175537\n",
      "Validation loss: 1.6997678875923157 RMSE: 1.3037512\n",
      "Validation loss: 8.209753513336182 RMSE: 2.865267\n",
      "Validation loss: 3.7261006832122803 RMSE: 1.9303108\n",
      "32 6 1.3978638648986816\n",
      "Validation loss: 3.634792923927307 RMSE: 1.9065133\n",
      "Validation loss: 23.70071315765381 RMSE: 4.8683376\n",
      "Validation loss: 2.1626139879226685 RMSE: 1.4705827\n",
      "35 5 2.1506173610687256\n",
      "Validation loss: 7.431595802307129 RMSE: 2.7260954\n",
      "Validation loss: 6.668334126472473 RMSE: 2.5823119\n",
      "Validation loss: 2.303868055343628 RMSE: 1.5178498\n",
      "38 4 5.720208168029785\n",
      "Validation loss: 10.526123046875 RMSE: 3.2443988\n",
      "Validation loss: 4.498493194580078 RMSE: 2.1209652\n",
      "Validation loss: 4.741388440132141 RMSE: 2.1774728\n",
      "41 3 1.1128249168395996\n",
      "Validation loss: 15.646967649459839 RMSE: 3.9556246\n",
      "Validation loss: 1.95924711227417 RMSE: 1.3997312\n",
      "Validation loss: 3.023181438446045 RMSE: 1.7387301\n",
      "44 2 1.488884449005127\n",
      "Validation loss: 8.31072986125946 RMSE: 2.8828337\n",
      "Validation loss: 3.8074792623519897 RMSE: 1.9512762\n",
      "Validation loss: 15.148390293121338 RMSE: 3.8920937\n",
      "47 1 1.3609793186187744\n",
      "Validation loss: 11.078186750411987 RMSE: 3.3283908\n",
      "Validation loss: 2.7154356241226196 RMSE: 1.6478577\n",
      "Validation loss: 9.06560730934143 RMSE: 3.0109143\n",
      "50 0 1.7610065937042236\n",
      "Validation loss: 5.494647741317749 RMSE: 2.3440666\n",
      "Validation loss: 4.9150495529174805 RMSE: 2.216991\n",
      "52 16 4.530332565307617\n",
      "Validation loss: 2.4536731243133545 RMSE: 1.5664204\n",
      "Validation loss: 7.778294801712036 RMSE: 2.7889593\n",
      "Validation loss: 11.89563512802124 RMSE: 3.4490044\n",
      "55 15 1.5943224430084229\n",
      "Validation loss: 8.33542251586914 RMSE: 2.887113\n",
      "Validation loss: 4.603169918060303 RMSE: 2.1454997\n",
      "Validation loss: 8.7169189453125 RMSE: 2.9524424\n",
      "58 14 6.113883972167969\n",
      "Validation loss: 3.524573802947998 RMSE: 1.8773848\n",
      "Validation loss: 3.1536667346954346 RMSE: 1.7758563\n",
      "Validation loss: 2.669476866722107 RMSE: 1.6338534\n",
      "61 13 3.593710422515869\n",
      "Validation loss: 2.729556918144226 RMSE: 1.6521369\n",
      "Validation loss: 1.4966734647750854 RMSE: 1.223386\n",
      "Validation loss: 10.484611511230469 RMSE: 3.2379944\n",
      "64 12 1.4421100616455078\n",
      "Validation loss: 15.941698551177979 RMSE: 3.9927056\n",
      "Validation loss: 2.03757643699646 RMSE: 1.4274371\n",
      "Validation loss: 4.891714930534363 RMSE: 2.2117226\n",
      "67 11 0.8361790180206299\n",
      "Validation loss: 1.9478195905685425 RMSE: 1.3956432\n",
      "Validation loss: 4.604289650917053 RMSE: 2.1457608\n",
      "Validation loss: 6.783350348472595 RMSE: 2.6044862\n",
      "70 10 2.0118415355682373\n",
      "Validation loss: 5.839918375015259 RMSE: 2.4165924\n",
      "Validation loss: 8.688260555267334 RMSE: 2.9475856\n",
      "Validation loss: 2.0918793082237244 RMSE: 1.446333\n",
      "73 9 0.996292233467102\n",
      "Validation loss: 2.770226240158081 RMSE: 1.6643996\n",
      "Validation loss: 3.364711880683899 RMSE: 1.8343152\n",
      "Validation loss: 2.3308213353157043 RMSE: 1.5267028\n",
      "76 8 1.6383395195007324\n",
      "Validation loss: 2.394361436367035 RMSE: 1.5473725\n",
      "Validation loss: 1.2451589703559875 RMSE: 1.1158668\n",
      "Validation loss: 9.45081090927124 RMSE: 3.0742173\n",
      "79 7 2.138673782348633\n",
      "Validation loss: 6.314155101776123 RMSE: 2.5127983\n",
      "Validation loss: 11.514304161071777 RMSE: 3.3932729\n",
      "Validation loss: 1.5443845093250275 RMSE: 1.2427326\n",
      "82 6 1.8259228467941284\n",
      "Validation loss: 2.806889295578003 RMSE: 1.6753772\n",
      "Validation loss: 2.180079460144043 RMSE: 1.4765092\n",
      "Validation loss: 13.70202922821045 RMSE: 3.7016258\n",
      "85 5 1.7427281141281128\n",
      "Validation loss: 2.6307966709136963 RMSE: 1.6219732\n",
      "Validation loss: 5.62000185251236 RMSE: 2.370654\n",
      "Validation loss: 2.6636844873428345 RMSE: 1.6320797\n",
      "88 4 0.8470762968063354\n",
      "Validation loss: 1.5560477375984192 RMSE: 1.2474165\n",
      "Validation loss: 6.000298738479614 RMSE: 2.4495509\n",
      "Validation loss: 3.4865431785583496 RMSE: 1.867229\n",
      "91 3 0.5117723345756531\n",
      "Validation loss: 1.3801007866859436 RMSE: 1.1747768\n",
      "Validation loss: 3.1592814922332764 RMSE: 1.7774367\n",
      "Validation loss: 2.77260684967041 RMSE: 1.6651146\n",
      "94 2 2.832580327987671\n",
      "Validation loss: 2.2825942039489746 RMSE: 1.5108255\n",
      "Validation loss: 1.2492355108261108 RMSE: 1.117692\n",
      "Validation loss: 4.164891719818115 RMSE: 2.0408063\n",
      "97 1 2.0177931785583496\n",
      "Validation loss: 3.6904574632644653 RMSE: 1.9210564\n",
      "Validation loss: 5.913467675447464 RMSE: 2.4317622\n",
      "Validation loss: 1.072115421295166 RMSE: 1.0354302\n",
      "100 0 1.43095862865448\n",
      "Validation loss: 2.359165281057358 RMSE: 1.5359572\n",
      "Validation loss: 3.742803692817688 RMSE: 1.9346328\n",
      "102 16 0.6668446660041809\n",
      "Validation loss: 2.897221565246582 RMSE: 1.7021226\n",
      "Validation loss: 1.9340905249118805 RMSE: 1.3907158\n",
      "Validation loss: 3.806449294090271 RMSE: 1.9510124\n",
      "105 15 2.747955322265625\n",
      "Validation loss: 2.2267810106277466 RMSE: 1.4922402\n",
      "Validation loss: 2.8945993185043335 RMSE: 1.7013521\n",
      "Validation loss: 2.2886324524879456 RMSE: 1.5128231\n",
      "108 14 1.856547236442566\n",
      "Validation loss: 5.9955174922943115 RMSE: 2.4485748\n",
      "Validation loss: 1.249184787273407 RMSE: 1.1176693\n",
      "Validation loss: 3.0336804389953613 RMSE: 1.7417461\n",
      "111 13 0.8801333904266357\n",
      "Validation loss: 1.4764221906661987 RMSE: 1.2150811\n",
      "Validation loss: 6.9158570766448975 RMSE: 2.6298018\n",
      "Validation loss: 1.338023066520691 RMSE: 1.1567296\n",
      "114 12 1.0879697799682617\n",
      "Validation loss: 2.7781455516815186 RMSE: 1.6667773\n",
      "Validation loss: 4.137959718704224 RMSE: 2.0341976\n",
      "Validation loss: 2.805605858564377 RMSE: 1.6749942\n",
      "117 11 1.7006199359893799\n",
      "Validation loss: 2.56577730178833 RMSE: 1.6018044\n",
      "Validation loss: 1.2324445247650146 RMSE: 1.1101552\n",
      "Validation loss: 1.9795788526535034 RMSE: 1.4069748\n",
      "120 10 1.2716553211212158\n",
      "Validation loss: 1.340436190366745 RMSE: 1.157772\n",
      "Validation loss: 1.4255233108997345 RMSE: 1.1939527\n",
      "Validation loss: 6.794361591339111 RMSE: 2.6065993\n",
      "123 9 1.2550132274627686\n",
      "Validation loss: 1.4356001019477844 RMSE: 1.1981653\n",
      "Validation loss: 5.750680923461914 RMSE: 2.3980577\n",
      "Validation loss: 1.3114582300186157 RMSE: 1.145189\n",
      "126 8 1.7500109672546387\n",
      "Validation loss: 4.8761069774627686 RMSE: 2.208191\n",
      "Validation loss: 1.45756334066391 RMSE: 1.2072957\n",
      "Validation loss: 1.1161895394325256 RMSE: 1.0564988\n",
      "129 7 0.6116553544998169\n",
      "Validation loss: 1.739623486995697 RMSE: 1.3189479\n",
      "Validation loss: 3.356816291809082 RMSE: 1.8321614\n",
      "Validation loss: 3.7187633514404297 RMSE: 1.9284096\n",
      "132 6 1.253115177154541\n",
      "Validation loss: 1.6520788371562958 RMSE: 1.2853323\n",
      "Validation loss: 1.0393221378326416 RMSE: 1.0194714\n",
      "Validation loss: 2.311925768852234 RMSE: 1.5205017\n",
      "135 5 0.5788388848304749\n",
      "Validation loss: 2.79392671585083 RMSE: 1.6715044\n",
      "Validation loss: 2.091659724712372 RMSE: 1.4462571\n",
      "Validation loss: 1.4883313179016113 RMSE: 1.2199719\n",
      "138 4 1.0223125219345093\n",
      "Validation loss: 2.0054184198379517 RMSE: 1.4161279\n",
      "Validation loss: 1.3556776642799377 RMSE: 1.1643357\n",
      "Validation loss: 1.5639651715755463 RMSE: 1.2505859\n",
      "141 3 1.521355152130127\n",
      "Validation loss: 0.9317711591720581 RMSE: 0.9652828\n",
      "Validation loss: 1.3833388686180115 RMSE: 1.1761543\n",
      "Validation loss: 1.6674079895019531 RMSE: 1.2912816\n",
      "144 2 1.359778642654419\n",
      "Validation loss: 6.275326251983643 RMSE: 2.5050602\n",
      "Validation loss: 2.056834638118744 RMSE: 1.4341669\n",
      "Validation loss: 1.3124879002571106 RMSE: 1.1456387\n",
      "147 1 1.4902451038360596\n",
      "Validation loss: 1.7479283809661865 RMSE: 1.3220924\n",
      "Validation loss: 3.0501595735549927 RMSE: 1.7464706\n",
      "Validation loss: 2.12426096200943 RMSE: 1.4574845\n",
      "150 0 0.7807320356369019\n",
      "Validation loss: 1.9108963012695312 RMSE: 1.3823519\n",
      "Validation loss: 1.4102342128753662 RMSE: 1.1875328\n",
      "152 16 0.3569975197315216\n",
      "Validation loss: 1.5386036038398743 RMSE: 1.2404047\n",
      "Validation loss: 0.9323343336582184 RMSE: 0.9655746\n",
      "Validation loss: 1.196922391653061 RMSE: 1.0940394\n",
      "155 15 1.0107722282409668\n",
      "Validation loss: 3.7611218690872192 RMSE: 1.9393616\n",
      "Validation loss: 3.757067382335663 RMSE: 1.9383152\n",
      "Validation loss: 5.740548729896545 RMSE: 2.395944\n",
      "158 14 2.9125609397888184\n",
      "Validation loss: 1.1788890063762665 RMSE: 1.0857666\n",
      "Validation loss: 1.6328120231628418 RMSE: 1.2778153\n",
      "Validation loss: 1.2574752569198608 RMSE: 1.1213721\n",
      "161 13 1.1761541366577148\n",
      "Validation loss: 4.028734087944031 RMSE: 2.0071704\n",
      "Validation loss: 0.9544443190097809 RMSE: 0.97695667\n",
      "Validation loss: 1.6894562244415283 RMSE: 1.2997906\n",
      "164 12 0.6288465261459351\n",
      "Validation loss: 1.0672178864479065 RMSE: 1.0330625\n",
      "Validation loss: 0.966910719871521 RMSE: 0.98331624\n",
      "Validation loss: 1.1631883382797241 RMSE: 1.0785121\n",
      "167 11 0.9410708546638489\n",
      "Validation loss: 1.181836485862732 RMSE: 1.087123\n",
      "Validation loss: 1.3985340595245361 RMSE: 1.1825962\n",
      "Validation loss: 3.3499534130096436 RMSE: 1.8302876\n",
      "170 10 1.0085904598236084\n",
      "Validation loss: 2.5982906818389893 RMSE: 1.6119213\n",
      "Validation loss: 1.89385986328125 RMSE: 1.3761758\n",
      "Validation loss: 1.6625536680221558 RMSE: 1.2894006\n",
      "173 9 0.9965384006500244\n",
      "Validation loss: 4.122945308685303 RMSE: 2.0305035\n",
      "Validation loss: 1.4736483097076416 RMSE: 1.2139392\n",
      "Validation loss: 0.7938440442085266 RMSE: 0.8909793\n",
      "176 8 1.9352898597717285\n",
      "Validation loss: 1.3663355112075806 RMSE: 1.1689036\n",
      "Validation loss: 1.0736965537071228 RMSE: 1.0361933\n",
      "Validation loss: 5.307312250137329 RMSE: 2.3037603\n",
      "179 7 1.3857712745666504\n",
      "Validation loss: 1.7182368636131287 RMSE: 1.3108155\n",
      "Validation loss: 1.4312275648117065 RMSE: 1.1963391\n",
      "Validation loss: 0.9997207522392273 RMSE: 0.9998604\n",
      "182 6 0.9685672521591187\n",
      "Validation loss: 1.9417122602462769 RMSE: 1.3934534\n",
      "Validation loss: 1.6379902362823486 RMSE: 1.27984\n",
      "Validation loss: 1.0898262858390808 RMSE: 1.0439473\n",
      "185 5 1.3943133354187012\n",
      "Validation loss: 1.1425244808197021 RMSE: 1.0688894\n",
      "Validation loss: 1.9647348523139954 RMSE: 1.4016898\n",
      "Validation loss: 1.6245195865631104 RMSE: 1.2745663\n",
      "188 4 0.5735785961151123\n",
      "Validation loss: 2.1779500246047974 RMSE: 1.4757879\n",
      "Validation loss: 1.380947470664978 RMSE: 1.1751372\n",
      "Validation loss: 1.5370616912841797 RMSE: 1.239783\n",
      "191 3 0.5352512001991272\n",
      "Validation loss: 2.5072742700576782 RMSE: 1.5834376\n",
      "Validation loss: 1.66990065574646 RMSE: 1.2922465\n",
      "Validation loss: 3.1005266904830933 RMSE: 1.7608316\n",
      "194 2 1.3924925327301025\n",
      "Validation loss: 4.185925126075745 RMSE: 2.0459528\n",
      "Validation loss: 0.7411659359931946 RMSE: 0.86090994\n",
      "Validation loss: 4.66570520401001 RMSE: 2.1600242\n",
      "197 1 1.9714056253433228\n",
      "Validation loss: 2.1266363859176636 RMSE: 1.4582992\n",
      "Validation loss: 0.8483651876449585 RMSE: 0.9210674\n",
      "Validation loss: 3.0864381194114685 RMSE: 1.7568266\n",
      "Loaded trained model with success.\n",
      "Test loss: 1.9685813188552856 Test RMSE: 1.4030615\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:0', 'task_name': 'freesolv', 'model_type': 'gin', 'model': {'num_layer': 5, 'emb_dim': 300, 'feat_dim': 300, 'drop_ratio': 0.3, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'random', 'seed': 778, 'task': 'regression', 'data_path': 'data/freesolv/freesolv.csv', 'target': ['expt']}}\n",
      "Running on: cuda:0\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 16.177167892456055\n",
      "Validation loss: 24.261737823486328 RMSE: 4.9256206\n",
      "Validation loss: 19.728859424591064 RMSE: 4.441718\n",
      "2 16 17.045284271240234\n",
      "Validation loss: 19.012720108032227 RMSE: 4.360358\n",
      "Validation loss: 6.400212526321411 RMSE: 2.5298643\n",
      "Validation loss: 17.22266435623169 RMSE: 4.1500196\n",
      "5 15 3.8309154510498047\n",
      "Validation loss: 5.235531568527222 RMSE: 2.2881284\n",
      "Validation loss: 27.575143814086914 RMSE: 5.251204\n",
      "Validation loss: 5.97870135307312 RMSE: 2.4451385\n",
      "8 14 3.077336311340332\n",
      "Validation loss: 6.119457244873047 RMSE: 2.4737537\n",
      "Validation loss: 5.8354332447052 RMSE: 2.4156642\n",
      "Validation loss: 7.723174333572388 RMSE: 2.7790601\n",
      "11 13 3.958364963531494\n",
      "Validation loss: 6.013454914093018 RMSE: 2.4522345\n",
      "Validation loss: 5.204346179962158 RMSE: 2.2813036\n",
      "Validation loss: 6.283418893814087 RMSE: 2.506675\n",
      "14 12 2.3135015964508057\n",
      "Validation loss: 4.100240707397461 RMSE: 2.0249052\n",
      "Validation loss: 9.616214275360107 RMSE: 3.1010022\n",
      "Validation loss: 7.279036045074463 RMSE: 2.6979687\n",
      "17 11 3.8547940254211426\n",
      "Validation loss: 9.974107682704926 RMSE: 3.1581812\n",
      "Validation loss: 3.30516254901886 RMSE: 1.8180106\n",
      "Validation loss: 2.5077884197235107 RMSE: 1.5835998\n",
      "20 10 2.145199775695801\n",
      "Validation loss: 3.2945258617401123 RMSE: 1.815083\n",
      "Validation loss: 7.0554587841033936 RMSE: 2.6562114\n",
      "Validation loss: 3.3896178603172302 RMSE: 1.8410914\n",
      "23 9 1.5921759605407715\n",
      "Validation loss: 5.975407600402832 RMSE: 2.4444647\n",
      "Validation loss: 3.8915568590164185 RMSE: 1.9727031\n",
      "Validation loss: 3.271517276763916 RMSE: 1.8087337\n",
      "26 8 1.3833270072937012\n",
      "Validation loss: 3.5469539165496826 RMSE: 1.8833357\n",
      "Validation loss: 3.483122944831848 RMSE: 1.8663127\n",
      "Validation loss: 6.065205097198486 RMSE: 2.4627638\n",
      "29 7 1.1739394664764404\n",
      "Validation loss: 6.60438871383667 RMSE: 2.5699005\n",
      "Validation loss: 3.480749785900116 RMSE: 1.8656768\n",
      "Validation loss: 2.2743804454803467 RMSE: 1.508105\n",
      "32 6 1.3863418102264404\n",
      "Validation loss: 8.4001704454422 RMSE: 2.8983047\n",
      "Validation loss: 3.368551254272461 RMSE: 1.8353615\n",
      "Validation loss: 7.511385917663574 RMSE: 2.7406907\n",
      "35 5 2.5965898036956787\n",
      "Validation loss: 3.83659827709198 RMSE: 1.9587238\n",
      "Validation loss: 3.257595717906952 RMSE: 1.804881\n",
      "Validation loss: 2.791750907897949 RMSE: 1.6708534\n",
      "38 4 1.881471037864685\n",
      "Validation loss: 9.365322828292847 RMSE: 3.0602813\n",
      "Validation loss: 11.13070011138916 RMSE: 3.3362706\n",
      "Validation loss: 6.849674940109253 RMSE: 2.6171882\n",
      "41 3 3.5577640533447266\n",
      "Validation loss: 4.5565197467803955 RMSE: 2.1346006\n",
      "Validation loss: 5.489298582077026 RMSE: 2.3429246\n",
      "Validation loss: 2.858259618282318 RMSE: 1.6906388\n",
      "44 2 2.3805320262908936\n",
      "Validation loss: 4.33515477180481 RMSE: 2.0821033\n",
      "Validation loss: 3.2206575870513916 RMSE: 1.7946192\n",
      "Validation loss: 5.4128406047821045 RMSE: 2.326551\n",
      "47 1 1.824760913848877\n",
      "Validation loss: 3.548068881034851 RMSE: 1.883632\n",
      "Validation loss: 4.326249122619629 RMSE: 2.0799637\n",
      "Validation loss: 3.7500126361846924 RMSE: 1.9364951\n",
      "50 0 2.920224189758301\n",
      "Validation loss: 2.8610774278640747 RMSE: 1.6914718\n",
      "Validation loss: 4.7691570520401 RMSE: 2.18384\n",
      "52 16 8.06419563293457\n",
      "Validation loss: 5.873327732086182 RMSE: 2.423495\n",
      "Validation loss: 4.766313314437866 RMSE: 2.1831892\n",
      "Validation loss: 5.336477994918823 RMSE: 2.310082\n",
      "55 15 0.8392414450645447\n",
      "Validation loss: 4.3221787214279175 RMSE: 2.0789852\n",
      "Validation loss: 4.687639951705933 RMSE: 2.1650958\n",
      "Validation loss: 5.042495012283325 RMSE: 2.2455502\n",
      "58 14 0.9096124768257141\n",
      "Validation loss: 3.4262540340423584 RMSE: 1.8510143\n",
      "Validation loss: 3.1890887022018433 RMSE: 1.7858019\n",
      "Validation loss: 8.866388320922852 RMSE: 2.9776487\n",
      "61 13 1.245629072189331\n",
      "Validation loss: 4.22921359539032 RMSE: 2.0565054\n",
      "Validation loss: 5.495846509933472 RMSE: 2.344322\n",
      "Validation loss: 3.5939204692840576 RMSE: 1.8957638\n",
      "64 12 0.957255482673645\n",
      "Validation loss: 3.9504001140594482 RMSE: 1.9875616\n",
      "Validation loss: 3.2795557975769043 RMSE: 1.8109546\n",
      "Validation loss: 6.611280679702759 RMSE: 2.5712411\n",
      "67 11 0.6362749338150024\n",
      "Validation loss: 3.502771496772766 RMSE: 1.8715692\n",
      "Validation loss: 4.1181793212890625 RMSE: 2.0293298\n",
      "Validation loss: 7.210947036743164 RMSE: 2.6853206\n",
      "70 10 2.4402623176574707\n",
      "Validation loss: 2.392658770084381 RMSE: 1.5468222\n",
      "Validation loss: 4.481552720069885 RMSE: 2.1169677\n",
      "Validation loss: 4.823304891586304 RMSE: 2.1962025\n",
      "73 9 0.842302680015564\n",
      "Validation loss: 7.994356632232666 RMSE: 2.8274293\n",
      "Validation loss: 2.0276458263397217 RMSE: 1.4239542\n",
      "Validation loss: 2.9081162214279175 RMSE: 1.70532\n",
      "76 8 1.0372307300567627\n",
      "Validation loss: 2.5272045135498047 RMSE: 1.5897183\n",
      "Validation loss: 3.901069402694702 RMSE: 1.9751126\n",
      "Validation loss: 1.6934426426887512 RMSE: 1.3013235\n",
      "79 7 5.582481861114502\n",
      "Validation loss: 4.278210818767548 RMSE: 2.0683837\n",
      "Validation loss: 2.150050461292267 RMSE: 1.4663051\n",
      "Validation loss: 4.639265656471252 RMSE: 2.1538954\n",
      "82 6 1.0610411167144775\n",
      "Validation loss: 3.7479405403137207 RMSE: 1.9359597\n",
      "Validation loss: 2.750980257987976 RMSE: 1.658608\n",
      "Validation loss: 7.1140453815460205 RMSE: 2.6672168\n",
      "85 5 2.820385456085205\n",
      "Validation loss: 2.742839813232422 RMSE: 1.6561521\n",
      "Validation loss: 6.140491247177124 RMSE: 2.4780014\n",
      "Validation loss: 4.836590051651001 RMSE: 2.199225\n",
      "88 4 1.0982080698013306\n",
      "Validation loss: 3.1317538022994995 RMSE: 1.7696763\n",
      "Validation loss: 6.137798070907593 RMSE: 2.477458\n",
      "Validation loss: 2.7881836891174316 RMSE: 1.6697855\n",
      "91 3 0.6676109433174133\n",
      "Validation loss: 3.0917364358901978 RMSE: 1.7583334\n",
      "Validation loss: 3.5074254274368286 RMSE: 1.8728125\n",
      "Validation loss: 3.1494717597961426 RMSE: 1.7746753\n",
      "94 2 1.818185567855835\n",
      "Validation loss: 3.318026840686798 RMSE: 1.8215451\n",
      "Validation loss: 3.2647088766098022 RMSE: 1.8068506\n",
      "Validation loss: 3.2623045444488525 RMSE: 1.806185\n",
      "97 1 3.451174736022949\n",
      "Validation loss: 4.204235553741455 RMSE: 2.0504236\n",
      "Validation loss: 2.0708935260772705 RMSE: 1.4390599\n",
      "Validation loss: 2.3481072187423706 RMSE: 1.5323536\n",
      "100 0 0.7608904838562012\n",
      "Validation loss: 1.9471333026885986 RMSE: 1.3953973\n",
      "Validation loss: 1.894763708114624 RMSE: 1.3765043\n",
      "102 16 0.40318411588668823\n",
      "Validation loss: 3.6346575021743774 RMSE: 1.906478\n",
      "Validation loss: 2.1790401935577393 RMSE: 1.4761574\n",
      "Validation loss: 2.420938491821289 RMSE: 1.5559366\n",
      "105 15 6.433834075927734\n",
      "Validation loss: 1.8929924964904785 RMSE: 1.3758606\n",
      "Validation loss: 2.8596739768981934 RMSE: 1.6910571\n",
      "Validation loss: 8.327637195587158 RMSE: 2.8857648\n",
      "108 14 0.6014808416366577\n",
      "Validation loss: 14.256386280059814 RMSE: 3.775763\n",
      "Validation loss: 15.315126657485962 RMSE: 3.9134548\n",
      "Validation loss: 2.776508629322052 RMSE: 1.666286\n",
      "111 13 1.560638666152954\n",
      "Validation loss: 4.6485559940338135 RMSE: 2.156051\n",
      "Validation loss: 2.097421407699585 RMSE: 1.4482478\n",
      "Validation loss: 2.6311044096946716 RMSE: 1.6220679\n",
      "114 12 1.1249645948410034\n",
      "Validation loss: 2.067474067211151 RMSE: 1.4378713\n",
      "Validation loss: 2.8440637588500977 RMSE: 1.6864352\n",
      "Validation loss: 10.749679565429688 RMSE: 3.2786703\n",
      "117 11 0.7704058885574341\n",
      "Validation loss: 2.4998421669006348 RMSE: 1.5810889\n",
      "Validation loss: 2.1385265588760376 RMSE: 1.4623704\n",
      "Validation loss: 2.2349127531051636 RMSE: 1.4949625\n",
      "120 10 2.030350923538208\n",
      "Validation loss: 2.8448140621185303 RMSE: 1.6866575\n",
      "Validation loss: 6.282335042953491 RMSE: 2.506459\n",
      "Validation loss: 3.8403666019439697 RMSE: 1.9596854\n",
      "123 9 0.34163355827331543\n",
      "Validation loss: 4.635653853416443 RMSE: 2.153057\n",
      "Validation loss: 3.7509816884994507 RMSE: 1.936745\n",
      "Validation loss: 4.0451420545578 RMSE: 2.011254\n",
      "126 8 0.39654847979545593\n",
      "Validation loss: 2.977458953857422 RMSE: 1.7255315\n",
      "Validation loss: 1.9943581819534302 RMSE: 1.4122175\n",
      "Validation loss: 9.022719502449036 RMSE: 3.0037842\n",
      "129 7 0.6537832021713257\n",
      "Validation loss: 2.475521445274353 RMSE: 1.573379\n",
      "Validation loss: 3.081158995628357 RMSE: 1.7553228\n",
      "Validation loss: 6.174708366394043 RMSE: 2.484896\n",
      "132 6 2.7577638626098633\n",
      "Validation loss: 6.466942071914673 RMSE: 2.5430183\n",
      "Validation loss: 2.8816038370132446 RMSE: 1.697529\n",
      "Validation loss: 2.4262691736221313 RMSE: 1.5576488\n",
      "135 5 1.3890268802642822\n",
      "Validation loss: 3.71411395072937 RMSE: 1.9272038\n",
      "Validation loss: 5.127269744873047 RMSE: 2.2643476\n",
      "Validation loss: 6.524946093559265 RMSE: 2.554398\n",
      "138 4 0.7051113247871399\n",
      "Validation loss: 2.26407253742218 RMSE: 1.5046835\n",
      "Validation loss: 5.4928752183914185 RMSE: 2.3436882\n",
      "Validation loss: 5.883825302124023 RMSE: 2.4256597\n",
      "141 3 3.058467149734497\n",
      "Validation loss: 2.166216492652893 RMSE: 1.4718072\n",
      "Validation loss: 2.9101377725601196 RMSE: 1.7059125\n",
      "Validation loss: 6.473652362823486 RMSE: 2.5443375\n",
      "144 2 1.248665452003479\n",
      "Validation loss: 3.6994166374206543 RMSE: 1.9233868\n",
      "Validation loss: 3.130277633666992 RMSE: 1.7692591\n",
      "Validation loss: 4.930642366409302 RMSE: 2.2205055\n",
      "147 1 1.0608155727386475\n",
      "Validation loss: 5.398394346237183 RMSE: 2.3234441\n",
      "Validation loss: 1.9192267656326294 RMSE: 1.3853614\n",
      "Validation loss: 2.368110775947571 RMSE: 1.5388666\n",
      "150 0 0.34353500604629517\n",
      "Validation loss: 3.953379511833191 RMSE: 1.9883107\n",
      "Validation loss: 2.4039742946624756 RMSE: 1.5504756\n",
      "152 16 8.636552810668945\n",
      "Validation loss: 2.7699525356292725 RMSE: 1.6643175\n",
      "Validation loss: 2.876649796962738 RMSE: 1.696069\n",
      "Validation loss: 6.884784698486328 RMSE: 2.6238875\n",
      "155 15 0.9985655546188354\n",
      "Validation loss: 1.9693673849105835 RMSE: 1.4033415\n",
      "Validation loss: 2.6073597073554993 RMSE: 1.6147323\n",
      "Validation loss: 2.4745023250579834 RMSE: 1.5730551\n",
      "158 14 1.2581570148468018\n",
      "Validation loss: 4.624174475669861 RMSE: 2.1503892\n",
      "Validation loss: 2.2568711042404175 RMSE: 1.5022886\n",
      "Validation loss: 3.3900887966156006 RMSE: 1.8412194\n",
      "161 13 0.8555150032043457\n",
      "Validation loss: 2.448672890663147 RMSE: 1.5648235\n",
      "Validation loss: 4.446964740753174 RMSE: 2.1087828\n",
      "Validation loss: 3.0658514499664307 RMSE: 1.7509575\n",
      "164 12 0.4280672073364258\n",
      "Validation loss: 2.7690131664276123 RMSE: 1.6640353\n",
      "Validation loss: 2.9017393589019775 RMSE: 1.7034489\n",
      "Validation loss: 2.7440223693847656 RMSE: 1.6565093\n",
      "167 11 4.516729354858398\n",
      "Validation loss: 16.870887279510498 RMSE: 4.1074195\n",
      "Validation loss: 4.072938561439514 RMSE: 2.0181525\n",
      "Validation loss: 3.5237984657287598 RMSE: 1.8771784\n",
      "170 10 3.036881923675537\n",
      "Validation loss: 3.036983370780945 RMSE: 1.7426944\n",
      "Validation loss: 2.44356369972229 RMSE: 1.5631903\n",
      "Validation loss: 3.3137908577919006 RMSE: 1.820382\n",
      "173 9 0.9306402206420898\n",
      "Validation loss: 3.2512301206588745 RMSE: 1.803117\n",
      "Validation loss: 2.7262980937957764 RMSE: 1.6511506\n",
      "Validation loss: 3.3111889362335205 RMSE: 1.819667\n",
      "176 8 1.9048376083374023\n",
      "Validation loss: 3.0413849353790283 RMSE: 1.7439568\n",
      "Validation loss: 2.903261661529541 RMSE: 1.703896\n",
      "Validation loss: 2.482684373855591 RMSE: 1.5756537\n",
      "179 7 1.1275672912597656\n",
      "Validation loss: 3.1045680046081543 RMSE: 1.7619786\n",
      "Validation loss: 2.8164854645729065 RMSE: 1.6782386\n",
      "Validation loss: 3.712785482406616 RMSE: 1.926859\n",
      "182 6 0.6586277484893799\n",
      "Validation loss: 10.226084232330322 RMSE: 3.197825\n",
      "Validation loss: 1.4486350417137146 RMSE: 1.2035925\n",
      "Validation loss: 2.521359086036682 RMSE: 1.5878788\n",
      "185 5 0.3939151465892792\n",
      "Validation loss: 2.095668315887451 RMSE: 1.4476424\n",
      "Validation loss: 2.631614923477173 RMSE: 1.6222253\n",
      "Validation loss: 2.4122458696365356 RMSE: 1.5531406\n",
      "188 4 0.7573904991149902\n",
      "Validation loss: 3.088388741016388 RMSE: 1.7573813\n",
      "Validation loss: 2.1823490858078003 RMSE: 1.4772775\n",
      "Validation loss: 2.7811315059661865 RMSE: 1.6676725\n",
      "191 3 0.6242269277572632\n",
      "Validation loss: 2.3720672130584717 RMSE: 1.5401515\n",
      "Validation loss: 1.6066965460777283 RMSE: 1.2675554\n",
      "Validation loss: 1.9161116480827332 RMSE: 1.3842369\n",
      "194 2 0.40893393754959106\n",
      "Validation loss: 2.829254627227783 RMSE: 1.682039\n",
      "Validation loss: 3.5460554361343384 RMSE: 1.883097\n",
      "Validation loss: 3.258698523044586 RMSE: 1.8051865\n",
      "197 1 0.43936824798583984\n",
      "Validation loss: 2.2613322734832764 RMSE: 1.5037729\n",
      "Validation loss: 3.0301384925842285 RMSE: 1.7407295\n",
      "Validation loss: 2.5140167474746704 RMSE: 1.5855651\n",
      "Loaded trained model with success.\n",
      "Test loss: 1.296982854604721 Test RMSE: 1.1388515\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:0', 'task_name': 'freesolv', 'model_type': 'gin', 'model': {'num_layer': 5, 'emb_dim': 300, 'feat_dim': 300, 'drop_ratio': 0.3, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'random', 'seed': 779, 'task': 'regression', 'data_path': 'data/freesolv/freesolv.csv', 'target': ['expt']}}\n",
      "Running on: cuda:0\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 25.34465789794922\n",
      "Validation loss: 12.407707452774048 RMSE: 3.5224576\n",
      "Validation loss: 10.950044393539429 RMSE: 3.3090851\n",
      "2 16 9.865896224975586\n",
      "Validation loss: 7.281047344207764 RMSE: 2.6983416\n",
      "Validation loss: 3.1205453872680664 RMSE: 1.7665067\n",
      "Validation loss: 8.189875364303589 RMSE: 2.8617957\n",
      "5 15 5.4235687255859375\n",
      "Validation loss: 11.925300598144531 RMSE: 3.4533033\n",
      "Validation loss: 11.280821323394775 RMSE: 3.3586934\n",
      "Validation loss: 9.048739194869995 RMSE: 3.0081117\n",
      "8 14 3.725132942199707\n",
      "Validation loss: 8.653762340545654 RMSE: 2.9417279\n",
      "Validation loss: 8.24027967453003 RMSE: 2.8705888\n",
      "Validation loss: 13.310264706611633 RMSE: 3.6483235\n",
      "11 13 3.842151165008545\n",
      "Validation loss: 13.993754386901855 RMSE: 3.7408228\n",
      "Validation loss: 40.17470169067383 RMSE: 6.3383517\n",
      "Validation loss: 5.9025468826293945 RMSE: 2.429516\n",
      "14 12 2.875326633453369\n",
      "Validation loss: 15.69568932056427 RMSE: 3.9617782\n",
      "Validation loss: 12.538475036621094 RMSE: 3.5409706\n",
      "Validation loss: 2.867928624153137 RMSE: 1.6934958\n",
      "17 11 1.5011425018310547\n",
      "Validation loss: 11.986506223678589 RMSE: 3.4621537\n",
      "Validation loss: 19.080206632614136 RMSE: 4.368089\n",
      "Validation loss: 9.991400957107544 RMSE: 3.1609185\n",
      "20 10 2.6338210105895996\n",
      "Validation loss: 7.560392141342163 RMSE: 2.7496164\n",
      "Validation loss: 6.972215056419373 RMSE: 2.640495\n",
      "Validation loss: 20.8851158618927 RMSE: 4.5700235\n",
      "23 9 2.8116395473480225\n",
      "Validation loss: 8.898826122283936 RMSE: 2.9830902\n",
      "Validation loss: 13.516381740570068 RMSE: 3.6764634\n",
      "Validation loss: 8.907856225967407 RMSE: 2.9846036\n",
      "26 8 3.271012306213379\n",
      "Validation loss: 4.919792026281357 RMSE: 2.21806\n",
      "Validation loss: 15.52579927444458 RMSE: 3.9402792\n",
      "Validation loss: 16.0887770652771 RMSE: 4.0110817\n",
      "29 7 2.6327834129333496\n",
      "Validation loss: 9.56667709350586 RMSE: 3.093005\n",
      "Validation loss: 18.736005783081055 RMSE: 4.328511\n",
      "Validation loss: 9.579350233078003 RMSE: 3.0950525\n",
      "32 6 7.236246109008789\n",
      "Validation loss: 13.506068885326385 RMSE: 3.6750605\n",
      "Validation loss: 7.609158158302307 RMSE: 2.7584705\n",
      "Validation loss: 9.251606613397598 RMSE: 3.0416455\n",
      "35 5 2.20535945892334\n",
      "Validation loss: 9.817100524902344 RMSE: 3.1332252\n",
      "Validation loss: 5.903929233551025 RMSE: 2.4298003\n",
      "Validation loss: 8.708036184310913 RMSE: 2.9509382\n",
      "38 4 2.9949190616607666\n",
      "Validation loss: 7.169894695281982 RMSE: 2.6776655\n",
      "Validation loss: 12.196779012680054 RMSE: 3.4923887\n",
      "Validation loss: 17.020306825637817 RMSE: 4.1255674\n",
      "41 3 0.8254646062850952\n",
      "Validation loss: 11.572498798370361 RMSE: 3.4018376\n",
      "Validation loss: 10.738515615463257 RMSE: 3.2769675\n",
      "Validation loss: 12.545060276985168 RMSE: 3.5419002\n",
      "44 2 1.2029037475585938\n",
      "Validation loss: 15.750840663909912 RMSE: 3.9687326\n",
      "Validation loss: 23.04930591583252 RMSE: 4.800969\n",
      "Validation loss: 5.263496160507202 RMSE: 2.2942312\n",
      "47 1 2.828524112701416\n",
      "Validation loss: 5.428640604019165 RMSE: 2.3299444\n",
      "Validation loss: 8.358381688594818 RMSE: 2.8910866\n",
      "Validation loss: 12.04343867301941 RMSE: 3.4703658\n",
      "50 0 3.5597305297851562\n",
      "Validation loss: 4.632461130619049 RMSE: 2.1523154\n",
      "Validation loss: 10.834916353225708 RMSE: 3.2916436\n",
      "52 16 0.014107247814536095\n",
      "Validation loss: 7.682161629199982 RMSE: 2.7716713\n",
      "Validation loss: 9.323116421699524 RMSE: 3.0533776\n",
      "Validation loss: 10.95346474647522 RMSE: 3.3096018\n",
      "55 15 1.3543438911437988\n",
      "Validation loss: 10.487406373023987 RMSE: 3.2384264\n",
      "Validation loss: 12.428297877311707 RMSE: 3.5253794\n",
      "Validation loss: 17.106172800064087 RMSE: 4.135961\n",
      "58 14 1.1939666271209717\n",
      "Validation loss: 3.819244623184204 RMSE: 1.9542888\n",
      "Validation loss: 8.40926730632782 RMSE: 2.8998737\n",
      "Validation loss: 4.636030197143555 RMSE: 2.1531446\n",
      "61 13 0.6500518321990967\n",
      "Validation loss: 11.045481145381927 RMSE: 3.3234744\n",
      "Validation loss: 12.095356941223145 RMSE: 3.477838\n",
      "Validation loss: 2.532334089279175 RMSE: 1.591331\n",
      "64 12 1.9962137937545776\n",
      "Validation loss: 12.296108186244965 RMSE: 3.5065806\n",
      "Validation loss: 9.195169925689697 RMSE: 3.0323539\n",
      "Validation loss: 7.6196160316467285 RMSE: 2.7603652\n",
      "67 11 0.8398916721343994\n",
      "Validation loss: 11.689944744110107 RMSE: 3.419056\n",
      "Validation loss: 11.393112003803253 RMSE: 3.3753684\n",
      "Validation loss: 8.976502358913422 RMSE: 2.996081\n",
      "70 10 0.8679982423782349\n",
      "Validation loss: 4.520162433385849 RMSE: 2.1260674\n",
      "Validation loss: 13.051826000213623 RMSE: 3.612731\n",
      "Validation loss: 5.0417174100875854 RMSE: 2.2453768\n",
      "73 9 3.008023262023926\n",
      "Validation loss: 13.149206697940826 RMSE: 3.6261835\n",
      "Validation loss: 6.895186543464661 RMSE: 2.6258693\n",
      "Validation loss: 10.904086887836456 RMSE: 3.3021333\n",
      "76 8 3.3582215309143066\n",
      "Validation loss: 6.922343492507935 RMSE: 2.6310346\n",
      "Validation loss: 8.562838077545166 RMSE: 2.9262328\n",
      "Validation loss: 9.650052189826965 RMSE: 3.1064532\n",
      "79 7 0.5592973232269287\n",
      "Validation loss: 5.833475112915039 RMSE: 2.415259\n",
      "Validation loss: 9.370941936969757 RMSE: 3.0611997\n",
      "Validation loss: 2.1663047075271606 RMSE: 1.471837\n",
      "82 6 2.181236505508423\n",
      "Validation loss: 8.572536408901215 RMSE: 2.9278898\n",
      "Validation loss: 14.920010566711426 RMSE: 3.862643\n",
      "Validation loss: 7.8420023918151855 RMSE: 2.800358\n",
      "85 5 1.2096376419067383\n",
      "Validation loss: 7.246490836143494 RMSE: 2.6919308\n",
      "Validation loss: 6.272541761398315 RMSE: 2.5045042\n",
      "Validation loss: 2.543134033679962 RMSE: 1.5947206\n",
      "88 4 1.7187529802322388\n",
      "Validation loss: 6.0916712284088135 RMSE: 2.4681315\n",
      "Validation loss: 9.425927877426147 RMSE: 3.0701678\n",
      "Validation loss: 11.839131355285645 RMSE: 3.4408035\n",
      "91 3 1.6918084621429443\n",
      "Validation loss: 4.942620515823364 RMSE: 2.2232006\n",
      "Validation loss: 8.239064931869507 RMSE: 2.870377\n",
      "Validation loss: 6.177694797515869 RMSE: 2.485497\n",
      "94 2 3.3657453060150146\n",
      "Validation loss: 5.081050276756287 RMSE: 2.254119\n",
      "Validation loss: 6.333524644374847 RMSE: 2.5166495\n",
      "Validation loss: 4.724359154701233 RMSE: 2.173559\n",
      "97 1 1.5374150276184082\n",
      "Validation loss: 10.146036028862 RMSE: 3.1852846\n",
      "Validation loss: 3.6952667236328125 RMSE: 1.9223077\n",
      "Validation loss: 4.303432643413544 RMSE: 2.0744717\n",
      "100 0 1.144290804862976\n",
      "Validation loss: 3.2733246088027954 RMSE: 1.8092335\n",
      "Validation loss: 6.557059973478317 RMSE: 2.5606756\n",
      "102 16 26.50275993347168\n",
      "Validation loss: 4.829246401786804 RMSE: 2.1975548\n",
      "Validation loss: 4.797077119350433 RMSE: 2.190223\n",
      "Validation loss: 1.6351289749145508 RMSE: 1.2787216\n",
      "105 15 1.4952147006988525\n",
      "Validation loss: 16.360883355140686 RMSE: 4.044859\n",
      "Validation loss: 13.22340178489685 RMSE: 3.6363995\n",
      "Validation loss: 2.176056385040283 RMSE: 1.4751463\n",
      "108 14 1.1397364139556885\n",
      "Validation loss: 2.25118088722229 RMSE: 1.5003936\n",
      "Validation loss: 2.4837217330932617 RMSE: 1.5759828\n",
      "Validation loss: 2.353823661804199 RMSE: 1.5342176\n",
      "111 13 1.7176434993743896\n",
      "Validation loss: 5.941526770591736 RMSE: 2.4375246\n",
      "Validation loss: 7.349184036254883 RMSE: 2.710938\n",
      "Validation loss: 5.214462041854858 RMSE: 2.2835197\n",
      "114 12 1.4763509035110474\n",
      "Validation loss: 3.8320605754852295 RMSE: 1.9575648\n",
      "Validation loss: 4.372048258781433 RMSE: 2.0909443\n",
      "Validation loss: 4.4103734493255615 RMSE: 2.100089\n",
      "117 11 1.1774027347564697\n",
      "Validation loss: 8.59807574748993 RMSE: 2.9322476\n",
      "Validation loss: 3.3164197206497192 RMSE: 1.8211039\n",
      "Validation loss: 7.782244324684143 RMSE: 2.7896674\n",
      "120 10 1.4214004278182983\n",
      "Validation loss: 2.061392664909363 RMSE: 1.4357553\n",
      "Validation loss: 3.192260265350342 RMSE: 1.7866898\n",
      "Validation loss: 2.5014848709106445 RMSE: 1.5816084\n",
      "123 9 1.549255132675171\n",
      "Validation loss: 7.7422440350055695 RMSE: 2.7824883\n",
      "Validation loss: 4.844755828380585 RMSE: 2.201081\n",
      "Validation loss: 2.65641325712204 RMSE: 1.6298505\n",
      "126 8 1.752204418182373\n",
      "Validation loss: 7.358728766441345 RMSE: 2.7126977\n",
      "Validation loss: 5.25514554977417 RMSE: 2.2924106\n",
      "Validation loss: 3.194562792778015 RMSE: 1.787334\n",
      "129 7 1.192499041557312\n",
      "Validation loss: 2.7923807501792908 RMSE: 1.6710418\n",
      "Validation loss: 13.555424094200134 RMSE: 3.6817691\n",
      "Validation loss: 3.234817624092102 RMSE: 1.7985598\n",
      "132 6 1.3933422565460205\n",
      "Validation loss: 3.969472259283066 RMSE: 1.9923533\n",
      "Validation loss: 4.381146192550659 RMSE: 2.093119\n",
      "Validation loss: 5.872615694999695 RMSE: 2.423348\n",
      "135 5 1.9760018587112427\n",
      "Validation loss: 3.626494348049164 RMSE: 1.9043357\n",
      "Validation loss: 3.935815393924713 RMSE: 1.9838895\n",
      "Validation loss: 8.364284634590149 RMSE: 2.8921072\n",
      "138 4 2.033379316329956\n",
      "Validation loss: 6.116712212562561 RMSE: 2.4731987\n",
      "Validation loss: 4.067933976650238 RMSE: 2.0169122\n",
      "Validation loss: 5.310504674911499 RMSE: 2.3044527\n",
      "141 3 1.7096061706542969\n",
      "Validation loss: 1.2207102179527283 RMSE: 1.1048576\n",
      "Validation loss: 4.561801373958588 RMSE: 2.135837\n",
      "Validation loss: 2.648259162902832 RMSE: 1.6273472\n",
      "144 2 0.7973589897155762\n",
      "Validation loss: 2.383908271789551 RMSE: 1.5439911\n",
      "Validation loss: 5.260348558425903 RMSE: 2.293545\n",
      "Validation loss: 4.5857884883880615 RMSE: 2.1414452\n",
      "147 1 0.9492155313491821\n",
      "Validation loss: 6.9498714208602905 RMSE: 2.6362607\n",
      "Validation loss: 3.611623227596283 RMSE: 1.9004271\n",
      "Validation loss: 2.5452182292938232 RMSE: 1.595374\n",
      "150 0 0.3967480957508087\n",
      "Validation loss: 3.5009982585906982 RMSE: 1.8710954\n",
      "Validation loss: 3.1697704792022705 RMSE: 1.7803854\n",
      "152 16 28.2015380859375\n",
      "Validation loss: 7.195996642112732 RMSE: 2.682536\n",
      "Validation loss: 2.9755430817604065 RMSE: 1.7249761\n",
      "Validation loss: 1.8819925785064697 RMSE: 1.3718574\n",
      "155 15 1.3871794939041138\n",
      "Validation loss: 9.145497441291809 RMSE: 3.0241523\n",
      "Validation loss: 2.7685552835464478 RMSE: 1.6638978\n",
      "Validation loss: 9.539920210838318 RMSE: 3.088676\n",
      "158 14 0.4960346817970276\n",
      "Validation loss: 1.184550404548645 RMSE: 1.0883704\n",
      "Validation loss: 4.768923878669739 RMSE: 2.1837866\n",
      "Validation loss: 4.139249265193939 RMSE: 2.0345144\n",
      "161 13 2.6640396118164062\n",
      "Validation loss: 3.756318509578705 RMSE: 1.9381224\n",
      "Validation loss: 2.733267068862915 RMSE: 1.6532598\n"
     ]
    }
   ],
   "source": [
    "seeds = list(range(777,782))\n",
    "# datasets = [\"bace\",  \"bbbp\", \"tox21\", \"toxcast\", \"sider\",  ]\n",
    "datasets = [\"freeSolv\",  \"lipo\", \"esol\", \"qm7\", \"bace\",  \"bbbp\", ]\n",
    "\n",
    "for dataset in datasets:\n",
    "    for seed in seeds: \n",
    "        !python finetuneRecon.py \\\n",
    "        --task_name {dataset} \\\n",
    "        --splitting scaffold \\\n",
    "        --seed {seed} \\\n",
    "        --alpha 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6764b07a-c44b-44d3-ab6b-161100c0d53b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:0', 'task_name': 'esol', 'model_type': 'gin', 'model': {'num_layer': 5, 'emb_dim': 300, 'feat_dim': 300, 'drop_ratio': 0.3, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 751, 'task': 'regression', 'data_path': 'data/esol/esol.csv', 'target': ['measured log solubility in mols per litre']}}\n",
      "Running on: cuda:0\n",
      "1127\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/1127\n",
      "Generating scaffold 1000/1127\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "tensor([[ 0.0806, -0.0410, -0.0872,  ..., -0.0789, -0.0887, -0.1107],\n",
      "        [ 0.0313,  0.0164,  0.1077,  ..., -0.0785,  0.1171,  0.1046],\n",
      "        [-0.0002, -0.0291, -0.0762,  ...,  0.0110,  0.0630,  0.0511],\n",
      "        ...,\n",
      "        [ 0.0313,  0.0164,  0.1077,  ..., -0.0785,  0.1171,  0.1046],\n",
      "        [-0.0175,  0.1190, -0.0096,  ...,  0.0821,  0.0621,  0.0488],\n",
      "        [ 0.0313,  0.0164,  0.1077,  ..., -0.0785,  0.1171,  0.1046]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "0 0 8.955981254577637\n",
      "tensor([[ 0.0312,  0.0165,  0.1078,  ..., -0.0786,  0.1172,  0.1047],\n",
      "        [ 0.0312,  0.0165,  0.1078,  ..., -0.0786,  0.1172,  0.1047],\n",
      "        [ 0.0312,  0.0165,  0.1078,  ..., -0.0786,  0.1172,  0.1047],\n",
      "        ...,\n",
      "        [ 0.0312,  0.0165,  0.1078,  ..., -0.0786,  0.1172,  0.1047],\n",
      "        [ 0.0312,  0.0165,  0.1078,  ..., -0.0786,  0.1172,  0.1047],\n",
      "        [ 0.0312,  0.0165,  0.1078,  ..., -0.0786,  0.1172,  0.1047]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0004, -0.0289, -0.0764,  ...,  0.0108,  0.0629,  0.0509],\n",
      "        [-0.0004, -0.0289, -0.0764,  ...,  0.0108,  0.0629,  0.0509],\n",
      "        [-0.0004, -0.0289, -0.0764,  ...,  0.0108,  0.0629,  0.0509],\n",
      "        ...,\n",
      "        [ 0.0311,  0.0166,  0.1079,  ..., -0.0787,  0.1173,  0.1048],\n",
      "        [ 0.0311,  0.0166,  0.1079,  ..., -0.0787,  0.1173,  0.1048],\n",
      "        [ 0.0311,  0.0166,  0.1079,  ..., -0.0787,  0.1173,  0.1048]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0004, -0.0288, -0.0764,  ...,  0.0108,  0.0629,  0.0509],\n",
      "        [-0.0004, -0.0288, -0.0764,  ...,  0.0108,  0.0629,  0.0509],\n",
      "        [ 0.0311,  0.0167,  0.1079,  ..., -0.0788,  0.1174,  0.1048],\n",
      "        ...,\n",
      "        [ 0.0187, -0.0968, -0.0305,  ...,  0.0381, -0.0655, -0.0279],\n",
      "        [ 0.0311,  0.0167,  0.1079,  ..., -0.0788,  0.1174,  0.1048],\n",
      "        [ 0.0311,  0.0167,  0.1079,  ..., -0.0788,  0.1174,  0.1048]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0310,  0.0168,  0.1079,  ..., -0.0789,  0.1174,  0.1049],\n",
      "        [-0.0003, -0.0287, -0.0764,  ...,  0.0107,  0.0629,  0.0509],\n",
      "        [-0.0003, -0.0287, -0.0764,  ...,  0.0107,  0.0629,  0.0509],\n",
      "        ...,\n",
      "        [ 0.0310,  0.0168,  0.1079,  ..., -0.0789,  0.1174,  0.1049],\n",
      "        [ 0.0310,  0.0168,  0.1079,  ..., -0.0789,  0.1174,  0.1049],\n",
      "        [ 0.0310,  0.0168,  0.1079,  ..., -0.0789,  0.1174,  0.1049]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0309,  0.0168,  0.1080,  ..., -0.0789,  0.1175,  0.1050],\n",
      "        [ 0.0309,  0.0168,  0.1080,  ..., -0.0789,  0.1175,  0.1050],\n",
      "        [-0.0961, -0.0527,  0.0569,  ...,  0.0218, -0.0656,  0.0982],\n",
      "        ...,\n",
      "        [ 0.0309,  0.0168,  0.1080,  ..., -0.0789,  0.1175,  0.1050],\n",
      "        [-0.0961, -0.0527,  0.0569,  ...,  0.0218, -0.0656,  0.0982],\n",
      "        [ 0.0065, -0.1107,  0.0619,  ...,  0.0173,  0.0921, -0.0094]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0065, -0.1107,  0.0619,  ...,  0.0174,  0.0921, -0.0095],\n",
      "        [ 0.0065, -0.1107,  0.0619,  ...,  0.0174,  0.0921, -0.0095],\n",
      "        [ 0.0309,  0.0169,  0.1080,  ..., -0.0790,  0.1176,  0.1050],\n",
      "        ...,\n",
      "        [-0.0961, -0.0528,  0.0569,  ...,  0.0219, -0.0655,  0.0981],\n",
      "        [ 0.0309,  0.0169,  0.1080,  ..., -0.0790,  0.1176,  0.1050],\n",
      "        [ 0.0065, -0.1107,  0.0619,  ...,  0.0174,  0.0921, -0.0095]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0002, -0.0285, -0.0763,  ...,  0.0108,  0.0630,  0.0507],\n",
      "        [-0.0002, -0.0285, -0.0763,  ...,  0.0108,  0.0630,  0.0507],\n",
      "        [ 0.0309,  0.0169,  0.1081,  ..., -0.0791,  0.1177,  0.1051],\n",
      "        ...,\n",
      "        [ 0.0309,  0.0169,  0.1081,  ..., -0.0791,  0.1177,  0.1051],\n",
      "        [ 0.0309,  0.0169,  0.1081,  ..., -0.0791,  0.1177,  0.1051],\n",
      "        [-0.0123,  0.1087, -0.0120,  ...,  0.0790,  0.0517, -0.0861]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0961, -0.0528,  0.0569,  ...,  0.0220, -0.0653,  0.0979],\n",
      "        [-0.0961, -0.0528,  0.0569,  ...,  0.0220, -0.0653,  0.0979],\n",
      "        [ 0.0804, -0.0412, -0.0873,  ..., -0.0789, -0.0891, -0.1105],\n",
      "        ...,\n",
      "        [ 0.0309,  0.0170,  0.1081,  ..., -0.0791,  0.1178,  0.1052],\n",
      "        [ 0.0309,  0.0170,  0.1081,  ..., -0.0791,  0.1178,  0.1052],\n",
      "        [-0.0961, -0.0528,  0.0569,  ...,  0.0220, -0.0653,  0.0979]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0002, -0.0284, -0.0764,  ...,  0.0108,  0.0631,  0.0506],\n",
      "        [-0.0002, -0.0284, -0.0764,  ...,  0.0108,  0.0631,  0.0506],\n",
      "        [ 0.0309,  0.0170,  0.1082,  ..., -0.0792,  0.1179,  0.1052],\n",
      "        ...,\n",
      "        [ 0.0309,  0.0170,  0.1082,  ..., -0.0792,  0.1179,  0.1052],\n",
      "        [ 0.0309,  0.0170,  0.1082,  ..., -0.0792,  0.1179,  0.1052],\n",
      "        [ 0.0309,  0.0170,  0.1082,  ..., -0.0792,  0.1179,  0.1052]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0310,  0.0170,  0.1083,  ..., -0.0792,  0.1179,  0.1053],\n",
      "        [-0.0002, -0.0284, -0.0765,  ...,  0.0108,  0.0631,  0.0505],\n",
      "        [-0.0002, -0.0284, -0.0765,  ...,  0.0108,  0.0631,  0.0505],\n",
      "        ...,\n",
      "        [ 0.0310,  0.0170,  0.1083,  ..., -0.0792,  0.1179,  0.1053],\n",
      "        [ 0.0310,  0.0170,  0.1083,  ..., -0.0792,  0.1179,  0.1053],\n",
      "        [ 0.0310,  0.0170,  0.1083,  ..., -0.0792,  0.1179,  0.1053]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0310,  0.0171,  0.1083,  ..., -0.0793,  0.1179,  0.1054],\n",
      "        [ 0.0070, -0.1110,  0.0617,  ...,  0.0178,  0.0925, -0.0099],\n",
      "        [ 0.0310,  0.0171,  0.1083,  ..., -0.0793,  0.1179,  0.1054],\n",
      "        ...,\n",
      "        [ 0.0310,  0.0171,  0.1083,  ..., -0.0793,  0.1179,  0.1054],\n",
      "        [ 0.0310,  0.0171,  0.1083,  ..., -0.0793,  0.1179,  0.1054],\n",
      "        [-0.0961, -0.0529,  0.0568,  ...,  0.0223, -0.0651,  0.0977]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0309,  0.0171,  0.1084,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [ 0.0309,  0.0171,  0.1084,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [-0.0961, -0.0529,  0.0568,  ...,  0.0223, -0.0650,  0.0976],\n",
      "        ...,\n",
      "        [ 0.0309,  0.0171,  0.1084,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [ 0.0309,  0.0171,  0.1084,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [ 0.0309,  0.0171,  0.1084,  ..., -0.0794,  0.1179,  0.1054]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0961, -0.0529,  0.0568,  ...,  0.0224, -0.0649,  0.0975],\n",
      "        [-0.0003, -0.0284, -0.0766,  ...,  0.0107,  0.0633,  0.0504],\n",
      "        [-0.0003, -0.0284, -0.0766,  ...,  0.0107,  0.0633,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0309,  0.0171,  0.1085,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [ 0.0071, -0.1112,  0.0617,  ...,  0.0179,  0.0926, -0.0100],\n",
      "        [ 0.0309,  0.0171,  0.1085,  ..., -0.0794,  0.1179,  0.1054]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0309,  0.0171,  0.1085,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [ 0.0309,  0.0171,  0.1085,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [ 0.0309,  0.0171,  0.1085,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        ...,\n",
      "        [ 0.0071, -0.1112,  0.0616,  ...,  0.0180,  0.0927, -0.0100],\n",
      "        [ 0.0309,  0.0171,  0.1085,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [ 0.0309,  0.0171,  0.1085,  ..., -0.0794,  0.1179,  0.1054]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0309,  0.0171,  0.1086,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [ 0.0309,  0.0171,  0.1086,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [ 0.0309,  0.0171,  0.1086,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        ...,\n",
      "        [ 0.0309,  0.0171,  0.1086,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [-0.0962, -0.0529,  0.0568,  ...,  0.0225, -0.0647,  0.0974],\n",
      "        [-0.0127,  0.1084, -0.0120,  ...,  0.0788,  0.0512, -0.0857]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0309,  0.0171,  0.1087,  ..., -0.0795,  0.1179,  0.1054],\n",
      "        [-0.0002, -0.0284, -0.0766,  ...,  0.0107,  0.0632,  0.0503],\n",
      "        [-0.0002, -0.0284, -0.0766,  ...,  0.0107,  0.0632,  0.0503],\n",
      "        ...,\n",
      "        [ 0.0309,  0.0171,  0.1087,  ..., -0.0795,  0.1179,  0.1054],\n",
      "        [ 0.0309,  0.0171,  0.1087,  ..., -0.0795,  0.1179,  0.1054],\n",
      "        [ 0.0309,  0.0171,  0.1087,  ..., -0.0795,  0.1179,  0.1054]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 3.0917e-02,  1.7134e-02,  1.0878e-01,  ..., -7.9471e-02,\n",
      "          1.1795e-01,  1.0532e-01],\n",
      "        [-1.1599e-04, -2.8463e-02, -7.6550e-02,  ...,  1.0742e-02,\n",
      "          6.3165e-02,  5.0226e-02],\n",
      "        [-1.1599e-04, -2.8463e-02, -7.6550e-02,  ...,  1.0742e-02,\n",
      "          6.3165e-02,  5.0226e-02],\n",
      "        ...,\n",
      "        [ 3.0917e-02,  1.7134e-02,  1.0878e-01,  ..., -7.9471e-02,\n",
      "          1.1795e-01,  1.0532e-01],\n",
      "        [ 3.0917e-02,  1.7134e-02,  1.0878e-01,  ..., -7.9471e-02,\n",
      "          1.1795e-01,  1.0532e-01],\n",
      "        [ 3.0917e-02,  1.7134e-02,  1.0878e-01,  ..., -7.9471e-02,\n",
      "          1.1795e-01,  1.0532e-01]], device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0309,  0.0172,  0.1088,  ..., -0.0795,  0.1180,  0.1053],\n",
      "        [ 0.0309,  0.0172,  0.1088,  ..., -0.0795,  0.1180,  0.1053],\n",
      "        [ 0.0309,  0.0172,  0.1088,  ..., -0.0795,  0.1180,  0.1053],\n",
      "        ...,\n",
      "        [ 0.0074, -0.1113,  0.0618,  ...,  0.0180,  0.0929, -0.0100],\n",
      "        [ 0.0309,  0.0172,  0.1088,  ..., -0.0795,  0.1180,  0.1053],\n",
      "        [-0.0964, -0.0527,  0.0568,  ...,  0.0224, -0.0647,  0.0975]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-9.6424e-02, -5.2667e-02,  5.6764e-02,  ...,  2.2343e-02,\n",
      "         -6.4659e-02,  9.7490e-02],\n",
      "        [ 3.0807e-02,  1.7243e-02,  1.0880e-01,  ..., -7.9564e-02,\n",
      "          1.1803e-01,  1.0522e-01],\n",
      "        [-1.7022e-05, -2.8596e-02, -7.6470e-02,  ...,  1.0681e-02,\n",
      "          6.3049e-02,  5.0292e-02],\n",
      "        ...,\n",
      "        [ 3.0807e-02,  1.7243e-02,  1.0880e-01,  ..., -7.9564e-02,\n",
      "          1.1803e-01,  1.0522e-01],\n",
      "        [-1.7267e-02,  1.1858e-01, -1.0125e-02,  ...,  8.2536e-02,\n",
      "          6.1876e-02,  4.8307e-02],\n",
      "        [ 3.0807e-02,  1.7243e-02,  1.0880e-01,  ..., -7.9564e-02,\n",
      "          1.1803e-01,  1.0522e-01]], device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 3.0753e-02,  1.7299e-02,  1.0883e-01,  ..., -7.9583e-02,\n",
      "          1.1807e-01,  1.0516e-01],\n",
      "        [ 3.0753e-02,  1.7299e-02,  1.0883e-01,  ..., -7.9583e-02,\n",
      "          1.1807e-01,  1.0516e-01],\n",
      "        [ 4.1295e-05, -2.8627e-02, -7.6411e-02,  ...,  1.0639e-02,\n",
      "          6.3006e-02,  5.0348e-02],\n",
      "        ...,\n",
      "        [ 3.0753e-02,  1.7299e-02,  1.0883e-01,  ..., -7.9583e-02,\n",
      "          1.1807e-01,  1.0516e-01],\n",
      "        [-1.2426e-02,  1.0806e-01, -1.2240e-02,  ...,  7.9107e-02,\n",
      "          5.1456e-02, -8.5975e-02],\n",
      "        [ 3.0753e-02,  1.7299e-02,  1.0883e-01,  ..., -7.9583e-02,\n",
      "          1.1807e-01,  1.0516e-01]], device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0307,  0.0173,  0.1089,  ..., -0.0796,  0.1181,  0.1051],\n",
      "        [ 0.0307,  0.0173,  0.1089,  ..., -0.0796,  0.1181,  0.1051],\n",
      "        [ 0.0307,  0.0173,  0.1089,  ..., -0.0796,  0.1181,  0.1051],\n",
      "        ...,\n",
      "        [ 0.0076, -0.1114,  0.0619,  ...,  0.0181,  0.0930, -0.0100],\n",
      "        [ 0.0307,  0.0173,  0.1089,  ..., -0.0796,  0.1181,  0.1051],\n",
      "        [ 0.0307,  0.0173,  0.1089,  ..., -0.0796,  0.1181,  0.1051]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0307,  0.0174,  0.1089,  ..., -0.0796,  0.1181,  0.1051],\n",
      "        [ 0.0808, -0.0411, -0.0881,  ..., -0.0790, -0.0893, -0.1108],\n",
      "        [ 0.0076, -0.1113,  0.0619,  ...,  0.0181,  0.0930, -0.0100],\n",
      "        ...,\n",
      "        [ 0.0307,  0.0174,  0.1089,  ..., -0.0796,  0.1181,  0.1051],\n",
      "        [-0.0966, -0.0525,  0.0567,  ...,  0.0222, -0.0646,  0.0975],\n",
      "        [-0.0966, -0.0525,  0.0567,  ...,  0.0222, -0.0646,  0.0975]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0001, -0.0288, -0.0764,  ...,  0.0106,  0.0630,  0.0504],\n",
      "        [ 0.0898,  0.0321,  0.0691,  ...,  0.0426, -0.0070,  0.0773],\n",
      "        [ 0.0001, -0.0288, -0.0764,  ...,  0.0106,  0.0630,  0.0504],\n",
      "        ...,\n",
      "        [-0.0967, -0.0524,  0.0567,  ...,  0.0222, -0.0646,  0.0975],\n",
      "        [ 0.0307,  0.0174,  0.1089,  ..., -0.0797,  0.1182,  0.1051],\n",
      "        [-0.0967, -0.0524,  0.0567,  ...,  0.0222, -0.0646,  0.0975]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0968, -0.0523,  0.0567,  ...,  0.0222, -0.0646,  0.0974],\n",
      "        [ 0.0001, -0.0288, -0.0764,  ...,  0.0106,  0.0630,  0.0504],\n",
      "        [ 0.0001, -0.0288, -0.0764,  ...,  0.0106,  0.0630,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0307,  0.0174,  0.1090,  ..., -0.0797,  0.1182,  0.1052],\n",
      "        [-0.0968, -0.0523,  0.0567,  ...,  0.0222, -0.0646,  0.0974],\n",
      "        [ 0.0307,  0.0174,  0.1090,  ..., -0.0797,  0.1182,  0.1052]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0306,  0.0174,  0.1090,  ..., -0.0798,  0.1182,  0.1052],\n",
      "        [ 0.0001, -0.0289, -0.0764,  ...,  0.0106,  0.0630,  0.0504],\n",
      "        [ 0.0001, -0.0289, -0.0764,  ...,  0.0106,  0.0630,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0306,  0.0174,  0.1090,  ..., -0.0798,  0.1182,  0.1052],\n",
      "        [-0.0968, -0.0523,  0.0566,  ...,  0.0222, -0.0646,  0.0974],\n",
      "        [-0.0122,  0.1078, -0.0123,  ...,  0.0793,  0.0515, -0.0860]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0122,  0.1078, -0.0123,  ...,  0.0793,  0.0516, -0.0860],\n",
      "        [ 0.0306,  0.0174,  0.1091,  ..., -0.0798,  0.1182,  0.1053],\n",
      "        [ 0.0306,  0.0174,  0.1091,  ..., -0.0798,  0.1182,  0.1053],\n",
      "        ...,\n",
      "        [ 0.0306,  0.0174,  0.1091,  ..., -0.0798,  0.1182,  0.1053],\n",
      "        [ 0.0306,  0.0174,  0.1091,  ..., -0.0798,  0.1182,  0.1053],\n",
      "        [ 0.0306,  0.0174,  0.1091,  ..., -0.0798,  0.1182,  0.1053]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0306,  0.0174,  0.1091,  ..., -0.0799,  0.1182,  0.1053],\n",
      "        [-0.0121,  0.1078, -0.0122,  ...,  0.0793,  0.0516, -0.0860],\n",
      "        [ 0.0306,  0.0174,  0.1091,  ..., -0.0799,  0.1182,  0.1053],\n",
      "        ...,\n",
      "        [ 0.0306,  0.0174,  0.1091,  ..., -0.0799,  0.1182,  0.1053],\n",
      "        [ 0.0306,  0.0174,  0.1091,  ..., -0.0799,  0.1182,  0.1053],\n",
      "        [-0.0170,  0.1189, -0.0102,  ...,  0.0827,  0.0622,  0.0487]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0121,  0.1077, -0.0122,  ...,  0.0793,  0.0516, -0.0860],\n",
      "        [ 0.0305,  0.0174,  0.1092,  ..., -0.0799,  0.1182,  0.1053],\n",
      "        [ 0.0305,  0.0174,  0.1092,  ..., -0.0799,  0.1182,  0.1053],\n",
      "        ...,\n",
      "        [ 0.0305,  0.0174,  0.1092,  ..., -0.0799,  0.1182,  0.1053],\n",
      "        [ 0.0001, -0.0291, -0.0765,  ...,  0.0107,  0.0630,  0.0504],\n",
      "        [ 0.0001, -0.0291, -0.0765,  ...,  0.0107,  0.0630,  0.0504]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Validation loss: 2.2496231530619935 RMSE: 1.4998744\n",
      "tensor([[ 0.0305,  0.0174,  0.1092,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [-0.0970, -0.0521,  0.0565,  ...,  0.0224, -0.0646,  0.0973],\n",
      "        [ 0.0001, -0.0292, -0.0766,  ...,  0.0107,  0.0630,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0305,  0.0174,  0.1092,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [ 0.0305,  0.0174,  0.1092,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [ 0.0305,  0.0174,  0.1092,  ..., -0.0800,  0.1182,  0.1054]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0002, -0.0292, -0.0766,  ...,  0.0108,  0.0630,  0.0504],\n",
      "        [ 0.0305,  0.0174,  0.1092,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [ 0.0305,  0.0174,  0.1092,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        ...,\n",
      "        [ 0.0305,  0.0174,  0.1092,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [-0.0971, -0.0521,  0.0565,  ...,  0.0224, -0.0646,  0.0972],\n",
      "        [ 0.0078, -0.1111,  0.0621,  ...,  0.0182,  0.0933, -0.0101]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0971, -0.0521,  0.0565,  ...,  0.0224, -0.0647,  0.0972],\n",
      "        [ 0.0002, -0.0292, -0.0766,  ...,  0.0108,  0.0630,  0.0504],\n",
      "        [ 0.0002, -0.0292, -0.0766,  ...,  0.0108,  0.0630,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0305,  0.0174,  0.1093,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [ 0.0305,  0.0174,  0.1093,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [ 0.0078, -0.1111,  0.0621,  ...,  0.0182,  0.0934, -0.0101]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0304,  0.0174,  0.1093,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [ 0.0304,  0.0174,  0.1093,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [ 0.0002, -0.0293, -0.0766,  ...,  0.0108,  0.0630,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0304,  0.0174,  0.1093,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [ 0.0304,  0.0174,  0.1093,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [ 0.0304,  0.0174,  0.1093,  ..., -0.0800,  0.1182,  0.1054]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [-0.0120,  0.1077, -0.0121,  ...,  0.0793,  0.0515, -0.0860],\n",
      "        ...,\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [ 0.0809, -0.0410, -0.0884,  ..., -0.0791, -0.0894, -0.1111],\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [ 0.0003, -0.0293, -0.0766,  ...,  0.0108,  0.0631,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [-0.0120,  0.1076, -0.0121,  ...,  0.0793,  0.0515, -0.0860]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        ...,\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0304,  0.0174,  0.1095,  ..., -0.0801,  0.1182,  0.1055],\n",
      "        [ 0.0304,  0.0174,  0.1095,  ..., -0.0801,  0.1182,  0.1055],\n",
      "        [ 0.0304,  0.0174,  0.1095,  ..., -0.0801,  0.1182,  0.1055],\n",
      "        ...,\n",
      "        [-0.0974, -0.0521,  0.0564,  ...,  0.0225, -0.0648,  0.0970],\n",
      "        [ 0.0233,  0.0582, -0.1101,  ...,  0.0704,  0.0737,  0.0701],\n",
      "        [-0.0974, -0.0521,  0.0564,  ...,  0.0225, -0.0648,  0.0970]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0303,  0.0174,  0.1095,  ..., -0.0801,  0.1182,  0.1055],\n",
      "        [ 0.0303,  0.0174,  0.1095,  ..., -0.0801,  0.1182,  0.1055],\n",
      "        [ 0.0003, -0.0294, -0.0767,  ...,  0.0108,  0.0631,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0303,  0.0174,  0.1095,  ..., -0.0801,  0.1182,  0.1055],\n",
      "        [ 0.0303,  0.0174,  0.1095,  ..., -0.0801,  0.1182,  0.1055],\n",
      "        [-0.0974, -0.0522,  0.0564,  ...,  0.0225, -0.0649,  0.0970]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0003, -0.0295, -0.0767,  ...,  0.0108,  0.0631,  0.0503],\n",
      "        [ 0.0003, -0.0295, -0.0767,  ...,  0.0108,  0.0631,  0.0503],\n",
      "        [ 0.0303,  0.0174,  0.1096,  ..., -0.0801,  0.1182,  0.1055],\n",
      "        ...,\n",
      "        [ 0.0303,  0.0174,  0.1096,  ..., -0.0801,  0.1182,  0.1055],\n",
      "        [ 0.0303,  0.0174,  0.1096,  ..., -0.0801,  0.1182,  0.1055],\n",
      "        [ 0.0303,  0.0174,  0.1096,  ..., -0.0801,  0.1182,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0303,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1055],\n",
      "        [ 0.0003, -0.0295, -0.0767,  ...,  0.0108,  0.0631,  0.0503],\n",
      "        [ 0.0003, -0.0295, -0.0767,  ...,  0.0108,  0.0631,  0.0503],\n",
      "        ...,\n",
      "        [-0.0167,  0.1191, -0.0102,  ...,  0.0827,  0.0622,  0.0491],\n",
      "        [-0.0167,  0.1191, -0.0102,  ...,  0.0827,  0.0622,  0.0491],\n",
      "        [ 0.0303,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0302,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1055],\n",
      "        [ 0.0004, -0.0296, -0.0768,  ...,  0.0108,  0.0631,  0.0503],\n",
      "        [ 0.0004, -0.0296, -0.0768,  ...,  0.0108,  0.0631,  0.0503],\n",
      "        ...,\n",
      "        [ 0.0302,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1055],\n",
      "        [ 0.0302,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1055],\n",
      "        [ 0.0302,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0079, -0.1110,  0.0623,  ...,  0.0182,  0.0935, -0.0101],\n",
      "        [-0.0976, -0.0523,  0.0563,  ...,  0.0225, -0.0650,  0.0970],\n",
      "        [ 0.0302,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0302,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0302,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0302,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0302,  0.0174,  0.1097,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0302,  0.0174,  0.1097,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0302,  0.0174,  0.1097,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        ...,\n",
      "        [-0.0977, -0.0523,  0.0563,  ...,  0.0225, -0.0649,  0.0970],\n",
      "        [ 0.0302,  0.0174,  0.1097,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0302,  0.0174,  0.1097,  ..., -0.0802,  0.1182,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [-0.0977, -0.0524,  0.0563,  ...,  0.0225, -0.0649,  0.0969],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0118,  0.1075, -0.0120,  ...,  0.0793,  0.0513, -0.0859],\n",
      "        [-0.0118,  0.1075, -0.0120,  ...,  0.0793,  0.0513, -0.0859],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0803,  0.1182,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0803,  0.1182,  0.1056],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0803,  0.1182,  0.1056],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0803,  0.1182,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0003, -0.0297, -0.0768,  ...,  0.0108,  0.0631,  0.0504],\n",
      "        [ 0.0003, -0.0297, -0.0768,  ...,  0.0108,  0.0631,  0.0504],\n",
      "        [ 0.0003, -0.0297, -0.0768,  ...,  0.0108,  0.0631,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0082, -0.1109,  0.0626,  ...,  0.0178,  0.0937, -0.0099],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0803,  0.1181,  0.1056],\n",
      "        [ 0.0813, -0.0407, -0.0882,  ..., -0.0792, -0.0893, -0.1111]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0300,  0.0174,  0.1096,  ..., -0.0804,  0.1181,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0804,  0.1181,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0804,  0.1181,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0804,  0.1181,  0.1056],\n",
      "        [ 0.0082, -0.1109,  0.0626,  ...,  0.0178,  0.0938, -0.0099],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0804,  0.1181,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0002, -0.0298, -0.0769,  ...,  0.0107,  0.0631,  0.0504],\n",
      "        [ 0.0002, -0.0298, -0.0769,  ...,  0.0107,  0.0631,  0.0504],\n",
      "        [ 0.0002, -0.0298, -0.0769,  ...,  0.0107,  0.0631,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0804,  0.1181,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0804,  0.1181,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0804,  0.1181,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0300,  0.0174,  0.1096,  ..., -0.0805,  0.1181,  0.1055],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0805,  0.1181,  0.1055],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0805,  0.1181,  0.1055],\n",
      "        ...,\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0805,  0.1181,  0.1055],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0805,  0.1181,  0.1055],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0805,  0.1181,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0174,  0.1097,  ..., -0.0805,  0.1181,  0.1055],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0805,  0.1181,  0.1055],\n",
      "        [ 0.0001, -0.0298, -0.0769,  ...,  0.0107,  0.0631,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0805,  0.1181,  0.1055],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0805,  0.1181,  0.1055],\n",
      "        [ 0.0900,  0.0317,  0.0689,  ...,  0.0427, -0.0067,  0.0771]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "1 21 1.6086289882659912\n",
      "tensor([[ 0.0299,  0.0174,  0.1097,  ..., -0.0806,  0.1181,  0.1055],\n",
      "        [ 0.0083, -0.1108,  0.0627,  ...,  0.0176,  0.0939, -0.0098],\n",
      "        [ 0.0083, -0.1108,  0.0627,  ...,  0.0176,  0.0939, -0.0098],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0806,  0.1181,  0.1055],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0806,  0.1181,  0.1055],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0806,  0.1181,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0174,  0.1097,  ..., -0.0806,  0.1181,  0.1056],\n",
      "        [-0.0983, -0.0523,  0.0559,  ...,  0.0224, -0.0645,  0.0968],\n",
      "        [-0.0983, -0.0523,  0.0559,  ...,  0.0224, -0.0645,  0.0968],\n",
      "        ...,\n",
      "        [ 0.0083, -0.1107,  0.0627,  ...,  0.0175,  0.0939, -0.0097],\n",
      "        [-0.0983, -0.0523,  0.0559,  ...,  0.0224, -0.0645,  0.0968],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0806,  0.1181,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0174,  0.1097,  ..., -0.0807,  0.1182,  0.1056],\n",
      "        [ 0.0002, -0.0300, -0.0769,  ...,  0.0107,  0.0631,  0.0505],\n",
      "        [ 0.0002, -0.0300, -0.0769,  ...,  0.0107,  0.0631,  0.0505],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0807,  0.1182,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0807,  0.1182,  0.1056],\n",
      "        [ 0.0901,  0.0316,  0.0689,  ...,  0.0427, -0.0068,  0.0770]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0174,  0.1097,  ..., -0.0807,  0.1182,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0807,  0.1182,  0.1056],\n",
      "        [ 0.0003, -0.0300, -0.0769,  ...,  0.0106,  0.0631,  0.0506],\n",
      "        ...,\n",
      "        [ 0.0901,  0.0316,  0.0689,  ...,  0.0427, -0.0069,  0.0771],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0807,  0.1182,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0807,  0.1182,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0174,  0.1097,  ..., -0.0808,  0.1182,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0808,  0.1182,  0.1056],\n",
      "        [-0.0116,  0.1073, -0.0120,  ...,  0.0794,  0.0513, -0.0860],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0808,  0.1182,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0808,  0.1182,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0808,  0.1182,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0986, -0.0523,  0.0560,  ...,  0.0223, -0.0644,  0.0966],\n",
      "        [ 0.0299,  0.0175,  0.1097,  ..., -0.0808,  0.1182,  0.1056],\n",
      "        [ 0.0299,  0.0175,  0.1097,  ..., -0.0808,  0.1182,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0175,  0.1097,  ..., -0.0808,  0.1182,  0.1056],\n",
      "        [ 0.0299,  0.0175,  0.1097,  ..., -0.0808,  0.1182,  0.1056],\n",
      "        [ 0.0299,  0.0175,  0.1097,  ..., -0.0808,  0.1182,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0082, -0.1106,  0.0628,  ...,  0.0174,  0.0939, -0.0098],\n",
      "        [-0.0987, -0.0523,  0.0560,  ...,  0.0223, -0.0644,  0.0966],\n",
      "        [ 0.0004, -0.0301, -0.0769,  ...,  0.0105,  0.0631,  0.0508],\n",
      "        ...,\n",
      "        [ 0.0004, -0.0301, -0.0769,  ...,  0.0105,  0.0631,  0.0508],\n",
      "        [ 0.0004, -0.0301, -0.0769,  ...,  0.0105,  0.0631,  0.0508],\n",
      "        [ 0.0004, -0.0301, -0.0769,  ...,  0.0105,  0.0631,  0.0508]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Validation loss: 17.010034324848547 RMSE: 4.1243224\n",
      "tensor([[ 0.0004, -0.0302, -0.0769,  ...,  0.0105,  0.0631,  0.0508],\n",
      "        [-0.0115,  0.1073, -0.0120,  ...,  0.0794,  0.0512, -0.0860],\n",
      "        [ 0.0299,  0.0175,  0.1096,  ..., -0.0809,  0.1182,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0082, -0.1106,  0.0628,  ...,  0.0174,  0.0939, -0.0097],\n",
      "        [ 0.0299,  0.0175,  0.1096,  ..., -0.0809,  0.1182,  0.1056],\n",
      "        [ 0.0299,  0.0175,  0.1096,  ..., -0.0809,  0.1182,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0175,  0.1096,  ..., -0.0809,  0.1181,  0.1056],\n",
      "        [ 0.0004, -0.0302, -0.0769,  ...,  0.0105,  0.0631,  0.0508],\n",
      "        [ 0.0004, -0.0302, -0.0769,  ...,  0.0105,  0.0631,  0.0508],\n",
      "        ...,\n",
      "        [-0.0989, -0.0523,  0.0560,  ...,  0.0224, -0.0644,  0.0966],\n",
      "        [ 0.0299,  0.0175,  0.1096,  ..., -0.0809,  0.1181,  0.1056],\n",
      "        [ 0.0299,  0.0175,  0.1096,  ..., -0.0809,  0.1181,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0175,  0.1096,  ..., -0.0810,  0.1181,  0.1056],\n",
      "        [ 0.0004, -0.0302, -0.0769,  ...,  0.0105,  0.0631,  0.0508],\n",
      "        [ 0.0004, -0.0302, -0.0769,  ...,  0.0105,  0.0631,  0.0508],\n",
      "        ...,\n",
      "        [-0.0989, -0.0523,  0.0560,  ...,  0.0225, -0.0644,  0.0966],\n",
      "        [ 0.0299,  0.0175,  0.1096,  ..., -0.0810,  0.1181,  0.1056],\n",
      "        [-0.0115,  0.1073, -0.0119,  ...,  0.0795,  0.0512, -0.0860]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0174,  0.1096,  ..., -0.0810,  0.1181,  0.1056],\n",
      "        [ 0.0004, -0.0303, -0.0770,  ...,  0.0104,  0.0630,  0.0508],\n",
      "        [ 0.0004, -0.0303, -0.0770,  ...,  0.0104,  0.0630,  0.0508],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0810,  0.1181,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0810,  0.1181,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0810,  0.1181,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056],\n",
      "        [-0.0115,  0.1073, -0.0118,  ...,  0.0795,  0.0512, -0.0859],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0300,  0.0174,  0.1096,  ..., -0.0811,  0.1179,  0.1056],\n",
      "        [-0.0993, -0.0524,  0.0558,  ...,  0.0228, -0.0645,  0.0966],\n",
      "        [ 0.0004, -0.0304, -0.0772,  ...,  0.0104,  0.0629,  0.0507],\n",
      "        ...,\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0811,  0.1179,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0811,  0.1179,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0811,  0.1179,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1179,  0.1056],\n",
      "        [-0.0115,  0.1073, -0.0117,  ...,  0.0795,  0.0512, -0.0859],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1179,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1179,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1179,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1179,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0994, -0.0524,  0.0558,  ...,  0.0229, -0.0646,  0.0966],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1179,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1179,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1179,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1179,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1179,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1178,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1178,  0.1056],\n",
      "        [ 0.0003, -0.0305, -0.0773,  ...,  0.0104,  0.0628,  0.0506],\n",
      "        ...,\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1178,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1178,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0995, -0.0524,  0.0558,  ...,  0.0229, -0.0646,  0.0966],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        ...,\n",
      "        [-0.0995, -0.0524,  0.0558,  ...,  0.0229, -0.0646,  0.0966],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0174,  0.1097,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0004, -0.0305, -0.0774,  ...,  0.0104,  0.0628,  0.0506],\n",
      "        [ 0.0004, -0.0305, -0.0774,  ...,  0.0104,  0.0628,  0.0506],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0165,  0.1185, -0.0105,  ...,  0.0828,  0.0629,  0.0487],\n",
      "        [ 0.0299,  0.0174,  0.1098,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0004, -0.0305, -0.0774,  ...,  0.0104,  0.0627,  0.0506],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0174,  0.1098,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1098,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1098,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0115,  0.1073, -0.0117,  ...,  0.0795,  0.0511, -0.0859],\n",
      "        [ 0.0299,  0.0174,  0.1098,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [-0.0997, -0.0524,  0.0558,  ...,  0.0230, -0.0647,  0.0966],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0174,  0.1098,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1098,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1098,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0298,  0.0175,  0.1099,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        ...,\n",
      "        [-0.0998, -0.0524,  0.0558,  ...,  0.0230, -0.0647,  0.0966],\n",
      "        [ 0.0234,  0.0589, -0.1091,  ...,  0.0714,  0.0753,  0.0714],\n",
      "        [ 0.0808, -0.0412, -0.0879,  ..., -0.0785, -0.0889, -0.1103]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0298,  0.0175,  0.1100,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1100,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0004, -0.0305, -0.0774,  ...,  0.0103,  0.0626,  0.0507],\n",
      "        ...,\n",
      "        [ 0.0298,  0.0175,  0.1100,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1100,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1100,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0298,  0.0175,  0.1100,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1100,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1100,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        ...,\n",
      "        [-0.0998, -0.0524,  0.0558,  ...,  0.0230, -0.0647,  0.0966],\n",
      "        [ 0.0298,  0.0175,  0.1100,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1100,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0005, -0.0305, -0.0774,  ...,  0.0103,  0.0626,  0.0507],\n",
      "        [ 0.0005, -0.0305, -0.0774,  ...,  0.0103,  0.0626,  0.0507],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0005, -0.0306, -0.0774,  ...,  0.0102,  0.0626,  0.0507],\n",
      "        [ 0.0005, -0.0306, -0.0774,  ...,  0.0102,  0.0626,  0.0507],\n",
      "        [ 0.0005, -0.0306, -0.0774,  ...,  0.0102,  0.0626,  0.0507],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.1000, -0.0523,  0.0557,  ...,  0.0230, -0.0646,  0.0964],\n",
      "        [ 0.0005, -0.0306, -0.0775,  ...,  0.0103,  0.0625,  0.0507],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0806, -0.0413, -0.0879,  ..., -0.0785, -0.0888, -0.1102],\n",
      "        [-0.1000, -0.0523,  0.0557,  ...,  0.0230, -0.0646,  0.0964],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0005, -0.0305, -0.0775,  ...,  0.0103,  0.0625,  0.0507],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0005, -0.0305, -0.0776,  ...,  0.0103,  0.0625,  0.0507],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [ 0.0005, -0.0305, -0.0776,  ...,  0.0103,  0.0625,  0.0507],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [-0.1001, -0.0524,  0.0557,  ...,  0.0231, -0.0646,  0.0963],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0112,  0.1071, -0.0116,  ...,  0.0795,  0.0511, -0.0860],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [ 0.0087, -0.1107,  0.0631,  ...,  0.0173,  0.0944, -0.0097],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [-0.1001, -0.0524,  0.0557,  ...,  0.0232, -0.0646,  0.0963],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.1001, -0.0524,  0.0556,  ...,  0.0232, -0.0645,  0.0963],\n",
      "        [-0.1001, -0.0524,  0.0556,  ...,  0.0232, -0.0645,  0.0963],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [ 0.0005, -0.0306, -0.0777,  ...,  0.0103,  0.0624,  0.0507],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [-0.1002, -0.0524,  0.0556,  ...,  0.0232, -0.0645,  0.0962],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1177,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.1002, -0.0524,  0.0556,  ...,  0.0233, -0.0645,  0.0962],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1177,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1176,  0.1056],\n",
      "        [ 0.0005, -0.0305, -0.0778,  ...,  0.0104,  0.0624,  0.0506],\n",
      "        [ 0.0089, -0.1107,  0.0631,  ...,  0.0172,  0.0945, -0.0097],\n",
      "        ...,\n",
      "        [-0.1002, -0.0524,  0.0556,  ...,  0.0233, -0.0645,  0.0962],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1176,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Validation loss: 3.044654910543324 RMSE: 1.744894\n",
      "tensor([[ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1176,  0.1056],\n",
      "        ...,\n",
      "        [-0.1002, -0.0523,  0.0556,  ...,  0.0233, -0.0645,  0.0962],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1176,  0.1056],\n",
      "        [ 0.0089, -0.1107,  0.0632,  ...,  0.0171,  0.0946, -0.0098]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0005, -0.0306, -0.0779,  ...,  0.0104,  0.0623,  0.0507],\n",
      "        [ 0.0005, -0.0306, -0.0779,  ...,  0.0104,  0.0623,  0.0507],\n",
      "        [ 0.0297,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0005, -0.0306, -0.0779,  ...,  0.0105,  0.0622,  0.0507],\n",
      "        [ 0.0005, -0.0306, -0.0779,  ...,  0.0105,  0.0622,  0.0507],\n",
      "        [ 0.0005, -0.0306, -0.0779,  ...,  0.0105,  0.0622,  0.0507],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0297,  0.0175,  0.1100,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0175,  0.1100,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0175,  0.1100,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0175,  0.1100,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        [-0.1002, -0.0523,  0.0556,  ...,  0.0233, -0.0645,  0.0962],\n",
      "        [ 0.0297,  0.0175,  0.1100,  ..., -0.0811,  0.1176,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0297,  0.0175,  0.1099,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        [-0.1002, -0.0523,  0.0556,  ...,  0.0233, -0.0645,  0.0962],\n",
      "        [ 0.0005, -0.0306, -0.0779,  ...,  0.0105,  0.0621,  0.0507],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0175,  0.1099,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0175,  0.1099,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0175,  0.1099,  ..., -0.0811,  0.1176,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0113,  0.1071, -0.0114,  ...,  0.0795,  0.0511, -0.0859],\n",
      "        [ 0.0297,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [ 0.0005, -0.0306, -0.0779,  ...,  0.0105,  0.0621,  0.0507],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0297,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [ 0.0005, -0.0306, -0.0779,  ...,  0.0105,  0.0621,  0.0507],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [-0.1003, -0.0524,  0.0556,  ...,  0.0234, -0.0646,  0.0962],\n",
      "        [ 0.0297,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0909,  0.0324,  0.0702,  ...,  0.0420, -0.0075,  0.0781],\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0298,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [-0.1003, -0.0525,  0.0556,  ...,  0.0234, -0.0645,  0.0961],\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0298,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1055],\n",
      "        ...,\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1055],\n",
      "        [-0.1004, -0.0525,  0.0556,  ...,  0.0234, -0.0645,  0.0961]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.1004, -0.0525,  0.0556,  ...,  0.0234, -0.0645,  0.0961],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1176,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1176,  0.1055],\n",
      "        ...,\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1176,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1176,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1176,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0005, -0.0305, -0.0779,  ...,  0.0106,  0.0620,  0.0506],\n",
      "        ...,\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        ...,\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [-0.1004, -0.0525,  0.0556,  ...,  0.0234, -0.0643,  0.0961],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        ...,\n",
      "        [ 0.0088, -0.1103,  0.0630,  ...,  0.0169,  0.0950, -0.0102],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "3 13 1.791797399520874\n",
      "tensor([[ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0239,  0.0583, -0.1088,  ...,  0.0713,  0.0755,  0.0716],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        ...,\n",
      "        [ 0.0089, -0.1103,  0.0630,  ...,  0.0169,  0.0951, -0.0102],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0910,  0.0325,  0.0701,  ...,  0.0420, -0.0072,  0.0780],\n",
      "        [ 0.0005, -0.0306, -0.0780,  ...,  0.0108,  0.0618,  0.0505],\n",
      "        [ 0.0005, -0.0306, -0.0780,  ...,  0.0108,  0.0618,  0.0505],\n",
      "        ...,\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1054],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1054],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1054]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0111,  0.1072, -0.0113,  ...,  0.0795,  0.0512, -0.0859],\n",
      "        [-0.1005, -0.0524,  0.0556,  ...,  0.0235, -0.0642,  0.0960],\n",
      "        [ 0.0298,  0.0176,  0.1101,  ..., -0.0810,  0.1174,  0.1054],\n",
      "        ...,\n",
      "        [ 0.0298,  0.0176,  0.1101,  ..., -0.0810,  0.1174,  0.1054],\n",
      "        [ 0.0298,  0.0176,  0.1101,  ..., -0.0810,  0.1174,  0.1054],\n",
      "        [ 0.0298,  0.0176,  0.1101,  ..., -0.0810,  0.1174,  0.1054]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1174,  0.1054],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1174,  0.1054],\n",
      "        [ 0.0005, -0.0306, -0.0780,  ...,  0.0108,  0.0617,  0.0505],\n",
      "        ...,\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1174,  0.1054],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1174,  0.1054],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1174,  0.1054]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n"
     ]
    }
   ],
   "source": [
    "seeds = [range(777,782)]\n",
    "for seed in seeds:\n",
    "    !python finetuneRecon.py \\\n",
    "    --task_name bbbp \\\n",
    "    --splitting scaffold \\\n",
    "    --seed {seed}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ce4ecd8-eaee-4c19-8a8d-195f56a1af16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 1, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:0', 'task_name': 'freesolv', 'model_type': 'gin', 'model': {'num_layer': 5, 'emb_dim': 300, 'feat_dim': 300, 'drop_ratio': 0.3, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 750, 'task': 'regression', 'data_path': 'data/freesolv/freesolv.csv', 'target': ['expt']}}\n",
      "Running on: cuda:0\n",
      "641\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/641\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 43.755775451660156\n",
      "Validation loss: 49.51893424987793 RMSE: 7.036969\n",
      "Loaded trained model with success.\n",
      "Test loss: 25.182190352219802 Test RMSE: 5.018186\n"
     ]
    }
   ],
   "source": [
    "!python finetuneReconOriginEmbedding.py \\\n",
    "--task_name freesolv \\\n",
    "--splitting scaffold \\\n",
    "--seed 750 \\\n",
    "--random_masking 1 \\\n",
    "--epochs 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c350a18-74eb-42bb-ac32-ea54a4810977",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
