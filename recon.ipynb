{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cbc95c-8c82-4b39-9acb-eac3d7dea845",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:0', 'task_name': 'freesolv', 'model_type': 'gin', 'model': {'num_layer': 3, 'emb_dim': 64, 'feat_dim': 64, 'drop_ratio': 0.5, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 777, 'task': 'regression', 'data_path': 'data/freesolv/freesolv.csv', 'target': ['expt']}}\n",
      "Running on: cuda:0\n",
      "641\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/641\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 22.549190521240234\n",
      "Validation loss: 67.7631950378418 RMSE: 8.23184\n",
      "Validation loss: 61.79688262939453 RMSE: 7.8610992\n",
      "Validation loss: 53.3897762298584 RMSE: 7.306831\n",
      "3 2 12.403983116149902\n",
      "Validation loss: 37.65724468231201 RMSE: 6.13655\n",
      "Validation loss: 14.305150032043457 RMSE: 3.782215\n",
      "Validation loss: 12.83330488204956 RMSE: 3.5823603\n",
      "6 4 5.13505220413208\n",
      "Validation loss: 13.772623062133789 RMSE: 3.7111485\n",
      "Validation loss: 14.239217281341553 RMSE: 3.7734885\n",
      "Validation loss: 12.596134185791016 RMSE: 3.5491033\n",
      "9 6 4.963886737823486\n",
      "Validation loss: 11.067981719970703 RMSE: 3.3268576\n",
      "Validation loss: 13.2083740234375 RMSE: 3.6343327\n",
      "Validation loss: 11.952752590179443 RMSE: 3.4572754\n",
      "12 8 5.027112007141113\n",
      "Validation loss: 12.18651008605957 RMSE: 3.4909182\n",
      "Validation loss: 13.059415817260742 RMSE: 3.6137817\n",
      "Validation loss: 12.424306392669678 RMSE: 3.524813\n",
      "15 10 4.546539306640625\n",
      "Validation loss: 12.074656009674072 RMSE: 3.4748604\n",
      "Validation loss: 12.909822463989258 RMSE: 3.5930243\n",
      "Validation loss: 12.16226053237915 RMSE: 3.4874432\n",
      "18 12 4.898680210113525\n",
      "Validation loss: 11.22277545928955 RMSE: 3.3500412\n",
      "Validation loss: 12.21049690246582 RMSE: 3.4943523\n",
      "Validation loss: 14.792869091033936 RMSE: 3.84615\n",
      "21 14 3.9849209785461426\n",
      "Validation loss: 11.684587478637695 RMSE: 3.4182725\n",
      "Validation loss: 11.28807544708252 RMSE: 3.359773\n",
      "Validation loss: 11.336359977722168 RMSE: 3.3669512\n",
      "Validation loss: 13.585049629211426 RMSE: 3.68579\n",
      "25 0 4.1924896240234375\n",
      "Validation loss: 10.254653930664062 RMSE: 3.2022889\n",
      "Validation loss: 9.875267267227173 RMSE: 3.1424937\n",
      "Validation loss: 12.036911010742188 RMSE: 3.4694252\n",
      "28 2 4.190744400024414\n",
      "Validation loss: 10.933922290802002 RMSE: 3.3066483\n",
      "Validation loss: 11.962154388427734 RMSE: 3.458635\n",
      "Validation loss: 14.424395561218262 RMSE: 3.7979462\n",
      "31 4 3.8356542587280273\n",
      "Validation loss: 20.140159606933594 RMSE: 4.487779\n",
      "Validation loss: 10.828535079956055 RMSE: 3.2906742\n",
      "Validation loss: 14.699629783630371 RMSE: 3.8340094\n",
      "34 6 5.650207996368408\n",
      "Validation loss: 11.140484809875488 RMSE: 3.3377364\n",
      "Validation loss: 13.194233417510986 RMSE: 3.632387\n",
      "Validation loss: 11.797041893005371 RMSE: 3.4346824\n",
      "37 8 2.8370203971862793\n",
      "Validation loss: 13.240713119506836 RMSE: 3.6387792\n",
      "Validation loss: 15.983142852783203 RMSE: 3.9978921\n",
      "Validation loss: 11.491111755371094 RMSE: 3.3898542\n",
      "40 10 4.2237935066223145\n",
      "Validation loss: 11.127639770507812 RMSE: 3.3358119\n",
      "Validation loss: 14.592778205871582 RMSE: 3.8200495\n",
      "Validation loss: 19.64497947692871 RMSE: 4.4322658\n",
      "43 12 2.5851047039031982\n",
      "Validation loss: 13.229105949401855 RMSE: 3.6371837\n",
      "Validation loss: 12.656468391418457 RMSE: 3.5575933\n",
      "Validation loss: 9.828158378601074 RMSE: 3.1349893\n",
      "46 14 2.2704977989196777\n",
      "Validation loss: 11.367419242858887 RMSE: 3.37156\n",
      "Validation loss: 9.714212417602539 RMSE: 3.116763\n",
      "Validation loss: 9.147469520568848 RMSE: 3.0244784\n",
      "Validation loss: 12.618237495422363 RMSE: 3.552216\n",
      "50 0 3.6810953617095947\n",
      "Validation loss: 11.105815410614014 RMSE: 3.3325388\n",
      "Validation loss: 18.14591407775879 RMSE: 4.2598023\n",
      "Validation loss: 10.12650203704834 RMSE: 3.1822164\n",
      "53 2 1.552926778793335\n",
      "Validation loss: 11.575499057769775 RMSE: 3.4022787\n",
      "Validation loss: 14.383668422698975 RMSE: 3.7925806\n",
      "Validation loss: 12.0302095413208 RMSE: 3.4684594\n",
      "56 4 2.5986709594726562\n",
      "Validation loss: 12.024067401885986 RMSE: 3.4675736\n",
      "Validation loss: 11.553879737854004 RMSE: 3.3990996\n",
      "Validation loss: 12.92011547088623 RMSE: 3.5944562\n",
      "59 6 3.5236434936523438\n",
      "Validation loss: 10.523506164550781 RMSE: 3.2439954\n",
      "Validation loss: 12.08469295501709 RMSE: 3.476305\n",
      "Validation loss: 10.303243637084961 RMSE: 3.2098665\n",
      "62 8 2.0236356258392334\n",
      "Validation loss: 11.693474292755127 RMSE: 3.4195724\n",
      "Validation loss: 13.870143413543701 RMSE: 3.7242641\n",
      "Validation loss: 10.33800458908081 RMSE: 3.2152767\n",
      "65 10 2.256991147994995\n",
      "Validation loss: 10.360054969787598 RMSE: 3.2187037\n",
      "Validation loss: 16.026721000671387 RMSE: 4.003339\n",
      "Validation loss: 11.51496696472168 RMSE: 3.3933709\n",
      "68 12 2.1716978549957275\n",
      "Validation loss: 11.29664659500122 RMSE: 3.3610482\n",
      "Validation loss: 12.41639232635498 RMSE: 3.52369\n",
      "Validation loss: 13.804309844970703 RMSE: 3.7154152\n",
      "71 14 2.242072582244873\n",
      "Validation loss: 8.881848812103271 RMSE: 2.9802432\n",
      "Validation loss: 13.040230751037598 RMSE: 3.611126\n",
      "Validation loss: 11.079503059387207 RMSE: 3.3285887\n",
      "Validation loss: 12.088497161865234 RMSE: 3.4768517\n",
      "75 0 1.9586255550384521\n",
      "Validation loss: 12.086129665374756 RMSE: 3.476511\n",
      "Validation loss: 14.604249000549316 RMSE: 3.8215506\n",
      "Validation loss: 9.899453163146973 RMSE: 3.1463394\n",
      "78 2 2.1605384349823\n",
      "Validation loss: 11.133007049560547 RMSE: 3.336616\n",
      "Validation loss: 11.67148494720459 RMSE: 3.4163556\n",
      "Validation loss: 8.19176721572876 RMSE: 2.8621264\n",
      "81 4 1.3272755146026611\n",
      "Validation loss: 13.745340824127197 RMSE: 3.7074711\n",
      "Validation loss: 11.092909812927246 RMSE: 3.3306024\n",
      "Validation loss: 12.001056671142578 RMSE: 3.4642541\n",
      "84 6 2.5570497512817383\n",
      "Validation loss: 12.281370162963867 RMSE: 3.5044787\n",
      "Validation loss: 10.011175155639648 RMSE: 3.1640441\n",
      "Validation loss: 9.879618644714355 RMSE: 3.1431863\n",
      "87 8 2.175231456756592\n",
      "Validation loss: 9.001417636871338 RMSE: 3.0002363\n",
      "Validation loss: 10.332190990447998 RMSE: 3.2143724\n",
      "Validation loss: 12.766836643218994 RMSE: 3.573071\n",
      "90 10 2.488922595977783\n",
      "Validation loss: 11.237708568572998 RMSE: 3.352269\n",
      "Validation loss: 10.722921371459961 RMSE: 3.2745874\n",
      "Validation loss: 9.062648296356201 RMSE: 3.0104234\n",
      "93 12 2.3657400608062744\n",
      "Validation loss: 15.660734176635742 RMSE: 3.9573646\n",
      "Validation loss: 10.736696720123291 RMSE: 3.2766898\n",
      "Validation loss: 10.288775444030762 RMSE: 3.207612\n",
      "96 14 2.2721946239471436\n",
      "Validation loss: 9.445882558822632 RMSE: 3.0734155\n",
      "Validation loss: 10.250828266143799 RMSE: 3.2016914\n",
      "Validation loss: 13.871039390563965 RMSE: 3.7243845\n",
      "Validation loss: 16.479034423828125 RMSE: 4.0594378\n",
      "100 0 2.2956976890563965\n",
      "Validation loss: 12.276054382324219 RMSE: 3.50372\n",
      "Validation loss: 14.12689208984375 RMSE: 3.7585762\n",
      "Validation loss: 11.41400671005249 RMSE: 3.3784623\n",
      "103 2 2.1061413288116455\n",
      "Validation loss: 9.925047874450684 RMSE: 3.1504042\n",
      "Validation loss: 9.254236221313477 RMSE: 3.0420775\n",
      "Validation loss: 11.874227523803711 RMSE: 3.4459002\n",
      "106 4 6.191744804382324\n",
      "Validation loss: 15.03623914718628 RMSE: 3.8776588\n",
      "Validation loss: 11.520757675170898 RMSE: 3.3942242\n",
      "Validation loss: 12.514562129974365 RMSE: 3.5375924\n",
      "109 6 1.8115155696868896\n",
      "Validation loss: 12.988062143325806 RMSE: 3.6038954\n",
      "Validation loss: 21.263700485229492 RMSE: 4.611258\n",
      "Validation loss: 12.90539836883545 RMSE: 3.5924084\n",
      "112 8 1.6073778867721558\n",
      "Validation loss: 13.358107566833496 RMSE: 3.6548743\n",
      "Validation loss: 10.081199645996094 RMSE: 3.1750906\n",
      "Validation loss: 10.567975997924805 RMSE: 3.2508423\n",
      "115 10 1.5539246797561646\n",
      "Validation loss: 15.60013484954834 RMSE: 3.9497008\n",
      "Validation loss: 14.43445110321045 RMSE: 3.7992697\n",
      "Validation loss: 8.663603782653809 RMSE: 2.9434001\n",
      "118 12 2.3413338661193848\n",
      "Validation loss: 14.592028617858887 RMSE: 3.819951\n",
      "Validation loss: 15.616495132446289 RMSE: 3.9517715\n",
      "Validation loss: 13.856782913208008 RMSE: 3.72247\n",
      "121 14 5.46366024017334\n",
      "Validation loss: 8.218266487121582 RMSE: 2.8667521\n",
      "Validation loss: 16.804208755493164 RMSE: 4.0992937\n",
      "Validation loss: 9.968191862106323 RMSE: 3.1572444\n",
      "Validation loss: 16.83217763900757 RMSE: 4.1027036\n",
      "125 0 2.8019680976867676\n",
      "Validation loss: 10.052502155303955 RMSE: 3.1705682\n",
      "Validation loss: 11.991596221923828 RMSE: 3.4628885\n",
      "Validation loss: 13.610822200775146 RMSE: 3.689285\n",
      "128 2 2.390115737915039\n",
      "Validation loss: 10.736181735992432 RMSE: 3.2766113\n",
      "Validation loss: 15.158637046813965 RMSE: 3.893409\n",
      "Validation loss: 14.112607955932617 RMSE: 3.756675\n",
      "131 4 1.8040575981140137\n",
      "Validation loss: 11.577507019042969 RMSE: 3.4025736\n",
      "Validation loss: 11.492897987365723 RMSE: 3.390118\n",
      "Validation loss: 9.443808555603027 RMSE: 3.073078\n",
      "134 6 1.3685206174850464\n",
      "Validation loss: 11.475550651550293 RMSE: 3.3875582\n",
      "Validation loss: 11.163554191589355 RMSE: 3.3411903\n",
      "Validation loss: 24.37071418762207 RMSE: 4.9366703\n",
      "137 8 1.3723859786987305\n",
      "Validation loss: 9.92880916595459 RMSE: 3.1510012\n",
      "Validation loss: 12.078185558319092 RMSE: 3.4753685\n",
      "Validation loss: 11.931907176971436 RMSE: 3.4542587\n",
      "140 10 1.7149642705917358\n",
      "Validation loss: 9.639654636383057 RMSE: 3.1047795\n",
      "Validation loss: 11.384411334991455 RMSE: 3.3740792\n",
      "Validation loss: 11.630992412567139 RMSE: 3.4104242\n",
      "143 12 2.3988118171691895\n",
      "Validation loss: 10.715644836425781 RMSE: 3.273476\n",
      "Validation loss: 12.262885570526123 RMSE: 3.5018404\n",
      "Validation loss: 12.93618392944336 RMSE: 3.5966907\n",
      "146 14 1.568603277206421\n",
      "Validation loss: 11.38478946685791 RMSE: 3.3741355\n",
      "Validation loss: 15.390948295593262 RMSE: 3.92313\n",
      "Validation loss: 19.32258701324463 RMSE: 4.395746\n",
      "Validation loss: 18.29245948791504 RMSE: 4.2769685\n",
      "150 0 2.678004741668701\n",
      "Validation loss: 11.432832717895508 RMSE: 3.3812473\n",
      "Validation loss: 11.539060115814209 RMSE: 3.3969193\n",
      "Validation loss: 13.292777061462402 RMSE: 3.645926\n",
      "153 2 2.4110348224639893\n",
      "Validation loss: 16.996045112609863 RMSE: 4.122626\n",
      "Validation loss: 13.547014236450195 RMSE: 3.6806269\n",
      "Validation loss: 16.41724967956543 RMSE: 4.0518208\n",
      "156 4 1.9415141344070435\n",
      "Validation loss: 17.079215049743652 RMSE: 4.132701\n",
      "Validation loss: 10.233212947845459 RMSE: 3.1989396\n",
      "Validation loss: 15.832491397857666 RMSE: 3.9790063\n",
      "159 6 1.8743761777877808\n",
      "Validation loss: 10.948290824890137 RMSE: 3.30882\n",
      "Validation loss: 17.07927894592285 RMSE: 4.1327085\n",
      "Validation loss: 14.827244758605957 RMSE: 3.850616\n",
      "162 8 2.2153942584991455\n",
      "Validation loss: 13.833048820495605 RMSE: 3.7192805\n",
      "Validation loss: 11.617283821105957 RMSE: 3.4084141\n",
      "Validation loss: 16.12813377380371 RMSE: 4.0159855\n",
      "165 10 2.121843099594116\n",
      "Validation loss: 13.604290008544922 RMSE: 3.6883993\n",
      "Validation loss: 15.065829277038574 RMSE: 3.8814726\n",
      "Validation loss: 14.201207637786865 RMSE: 3.768449\n",
      "168 12 1.819959044456482\n",
      "Validation loss: 15.977865219116211 RMSE: 3.9972322\n",
      "Validation loss: 17.049546241760254 RMSE: 4.12911\n",
      "Validation loss: 15.858026027679443 RMSE: 3.982214\n",
      "171 14 1.9656736850738525\n",
      "Validation loss: 21.181049346923828 RMSE: 4.6022873\n",
      "Validation loss: 9.922584533691406 RMSE: 3.1500134\n",
      "Validation loss: 16.639989376068115 RMSE: 4.079214\n",
      "Validation loss: 18.20258092880249 RMSE: 4.2664485\n",
      "175 0 1.0733965635299683\n",
      "Validation loss: 14.27123498916626 RMSE: 3.7777288\n",
      "Validation loss: 17.361735343933105 RMSE: 4.1667414\n",
      "Validation loss: 15.016939640045166 RMSE: 3.8751695\n",
      "178 2 0.8702579736709595\n",
      "Validation loss: 12.013015270233154 RMSE: 3.4659798\n",
      "Validation loss: 20.471017837524414 RMSE: 4.5244913\n",
      "Validation loss: 16.65596342086792 RMSE: 4.081172\n",
      "181 4 1.854993224143982\n",
      "Validation loss: 19.725913047790527 RMSE: 4.441386\n",
      "Validation loss: 17.233354568481445 RMSE: 4.1513076\n",
      "Validation loss: 13.261101722717285 RMSE: 3.6415794\n",
      "184 6 2.5858967304229736\n",
      "Validation loss: 15.784457683563232 RMSE: 3.9729657\n",
      "Validation loss: 15.057244300842285 RMSE: 3.8803666\n",
      "Validation loss: 21.5755615234375 RMSE: 4.64495\n",
      "187 8 2.077307939529419\n",
      "Validation loss: 17.16892147064209 RMSE: 4.1435394\n",
      "Validation loss: 16.82386541366577 RMSE: 4.1016903\n",
      "Validation loss: 21.852447509765625 RMSE: 4.6746607\n",
      "190 10 2.4887216091156006\n",
      "Validation loss: 20.8150634765625 RMSE: 4.562353\n",
      "Validation loss: 15.363621234893799 RMSE: 3.9196455\n",
      "Validation loss: 16.856054306030273 RMSE: 4.1056123\n",
      "193 12 2.052631139755249\n",
      "Validation loss: 13.683601379394531 RMSE: 3.6991353\n",
      "Validation loss: 15.666497230529785 RMSE: 3.9580927\n",
      "Validation loss: 16.746492385864258 RMSE: 4.0922475\n",
      "196 14 1.1727242469787598\n",
      "Validation loss: 12.375357151031494 RMSE: 3.5178626\n",
      "Validation loss: 9.997999906539917 RMSE: 3.1619613\n",
      "Validation loss: 13.780183792114258 RMSE: 3.712167\n",
      "Validation loss: 11.310431957244873 RMSE: 3.3630984\n",
      "Loaded trained model with success.\n",
      "Test loss: 10.200352360652044 Test RMSE: 3.193799\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:0', 'task_name': 'freesolv', 'model_type': 'gin', 'model': {'num_layer': 3, 'emb_dim': 64, 'feat_dim': 64, 'drop_ratio': 0.5, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 778, 'task': 'regression', 'data_path': 'data/freesolv/freesolv.csv', 'target': ['expt']}}\n",
      "Running on: cuda:0\n",
      "641\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/641\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 18.694581985473633\n",
      "Validation loss: 63.24338340759277 RMSE: 7.952571\n",
      "Validation loss: 57.56593322753906 RMSE: 7.587222\n",
      "Validation loss: 50.05668830871582 RMSE: 7.075075\n",
      "3 2 12.791915893554688\n",
      "Validation loss: 40.31209945678711 RMSE: 6.349181\n",
      "Validation loss: 33.88188552856445 RMSE: 5.820815\n",
      "Validation loss: 22.65110731124878 RMSE: 4.759318\n",
      "6 4 11.101362228393555\n",
      "Validation loss: 21.292930603027344 RMSE: 4.614426\n",
      "Validation loss: 21.46387481689453 RMSE: 4.632912\n",
      "Validation loss: 19.237884521484375 RMSE: 4.3861012\n",
      "9 6 5.027551651000977\n",
      "Validation loss: 23.24108600616455 RMSE: 4.8209014\n",
      "Validation loss: 17.421008110046387 RMSE: 4.173848\n",
      "Validation loss: 17.771122932434082 RMSE: 4.215581\n",
      "12 8 8.440978050231934\n",
      "Validation loss: 18.452296257019043 RMSE: 4.295614\n",
      "Validation loss: 14.884775161743164 RMSE: 3.8580792\n",
      "Validation loss: 13.92431354522705 RMSE: 3.7315297\n",
      "15 10 3.364379644393921\n",
      "Validation loss: 14.218479633331299 RMSE: 3.7707398\n",
      "Validation loss: 14.721628665924072 RMSE: 3.8368773\n",
      "Validation loss: 16.23388385772705 RMSE: 4.0291295\n",
      "18 12 6.7431230545043945\n",
      "Validation loss: 15.483531951904297 RMSE: 3.934912\n",
      "Validation loss: 15.613093376159668 RMSE: 3.951341\n",
      "Validation loss: 16.155658721923828 RMSE: 4.01941\n",
      "21 14 3.8003432750701904\n",
      "Validation loss: 14.433802604675293 RMSE: 3.7991846\n",
      "Validation loss: 14.355147361755371 RMSE: 3.7888188\n",
      "Validation loss: 20.291383743286133 RMSE: 4.5045958\n",
      "Validation loss: 14.854076862335205 RMSE: 3.8540986\n",
      "25 0 2.905398368835449\n",
      "Validation loss: 14.726908683776855 RMSE: 3.8375654\n",
      "Validation loss: 12.99168348312378 RMSE: 3.6043978\n",
      "Validation loss: 13.396605491638184 RMSE: 3.6601374\n",
      "28 2 2.892705202102661\n",
      "Validation loss: 11.080097198486328 RMSE: 3.3286781\n",
      "Validation loss: 11.46390151977539 RMSE: 3.3858383\n",
      "Validation loss: 13.60392141342163 RMSE: 3.6883495\n",
      "31 4 4.876672267913818\n",
      "Validation loss: 13.411251068115234 RMSE: 3.6621375\n",
      "Validation loss: 13.387951850891113 RMSE: 3.658955\n",
      "Validation loss: 13.395808696746826 RMSE: 3.6600285\n",
      "34 6 3.8221943378448486\n",
      "Validation loss: 16.286523818969727 RMSE: 4.0356565\n",
      "Validation loss: 14.657499313354492 RMSE: 3.8285115\n",
      "Validation loss: 13.46224594116211 RMSE: 3.6690934\n",
      "37 8 3.3992362022399902\n",
      "Validation loss: 17.33163070678711 RMSE: 4.1631274\n",
      "Validation loss: 17.052177906036377 RMSE: 4.1294284\n",
      "Validation loss: 14.320389747619629 RMSE: 3.784229\n",
      "40 10 1.8734662532806396\n",
      "Validation loss: 16.48659610748291 RMSE: 4.060369\n",
      "Validation loss: 17.48896312713623 RMSE: 4.181981\n",
      "Validation loss: 19.341684341430664 RMSE: 4.397918\n",
      "43 12 2.6317684650421143\n",
      "Validation loss: 19.100065231323242 RMSE: 4.370362\n",
      "Validation loss: 16.928608894348145 RMSE: 4.1144395\n",
      "Validation loss: 17.37483024597168 RMSE: 4.1683125\n",
      "46 14 2.5023176670074463\n",
      "Validation loss: 13.826045036315918 RMSE: 3.718339\n",
      "Validation loss: 16.752172470092773 RMSE: 4.0929418\n",
      "Validation loss: 14.78768253326416 RMSE: 3.8454754\n",
      "Validation loss: 14.818618774414062 RMSE: 3.849496\n",
      "50 0 2.3169567584991455\n",
      "Validation loss: 17.791497230529785 RMSE: 4.2179966\n",
      "Validation loss: 16.954349517822266 RMSE: 4.117566\n",
      "Validation loss: 16.19781255722046 RMSE: 4.0246506\n",
      "53 2 2.5435314178466797\n",
      "Validation loss: 20.09669589996338 RMSE: 4.4829335\n",
      "Validation loss: 20.4369158744812 RMSE: 4.520721\n",
      "Validation loss: 15.643176555633545 RMSE: 3.9551454\n",
      "56 4 3.818439483642578\n",
      "Validation loss: 19.244404792785645 RMSE: 4.3868446\n",
      "Validation loss: 19.327372550964355 RMSE: 4.396291\n",
      "Validation loss: 14.45944595336914 RMSE: 3.8025577\n",
      "59 6 1.518410086631775\n",
      "Validation loss: 16.035005569458008 RMSE: 4.0043736\n",
      "Validation loss: 16.396037101745605 RMSE: 4.049202\n",
      "Validation loss: 18.757659912109375 RMSE: 4.3310113\n",
      "62 8 2.2715697288513184\n",
      "Validation loss: 21.69473361968994 RMSE: 4.6577606\n",
      "Validation loss: 14.13969898223877 RMSE: 3.760279\n",
      "Validation loss: 19.57997226715088 RMSE: 4.4249263\n",
      "65 10 4.688320159912109\n",
      "Validation loss: 19.282913208007812 RMSE: 4.3912315\n",
      "Validation loss: 15.822707653045654 RMSE: 3.977777\n",
      "Validation loss: 21.765226364135742 RMSE: 4.665322\n",
      "68 12 1.991599678993225\n",
      "Validation loss: 17.08963108062744 RMSE: 4.1339607\n",
      "Validation loss: 17.030489921569824 RMSE: 4.1268015\n",
      "Validation loss: 19.120628356933594 RMSE: 4.372714\n",
      "71 14 2.5754237174987793\n",
      "Validation loss: 19.290294647216797 RMSE: 4.3920717\n",
      "Validation loss: 18.032286643981934 RMSE: 4.2464437\n",
      "Validation loss: 16.392903804779053 RMSE: 4.048815\n",
      "Validation loss: 18.68018674850464 RMSE: 4.322058\n",
      "75 0 1.56691312789917\n",
      "Validation loss: 18.886545181274414 RMSE: 4.3458657\n",
      "Validation loss: 17.479650497436523 RMSE: 4.180867\n",
      "Validation loss: 17.78088617324829 RMSE: 4.216739\n",
      "78 2 1.8751327991485596\n",
      "Validation loss: 19.92282485961914 RMSE: 4.463499\n",
      "Validation loss: 16.464448928833008 RMSE: 4.057641\n",
      "Validation loss: 22.606773376464844 RMSE: 4.754658\n",
      "81 4 4.863709449768066\n",
      "Validation loss: 16.283557891845703 RMSE: 4.0352893\n",
      "Validation loss: 20.306010246276855 RMSE: 4.5062194\n",
      "Validation loss: 19.73851776123047 RMSE: 4.442806\n",
      "84 6 2.352735996246338\n",
      "Validation loss: 20.305700302124023 RMSE: 4.5061846\n",
      "Validation loss: 19.883103370666504 RMSE: 4.4590473\n",
      "Validation loss: 23.404091835021973 RMSE: 4.8377776\n",
      "87 8 2.702502727508545\n",
      "Validation loss: 20.15921688079834 RMSE: 4.4899015\n",
      "Validation loss: 15.648903846740723 RMSE: 3.9558697\n",
      "Validation loss: 17.024839401245117 RMSE: 4.1261168\n",
      "90 10 2.8507304191589355\n",
      "Validation loss: 24.69016456604004 RMSE: 4.96892\n",
      "Validation loss: 17.541869163513184 RMSE: 4.1883016\n",
      "Validation loss: 17.295652389526367 RMSE: 4.158804\n",
      "93 12 2.975867509841919\n",
      "Validation loss: 17.714969635009766 RMSE: 4.2089157\n",
      "Validation loss: 16.312137126922607 RMSE: 4.0388284\n",
      "Validation loss: 20.04340934753418 RMSE: 4.4769864\n",
      "96 14 2.1789560317993164\n",
      "Validation loss: 17.81161880493164 RMSE: 4.2203813\n",
      "Validation loss: 15.682901859283447 RMSE: 3.9601645\n",
      "Validation loss: 20.861051559448242 RMSE: 4.56739\n",
      "Validation loss: 16.061055660247803 RMSE: 4.0076246\n",
      "100 0 1.8073886632919312\n",
      "Validation loss: 20.871898651123047 RMSE: 4.5685773\n",
      "Validation loss: 17.977736949920654 RMSE: 4.240016\n",
      "Validation loss: 17.646647453308105 RMSE: 4.2007914\n",
      "103 2 1.0609091520309448\n",
      "Validation loss: 19.42259979248047 RMSE: 4.4071074\n",
      "Validation loss: 17.196255683898926 RMSE: 4.1468368\n",
      "Validation loss: 21.28785991668701 RMSE: 4.613877\n",
      "106 4 4.326202869415283\n",
      "Validation loss: 17.136027812957764 RMSE: 4.139569\n",
      "Validation loss: 18.128307342529297 RMSE: 4.257735\n",
      "Validation loss: 19.49326801300049 RMSE: 4.415118\n",
      "109 6 1.2956769466400146\n",
      "Validation loss: 16.756783485412598 RMSE: 4.0935054\n",
      "Validation loss: 17.32544755935669 RMSE: 4.162385\n",
      "Validation loss: 15.631975650787354 RMSE: 3.9537294\n",
      "112 8 2.190760850906372\n",
      "Validation loss: 17.28109121322632 RMSE: 4.1570535\n",
      "Validation loss: 18.002090454101562 RMSE: 4.242887\n",
      "Validation loss: 18.582345962524414 RMSE: 4.3107243\n",
      "115 10 1.778804898262024\n",
      "Validation loss: 12.985628128051758 RMSE: 3.6035576\n",
      "Validation loss: 19.04081630706787 RMSE: 4.3635783\n",
      "Validation loss: 17.19888687133789 RMSE: 4.147154\n",
      "118 12 2.340620517730713\n",
      "Validation loss: 15.586163520812988 RMSE: 3.9479318\n",
      "Validation loss: 17.865038871765137 RMSE: 4.226705\n",
      "Validation loss: 17.190585136413574 RMSE: 4.146153\n",
      "121 14 1.950415015220642\n",
      "Validation loss: 19.07777976989746 RMSE: 4.3678117\n",
      "Validation loss: 17.731072425842285 RMSE: 4.210828\n",
      "Validation loss: 18.083925247192383 RMSE: 4.25252\n",
      "Validation loss: 14.188544750213623 RMSE: 3.7667685\n",
      "125 0 1.3594343662261963\n",
      "Validation loss: 17.789238452911377 RMSE: 4.217729\n",
      "Validation loss: 19.610017776489258 RMSE: 4.42832\n",
      "Validation loss: 16.595417976379395 RMSE: 4.073747\n",
      "128 2 2.56359601020813\n",
      "Validation loss: 20.73314094543457 RMSE: 4.553366\n",
      "Validation loss: 18.173112869262695 RMSE: 4.2629933\n",
      "Validation loss: 18.932528495788574 RMSE: 4.3511524\n",
      "131 4 1.713088035583496\n",
      "Validation loss: 18.251744270324707 RMSE: 4.2722063\n",
      "Validation loss: 17.275749683380127 RMSE: 4.1564107\n",
      "Validation loss: 21.511481285095215 RMSE: 4.638047\n",
      "134 6 2.9806206226348877\n",
      "Validation loss: 18.0107364654541 RMSE: 4.243906\n",
      "Validation loss: 15.9191312789917 RMSE: 3.989879\n",
      "Validation loss: 18.373918533325195 RMSE: 4.286481\n",
      "137 8 2.2457339763641357\n",
      "Validation loss: 16.74803400039673 RMSE: 4.0924363\n",
      "Validation loss: 17.29892635345459 RMSE: 4.159198\n",
      "Validation loss: 17.68351125717163 RMSE: 4.205177\n",
      "140 10 1.4299829006195068\n",
      "Validation loss: 18.841086387634277 RMSE: 4.340632\n",
      "Validation loss: 18.861103057861328 RMSE: 4.342937\n",
      "Validation loss: 18.467618942260742 RMSE: 4.2973967\n",
      "143 12 2.1190407276153564\n",
      "Validation loss: 17.111302375793457 RMSE: 4.136581\n",
      "Validation loss: 18.66129970550537 RMSE: 4.3198724\n",
      "Validation loss: 17.83674430847168 RMSE: 4.223357\n",
      "146 14 1.427722692489624\n",
      "Validation loss: 17.207656860351562 RMSE: 4.1482115\n",
      "Validation loss: 21.07792377471924 RMSE: 4.59107\n",
      "Validation loss: 19.134718894958496 RMSE: 4.374325\n",
      "Validation loss: 15.982222557067871 RMSE: 3.9977772\n",
      "150 0 1.1457332372665405\n",
      "Validation loss: 21.2012996673584 RMSE: 4.604487\n",
      "Validation loss: 16.843729972839355 RMSE: 4.104111\n",
      "Validation loss: 17.239668369293213 RMSE: 4.152068\n",
      "153 2 3.4403154850006104\n",
      "Validation loss: 17.923995971679688 RMSE: 4.233674\n",
      "Validation loss: 18.501325607299805 RMSE: 4.3013167\n",
      "Validation loss: 17.62917137145996 RMSE: 4.1987104\n",
      "156 4 1.4780272245407104\n",
      "Validation loss: 17.98846673965454 RMSE: 4.241281\n",
      "Validation loss: 20.25899314880371 RMSE: 4.5009995\n",
      "Validation loss: 21.575489044189453 RMSE: 4.6449423\n",
      "159 6 2.080706834793091\n",
      "Validation loss: 23.475645065307617 RMSE: 4.845167\n",
      "Validation loss: 19.18631362915039 RMSE: 4.3802185\n",
      "Validation loss: 18.156829833984375 RMSE: 4.261083\n",
      "162 8 2.3284494876861572\n",
      "Validation loss: 16.432852745056152 RMSE: 4.0537453\n",
      "Validation loss: 19.354591369628906 RMSE: 4.3993855\n",
      "Validation loss: 18.543426036834717 RMSE: 4.3062077\n",
      "165 10 1.4767775535583496\n",
      "Validation loss: 20.64064121246338 RMSE: 4.543197\n",
      "Validation loss: 18.631291389465332 RMSE: 4.3163977\n",
      "Validation loss: 17.639626502990723 RMSE: 4.1999555\n",
      "168 12 1.7745493650436401\n",
      "Validation loss: 18.037411212921143 RMSE: 4.2470474\n",
      "Validation loss: 24.34391498565674 RMSE: 4.933955\n",
      "Validation loss: 19.826266288757324 RMSE: 4.4526696\n",
      "171 14 2.809218168258667\n",
      "Validation loss: 20.848063468933105 RMSE: 4.565968\n",
      "Validation loss: 17.855666160583496 RMSE: 4.2255964\n",
      "Validation loss: 19.864660263061523 RMSE: 4.456979\n",
      "Validation loss: 17.525931358337402 RMSE: 4.1863985\n",
      "175 0 2.7481861114501953\n",
      "Validation loss: 22.90618133544922 RMSE: 4.7860403\n",
      "Validation loss: 15.611702919006348 RMSE: 3.9511647\n",
      "Validation loss: 16.97070026397705 RMSE: 4.1195507\n",
      "178 2 2.51078200340271\n",
      "Validation loss: 19.299257278442383 RMSE: 4.393092\n",
      "Validation loss: 17.101163864135742 RMSE: 4.1353555\n",
      "Validation loss: 18.144248008728027 RMSE: 4.259607\n",
      "181 4 1.1957740783691406\n",
      "Validation loss: 18.975231170654297 RMSE: 4.3560567\n",
      "Validation loss: 15.707030296325684 RMSE: 3.9632096\n",
      "Validation loss: 23.054970741271973 RMSE: 4.801559\n",
      "184 6 2.291348457336426\n",
      "Validation loss: 15.112464904785156 RMSE: 3.8874755\n",
      "Validation loss: 21.582880973815918 RMSE: 4.645738\n",
      "Validation loss: 25.16065216064453 RMSE: 5.0160394\n",
      "187 8 2.017887830734253\n",
      "Validation loss: 18.152287483215332 RMSE: 4.26055\n",
      "Validation loss: 20.10045623779297 RMSE: 4.483353\n",
      "Validation loss: 19.266559600830078 RMSE: 4.389369\n",
      "190 10 1.661246657371521\n",
      "Validation loss: 23.61006259918213 RMSE: 4.859019\n",
      "Validation loss: 16.77934741973877 RMSE: 4.09626\n",
      "Validation loss: 19.999969482421875 RMSE: 4.4721327\n",
      "193 12 1.9530020952224731\n",
      "Validation loss: 16.80448627471924 RMSE: 4.0993276\n",
      "Validation loss: 19.988773345947266 RMSE: 4.4708805\n",
      "Validation loss: 24.781607627868652 RMSE: 4.978113\n",
      "196 14 1.948118805885315\n",
      "Validation loss: 23.488834381103516 RMSE: 4.846528\n",
      "Validation loss: 19.309630393981934 RMSE: 4.3942723\n",
      "Validation loss: 20.59526538848877 RMSE: 4.5382004\n",
      "Validation loss: 22.08751916885376 RMSE: 4.699736\n",
      "Loaded trained model with success.\n",
      "Test loss: 13.312103432875412 Test RMSE: 3.6485755\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:0', 'task_name': 'freesolv', 'model_type': 'gin', 'model': {'num_layer': 3, 'emb_dim': 64, 'feat_dim': 64, 'drop_ratio': 0.5, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 779, 'task': 'regression', 'data_path': 'data/freesolv/freesolv.csv', 'target': ['expt']}}\n",
      "Running on: cuda:0\n",
      "641\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/641\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 25.594799041748047\n",
      "Validation loss: 72.09231567382812 RMSE: 8.49072\n",
      "Validation loss: 66.25250625610352 RMSE: 8.139564\n",
      "Validation loss: 58.27390480041504 RMSE: 7.6337347\n",
      "3 2 13.757444381713867\n",
      "Validation loss: 48.18612289428711 RMSE: 6.9416227\n",
      "Validation loss: 32.021846771240234 RMSE: 5.658785\n",
      "Validation loss: 19.885674476623535 RMSE: 4.459336\n",
      "6 4 7.84401798248291\n",
      "Validation loss: 14.766477108001709 RMSE: 3.8427174\n",
      "Validation loss: 17.62848472595215 RMSE: 4.198629\n",
      "Validation loss: 14.22976303100586 RMSE: 3.7722359\n",
      "9 6 5.090030193328857\n",
      "Validation loss: 12.343306064605713 RMSE: 3.513304\n",
      "Validation loss: 14.390365600585938 RMSE: 3.7934632\n",
      "Validation loss: 13.131308555603027 RMSE: 3.6237147\n",
      "12 8 6.130232334136963\n",
      "Validation loss: 15.481908798217773 RMSE: 3.9347057\n",
      "Validation loss: 14.552437782287598 RMSE: 3.8147657\n",
      "Validation loss: 14.550882339477539 RMSE: 3.8145616\n",
      "15 10 3.6715168952941895\n",
      "Validation loss: 13.200555801391602 RMSE: 3.6332572\n",
      "Validation loss: 11.293691635131836 RMSE: 3.360609\n",
      "Validation loss: 13.461721897125244 RMSE: 3.6690218\n",
      "18 12 4.600168704986572\n",
      "Validation loss: 13.334278583526611 RMSE: 3.651613\n",
      "Validation loss: 12.640996932983398 RMSE: 3.5554183\n",
      "Validation loss: 13.184962272644043 RMSE: 3.6311104\n",
      "21 14 3.735121250152588\n",
      "Validation loss: 13.077543258666992 RMSE: 3.6162887\n",
      "Validation loss: 13.520943641662598 RMSE: 3.6770837\n",
      "Validation loss: 14.082987785339355 RMSE: 3.7527308\n",
      "Validation loss: 14.085487842559814 RMSE: 3.753064\n",
      "25 0 3.0353903770446777\n",
      "Validation loss: 15.77650785446167 RMSE: 3.9719653\n",
      "Validation loss: 17.28047275543213 RMSE: 4.1569786\n",
      "Validation loss: 15.385218143463135 RMSE: 3.9223993\n",
      "28 2 3.2291598320007324\n",
      "Validation loss: 15.639126777648926 RMSE: 3.9546337\n",
      "Validation loss: 14.593392372131348 RMSE: 3.82013\n",
      "Validation loss: 17.22609567642212 RMSE: 4.150433\n",
      "31 4 1.7063628435134888\n",
      "Validation loss: 16.624140739440918 RMSE: 4.077271\n",
      "Validation loss: 18.259498119354248 RMSE: 4.2731133\n",
      "Validation loss: 15.031328201293945 RMSE: 3.8770256\n",
      "34 6 2.6218390464782715\n",
      "Validation loss: 17.11830425262451 RMSE: 4.137427\n",
      "Validation loss: 19.233922958374023 RMSE: 4.3856497\n",
      "Validation loss: 16.9979829788208 RMSE: 4.122861\n",
      "37 8 2.5038375854492188\n",
      "Validation loss: 14.762496948242188 RMSE: 3.8421996\n",
      "Validation loss: 13.234886169433594 RMSE: 3.6379783\n",
      "Validation loss: 14.008732795715332 RMSE: 3.7428238\n",
      "40 10 3.1160805225372314\n",
      "Validation loss: 13.395968437194824 RMSE: 3.6600502\n",
      "Validation loss: 13.076920509338379 RMSE: 3.6162026\n",
      "Validation loss: 13.262296676635742 RMSE: 3.6417437\n",
      "43 12 5.674530982971191\n",
      "Validation loss: 16.83442497253418 RMSE: 4.1029773\n",
      "Validation loss: 14.45858907699585 RMSE: 3.8024454\n",
      "Validation loss: 11.968676567077637 RMSE: 3.4595776\n",
      "46 14 3.281719446182251\n",
      "Validation loss: 15.799678802490234 RMSE: 3.974881\n",
      "Validation loss: 14.635723114013672 RMSE: 3.8256667\n",
      "Validation loss: 15.522788047790527 RMSE: 3.9398968\n",
      "Validation loss: 16.162543773651123 RMSE: 4.0202665\n",
      "50 0 3.5343127250671387\n",
      "Validation loss: 15.288685321807861 RMSE: 3.910075\n",
      "Validation loss: 13.798784732818604 RMSE: 3.7146719\n",
      "Validation loss: 13.666167259216309 RMSE: 3.6967783\n",
      "53 2 1.9770636558532715\n",
      "Validation loss: 15.429525375366211 RMSE: 3.9280436\n",
      "Validation loss: 16.109333038330078 RMSE: 4.0136433\n",
      "Validation loss: 12.669984340667725 RMSE: 3.5594919\n",
      "56 4 3.020200729370117\n",
      "Validation loss: 11.903962135314941 RMSE: 3.4502118\n",
      "Validation loss: 11.944151401519775 RMSE: 3.4560313\n",
      "Validation loss: 12.311467170715332 RMSE: 3.50877\n",
      "59 6 2.2855637073516846\n",
      "Validation loss: 15.277933120727539 RMSE: 3.9086998\n",
      "Validation loss: 13.501357078552246 RMSE: 3.6744194\n",
      "Validation loss: 12.88409423828125 RMSE: 3.589442\n",
      "62 8 4.396273136138916\n",
      "Validation loss: 14.721323013305664 RMSE: 3.8368378\n",
      "Validation loss: 12.756543397903442 RMSE: 3.5716305\n",
      "Validation loss: 14.588359355926514 RMSE: 3.8194711\n",
      "65 10 2.336728572845459\n",
      "Validation loss: 10.958462238311768 RMSE: 3.3103569\n",
      "Validation loss: 13.11781120300293 RMSE: 3.621852\n",
      "Validation loss: 14.414607048034668 RMSE: 3.7966576\n",
      "68 12 3.062828302383423\n",
      "Validation loss: 11.541993141174316 RMSE: 3.397351\n",
      "Validation loss: 13.785731792449951 RMSE: 3.712914\n",
      "Validation loss: 13.0703763961792 RMSE: 3.6152978\n",
      "71 14 3.17891001701355\n",
      "Validation loss: 12.471315383911133 RMSE: 3.531475\n",
      "Validation loss: 11.754141807556152 RMSE: 3.4284313\n",
      "Validation loss: 11.278167724609375 RMSE: 3.3582985\n",
      "Validation loss: 12.4930739402771 RMSE: 3.5345542\n",
      "75 0 2.5603482723236084\n",
      "Validation loss: 10.801154136657715 RMSE: 3.286511\n",
      "Validation loss: 11.626416683197021 RMSE: 3.409753\n",
      "Validation loss: 11.478271245956421 RMSE: 3.3879597\n",
      "78 2 3.7914867401123047\n",
      "Validation loss: 11.455623626708984 RMSE: 3.3846157\n",
      "Validation loss: 10.637775421142578 RMSE: 3.2615604\n",
      "Validation loss: 10.864487171173096 RMSE: 3.296132\n",
      "81 4 3.12916898727417\n",
      "Validation loss: 12.730350017547607 RMSE: 3.5679615\n",
      "Validation loss: 11.133076190948486 RMSE: 3.3366263\n",
      "Validation loss: 14.965944766998291 RMSE: 3.8685842\n",
      "84 6 2.8630056381225586\n",
      "Validation loss: 12.426227569580078 RMSE: 3.5250854\n",
      "Validation loss: 11.785633087158203 RMSE: 3.433021\n",
      "Validation loss: 12.653763771057129 RMSE: 3.5572128\n",
      "87 8 2.438075542449951\n",
      "Validation loss: 14.004358291625977 RMSE: 3.74224\n",
      "Validation loss: 12.214014053344727 RMSE: 3.4948554\n",
      "Validation loss: 11.78705358505249 RMSE: 3.4332278\n",
      "90 10 2.1962432861328125\n",
      "Validation loss: 11.613656997680664 RMSE: 3.4078817\n",
      "Validation loss: 12.752636909484863 RMSE: 3.5710833\n",
      "Validation loss: 15.066385746002197 RMSE: 3.881544\n",
      "93 12 4.429516792297363\n",
      "Validation loss: 12.903411865234375 RMSE: 3.5921319\n",
      "Validation loss: 11.876815795898438 RMSE: 3.4462755\n",
      "Validation loss: 11.00537633895874 RMSE: 3.317435\n",
      "96 14 1.8381876945495605\n",
      "Validation loss: 11.076823711395264 RMSE: 3.3281863\n",
      "Validation loss: 12.327035427093506 RMSE: 3.510988\n",
      "Validation loss: 11.725306987762451 RMSE: 3.4242237\n",
      "Validation loss: 11.31521463394165 RMSE: 3.3638096\n",
      "100 0 6.918654441833496\n",
      "Validation loss: 9.907795906066895 RMSE: 3.1476653\n",
      "Validation loss: 10.432677745819092 RMSE: 3.2299654\n",
      "Validation loss: 10.006510972976685 RMSE: 3.1633072\n",
      "103 2 4.26997709274292\n",
      "Validation loss: 11.60356855392456 RMSE: 3.4064012\n",
      "Validation loss: 9.879863739013672 RMSE: 3.143225\n",
      "Validation loss: 11.251795768737793 RMSE: 3.3543699\n",
      "106 4 5.162434101104736\n",
      "Validation loss: 11.588490009307861 RMSE: 3.4041872\n",
      "Validation loss: 11.33890676498413 RMSE: 3.3673294\n",
      "Validation loss: 10.880665302276611 RMSE: 3.2985852\n",
      "109 6 2.1951749324798584\n",
      "Validation loss: 10.753161430358887 RMSE: 3.2792013\n",
      "Validation loss: 12.544294834136963 RMSE: 3.5417926\n",
      "Validation loss: 9.969505310058594 RMSE: 3.1574526\n",
      "112 8 2.4525938034057617\n",
      "Validation loss: 10.582408428192139 RMSE: 3.2530615\n",
      "Validation loss: 11.520095348358154 RMSE: 3.394127\n",
      "Validation loss: 12.319879531860352 RMSE: 3.5099685\n",
      "115 10 1.877767562866211\n",
      "Validation loss: 10.999082565307617 RMSE: 3.3164866\n",
      "Validation loss: 10.644487857818604 RMSE: 3.262589\n",
      "Validation loss: 10.291573762893677 RMSE: 3.2080483\n",
      "118 12 1.7881391048431396\n",
      "Validation loss: 10.77755069732666 RMSE: 3.282918\n",
      "Validation loss: 12.381296157836914 RMSE: 3.5187066\n",
      "Validation loss: 10.601383686065674 RMSE: 3.2559764\n",
      "121 14 1.8943946361541748\n",
      "Validation loss: 11.058001041412354 RMSE: 3.3253572\n",
      "Validation loss: 8.213264226913452 RMSE: 2.8658793\n",
      "Validation loss: 9.302233219146729 RMSE: 3.049956\n",
      "Validation loss: 9.177209854125977 RMSE: 3.029391\n",
      "125 0 2.8031411170959473\n",
      "Validation loss: 10.91256070137024 RMSE: 3.3034165\n",
      "Validation loss: 10.036520957946777 RMSE: 3.1680467\n",
      "Validation loss: 9.066073179244995 RMSE: 3.010992\n",
      "128 2 2.0694596767425537\n",
      "Validation loss: 14.891324043273926 RMSE: 3.8589272\n",
      "Validation loss: 13.421561479568481 RMSE: 3.6635447\n",
      "Validation loss: 12.076438903808594 RMSE: 3.4751172\n",
      "131 4 3.2908294200897217\n",
      "Validation loss: 10.14597749710083 RMSE: 3.1852753\n",
      "Validation loss: 11.95606517791748 RMSE: 3.4577544\n",
      "Validation loss: 13.224198341369629 RMSE: 3.636509\n",
      "134 6 1.4944111108779907\n",
      "Validation loss: 14.063344478607178 RMSE: 3.750113\n",
      "Validation loss: 11.023579120635986 RMSE: 3.3201776\n",
      "Validation loss: 12.031491756439209 RMSE: 3.468644\n",
      "137 8 5.330763339996338\n",
      "Validation loss: 11.352291584014893 RMSE: 3.3693159\n",
      "Validation loss: 10.45367956161499 RMSE: 3.233215\n",
      "Validation loss: 12.779619216918945 RMSE: 3.5748591\n",
      "140 10 0.8812143802642822\n",
      "Validation loss: 11.375544548034668 RMSE: 3.3727653\n",
      "Validation loss: 11.664266586303711 RMSE: 3.415299\n",
      "Validation loss: 15.755606651306152 RMSE: 3.969333\n",
      "143 12 3.083909273147583\n",
      "Validation loss: 11.304507732391357 RMSE: 3.3622177\n",
      "Validation loss: 11.076502323150635 RMSE: 3.3281379\n",
      "Validation loss: 12.1568284034729 RMSE: 3.4866645\n",
      "146 14 3.101963996887207\n",
      "Validation loss: 13.32580280303955 RMSE: 3.6504526\n",
      "Validation loss: 13.69098711013794 RMSE: 3.700133\n",
      "Validation loss: 11.180161476135254 RMSE: 3.3436747\n",
      "Validation loss: 11.222232818603516 RMSE: 3.3499603\n",
      "150 0 1.3323665857315063\n",
      "Validation loss: 10.333234310150146 RMSE: 3.214535\n",
      "Validation loss: 10.217477798461914 RMSE: 3.1964788\n",
      "Validation loss: 13.215609550476074 RMSE: 3.635328\n",
      "153 2 2.119079113006592\n",
      "Validation loss: 10.766644477844238 RMSE: 3.2812567\n",
      "Validation loss: 9.230499267578125 RMSE: 3.0381737\n",
      "Validation loss: 11.061919212341309 RMSE: 3.3259466\n",
      "156 4 1.3474256992340088\n",
      "Validation loss: 11.406308650970459 RMSE: 3.3773227\n",
      "Validation loss: 11.136239290237427 RMSE: 3.3371005\n",
      "Validation loss: 11.515992164611816 RMSE: 3.393522\n",
      "159 6 1.7404989004135132\n",
      "Validation loss: 12.848736763000488 RMSE: 3.5845137\n",
      "Validation loss: 12.287148475646973 RMSE: 3.5053027\n",
      "Validation loss: 12.006399154663086 RMSE: 3.465025\n",
      "162 8 1.8671234846115112\n",
      "Validation loss: 11.119643688201904 RMSE: 3.3346128\n",
      "Validation loss: 10.25551176071167 RMSE: 3.2024229\n",
      "Validation loss: 11.927793979644775 RMSE: 3.453664\n",
      "165 10 1.7827448844909668\n",
      "Validation loss: 10.057277202606201 RMSE: 3.171321\n",
      "Validation loss: 9.694464683532715 RMSE: 3.1135936\n",
      "Validation loss: 9.897557973861694 RMSE: 3.1460383\n",
      "168 12 2.487687349319458\n",
      "Validation loss: 13.157390594482422 RMSE: 3.6273117\n",
      "Validation loss: 10.899205684661865 RMSE: 3.3013947\n",
      "Validation loss: 12.646475791931152 RMSE: 3.5561883\n",
      "171 14 1.469394326210022\n",
      "Validation loss: 12.849332332611084 RMSE: 3.5845966\n",
      "Validation loss: 12.061071395874023 RMSE: 3.4729054\n",
      "Validation loss: 11.559700965881348 RMSE: 3.3999565\n",
      "Validation loss: 10.00216817855835 RMSE: 3.1626205\n",
      "175 0 0.9638035893440247\n",
      "Validation loss: 11.995609283447266 RMSE: 3.4634678\n",
      "Validation loss: 13.384610176086426 RMSE: 3.6584985\n",
      "Validation loss: 11.561594009399414 RMSE: 3.4002345\n",
      "178 2 2.864607095718384\n",
      "Validation loss: 13.621935844421387 RMSE: 3.6907902\n",
      "Validation loss: 11.81108045578003 RMSE: 3.4367254\n",
      "Validation loss: 11.26634168624878 RMSE: 3.3565373\n",
      "181 4 2.221299886703491\n",
      "Validation loss: 11.863197326660156 RMSE: 3.4442995\n",
      "Validation loss: 11.062262535095215 RMSE: 3.325998\n",
      "Validation loss: 12.47910213470459 RMSE: 3.5325775\n",
      "184 6 1.6820448637008667\n",
      "Validation loss: 13.07456111907959 RMSE: 3.6158762\n",
      "Validation loss: 12.221150875091553 RMSE: 3.495876\n",
      "Validation loss: 12.686707496643066 RMSE: 3.5618405\n",
      "187 8 0.9268606901168823\n",
      "Validation loss: 10.483589887619019 RMSE: 3.2378373\n",
      "Validation loss: 10.547866344451904 RMSE: 3.247748\n",
      "Validation loss: 10.89119815826416 RMSE: 3.3001816\n",
      "190 10 1.5942000150680542\n",
      "Validation loss: 12.446281909942627 RMSE: 3.527929\n",
      "Validation loss: 12.068142890930176 RMSE: 3.4739232\n",
      "Validation loss: 11.39426326751709 RMSE: 3.375539\n",
      "193 12 1.1961623430252075\n",
      "Validation loss: 10.409526348114014 RMSE: 3.2263799\n",
      "Validation loss: 13.706621170043945 RMSE: 3.7022455\n",
      "Validation loss: 16.364669799804688 RMSE: 4.045326\n",
      "196 14 3.029444456100464\n",
      "Validation loss: 17.050976753234863 RMSE: 4.129283\n",
      "Validation loss: 15.568021774291992 RMSE: 3.945633\n",
      "Validation loss: 12.733533382415771 RMSE: 3.5684075\n",
      "Validation loss: 12.64376950263977 RMSE: 3.5558076\n",
      "Loaded trained model with success.\n",
      "Test loss: 6.993490776648888 Test RMSE: 2.644521\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:0', 'task_name': 'freesolv', 'model_type': 'gin', 'model': {'num_layer': 3, 'emb_dim': 64, 'feat_dim': 64, 'drop_ratio': 0.5, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 780, 'task': 'regression', 'data_path': 'data/freesolv/freesolv.csv', 'target': ['expt']}}\n",
      "Running on: cuda:0\n",
      "641\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/641\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 14.380531311035156\n",
      "Validation loss: 68.18072128295898 RMSE: 8.257161\n",
      "Validation loss: 62.154991149902344 RMSE: 7.883844\n",
      "Validation loss: 55.169498443603516 RMSE: 7.427617\n",
      "3 2 15.651215553283691\n",
      "Validation loss: 44.62763023376465 RMSE: 6.680392\n",
      "Validation loss: 32.034379959106445 RMSE: 5.659892\n",
      "Validation loss: 17.80509662628174 RMSE: 4.2196083\n",
      "6 4 6.604212760925293\n",
      "Validation loss: 18.9783878326416 RMSE: 4.356419\n",
      "Validation loss: 20.507080078125 RMSE: 4.5284743\n",
      "Validation loss: 20.287178993225098 RMSE: 4.504129\n",
      "9 6 12.248213768005371\n",
      "Validation loss: 16.481290817260742 RMSE: 4.0597157\n",
      "Validation loss: 18.169686794281006 RMSE: 4.2625914\n",
      "Validation loss: 20.561355590820312 RMSE: 4.534463\n",
      "12 8 5.626760959625244\n",
      "Validation loss: 18.21235179901123 RMSE: 4.2675934\n",
      "Validation loss: 20.02978801727295 RMSE: 4.4754653\n",
      "Validation loss: 16.06655502319336 RMSE: 4.0083103\n",
      "15 10 5.7204670906066895\n",
      "Validation loss: 17.246631622314453 RMSE: 4.1529064\n",
      "Validation loss: 17.899218559265137 RMSE: 4.2307467\n",
      "Validation loss: 18.187950134277344 RMSE: 4.2647333\n",
      "18 12 2.935084342956543\n",
      "Validation loss: 16.126327514648438 RMSE: 4.01576\n",
      "Validation loss: 20.18647003173828 RMSE: 4.4929357\n",
      "Validation loss: 16.583369731903076 RMSE: 4.072268\n",
      "21 14 5.349854469299316\n",
      "Validation loss: 15.98157024383545 RMSE: 3.9976957\n",
      "Validation loss: 16.38014316558838 RMSE: 4.0472393\n",
      "Validation loss: 16.219999313354492 RMSE: 4.0274057\n",
      "Validation loss: 20.63207244873047 RMSE: 4.542254\n",
      "25 0 2.558018445968628\n",
      "Validation loss: 17.8994083404541 RMSE: 4.230769\n",
      "Validation loss: 19.60798454284668 RMSE: 4.42809\n",
      "Validation loss: 18.142942428588867 RMSE: 4.2594533\n",
      "28 2 4.348515033721924\n",
      "Validation loss: 23.02505874633789 RMSE: 4.7984433\n",
      "Validation loss: 17.987674713134766 RMSE: 4.241188\n",
      "Validation loss: 19.924389839172363 RMSE: 4.4636745\n",
      "31 4 3.5012457370758057\n",
      "Validation loss: 19.23440933227539 RMSE: 4.385705\n",
      "Validation loss: 19.104792594909668 RMSE: 4.370903\n",
      "Validation loss: 17.703176498413086 RMSE: 4.2075143\n",
      "34 6 2.2451303005218506\n",
      "Validation loss: 20.066207885742188 RMSE: 4.4795322\n",
      "Validation loss: 16.710200309753418 RMSE: 4.087811\n",
      "Validation loss: 17.883532524108887 RMSE: 4.2288923\n",
      "37 8 3.85103178024292\n",
      "Validation loss: 16.04568338394165 RMSE: 4.0057063\n",
      "Validation loss: 16.906139373779297 RMSE: 4.111707\n",
      "Validation loss: 17.754834175109863 RMSE: 4.2136483\n",
      "40 10 3.2534003257751465\n",
      "Validation loss: 18.340134620666504 RMSE: 4.2825384\n",
      "Validation loss: 19.30962038040161 RMSE: 4.3942714\n",
      "Validation loss: 18.529105186462402 RMSE: 4.3045444\n",
      "43 12 4.901970863342285\n",
      "Validation loss: 16.64673948287964 RMSE: 4.080042\n",
      "Validation loss: 20.550986289978027 RMSE: 4.5333195\n",
      "Validation loss: 15.607494831085205 RMSE: 3.950632\n",
      "46 14 2.6864888668060303\n",
      "Validation loss: 18.786845207214355 RMSE: 4.3343797\n",
      "Validation loss: 17.52131748199463 RMSE: 4.1858473\n",
      "Validation loss: 18.731595993041992 RMSE: 4.328001\n",
      "Validation loss: 17.96615982055664 RMSE: 4.238651\n",
      "50 0 5.341622829437256\n",
      "Validation loss: 15.018949508666992 RMSE: 3.8754287\n",
      "Validation loss: 18.79732656478882 RMSE: 4.3355885\n",
      "Validation loss: 18.478053092956543 RMSE: 4.29861\n",
      "53 2 4.80063533782959\n",
      "Validation loss: 15.825626850128174 RMSE: 3.9781437\n",
      "Validation loss: 17.215681076049805 RMSE: 4.1491785\n",
      "Validation loss: 19.131229400634766 RMSE: 4.373926\n",
      "56 4 2.8817949295043945\n",
      "Validation loss: 16.45377779006958 RMSE: 4.056326\n",
      "Validation loss: 19.406024932861328 RMSE: 4.405227\n",
      "Validation loss: 15.661969184875488 RMSE: 3.9575205\n",
      "59 6 2.1933302879333496\n",
      "Validation loss: 17.438735961914062 RMSE: 4.175971\n",
      "Validation loss: 12.953465938568115 RMSE: 3.5990925\n",
      "Validation loss: 17.1976580619812 RMSE: 4.1470056\n",
      "62 8 2.267460346221924\n",
      "Validation loss: 15.949681282043457 RMSE: 3.993705\n",
      "Validation loss: 18.258375644683838 RMSE: 4.272982\n",
      "Validation loss: 20.49651527404785 RMSE: 4.5273075\n",
      "65 10 3.4937055110931396\n",
      "Validation loss: 17.507369995117188 RMSE: 4.1841807\n",
      "Validation loss: 13.054373741149902 RMSE: 3.6130836\n",
      "Validation loss: 17.360369205474854 RMSE: 4.166578\n",
      "68 12 1.977283239364624\n",
      "Validation loss: 16.317718029022217 RMSE: 4.03952\n",
      "Validation loss: 18.53538227081299 RMSE: 4.3052735\n",
      "Validation loss: 20.40557289123535 RMSE: 4.517253\n",
      "71 14 2.611920118331909\n",
      "Validation loss: 15.863600730895996 RMSE: 3.9829135\n",
      "Validation loss: 17.135692596435547 RMSE: 4.1395283\n",
      "Validation loss: 14.813265800476074 RMSE: 3.8488007\n",
      "Validation loss: 18.440436363220215 RMSE: 4.294233\n",
      "75 0 1.8771896362304688\n",
      "Validation loss: 16.797195434570312 RMSE: 4.0984383\n",
      "Validation loss: 17.445730209350586 RMSE: 4.176809\n",
      "Validation loss: 20.78763198852539 RMSE: 4.5593452\n",
      "78 2 2.169363021850586\n",
      "Validation loss: 17.978017330169678 RMSE: 4.240049\n",
      "Validation loss: 19.35875415802002 RMSE: 4.3998585\n",
      "Validation loss: 18.913901329040527 RMSE: 4.3490114\n",
      "81 4 2.0624372959136963\n",
      "Validation loss: 19.557220458984375 RMSE: 4.4223547\n",
      "Validation loss: 16.98308229446411 RMSE: 4.1210537\n",
      "Validation loss: 17.37854766845703 RMSE: 4.1687584\n",
      "84 6 1.6082323789596558\n",
      "Validation loss: 16.385093688964844 RMSE: 4.0478506\n",
      "Validation loss: 21.0258150100708 RMSE: 4.5853915\n",
      "Validation loss: 21.73908805847168 RMSE: 4.6625195\n",
      "87 8 3.529940366744995\n",
      "Validation loss: 21.319005966186523 RMSE: 4.617251\n",
      "Validation loss: 18.819602966308594 RMSE: 4.338156\n",
      "Validation loss: 16.89639949798584 RMSE: 4.1105227\n",
      "90 10 1.7962594032287598\n",
      "Validation loss: 21.482526779174805 RMSE: 4.6349244\n",
      "Validation loss: 19.169301986694336 RMSE: 4.3782763\n",
      "Validation loss: 19.56895637512207 RMSE: 4.4236813\n",
      "93 12 3.392590284347534\n",
      "Validation loss: 19.44534683227539 RMSE: 4.409688\n",
      "Validation loss: 22.96542263031006 RMSE: 4.792225\n",
      "Validation loss: 22.61923885345459 RMSE: 4.755969\n",
      "96 14 1.1163215637207031\n",
      "Validation loss: 20.25637912750244 RMSE: 4.5007086\n",
      "Validation loss: 18.65500545501709 RMSE: 4.3191442\n",
      "Validation loss: 19.157686233520508 RMSE: 4.3769493\n",
      "Validation loss: 21.905360221862793 RMSE: 4.6803164\n",
      "100 0 1.2474563121795654\n",
      "Validation loss: 20.31317901611328 RMSE: 4.5070148\n",
      "Validation loss: 22.396974563598633 RMSE: 4.732544\n",
      "Validation loss: 22.842185020446777 RMSE: 4.77935\n",
      "103 2 1.4008058309555054\n",
      "Validation loss: 17.41310214996338 RMSE: 4.172901\n",
      "Validation loss: 20.62465476989746 RMSE: 4.5414376\n",
      "Validation loss: 21.610032081604004 RMSE: 4.648659\n",
      "106 4 0.9423181414604187\n",
      "Validation loss: 19.114060401916504 RMSE: 4.371963\n",
      "Validation loss: 22.940818786621094 RMSE: 4.7896576\n",
      "Validation loss: 19.749855995178223 RMSE: 4.444081\n",
      "109 6 2.651172637939453\n",
      "Validation loss: 22.04652690887451 RMSE: 4.695373\n",
      "Validation loss: 20.013108253479004 RMSE: 4.4736013\n",
      "Validation loss: 20.19422721862793 RMSE: 4.4937987\n",
      "112 8 1.5276850461959839\n",
      "Validation loss: 20.62063694000244 RMSE: 4.540995\n",
      "Validation loss: 17.68079376220703 RMSE: 4.2048535\n",
      "Validation loss: 20.125590324401855 RMSE: 4.4861555\n",
      "115 10 1.6999136209487915\n",
      "Validation loss: 21.43900775909424 RMSE: 4.6302276\n",
      "Validation loss: 18.17482614517212 RMSE: 4.263194\n",
      "Validation loss: 13.851301193237305 RMSE: 3.7217336\n",
      "118 12 1.2238152027130127\n",
      "Validation loss: 18.736549377441406 RMSE: 4.3285737\n",
      "Validation loss: 17.881277084350586 RMSE: 4.228626\n",
      "Validation loss: 18.197325706481934 RMSE: 4.2658324\n",
      "121 14 1.7933275699615479\n",
      "Validation loss: 15.732185363769531 RMSE: 3.9663818\n",
      "Validation loss: 18.576258659362793 RMSE: 4.3100185\n",
      "Validation loss: 21.220452308654785 RMSE: 4.606566\n",
      "Validation loss: 19.063369274139404 RMSE: 4.366162\n",
      "125 0 1.4469646215438843\n",
      "Validation loss: 21.056365966796875 RMSE: 4.5887218\n",
      "Validation loss: 18.046624183654785 RMSE: 4.2481318\n",
      "Validation loss: 18.44749164581299 RMSE: 4.2950544\n",
      "128 2 1.8038899898529053\n",
      "Validation loss: 18.782384872436523 RMSE: 4.3338647\n",
      "Validation loss: 22.462539672851562 RMSE: 4.739466\n",
      "Validation loss: 20.99342918395996 RMSE: 4.5818586\n",
      "131 4 1.2822867631912231\n",
      "Validation loss: 19.060588359832764 RMSE: 4.3658433\n",
      "Validation loss: 21.75651264190674 RMSE: 4.6643877\n",
      "Validation loss: 18.932050704956055 RMSE: 4.3510976\n",
      "134 6 2.1680867671966553\n",
      "Validation loss: 19.175783157348633 RMSE: 4.379016\n",
      "Validation loss: 20.40130043029785 RMSE: 4.51678\n",
      "Validation loss: 20.51372718811035 RMSE: 4.529208\n",
      "137 8 3.8862783908843994\n",
      "Validation loss: 22.470561981201172 RMSE: 4.7403126\n",
      "Validation loss: 19.90148162841797 RMSE: 4.4611077\n",
      "Validation loss: 21.163941383361816 RMSE: 4.6004286\n",
      "140 10 2.8071959018707275\n",
      "Validation loss: 19.552833557128906 RMSE: 4.4218583\n",
      "Validation loss: 23.192270278930664 RMSE: 4.8158355\n",
      "Validation loss: 18.157179832458496 RMSE: 4.261124\n",
      "143 12 1.7124145030975342\n",
      "Validation loss: 16.600360870361328 RMSE: 4.074354\n",
      "Validation loss: 16.632922172546387 RMSE: 4.078348\n",
      "Validation loss: 16.934602737426758 RMSE: 4.1151676\n",
      "146 14 1.0388258695602417\n",
      "Validation loss: 16.159889221191406 RMSE: 4.0199366\n",
      "Validation loss: 15.169129371643066 RMSE: 3.8947566\n",
      "Validation loss: 18.58143901824951 RMSE: 4.3106194\n",
      "Validation loss: 19.049979209899902 RMSE: 4.3646283\n",
      "150 0 1.0238125324249268\n",
      "Validation loss: 17.812742233276367 RMSE: 4.220515\n",
      "Validation loss: 19.66976261138916 RMSE: 4.4350605\n",
      "Validation loss: 19.761052131652832 RMSE: 4.4453406\n",
      "153 2 1.4921408891677856\n",
      "Validation loss: 18.78057098388672 RMSE: 4.333656\n",
      "Validation loss: 17.959935188293457 RMSE: 4.2379165\n",
      "Validation loss: 18.616395950317383 RMSE: 4.314672\n",
      "156 4 1.4218930006027222\n",
      "Validation loss: 20.53299903869629 RMSE: 4.5313354\n",
      "Validation loss: 17.508441925048828 RMSE: 4.184309\n",
      "Validation loss: 15.536934852600098 RMSE: 3.9416916\n",
      "159 6 3.683330774307251\n",
      "Validation loss: 17.417717933654785 RMSE: 4.173454\n",
      "Validation loss: 19.001750946044922 RMSE: 4.3591\n",
      "Validation loss: 20.52531337738037 RMSE: 4.530487\n",
      "162 8 1.779770016670227\n",
      "Validation loss: 18.123714447021484 RMSE: 4.2571955\n",
      "Validation loss: 19.63251304626465 RMSE: 4.430859\n",
      "Validation loss: 19.359177589416504 RMSE: 4.3999066\n",
      "165 10 1.2725577354431152\n",
      "Validation loss: 19.069958686828613 RMSE: 4.366916\n",
      "Validation loss: 17.982218265533447 RMSE: 4.2405443\n",
      "Validation loss: 20.819899559020996 RMSE: 4.562883\n",
      "168 12 1.7129459381103516\n",
      "Validation loss: 19.643242835998535 RMSE: 4.43207\n",
      "Validation loss: 16.195977210998535 RMSE: 4.0244226\n",
      "Validation loss: 17.519285202026367 RMSE: 4.1856046\n",
      "171 14 3.1577048301696777\n",
      "Validation loss: 19.19508647918701 RMSE: 4.38122\n",
      "Validation loss: 19.67229652404785 RMSE: 4.435346\n",
      "Validation loss: 16.19573211669922 RMSE: 4.024392\n",
      "Validation loss: 16.220898628234863 RMSE: 4.027518\n",
      "175 0 2.8427913188934326\n",
      "Validation loss: 21.29168701171875 RMSE: 4.6142917\n",
      "Validation loss: 23.028932571411133 RMSE: 4.798847\n",
      "Validation loss: 20.336645126342773 RMSE: 4.509617\n",
      "178 2 1.8584229946136475\n",
      "Validation loss: 19.856298446655273 RMSE: 4.456041\n",
      "Validation loss: 20.12801456451416 RMSE: 4.4864254\n",
      "Validation loss: 18.646265029907227 RMSE: 4.318132\n",
      "181 4 1.4263349771499634\n",
      "Validation loss: 22.834209442138672 RMSE: 4.7785153\n",
      "Validation loss: 16.569658279418945 RMSE: 4.0705843\n",
      "Validation loss: 22.90320110321045 RMSE: 4.785729\n",
      "184 6 1.076298475265503\n",
      "Validation loss: 19.69961452484131 RMSE: 4.438425\n",
      "Validation loss: 17.94108819961548 RMSE: 4.235692\n",
      "Validation loss: 22.582866668701172 RMSE: 4.7521434\n",
      "187 8 0.8805935382843018\n",
      "Validation loss: 19.04134178161621 RMSE: 4.3636384\n",
      "Validation loss: 17.69955587387085 RMSE: 4.207084\n",
      "Validation loss: 21.117666244506836 RMSE: 4.595396\n",
      "190 10 1.7250502109527588\n",
      "Validation loss: 21.18165111541748 RMSE: 4.602353\n",
      "Validation loss: 23.76859951019287 RMSE: 4.875305\n",
      "Validation loss: 20.598061561584473 RMSE: 4.5385084\n",
      "193 12 1.2219200134277344\n",
      "Validation loss: 21.493666648864746 RMSE: 4.6361265\n",
      "Validation loss: 22.93106460571289 RMSE: 4.788639\n",
      "Validation loss: 20.062817573547363 RMSE: 4.4791536\n",
      "196 14 1.553781270980835\n",
      "Validation loss: 18.454192638397217 RMSE: 4.2958345\n",
      "Validation loss: 19.27839994430542 RMSE: 4.3907175\n",
      "Validation loss: 21.579153060913086 RMSE: 4.645337\n",
      "Validation loss: 19.62548828125 RMSE: 4.4300666\n",
      "Loaded trained model with success.\n",
      "Test loss: 14.891859846848709 Test RMSE: 3.8589973\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:0', 'task_name': 'freesolv', 'model_type': 'gin', 'model': {'num_layer': 3, 'emb_dim': 64, 'feat_dim': 64, 'drop_ratio': 0.5, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 781, 'task': 'regression', 'data_path': 'data/freesolv/freesolv.csv', 'target': ['expt']}}\n",
      "Running on: cuda:0\n",
      "641\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/641\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 16.494474411010742\n",
      "Validation loss: 63.37864685058594 RMSE: 7.9610705\n",
      "Validation loss: 57.70648765563965 RMSE: 7.5964785\n",
      "Validation loss: 50.85343360900879 RMSE: 7.1311593\n",
      "3 2 6.094778537750244\n",
      "Validation loss: 40.240577697753906 RMSE: 6.343546\n",
      "Validation loss: 27.776565551757812 RMSE: 5.2703476\n",
      "Validation loss: 22.397489547729492 RMSE: 4.732599\n",
      "6 4 4.120986461639404\n",
      "Validation loss: 19.77552604675293 RMSE: 4.446968\n",
      "Validation loss: 18.378849029541016 RMSE: 4.287056\n",
      "Validation loss: 16.487154483795166 RMSE: 4.0604377\n",
      "9 6 4.980007648468018\n",
      "Validation loss: 19.19532299041748 RMSE: 4.3812466\n",
      "Validation loss: 17.99343490600586 RMSE: 4.241867\n",
      "Validation loss: 16.301151275634766 RMSE: 4.0374684\n",
      "12 8 6.683112621307373\n",
      "Validation loss: 14.899425983428955 RMSE: 3.8599772\n",
      "Validation loss: 13.89769983291626 RMSE: 3.7279618\n",
      "Validation loss: 14.541420459747314 RMSE: 3.8133214\n",
      "15 10 6.320693492889404\n",
      "Validation loss: 15.722357749938965 RMSE: 3.9651427\n",
      "Validation loss: 14.670886993408203 RMSE: 3.830259\n",
      "Validation loss: 13.370734691619873 RMSE: 3.6566017\n",
      "18 12 3.367609977722168\n",
      "Validation loss: 12.803734302520752 RMSE: 3.5782306\n",
      "Validation loss: 13.327853202819824 RMSE: 3.6507335\n",
      "Validation loss: 12.263986110687256 RMSE: 3.5019975\n",
      "21 14 4.305177211761475\n",
      "Validation loss: 11.3036470413208 RMSE: 3.3620896\n",
      "Validation loss: 13.008333206176758 RMSE: 3.6067066\n",
      "Validation loss: 12.459859371185303 RMSE: 3.5298526\n",
      "Validation loss: 13.203208923339844 RMSE: 3.633622\n",
      "25 0 3.7855303287506104\n",
      "Validation loss: 11.428683280944824 RMSE: 3.3806334\n",
      "Validation loss: 12.950914859771729 RMSE: 3.598738\n",
      "Validation loss: 11.868778228759766 RMSE: 3.4451091\n",
      "28 2 4.444281101226807\n",
      "Validation loss: 12.892252922058105 RMSE: 3.5905786\n",
      "Validation loss: 11.972635269165039 RMSE: 3.4601495\n",
      "Validation loss: 10.6941237449646 RMSE: 3.2701867\n",
      "31 4 7.470726013183594\n",
      "Validation loss: 10.758691310882568 RMSE: 3.2800446\n",
      "Validation loss: 10.420026779174805 RMSE: 3.2280068\n",
      "Validation loss: 10.16171646118164 RMSE: 3.1877446\n",
      "34 6 2.5796356201171875\n",
      "Validation loss: 10.186299324035645 RMSE: 3.1915982\n",
      "Validation loss: 10.827912330627441 RMSE: 3.2905793\n",
      "Validation loss: 10.129017353057861 RMSE: 3.1826117\n",
      "37 8 4.232490539550781\n",
      "Validation loss: 9.64976453781128 RMSE: 3.1064072\n",
      "Validation loss: 9.925426483154297 RMSE: 3.1504645\n",
      "Validation loss: 8.89531660079956 RMSE: 2.9825017\n",
      "40 10 3.1296699047088623\n",
      "Validation loss: 8.46668004989624 RMSE: 2.9097562\n",
      "Validation loss: 8.43816351890564 RMSE: 2.9048517\n",
      "Validation loss: 9.801408767700195 RMSE: 3.1307201\n",
      "43 12 4.558887481689453\n",
      "Validation loss: 9.295392513275146 RMSE: 3.0488343\n",
      "Validation loss: 9.682642459869385 RMSE: 3.1116946\n",
      "Validation loss: 9.029189109802246 RMSE: 3.0048609\n",
      "46 14 4.816214084625244\n",
      "Validation loss: 10.255484342575073 RMSE: 3.2024183\n",
      "Validation loss: 9.415710926055908 RMSE: 3.0685031\n",
      "Validation loss: 10.098681449890137 RMSE: 3.1778421\n",
      "Validation loss: 8.845021724700928 RMSE: 2.9740582\n",
      "50 0 3.4493231773376465\n",
      "Validation loss: 9.38715648651123 RMSE: 3.0638466\n",
      "Validation loss: 9.653773784637451 RMSE: 3.107052\n",
      "Validation loss: 10.260643005371094 RMSE: 3.203224\n",
      "53 2 3.7386248111724854\n",
      "Validation loss: 9.95615839958191 RMSE: 3.155338\n",
      "Validation loss: 9.271942377090454 RMSE: 3.0449865\n",
      "Validation loss: 8.625911712646484 RMSE: 2.93699\n",
      "56 4 5.134434700012207\n",
      "Validation loss: 10.185263395309448 RMSE: 3.1914358\n",
      "Validation loss: 10.818313121795654 RMSE: 3.2891202\n",
      "Validation loss: 9.808629989624023 RMSE: 3.131873\n",
      "59 6 3.6999833583831787\n",
      "Validation loss: 10.817876815795898 RMSE: 3.2890542\n",
      "Validation loss: 11.72745943069458 RMSE: 3.4245377\n",
      "Validation loss: 11.566049098968506 RMSE: 3.4008894\n",
      "62 8 2.46457839012146\n",
      "Validation loss: 10.9123215675354 RMSE: 3.3033803\n",
      "Validation loss: 10.361541748046875 RMSE: 3.2189348\n",
      "Validation loss: 10.536602973937988 RMSE: 3.2460136\n",
      "65 10 3.799441337585449\n",
      "Validation loss: 12.46255350112915 RMSE: 3.5302343\n",
      "Validation loss: 12.113269329071045 RMSE: 3.4804122\n",
      "Validation loss: 10.801318168640137 RMSE: 3.2865357\n",
      "68 12 1.3783575296401978\n",
      "Validation loss: 8.876533031463623 RMSE: 2.9793508\n",
      "Validation loss: 13.17081880569458 RMSE: 3.6291618\n",
      "Validation loss: 10.46964406967163 RMSE: 3.235683\n",
      "71 14 2.3466994762420654\n",
      "Validation loss: 11.703318119049072 RMSE: 3.4210114\n",
      "Validation loss: 11.342588901519775 RMSE: 3.367876\n",
      "Validation loss: 8.159164905548096 RMSE: 2.856425\n",
      "Validation loss: 10.617296695709229 RMSE: 3.258419\n",
      "75 0 2.0590999126434326\n",
      "Validation loss: 11.2232084274292 RMSE: 3.3501058\n",
      "Validation loss: 9.52677583694458 RMSE: 3.0865474\n",
      "Validation loss: 10.761238098144531 RMSE: 3.2804327\n",
      "78 2 2.42635440826416\n",
      "Validation loss: 9.119832515716553 RMSE: 3.019906\n",
      "Validation loss: 9.243135690689087 RMSE: 3.0402527\n",
      "Validation loss: 11.747284412384033 RMSE: 3.427431\n",
      "81 4 3.5668787956237793\n",
      "Validation loss: 11.95081639289856 RMSE: 3.456995\n",
      "Validation loss: 11.016832828521729 RMSE: 3.3191612\n",
      "Validation loss: 11.568840742111206 RMSE: 3.4013\n",
      "84 6 3.0188307762145996\n",
      "Validation loss: 11.088129043579102 RMSE: 3.3298843\n",
      "Validation loss: 13.662787914276123 RMSE: 3.696321\n",
      "Validation loss: 11.977829456329346 RMSE: 3.4609\n",
      "87 8 1.0097551345825195\n",
      "Validation loss: 12.86894702911377 RMSE: 3.5873313\n",
      "Validation loss: 10.473198413848877 RMSE: 3.236232\n",
      "Validation loss: 13.13294792175293 RMSE: 3.6239407\n",
      "90 10 4.558632850646973\n",
      "Validation loss: 12.485996723175049 RMSE: 3.533553\n",
      "Validation loss: 12.173186302185059 RMSE: 3.4890094\n",
      "Validation loss: 12.278036117553711 RMSE: 3.5040028\n",
      "93 12 2.707979917526245\n",
      "Validation loss: 13.006382465362549 RMSE: 3.6064365\n",
      "Validation loss: 13.838664531707764 RMSE: 3.7200358\n",
      "Validation loss: 11.343766689300537 RMSE: 3.3680506\n",
      "96 14 2.3341171741485596\n",
      "Validation loss: 11.541078090667725 RMSE: 3.3972163\n",
      "Validation loss: 10.463056087493896 RMSE: 3.234665\n",
      "Validation loss: 13.188025951385498 RMSE: 3.631532\n",
      "Validation loss: 14.461104393005371 RMSE: 3.802776\n",
      "100 0 2.357612133026123\n",
      "Validation loss: 10.889169454574585 RMSE: 3.2998743\n",
      "Validation loss: 12.085124969482422 RMSE: 3.4763668\n",
      "Validation loss: 12.621600151062012 RMSE: 3.552689\n",
      "103 2 1.8545043468475342\n",
      "Validation loss: 10.40843391418457 RMSE: 3.2262104\n",
      "Validation loss: 11.969700813293457 RMSE: 3.4597256\n",
      "Validation loss: 13.356883525848389 RMSE: 3.654707\n",
      "106 4 1.9825621843338013\n",
      "Validation loss: 11.162297248840332 RMSE: 3.3410025\n",
      "Validation loss: 12.376768112182617 RMSE: 3.5180628\n",
      "Validation loss: 14.670852661132812 RMSE: 3.8302546\n",
      "109 6 1.872076153755188\n",
      "Validation loss: 12.338840007781982 RMSE: 3.5126686\n",
      "Validation loss: 10.370081901550293 RMSE: 3.220261\n",
      "Validation loss: 12.742056846618652 RMSE: 3.569602\n",
      "112 8 2.0754239559173584\n",
      "Validation loss: 14.376899719238281 RMSE: 3.7916882\n",
      "Validation loss: 12.826350212097168 RMSE: 3.5813897\n",
      "Validation loss: 11.85748815536499 RMSE: 3.4434702\n",
      "115 10 2.8973405361175537\n",
      "Validation loss: 12.820828437805176 RMSE: 3.5806186\n",
      "Validation loss: 11.170296669006348 RMSE: 3.3422\n",
      "Validation loss: 10.497042179107666 RMSE: 3.2399137\n",
      "118 12 2.836591958999634\n",
      "Validation loss: 12.519871234893799 RMSE: 3.5383427\n",
      "Validation loss: 11.311692953109741 RMSE: 3.363286\n",
      "Validation loss: 10.070010662078857 RMSE: 3.1733282\n",
      "121 14 1.5909678936004639\n",
      "Validation loss: 11.209843635559082 RMSE: 3.3481104\n",
      "Validation loss: 11.813373565673828 RMSE: 3.437059\n",
      "Validation loss: 10.203697204589844 RMSE: 3.1943228\n",
      "Validation loss: 11.082262992858887 RMSE: 3.3290033\n",
      "125 0 2.2514312267303467\n",
      "Validation loss: 13.425611972808838 RMSE: 3.6640978\n",
      "Validation loss: 12.173857688903809 RMSE: 3.4891055\n",
      "Validation loss: 14.714167594909668 RMSE: 3.835905\n",
      "128 2 1.0597751140594482\n",
      "Validation loss: 13.494952201843262 RMSE: 3.673548\n",
      "Validation loss: 13.876571655273438 RMSE: 3.725127\n",
      "Validation loss: 11.476705551147461 RMSE: 3.3877287\n",
      "131 4 2.051633596420288\n",
      "Validation loss: 13.271856307983398 RMSE: 3.6430557\n",
      "Validation loss: 12.102744102478027 RMSE: 3.4789\n",
      "Validation loss: 16.07072687149048 RMSE: 4.0088315\n",
      "134 6 1.7578357458114624\n",
      "Validation loss: 12.819045543670654 RMSE: 3.5803695\n",
      "Validation loss: 12.145698547363281 RMSE: 3.4850678\n",
      "Validation loss: 13.676431655883789 RMSE: 3.6981657\n",
      "137 8 2.2757813930511475\n",
      "Validation loss: 13.118485450744629 RMSE: 3.6219451\n",
      "Validation loss: 12.714537143707275 RMSE: 3.565745\n",
      "Validation loss: 13.430504322052002 RMSE: 3.6647651\n",
      "140 10 1.5929601192474365\n",
      "Validation loss: 11.728376388549805 RMSE: 3.4246714\n",
      "Validation loss: 11.336835861206055 RMSE: 3.3670218\n",
      "Validation loss: 15.09029245376587 RMSE: 3.8846223\n",
      "143 12 1.2604851722717285\n",
      "Validation loss: 10.571396350860596 RMSE: 3.2513683\n",
      "Validation loss: 11.148153305053711 RMSE: 3.338885\n",
      "Validation loss: 13.389955997467041 RMSE: 3.659229\n",
      "146 14 0.8803390860557556\n",
      "Validation loss: 14.54458236694336 RMSE: 3.813736\n",
      "Validation loss: 14.827224731445312 RMSE: 3.8506134\n",
      "Validation loss: 13.142364025115967 RMSE: 3.6252398\n",
      "Validation loss: 14.865447998046875 RMSE: 3.8555737\n",
      "150 0 1.71909499168396\n",
      "Validation loss: 14.73829698562622 RMSE: 3.839049\n",
      "Validation loss: 12.81090784072876 RMSE: 3.579233\n",
      "Validation loss: 15.548569679260254 RMSE: 3.9431674\n",
      "153 2 2.379701852798462\n",
      "Validation loss: 12.091678619384766 RMSE: 3.477309\n",
      "Validation loss: 13.965253353118896 RMSE: 3.7370114\n",
      "Validation loss: 16.740320682525635 RMSE: 4.091494\n",
      "156 4 4.768593788146973\n",
      "Validation loss: 18.84019923210144 RMSE: 4.34053\n",
      "Validation loss: 15.681625366210938 RMSE: 3.9600031\n",
      "Validation loss: 14.464266777038574 RMSE: 3.8031917\n",
      "159 6 2.492835760116577\n",
      "Validation loss: 14.03244924545288 RMSE: 3.745991\n",
      "Validation loss: 18.5999174118042 RMSE: 4.3127623\n",
      "Validation loss: 14.350432395935059 RMSE: 3.7881963\n",
      "162 8 2.942075252532959\n",
      "Validation loss: 14.15758228302002 RMSE: 3.7626562\n",
      "Validation loss: 14.651744842529297 RMSE: 3.82776\n",
      "Validation loss: 14.567110538482666 RMSE: 3.8166888\n",
      "165 10 1.8250887393951416\n",
      "Validation loss: 15.458728313446045 RMSE: 3.9317586\n",
      "Validation loss: 17.21955156326294 RMSE: 4.149645\n",
      "Validation loss: 16.16496181488037 RMSE: 4.0205674\n",
      "168 12 3.1260838508605957\n",
      "Validation loss: 16.210002422332764 RMSE: 4.026165\n",
      "Validation loss: 19.004860401153564 RMSE: 4.3594565\n",
      "Validation loss: 16.493040084838867 RMSE: 4.0611625\n",
      "171 14 1.5651849508285522\n",
      "Validation loss: 14.734803676605225 RMSE: 3.8385937\n",
      "Validation loss: 14.657246589660645 RMSE: 3.8284783\n",
      "Validation loss: 16.439247131347656 RMSE: 4.054534\n",
      "Validation loss: 13.756384372711182 RMSE: 3.70896\n",
      "175 0 0.9097182154655457\n",
      "Validation loss: 17.029406547546387 RMSE: 4.1266704\n",
      "Validation loss: 17.71472930908203 RMSE: 4.208887\n",
      "Validation loss: 17.900203227996826 RMSE: 4.2308626\n",
      "178 2 1.7311131954193115\n",
      "Validation loss: 15.723302841186523 RMSE: 3.9652622\n",
      "Validation loss: 13.76924991607666 RMSE: 3.710694\n",
      "Validation loss: 13.496948719024658 RMSE: 3.6738195\n",
      "181 4 1.8946154117584229\n",
      "Validation loss: 15.046507835388184 RMSE: 3.8789828\n",
      "Validation loss: 16.532819747924805 RMSE: 4.066057\n",
      "Validation loss: 14.201432228088379 RMSE: 3.7684789\n",
      "184 6 1.9744702577590942\n",
      "Validation loss: 14.870175838470459 RMSE: 3.8561866\n",
      "Validation loss: 15.28712010383606 RMSE: 3.9098744\n",
      "Validation loss: 15.896429538726807 RMSE: 3.9870327\n",
      "187 8 2.2684326171875\n",
      "Validation loss: 16.079360961914062 RMSE: 4.0099077\n",
      "Validation loss: 15.682478904724121 RMSE: 3.960111\n",
      "Validation loss: 17.761839866638184 RMSE: 4.2144794\n",
      "190 10 1.8345932960510254\n",
      "Validation loss: 20.14351463317871 RMSE: 4.4881525\n",
      "Validation loss: 20.359399795532227 RMSE: 4.512139\n",
      "Validation loss: 15.141329288482666 RMSE: 3.891186\n",
      "193 12 1.5341640710830688\n",
      "Validation loss: 17.04370880126953 RMSE: 4.1284027\n",
      "Validation loss: 17.307899475097656 RMSE: 4.160276\n",
      "Validation loss: 14.176856517791748 RMSE: 3.7652168\n",
      "196 14 1.9179497957229614\n",
      "Validation loss: 13.702188968658447 RMSE: 3.7016468\n",
      "Validation loss: 13.822587490081787 RMSE: 3.717874\n",
      "Validation loss: 14.571857929229736 RMSE: 3.81731\n",
      "Validation loss: 13.30838680267334 RMSE: 3.6480663\n",
      "Loaded trained model with success.\n",
      "Test loss: 7.114811974649246 Test RMSE: 2.6673608\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:0', 'task_name': 'esol', 'model_type': 'gin', 'model': {'num_layer': 5, 'emb_dim': 300, 'feat_dim': 300, 'drop_ratio': 0.3, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 777, 'task': 'regression', 'data_path': 'data/esol/esol.csv', 'target': ['measured log solubility in mols per litre']}}\n",
      "Running on: cuda:0\n",
      "1127\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/1127\n",
      "Generating scaffold 1000/1127\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 7.802577495574951\n",
      "Validation loss: 2.3783169235803383 RMSE: 1.5421792\n",
      "1 21 1.6155766248703003\n",
      "Validation loss: 6.791090771160294 RMSE: 2.605972\n",
      "Validation loss: 3.711505712661068 RMSE: 1.9265268\n",
      "3 13 1.3249702453613281\n",
      "Validation loss: 6.63696265431632 RMSE: 2.5762303\n",
      "Validation loss: 3.7594032287597656 RMSE: 1.938918\n",
      "5 5 1.3880724906921387\n",
      "Validation loss: 8.652576488731182 RMSE: 2.9415264\n",
      "6 26 1.5073169469833374\n",
      "Validation loss: 2.462568462422464 RMSE: 1.5692573\n",
      "Validation loss: 5.525371597931448 RMSE: 2.350611\n",
      "8 18 1.5046231746673584\n",
      "Validation loss: 4.569199051477213 RMSE: 2.1375685\n",
      "Validation loss: 3.6810026063328296 RMSE: 1.9185941\n",
      "10 10 1.3865035772323608\n",
      "Validation loss: 5.865889418441637 RMSE: 2.4219599\n",
      "Validation loss: 7.363182152267051 RMSE: 2.7135184\n",
      "12 2 0.894240140914917\n",
      "Validation loss: 3.893644628271592 RMSE: 1.973232\n",
      "13 23 0.926744818687439\n",
      "Validation loss: 3.305095963773474 RMSE: 1.8179924\n",
      "Validation loss: 5.518353495977621 RMSE: 2.3491175\n",
      "15 15 0.8470949530601501\n",
      "Validation loss: 4.0751069879109885 RMSE: 2.0186894\n",
      "Validation loss: 3.470285654067993 RMSE: 1.8628702\n",
      "17 7 1.4396799802780151\n",
      "Validation loss: 5.748883293793265 RMSE: 2.397683\n",
      "18 28 1.1284677982330322\n",
      "Validation loss: 5.3725648221716416 RMSE: 2.3178792\n",
      "Validation loss: 4.199879156804718 RMSE: 2.0493608\n",
      "20 20 1.6978216171264648\n",
      "Validation loss: 5.138842236679213 RMSE: 2.2669015\n",
      "Validation loss: 2.839826147113226 RMSE: 1.6851783\n",
      "22 12 0.8134515881538391\n",
      "Validation loss: 2.9722435959672504 RMSE: 1.7240196\n",
      "Validation loss: 6.166235738095984 RMSE: 2.4831908\n",
      "24 4 0.8437265753746033\n",
      "Validation loss: 5.526190521442785 RMSE: 2.3507853\n",
      "25 25 0.49604281783103943\n",
      "Validation loss: 3.13078530488816 RMSE: 1.7694026\n",
      "Validation loss: 3.844353462742493 RMSE: 1.9607023\n",
      "27 17 0.7879207134246826\n",
      "Validation loss: 5.00728998352996 RMSE: 2.2376976\n",
      "Validation loss: 3.600343750641409 RMSE: 1.8974572\n",
      "29 9 0.768017053604126\n",
      "Validation loss: 2.372789842892537 RMSE: 1.5403863\n",
      "Validation loss: 3.5731098166609234 RMSE: 1.890267\n",
      "31 1 0.8945668339729309\n",
      "Validation loss: 2.399088175950852 RMSE: 1.548899\n",
      "32 22 0.711030125617981\n",
      "Validation loss: 4.200402780971696 RMSE: 2.0494883\n",
      "Validation loss: 2.3461017735236513 RMSE: 1.531699\n",
      "34 14 1.0401901006698608\n",
      "Validation loss: 3.426351496603637 RMSE: 1.8510407\n",
      "Validation loss: 3.2982005418929377 RMSE: 1.8160949\n",
      "36 6 0.6564465165138245\n",
      "Validation loss: 2.8757027820148298 RMSE: 1.6957897\n",
      "37 27 1.254995584487915\n",
      "Validation loss: 3.0167923205721694 RMSE: 1.7368916\n",
      "Validation loss: 4.949059937907531 RMSE: 2.2246485\n",
      "39 19 0.6820041537284851\n",
      "Validation loss: 4.9381480174782 RMSE: 2.2221942\n",
      "Validation loss: 3.562027741322475 RMSE: 1.8873335\n",
      "41 11 1.1244685649871826\n",
      "Validation loss: 2.19733176822156 RMSE: 1.48234\n",
      "Validation loss: 2.5637675112327645 RMSE: 1.6011769\n",
      "43 3 0.5873299837112427\n",
      "Validation loss: 2.8642650589478755 RMSE: 1.692414\n",
      "44 24 0.9068792462348938\n",
      "Validation loss: 2.405570766567129 RMSE: 1.5509902\n",
      "Validation loss: 4.273771948519006 RMSE: 2.0673103\n",
      "46 16 0.5510007739067078\n",
      "Validation loss: 2.672073594236796 RMSE: 1.6346478\n",
      "Validation loss: 2.3841199410700167 RMSE: 1.5440596\n",
      "48 8 0.685354471206665\n",
      "Validation loss: 2.8365056620234936 RMSE: 1.6841929\n",
      "Validation loss: 3.685748338699341 RMSE: 1.9198303\n",
      "50 0 0.6334590315818787\n",
      "Validation loss: 3.50365897406519 RMSE: 1.8718063\n",
      "51 21 0.6282471418380737\n",
      "Validation loss: 3.307563766968989 RMSE: 1.8186709\n",
      "Validation loss: 2.3353374932719544 RMSE: 1.5281812\n",
      "53 13 0.6036016941070557\n",
      "Validation loss: 2.4265028006207627 RMSE: 1.5577238\n",
      "Validation loss: 2.5779539230650506 RMSE: 1.6056007\n",
      "55 5 1.2018705606460571\n",
      "Validation loss: 4.444832257464924 RMSE: 2.108277\n",
      "56 26 1.4508999586105347\n",
      "Validation loss: 2.0638253372327418 RMSE: 1.436602\n",
      "Validation loss: 4.103061986180533 RMSE: 2.0256016\n",
      "58 18 0.8475562930107117\n",
      "Validation loss: 3.1169156901604307 RMSE: 1.7654787\n",
      "Validation loss: 2.7352398486263985 RMSE: 1.653856\n",
      "60 10 0.7348601818084717\n",
      "Validation loss: 3.56153321899144 RMSE: 1.8872025\n",
      "Validation loss: 2.8026675017534104 RMSE: 1.6741168\n",
      "62 2 0.8089854121208191\n",
      "Validation loss: 5.516304366356503 RMSE: 2.3486814\n",
      "63 23 0.4119999408721924\n",
      "Validation loss: 3.699686223426751 RMSE: 1.9234568\n",
      "Validation loss: 3.2826246772192222 RMSE: 1.8118016\n",
      "65 15 0.34792616963386536\n",
      "Validation loss: 2.0107671507691913 RMSE: 1.4180151\n",
      "Validation loss: 1.9614846284410594 RMSE: 1.4005301\n",
      "67 7 0.6639271974563599\n",
      "Validation loss: 2.6328304172617143 RMSE: 1.6225998\n",
      "68 28 4.3045525550842285\n",
      "Validation loss: 2.7502417437798155 RMSE: 1.6583853\n",
      "Validation loss: 5.753452923445575 RMSE: 2.3986356\n",
      "70 20 0.718856930732727\n",
      "Validation loss: 4.422993887842229 RMSE: 2.1030915\n",
      "Validation loss: 2.6165734972574013 RMSE: 1.6175827\n",
      "72 12 0.6921956539154053\n",
      "Validation loss: 3.2476885761834877 RMSE: 1.8021344\n",
      "Validation loss: 3.6425076121777558 RMSE: 1.9085354\n",
      "74 4 1.0152939558029175\n",
      "Validation loss: 3.6505633877441945 RMSE: 1.9106448\n",
      "75 25 0.4922932982444763\n",
      "Validation loss: 3.7513680458068848 RMSE: 1.9368448\n",
      "Validation loss: 1.9644620534593025 RMSE: 1.4015927\n",
      "77 17 0.6123790144920349\n",
      "Validation loss: 2.4311859755389458 RMSE: 1.5592262\n",
      "Validation loss: 3.12136693971347 RMSE: 1.7667391\n",
      "79 9 0.587790310382843\n",
      "Validation loss: 2.155237725350709 RMSE: 1.4680728\n",
      "Validation loss: 2.4643613922912464 RMSE: 1.5698285\n",
      "81 1 0.6981324553489685\n",
      "Validation loss: 2.441464556001984 RMSE: 1.5625187\n",
      "82 22 0.631949245929718\n",
      "Validation loss: 2.638858567296931 RMSE: 1.6244564\n",
      "Validation loss: 2.53879031670832 RMSE: 1.593358\n",
      "84 14 0.807098388671875\n",
      "Validation loss: 2.58978464751117 RMSE: 1.6092808\n",
      "Validation loss: 2.0251104778948084 RMSE: 1.4230638\n",
      "86 6 1.0179920196533203\n",
      "Validation loss: 2.4100136039531335 RMSE: 1.5524219\n",
      "87 27 0.9434929490089417\n",
      "Validation loss: 2.7597313222631943 RMSE: 1.6612439\n",
      "Validation loss: 2.8053109371556646 RMSE: 1.6749064\n",
      "89 19 0.471164345741272\n",
      "Validation loss: 2.9589139592331066 RMSE: 1.7201494\n",
      "Validation loss: 7.091508616388372 RMSE: 2.6629884\n",
      "91 11 0.39093562960624695\n",
      "Validation loss: 3.540541288072029 RMSE: 1.8816326\n",
      "Validation loss: 3.753546708453018 RMSE: 1.9374073\n",
      "93 3 0.4239843189716339\n",
      "Validation loss: 2.0071823987285646 RMSE: 1.4167507\n",
      "94 24 0.41382622718811035\n",
      "Validation loss: 3.1311753028261977 RMSE: 1.7695127\n",
      "Validation loss: 2.653809548479266 RMSE: 1.6290517\n",
      "96 16 0.5758501291275024\n",
      "Validation loss: 2.7904887990613956 RMSE: 1.6704755\n",
      "Validation loss: 2.693956776002867 RMSE: 1.6413279\n",
      "98 8 0.811626136302948\n",
      "Validation loss: 6.817604402525235 RMSE: 2.6110544\n",
      "Validation loss: 2.063648654296335 RMSE: 1.4365405\n",
      "100 0 0.4569157361984253\n",
      "Validation loss: 2.4848877339236504 RMSE: 1.5763526\n",
      "101 21 0.3450818359851837\n",
      "Validation loss: 2.4620624116036742 RMSE: 1.5690961\n",
      "Validation loss: 3.276990856744547 RMSE: 1.8102461\n",
      "103 13 0.6978827118873596\n",
      "Validation loss: 2.7236421393082204 RMSE: 1.6503462\n",
      "Validation loss: 2.1143006983056534 RMSE: 1.4540635\n",
      "105 5 0.815376877784729\n",
      "Validation loss: 2.0780136954467907 RMSE: 1.4415317\n",
      "106 26 0.468317449092865\n",
      "Validation loss: 2.122175381246921 RMSE: 1.4567689\n",
      "Validation loss: 2.304234567996675 RMSE: 1.5179706\n",
      "108 18 0.4387788474559784\n",
      "Validation loss: 2.044181923950668 RMSE: 1.4297489\n",
      "Validation loss: 2.3828528539269374 RMSE: 1.5436492\n",
      "110 10 1.178152322769165\n",
      "Validation loss: 2.790389765680364 RMSE: 1.670446\n",
      "Validation loss: 2.46247264769225 RMSE: 1.5692269\n",
      "112 2 0.7838736772537231\n",
      "Validation loss: 2.7181771202424985 RMSE: 1.6486894\n",
      "113 23 0.5676261186599731\n",
      "Validation loss: 1.8388568605996867 RMSE: 1.3560445\n",
      "Validation loss: 1.8450302529124032 RMSE: 1.3583188\n",
      "115 15 0.7655528783798218\n",
      "Validation loss: 3.317580041632188 RMSE: 1.8214226\n",
      "Validation loss: 2.5005419655183774 RMSE: 1.5813102\n",
      "117 7 0.48868393898010254\n",
      "Validation loss: 2.5745173682153752 RMSE: 1.6045303\n",
      "118 28 0.7028970122337341\n",
      "Validation loss: 2.490059141564158 RMSE: 1.5779922\n",
      "Validation loss: 2.0545289221063125 RMSE: 1.4333628\n",
      "120 20 0.3682815730571747\n",
      "Validation loss: 2.4902990096438247 RMSE: 1.5780681\n",
      "Validation loss: 2.471094069227708 RMSE: 1.5719713\n",
      "122 12 0.5969679355621338\n",
      "Validation loss: 2.240352334174435 RMSE: 1.4967806\n",
      "Validation loss: 3.3636364388254893 RMSE: 1.8340219\n",
      "124 4 0.45507195591926575\n",
      "Validation loss: 2.1840104529287965 RMSE: 1.4778398\n",
      "125 25 0.35098710656166077\n",
      "Validation loss: 2.3090519883991343 RMSE: 1.5195565\n",
      "Validation loss: 2.7658404249005613 RMSE: 1.6630815\n",
      "127 17 0.3972700238227844\n",
      "Validation loss: 2.193452446861605 RMSE: 1.4810308\n",
      "Validation loss: 2.4943979356141215 RMSE: 1.5793663\n",
      "129 9 0.63113933801651\n",
      "Validation loss: 2.1014923353110793 RMSE: 1.4496524\n",
      "Validation loss: 2.2976056959776754 RMSE: 1.5157853\n",
      "131 1 0.6502986550331116\n",
      "Validation loss: 2.0785340207867917 RMSE: 1.4417123\n",
      "132 22 0.5232454538345337\n",
      "Validation loss: 1.8035317775422492 RMSE: 1.3429563\n",
      "Validation loss: 2.0006052012992117 RMSE: 1.4144275\n",
      "134 14 0.9063807725906372\n",
      "Validation loss: 2.9151888674339363 RMSE: 1.7073922\n",
      "Validation loss: 2.248013868796087 RMSE: 1.4993378\n",
      "136 6 0.8383398056030273\n",
      "Validation loss: 2.2066476946383453 RMSE: 1.485479\n",
      "137 27 0.2794804871082306\n",
      "Validation loss: 2.053281901157008 RMSE: 1.4329277\n",
      "Validation loss: 1.9382477967085037 RMSE: 1.3922096\n",
      "139 19 0.35686683654785156\n",
      "Validation loss: 2.9199106418980962 RMSE: 1.7087747\n",
      "Validation loss: 3.3214041068490627 RMSE: 1.822472\n",
      "141 11 0.3221178948879242\n",
      "Validation loss: 2.0346996604868797 RMSE: 1.426429\n",
      "Validation loss: 1.9373557535947952 RMSE: 1.3918893\n",
      "143 3 0.44962078332901\n",
      "Validation loss: 2.162468521995882 RMSE: 1.4705335\n",
      "144 24 0.44079309701919556\n",
      "Validation loss: 2.464174325487255 RMSE: 1.5697689\n",
      "Validation loss: 2.192678603450809 RMSE: 1.4807696\n",
      "146 16 0.7478309273719788\n",
      "Validation loss: 1.7343559919205387 RMSE: 1.3169495\n",
      "Validation loss: 2.85483175674371 RMSE: 1.6896247\n",
      "148 8 0.9166339635848999\n",
      "Validation loss: 1.7619177841507228 RMSE: 1.3273724\n",
      "Validation loss: 2.4601713745994904 RMSE: 1.5684932\n",
      "150 0 0.5782211422920227\n",
      "Validation loss: 2.457557909256589 RMSE: 1.56766\n",
      "151 21 0.44925859570503235\n",
      "Validation loss: 2.359114469680111 RMSE: 1.5359409\n",
      "Validation loss: 2.0611096103634456 RMSE: 1.4356565\n",
      "153 13 0.4106452465057373\n",
      "Validation loss: 2.9770727790562455 RMSE: 1.7254196\n",
      "Validation loss: 2.9892834773105856 RMSE: 1.7289543\n",
      "155 5 0.3305235207080841\n",
      "Validation loss: 3.1342242692424134 RMSE: 1.7703741\n",
      "156 26 0.6974412798881531\n",
      "Validation loss: 1.862579278713834 RMSE: 1.3647634\n",
      "Validation loss: 2.9939262181256723 RMSE: 1.7302965\n",
      "158 18 0.5717875957489014\n",
      "Validation loss: 2.1111853903373787 RMSE: 1.4529918\n",
      "Validation loss: 2.0084260902573576 RMSE: 1.4171894\n",
      "160 10 0.39085257053375244\n",
      "Validation loss: 2.8199639257076567 RMSE: 1.6792748\n",
      "Validation loss: 2.112227478913501 RMSE: 1.4533505\n",
      "162 2 0.2929425835609436\n",
      "Validation loss: 2.0594096162677866 RMSE: 1.4350643\n",
      "163 23 0.24606049060821533\n",
      "Validation loss: 2.5211364125783464 RMSE: 1.5878086\n",
      "Validation loss: 2.2834253722587516 RMSE: 1.5111006\n",
      "165 15 0.31833645701408386\n",
      "Validation loss: 2.630628786255828 RMSE: 1.6219213\n",
      "Validation loss: 2.631604893017659 RMSE: 1.6222223\n",
      "167 7 0.3915364742279053\n",
      "Validation loss: 1.9660069984672344 RMSE: 1.4021438\n",
      "168 28 0.5144690871238708\n",
      "Validation loss: 2.2895405060422105 RMSE: 1.5131228\n",
      "Validation loss: 2.2145445916504984 RMSE: 1.4881345\n",
      "170 20 0.34813621640205383\n",
      "Validation loss: 2.205462903048085 RMSE: 1.4850801\n",
      "Validation loss: 1.961953156817276 RMSE: 1.4006974\n",
      "172 12 0.2796713411808014\n",
      "Validation loss: 2.1928284503189865 RMSE: 1.4808202\n",
      "Validation loss: 1.924171582259963 RMSE: 1.3871452\n",
      "174 4 0.27721235156059265\n",
      "Validation loss: 2.710373180102458 RMSE: 1.6463212\n",
      "175 25 0.6044672131538391\n",
      "Validation loss: 2.2119490340747663 RMSE: 1.4872622\n",
      "Validation loss: 2.2091098295903837 RMSE: 1.4863075\n",
      "177 17 0.5610980987548828\n",
      "Validation loss: 2.092849521510369 RMSE: 1.4466684\n",
      "Validation loss: 1.8020785878189898 RMSE: 1.3424152\n",
      "179 9 0.3050346374511719\n",
      "Validation loss: 2.3586432385233653 RMSE: 1.5357875\n",
      "Validation loss: 2.0182221843078074 RMSE: 1.4206415\n",
      "181 1 0.19082039594650269\n",
      "Validation loss: 1.9984593855596222 RMSE: 1.4136688\n",
      "182 22 0.6446114778518677\n",
      "Validation loss: 1.942926155782379 RMSE: 1.3938888\n",
      "Validation loss: 1.9915864889600636 RMSE: 1.4112357\n",
      "184 14 0.5575447678565979\n",
      "Validation loss: 2.1150945477781042 RMSE: 1.4543365\n",
      "Validation loss: 1.992550603056376 RMSE: 1.4115773\n",
      "186 6 0.3111613094806671\n",
      "Validation loss: 2.2874985289784657 RMSE: 1.5124478\n",
      "187 27 0.45424970984458923\n",
      "Validation loss: 1.8542653100680462 RMSE: 1.3617141\n",
      "Validation loss: 2.0584898121589053 RMSE: 1.4347439\n",
      "189 19 0.2210354208946228\n",
      "Validation loss: 2.4427187933331043 RMSE: 1.56292\n",
      "Validation loss: 2.020824255141537 RMSE: 1.421557\n",
      "191 11 0.4482056796550751\n",
      "Validation loss: 2.09288834892543 RMSE: 1.446682\n",
      "Validation loss: 2.4575579672788095 RMSE: 1.5676601\n",
      "193 3 0.21184833347797394\n",
      "Validation loss: 2.057523003721659 RMSE: 1.4344069\n",
      "194 24 0.27013105154037476\n",
      "Validation loss: 2.318071496170179 RMSE: 1.5225214\n",
      "Validation loss: 1.9090659924313031 RMSE: 1.3816894\n",
      "196 16 0.3276977837085724\n",
      "Validation loss: 2.2528867257379854 RMSE: 1.5009619\n",
      "Validation loss: 2.483715171307589 RMSE: 1.5759807\n",
      "198 8 0.8536829948425293\n",
      "Validation loss: 2.3325377038094848 RMSE: 1.5272648\n",
      "Validation loss: 1.9566820279686852 RMSE: 1.3988144\n",
      "Loaded trained model with success.\n",
      "Test loss: 2.0818641428398874 Test RMSE: 1.4428667\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:0', 'task_name': 'esol', 'model_type': 'gin', 'model': {'num_layer': 5, 'emb_dim': 300, 'feat_dim': 300, 'drop_ratio': 0.3, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 778, 'task': 'regression', 'data_path': 'data/esol/esol.csv', 'target': ['measured log solubility in mols per litre']}}\n",
      "Running on: cuda:0\n",
      "1127\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/1127\n",
      "Generating scaffold 1000/1127\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 16.973329544067383\n",
      "Validation loss: 2.619575871830493 RMSE: 1.6185104\n",
      "1 21 2.5641369819641113\n",
      "Validation loss: 9.408414979951571 RMSE: 3.067314\n",
      "Validation loss: 2.5454969849206703 RMSE: 1.5954614\n",
      "3 13 2.1943798065185547\n",
      "Validation loss: 8.324674884829902 RMSE: 2.8852513\n",
      "Validation loss: 7.836532255189609 RMSE: 2.7993808\n",
      "5 5 1.601306676864624\n",
      "Validation loss: 4.169868389062122 RMSE: 2.0420256\n",
      "6 26 1.4040722846984863\n",
      "Validation loss: 5.860526013163339 RMSE: 2.4208522\n",
      "Validation loss: 6.6430990273973585 RMSE: 2.577421\n",
      "8 18 0.9593408107757568\n",
      "Validation loss: 3.3191950943617696 RMSE: 1.8218658\n",
      "Validation loss: 1.9630938884431282 RMSE: 1.4011046\n",
      "10 10 0.9142476320266724\n",
      "Validation loss: 3.1880397691135913 RMSE: 1.7855082\n",
      "Validation loss: 8.053078507955096 RMSE: 2.8377945\n",
      "12 2 0.8013052344322205\n",
      "Validation loss: 4.497111809992157 RMSE: 2.1206396\n",
      "13 23 0.7651420831680298\n",
      "Validation loss: 6.789365797971202 RMSE: 2.6056411\n",
      "Validation loss: 6.821908068867911 RMSE: 2.6118784\n",
      "15 15 1.0921165943145752\n",
      "Validation loss: 5.6895535329801845 RMSE: 2.3852785\n",
      "Validation loss: 5.237299708138525 RMSE: 2.2885149\n",
      "17 7 1.9411087036132812\n",
      "Validation loss: 5.638831653426179 RMSE: 2.3746223\n",
      "18 28 1.0335983037948608\n",
      "Validation loss: 3.450440269655886 RMSE: 1.8575361\n",
      "Validation loss: 3.6171393099084366 RMSE: 1.9018778\n",
      "20 20 0.49485963582992554\n",
      "Validation loss: 4.488893605966483 RMSE: 2.118701\n",
      "Validation loss: 3.2113399716605127 RMSE: 1.7920212\n",
      "22 12 2.6390857696533203\n",
      "Validation loss: 4.548715878376918 RMSE: 2.1327717\n",
      "Validation loss: 3.698948809530883 RMSE: 1.9232653\n",
      "24 4 0.8931522965431213\n",
      "Validation loss: 3.1997483768294344 RMSE: 1.7887839\n",
      "25 25 1.1445239782333374\n",
      "Validation loss: 3.2500994395365757 RMSE: 1.8028033\n",
      "Validation loss: 4.401552380713741 RMSE: 2.0979877\n",
      "27 17 0.7158911824226379\n",
      "Validation loss: 4.617464854653957 RMSE: 2.148829\n",
      "Validation loss: 3.172195689868083 RMSE: 1.781066\n",
      "29 9 0.5295318961143494\n",
      "Validation loss: 3.088341383807427 RMSE: 1.7573677\n",
      "Validation loss: 2.424367902553187 RMSE: 1.5570382\n",
      "31 1 0.6556577682495117\n",
      "Validation loss: 2.737573129940877 RMSE: 1.6545613\n",
      "32 22 0.9021084308624268\n",
      "Validation loss: 3.508170619475103 RMSE: 1.8730111\n",
      "Validation loss: 3.6793338999284053 RMSE: 1.9181589\n",
      "34 14 0.46277424693107605\n",
      "Validation loss: 3.080450977899332 RMSE: 1.7551214\n",
      "Validation loss: 2.614730638740337 RMSE: 1.6170129\n",
      "36 6 0.8922715783119202\n",
      "Validation loss: 3.314627605201924 RMSE: 1.820612\n",
      "37 27 0.6527320742607117\n",
      "Validation loss: 8.748150850819275 RMSE: 2.9577272\n",
      "Validation loss: 2.6676303306512072 RMSE: 1.6332883\n",
      "39 19 0.5753816366195679\n",
      "Validation loss: 2.0525642823329013 RMSE: 1.4326773\n",
      "Validation loss: 5.582005184308618 RMSE: 2.3626268\n",
      "41 11 1.3570963144302368\n",
      "Validation loss: 6.104588339814042 RMSE: 2.4707465\n",
      "Validation loss: 3.9131538002891877 RMSE: 1.9781693\n",
      "43 3 1.208616018295288\n",
      "Validation loss: 2.5215680873499506 RMSE: 1.5879446\n",
      "44 24 0.5821788907051086\n",
      "Validation loss: 2.3969144852815476 RMSE: 1.5481972\n",
      "Validation loss: 1.8679367525387653 RMSE: 1.3667247\n",
      "46 16 0.5321993827819824\n",
      "Validation loss: 2.155433924852219 RMSE: 1.4681396\n",
      "Validation loss: 2.89004085760201 RMSE: 1.700012\n",
      "48 8 0.6455616354942322\n",
      "Validation loss: 2.426947057774637 RMSE: 1.5578663\n",
      "Validation loss: 3.343914152246661 RMSE: 1.8286374\n",
      "50 0 1.344321608543396\n",
      "Validation loss: 2.581092994824975 RMSE: 1.6065781\n",
      "51 21 0.32423046231269836\n",
      "Validation loss: 2.2019330695667096 RMSE: 1.4838911\n",
      "Validation loss: 2.406897954181232 RMSE: 1.5514178\n",
      "53 13 0.44780123233795166\n",
      "Validation loss: 1.9160197361380653 RMSE: 1.3842037\n",
      "Validation loss: 1.8244459207079051 RMSE: 1.3507204\n",
      "55 5 0.9938667416572571\n",
      "Validation loss: 3.6161536242054626 RMSE: 1.9016186\n",
      "56 26 1.734915852546692\n",
      "Validation loss: 3.8497819562928868 RMSE: 1.9620861\n",
      "Validation loss: 2.5980447499097976 RMSE: 1.6118451\n",
      "58 18 0.3790709972381592\n",
      "Validation loss: 2.2856739137025004 RMSE: 1.5118446\n",
      "Validation loss: 1.909698878769326 RMSE: 1.3819187\n",
      "60 10 0.8814273476600647\n",
      "Validation loss: 5.487138857883689 RMSE: 2.3424642\n",
      "Validation loss: 1.7980860819858788 RMSE: 1.3409274\n",
      "62 2 0.4742455780506134\n",
      "Validation loss: 2.0375700418928027 RMSE: 1.4274348\n",
      "63 23 0.602473258972168\n",
      "Validation loss: 2.994005281313331 RMSE: 1.7303194\n",
      "Validation loss: 2.1608038803117466 RMSE: 1.4699674\n",
      "65 15 0.8498629331588745\n",
      "Validation loss: 2.415291324125982 RMSE: 1.5541208\n",
      "Validation loss: 2.1698136435145825 RMSE: 1.4730288\n",
      "67 7 0.7771582007408142\n",
      "Validation loss: 2.648738903281963 RMSE: 1.6274947\n",
      "68 28 2.4596734046936035\n",
      "Validation loss: 1.7070775960398987 RMSE: 1.3065518\n",
      "Validation loss: 2.048451665228447 RMSE: 1.4312413\n",
      "70 20 0.7407054901123047\n",
      "Validation loss: 2.0799489105697226 RMSE: 1.4422027\n",
      "Validation loss: 1.6154026552639176 RMSE: 1.2709849\n",
      "72 12 0.656295657157898\n",
      "Validation loss: 2.822547499057466 RMSE: 1.6800438\n",
      "Validation loss: 1.885147308881304 RMSE: 1.3730067\n",
      "74 4 0.8422614932060242\n",
      "Validation loss: 2.0312806853150898 RMSE: 1.42523\n",
      "75 25 1.2025617361068726\n",
      "Validation loss: 2.079859761010229 RMSE: 1.4421719\n",
      "Validation loss: 1.924175283550161 RMSE: 1.3871465\n",
      "77 17 0.5470389127731323\n",
      "Validation loss: 1.8003157134604666 RMSE: 1.3417584\n",
      "Validation loss: 1.6181437832064334 RMSE: 1.2720628\n",
      "79 9 0.37693893909454346\n",
      "Validation loss: 1.8015803404613935 RMSE: 1.3422296\n",
      "Validation loss: 1.8842136052857459 RMSE: 1.3726666\n",
      "81 1 0.5968379974365234\n",
      "Validation loss: 1.7166064935447896 RMSE: 1.3101933\n",
      "82 22 0.5978913903236389\n",
      "Validation loss: 1.8195281113143515 RMSE: 1.3488989\n",
      "Validation loss: 1.8612425517191928 RMSE: 1.3642735\n",
      "84 14 0.57929527759552\n",
      "Validation loss: 1.784834709842648 RMSE: 1.3359771\n",
      "Validation loss: 2.1466281857110756 RMSE: 1.4651376\n",
      "86 6 0.7006015181541443\n",
      "Validation loss: 2.1690703514402947 RMSE: 1.4727765\n",
      "87 27 0.7252326607704163\n",
      "Validation loss: 1.9306856974036293 RMSE: 1.3894911\n",
      "Validation loss: 2.1157374318722075 RMSE: 1.4545574\n",
      "89 19 0.331971138715744\n",
      "Validation loss: 2.55745131779561 RMSE: 1.5992032\n",
      "Validation loss: 1.6666166276003407 RMSE: 1.2909751\n",
      "91 11 0.9945710301399231\n",
      "Validation loss: 2.148441183883532 RMSE: 1.4657562\n",
      "Validation loss: 2.465644311060948 RMSE: 1.5702372\n",
      "93 3 0.8722163438796997\n",
      "Validation loss: 1.8429481930437341 RMSE: 1.3575523\n",
      "94 24 0.5384295582771301\n",
      "Validation loss: 1.6325316608479592 RMSE: 1.2777057\n",
      "Validation loss: 1.654460616871319 RMSE: 1.2862583\n",
      "96 16 0.5301472544670105\n",
      "Validation loss: 2.058404023668407 RMSE: 1.4347138\n",
      "Validation loss: 1.8875436761737925 RMSE: 1.3738791\n",
      "98 8 0.5449192523956299\n",
      "Validation loss: 2.307655072845189 RMSE: 1.5190969\n",
      "Validation loss: 2.1435261585016168 RMSE: 1.4640785\n",
      "100 0 0.8477634191513062\n",
      "Validation loss: 2.7668184073625413 RMSE: 1.6633755\n",
      "101 21 0.31501758098602295\n",
      "Validation loss: 1.7788847895850122 RMSE: 1.3337483\n",
      "Validation loss: 2.460339136883221 RMSE: 1.568547\n",
      "103 13 0.5237648487091064\n",
      "Validation loss: 2.2222942145524827 RMSE: 1.4907361\n",
      "Validation loss: 2.362644129094824 RMSE: 1.5370895\n",
      "105 5 0.7799127697944641\n",
      "Validation loss: 2.196824860783805 RMSE: 1.4821689\n",
      "106 26 1.3645676374435425\n",
      "Validation loss: 1.9864795624682334 RMSE: 1.4094253\n",
      "Validation loss: 1.765241703100964 RMSE: 1.328624\n",
      "108 18 0.2769458591938019\n",
      "Validation loss: 2.5161311832149473 RMSE: 1.5862317\n",
      "Validation loss: 1.970074444745494 RMSE: 1.4035934\n",
      "110 10 0.4193962812423706\n",
      "Validation loss: 2.853030455850922 RMSE: 1.6890918\n",
      "Validation loss: 2.2148791477743504 RMSE: 1.4882472\n",
      "112 2 0.420962929725647\n",
      "Validation loss: 2.4467972464266077 RMSE: 1.5642241\n",
      "113 23 0.698369562625885\n",
      "Validation loss: 2.4119561667990896 RMSE: 1.5530474\n",
      "Validation loss: 1.5260216757259537 RMSE: 1.2353225\n",
      "115 15 0.5792765021324158\n",
      "Validation loss: 2.4996185872407084 RMSE: 1.5810183\n",
      "Validation loss: 2.0482681514942542 RMSE: 1.4311771\n",
      "117 7 0.782431960105896\n",
      "Validation loss: 1.6275548913837534 RMSE: 1.2757566\n",
      "118 28 0.6340715885162354\n",
      "Validation loss: 2.0797319348934478 RMSE: 1.4421276\n",
      "Validation loss: 2.021425555237627 RMSE: 1.4217685\n",
      "120 20 0.43470579385757446\n",
      "Validation loss: 1.7701017645608008 RMSE: 1.3304517\n",
      "Validation loss: 1.9509695937148237 RMSE: 1.3967711\n",
      "122 12 0.38531941175460815\n",
      "Validation loss: 2.199369890499959 RMSE: 1.4830272\n",
      "Validation loss: 1.7554035075997885 RMSE: 1.3249164\n",
      "124 4 0.6627715229988098\n",
      "Validation loss: 1.685826063156128 RMSE: 1.2983936\n",
      "125 25 0.7927924394607544\n",
      "Validation loss: 1.8126659129573182 RMSE: 1.3463528\n",
      "Validation loss: 1.6426259263426857 RMSE: 1.2816497\n",
      "127 17 0.35551658272743225\n",
      "Validation loss: 1.6268701516421495 RMSE: 1.2754881\n",
      "Validation loss: 2.1847268142531404 RMSE: 1.4780821\n",
      "129 9 0.7173911929130554\n",
      "Validation loss: 1.7091570396338944 RMSE: 1.3073473\n",
      "Validation loss: 2.2170030226749655 RMSE: 1.4889604\n",
      "131 1 1.2314091920852661\n",
      "Validation loss: 2.868865935148391 RMSE: 1.6937728\n",
      "132 22 1.0548453330993652\n",
      "Validation loss: 1.9465103413151428 RMSE: 1.3951739\n",
      "Validation loss: 1.885178745320413 RMSE: 1.3730181\n",
      "134 14 0.9279013872146606\n",
      "Validation loss: 2.508945216119817 RMSE: 1.5839651\n",
      "Validation loss: 1.6856189839607847 RMSE: 1.298314\n",
      "136 6 1.1899060010910034\n",
      "Validation loss: 1.7782485843759723 RMSE: 1.3335099\n",
      "137 27 0.6217537522315979\n",
      "Validation loss: 1.892229966357746 RMSE: 1.3755835\n",
      "Validation loss: 2.024414334676962 RMSE: 1.4228193\n",
      "139 19 0.3018767237663269\n",
      "Validation loss: 1.8516034067204568 RMSE: 1.3607364\n",
      "Validation loss: 2.358036705877929 RMSE: 1.53559\n",
      "141 11 0.315182626247406\n",
      "Validation loss: 1.5024602455375469 RMSE: 1.2257488\n",
      "Validation loss: 1.8231136925452578 RMSE: 1.3502274\n",
      "143 3 0.4584343135356903\n",
      "Validation loss: 1.77737417896237 RMSE: 1.333182\n",
      "144 24 0.6191144585609436\n",
      "Validation loss: 2.2129553746333164 RMSE: 1.4876006\n",
      "Validation loss: 1.3940023531955956 RMSE: 1.1806788\n",
      "146 16 0.372761070728302\n",
      "Validation loss: 1.6335122237163306 RMSE: 1.2780893\n",
      "Validation loss: 2.8998749509321904 RMSE: 1.7029018\n",
      "148 8 0.3134337365627289\n",
      "Validation loss: 2.0241437338094794 RMSE: 1.4227241\n",
      "Validation loss: 2.352155200148051 RMSE: 1.5336738\n",
      "150 0 0.42612388730049133\n",
      "Validation loss: 2.3769730361162034 RMSE: 1.5417435\n",
      "151 21 0.5619733333587646\n",
      "Validation loss: 2.391278634029152 RMSE: 1.546376\n",
      "Validation loss: 2.2682634180625985 RMSE: 1.5060755\n",
      "153 13 0.34427809715270996\n",
      "Validation loss: 3.404045697862068 RMSE: 1.8450058\n",
      "Validation loss: 1.891102369907683 RMSE: 1.3751737\n",
      "155 5 0.3792705535888672\n",
      "Validation loss: 2.138791764731956 RMSE: 1.462461\n",
      "156 26 0.5534073114395142\n",
      "Validation loss: 2.2352393512177255 RMSE: 1.4950715\n",
      "Validation loss: 2.034388928286797 RMSE: 1.42632\n",
      "158 18 0.41827863454818726\n",
      "Validation loss: 2.2043216059693194 RMSE: 1.4846958\n",
      "Validation loss: 2.071166714735791 RMSE: 1.4391547\n",
      "160 10 0.38241568207740784\n",
      "Validation loss: 1.6178184283518158 RMSE: 1.271935\n",
      "Validation loss: 1.548481173219934 RMSE: 1.2443799\n",
      "162 2 1.0816237926483154\n",
      "Validation loss: 1.6945794810236028 RMSE: 1.3017601\n",
      "163 23 0.9196969270706177\n",
      "Validation loss: 1.8401095360781239 RMSE: 1.3565063\n",
      "Validation loss: 1.4789300049300742 RMSE: 1.2161126\n",
      "165 15 0.5359899997711182\n",
      "Validation loss: 1.5576175082046373 RMSE: 1.2480454\n",
      "Validation loss: 1.709819814800161 RMSE: 1.3076007\n",
      "167 7 0.7238439917564392\n",
      "Validation loss: 1.7592153011170109 RMSE: 1.3263541\n",
      "168 28 0.1974978744983673\n",
      "Validation loss: 1.5376743996037847 RMSE: 1.24003\n",
      "Validation loss: 1.9541645197741753 RMSE: 1.3979144\n",
      "170 20 0.4480777084827423\n",
      "Validation loss: 2.115094362106998 RMSE: 1.4543364\n",
      "Validation loss: 1.8154264066071637 RMSE: 1.3473775\n",
      "172 12 0.4244942367076874\n",
      "Validation loss: 1.934836477304982 RMSE: 1.3909839\n",
      "Validation loss: 1.7426502198244618 RMSE: 1.3200947\n",
      "174 4 0.34173262119293213\n",
      "Validation loss: 1.6223774515422045 RMSE: 1.2737259\n",
      "175 25 0.4032565653324127\n",
      "Validation loss: 1.6113742579401067 RMSE: 1.2693992\n",
      "Validation loss: 1.5326194699886626 RMSE: 1.2379903\n",
      "177 17 0.3172113597393036\n",
      "Validation loss: 1.9682284883693255 RMSE: 1.4029356\n",
      "Validation loss: 1.8022802466839816 RMSE: 1.3424904\n",
      "179 9 0.37413156032562256\n",
      "Validation loss: 1.9370146325204225 RMSE: 1.3917667\n",
      "Validation loss: 1.723307381689021 RMSE: 1.3127481\n",
      "181 1 0.652779221534729\n",
      "Validation loss: 1.7300666502091737 RMSE: 1.31532\n",
      "182 22 0.8278559446334839\n",
      "Validation loss: 2.071723855702223 RMSE: 1.4393485\n",
      "Validation loss: 2.250482964304696 RMSE: 1.500161\n",
      "184 14 0.4339729845523834\n",
      "Validation loss: 2.1112387982089964 RMSE: 1.4530102\n",
      "Validation loss: 1.9772593721879268 RMSE: 1.4061506\n",
      "186 6 0.23697835206985474\n",
      "Validation loss: 3.2785035804309675 RMSE: 1.8106638\n",
      "187 27 0.21774666011333466\n",
      "Validation loss: 2.842830936465643 RMSE: 1.6860696\n",
      "Validation loss: 1.8449344202480484 RMSE: 1.3582836\n",
      "189 19 0.6121389865875244\n",
      "Validation loss: 2.6669566251535333 RMSE: 1.6330819\n",
      "Validation loss: 1.9108986960048169 RMSE: 1.3823526\n",
      "191 11 0.30776768922805786\n",
      "Validation loss: 1.7749284917274408 RMSE: 1.3322644\n",
      "Validation loss: 1.5177263970923636 RMSE: 1.2319605\n",
      "193 3 0.26953989267349243\n",
      "Validation loss: 1.9875358290376917 RMSE: 1.4097999\n",
      "194 24 0.5464832186698914\n",
      "Validation loss: 1.6330931418764907 RMSE: 1.2779254\n",
      "Validation loss: 1.9099382299237546 RMSE: 1.3820052\n",
      "196 16 0.2914208173751831\n",
      "Validation loss: 1.9449804873593086 RMSE: 1.3946255\n",
      "Validation loss: 1.7531961782843666 RMSE: 1.3240832\n",
      "198 8 0.701094925403595\n",
      "Validation loss: 1.831826528616711 RMSE: 1.3534498\n",
      "Validation loss: 1.6827550651752843 RMSE: 1.2972105\n",
      "Loaded trained model with success.\n",
      "Test loss: 1.774700630027636 Test RMSE: 1.332179\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:0', 'task_name': 'esol', 'model_type': 'gin', 'model': {'num_layer': 5, 'emb_dim': 300, 'feat_dim': 300, 'drop_ratio': 0.3, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 779, 'task': 'regression', 'data_path': 'data/esol/esol.csv', 'target': ['measured log solubility in mols per litre']}}\n",
      "Running on: cuda:0\n",
      "1127\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/1127\n",
      "Generating scaffold 1000/1127\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 14.839805603027344\n",
      "Validation loss: 2.7355015320060527 RMSE: 1.6539352\n",
      "1 21 1.9654078483581543\n",
      "Validation loss: 5.204608294816143 RMSE: 2.2813613\n",
      "Validation loss: 4.111663539852716 RMSE: 2.0277238\n",
      "3 13 1.7336199283599854\n",
      "Validation loss: 10.400829129514442 RMSE: 3.2250316\n",
      "Validation loss: 6.118032932281494 RMSE: 2.4734657\n",
      "5 5 1.494010329246521\n",
      "Validation loss: 3.423120817251965 RMSE: 1.8501679\n",
      "6 26 1.5517404079437256\n",
      "Validation loss: 3.129153622990161 RMSE: 1.7689415\n",
      "Validation loss: 3.0869073150432214 RMSE: 1.7569597\n",
      "8 18 1.2390174865722656\n",
      "Validation loss: 2.630299287559712 RMSE: 1.6218197\n",
      "Validation loss: 4.142586146835733 RMSE: 2.0353343\n",
      "10 10 1.1171714067459106\n",
      "Validation loss: 3.2705093611658147 RMSE: 1.8084551\n",
      "Validation loss: 4.133267638957606 RMSE: 2.0330439\n",
      "12 2 1.5136715173721313\n",
      "Validation loss: 2.2353190151991043 RMSE: 1.4950984\n",
      "13 23 1.1945469379425049\n",
      "Validation loss: 1.9264173043512665 RMSE: 1.3879544\n",
      "Validation loss: 3.7551536560058594 RMSE: 1.9378219\n",
      "15 15 1.5923044681549072\n",
      "Validation loss: 2.7131123426741204 RMSE: 1.6471528\n",
      "Validation loss: 4.198248728186683 RMSE: 2.0489628\n",
      "17 7 1.1660016775131226\n",
      "Validation loss: 3.485052119314143 RMSE: 1.8668295\n",
      "18 28 1.1373732089996338\n",
      "Validation loss: 4.301880937761965 RMSE: 2.0740976\n",
      "Validation loss: 8.172075600750679 RMSE: 2.8586843\n",
      "20 20 0.6464738845825195\n",
      "Validation loss: 4.841742325673061 RMSE: 2.2003958\n",
      "Validation loss: 2.6556893449968997 RMSE: 1.6296285\n",
      "22 12 1.6617478132247925\n",
      "Validation loss: 3.1408246428565643 RMSE: 1.7722372\n",
      "Validation loss: 5.460261509481785 RMSE: 2.3367202\n",
      "24 4 2.216991662979126\n",
      "Validation loss: 4.6745508624389105 RMSE: 2.162071\n",
      "25 25 0.7519726157188416\n",
      "Validation loss: 2.736093915669264 RMSE: 1.6541142\n",
      "Validation loss: 5.089285964459444 RMSE: 2.2559447\n",
      "27 17 0.6598265767097473\n",
      "Validation loss: 2.6787370116309783 RMSE: 1.6366848\n",
      "Validation loss: 2.8198470183178386 RMSE: 1.67924\n",
      "29 9 0.9393713474273682\n",
      "Validation loss: 2.779442578290416 RMSE: 1.6671658\n",
      "Validation loss: 5.40064417260938 RMSE: 2.3239286\n",
      "31 1 0.929041862487793\n",
      "Validation loss: 2.0178779753963503 RMSE: 1.4205203\n",
      "32 22 0.6732153296470642\n",
      "Validation loss: 3.266180287420222 RMSE: 1.8072577\n",
      "Validation loss: 2.1797850131988525 RMSE: 1.4764096\n",
      "34 14 1.3558567762374878\n",
      "Validation loss: 3.971305893585745 RMSE: 1.9928136\n",
      "Validation loss: 2.5921898985331038 RMSE: 1.6100279\n",
      "36 6 0.9791044592857361\n",
      "Validation loss: 1.9687870118470319 RMSE: 1.4031348\n",
      "37 27 0.5783706903457642\n",
      "Validation loss: 2.374948145013995 RMSE: 1.5410867\n",
      "Validation loss: 2.387045549080435 RMSE: 1.5450066\n",
      "39 19 1.6026315689086914\n",
      "Validation loss: 4.032840699221181 RMSE: 2.0081933\n",
      "Validation loss: 3.4129596642688314 RMSE: 1.8474197\n",
      "41 11 0.5116384029388428\n",
      "Validation loss: 6.494940217617339 RMSE: 2.548517\n",
      "Validation loss: 3.338598050902375 RMSE: 1.8271831\n",
      "43 3 0.6529607176780701\n",
      "Validation loss: 3.0035093843409446 RMSE: 1.7330636\n",
      "44 24 0.42622166872024536\n",
      "Validation loss: 2.4456987328233972 RMSE: 1.5638729\n",
      "Validation loss: 8.075228539188352 RMSE: 2.8416946\n",
      "46 16 1.0658293962478638\n",
      "Validation loss: 3.135887312678109 RMSE: 1.7708437\n",
      "Validation loss: 3.021752294185942 RMSE: 1.7383188\n",
      "48 8 0.5233972072601318\n",
      "Validation loss: 3.3982864438959983 RMSE: 1.8434441\n",
      "Validation loss: 3.0465496567498267 RMSE: 1.745437\n",
      "50 0 0.5350741744041443\n",
      "Validation loss: 1.9043289633978784 RMSE: 1.3799742\n",
      "51 21 0.9415429830551147\n",
      "Validation loss: 2.105052006983124 RMSE: 1.4508798\n",
      "Validation loss: 2.735991195239852 RMSE: 1.6540834\n",
      "53 13 0.6244257688522339\n",
      "Validation loss: 6.161486191032207 RMSE: 2.482234\n",
      "Validation loss: 5.634181942559977 RMSE: 2.373643\n",
      "55 5 0.43610304594039917\n",
      "Validation loss: 3.970210172433769 RMSE: 1.9925386\n",
      "56 26 1.500217318534851\n",
      "Validation loss: 3.7224652303003634 RMSE: 1.9293691\n",
      "Validation loss: 3.6950238827055535 RMSE: 1.9222444\n",
      "58 18 0.5793441534042358\n",
      "Validation loss: 2.7899986305068025 RMSE: 1.670329\n",
      "Validation loss: 3.002564170719248 RMSE: 1.7327908\n",
      "60 10 0.8974670171737671\n",
      "Validation loss: 2.6250851597406166 RMSE: 1.6202115\n",
      "Validation loss: 4.462189822070367 RMSE: 2.1123896\n",
      "62 2 0.45133209228515625\n",
      "Validation loss: 2.115786729660709 RMSE: 1.4545745\n",
      "63 23 0.32537901401519775\n",
      "Validation loss: 2.076442153052946 RMSE: 1.4409865\n",
      "Validation loss: 2.535603435693589 RMSE: 1.5923578\n",
      "65 15 1.771008014678955\n",
      "Validation loss: 3.349915974963028 RMSE: 1.8302777\n",
      "Validation loss: 2.950419381656478 RMSE: 1.7176785\n",
      "67 7 0.4929131269454956\n",
      "Validation loss: 2.5605003453988946 RMSE: 1.6001563\n",
      "68 28 1.4919302463531494\n",
      "Validation loss: 4.658553887257534 RMSE: 2.1583683\n",
      "Validation loss: 2.179335889563096 RMSE: 1.4762573\n",
      "70 20 0.6583980321884155\n",
      "Validation loss: 3.7409856446021426 RMSE: 1.9341627\n",
      "Validation loss: 2.932682537399562 RMSE: 1.7125077\n",
      "72 12 0.7537688612937927\n",
      "Validation loss: 4.8441567906236225 RMSE: 2.2009444\n",
      "Validation loss: 3.4788007736206055 RMSE: 1.8651543\n",
      "74 4 0.48307836055755615\n",
      "Validation loss: 3.2238782021851664 RMSE: 1.7955161\n",
      "75 25 0.5282208919525146\n",
      "Validation loss: 2.0856489470574706 RMSE: 1.4441776\n",
      "Validation loss: 1.781076417560071 RMSE: 1.3345697\n",
      "77 17 0.9923668503761292\n",
      "Validation loss: 2.056528900576904 RMSE: 1.4340603\n",
      "Validation loss: 2.186946974391431 RMSE: 1.4788331\n",
      "79 9 0.5273521542549133\n",
      "Validation loss: 2.2983298575983637 RMSE: 1.5160245\n",
      "Validation loss: 2.6183298815668157 RMSE: 1.6181254\n",
      "81 1 0.7901751399040222\n",
      "Validation loss: 3.468387086834528 RMSE: 1.8623607\n",
      "82 22 0.7957643270492554\n",
      "Validation loss: 4.2043298282454495 RMSE: 2.0504463\n",
      "Validation loss: 1.905871891342433 RMSE: 1.3805332\n",
      "84 14 0.6146484017372131\n",
      "Validation loss: 3.2338203149559224 RMSE: 1.7982826\n",
      "Validation loss: 2.306424067083713 RMSE: 1.5186915\n",
      "86 6 0.7280957102775574\n",
      "Validation loss: 3.06620527790711 RMSE: 1.7510582\n",
      "87 27 0.7613130211830139\n",
      "Validation loss: 3.010318012364143 RMSE: 1.7350268\n",
      "Validation loss: 4.078186925533599 RMSE: 2.019452\n",
      "89 19 0.6158419847488403\n",
      "Validation loss: 1.748524209039401 RMSE: 1.3223177\n",
      "Validation loss: 2.2222043307481614 RMSE: 1.490706\n",
      "91 11 0.7435609102249146\n",
      "Validation loss: 1.8822183788350197 RMSE: 1.3719397\n",
      "Validation loss: 2.663777454764442 RMSE: 1.6321084\n",
      "93 3 0.6144993305206299\n",
      "Validation loss: 2.853432626850837 RMSE: 1.6892105\n",
      "94 24 0.49476057291030884\n",
      "Validation loss: 2.9476075510008144 RMSE: 1.7168598\n",
      "Validation loss: 2.035701707401107 RMSE: 1.4267802\n",
      "96 16 0.4822043180465698\n",
      "Validation loss: 2.0816485174989277 RMSE: 1.4427919\n",
      "Validation loss: 2.4104399944828674 RMSE: 1.5525591\n",
      "98 8 0.40929049253463745\n",
      "Validation loss: 2.5111003175245976 RMSE: 1.5846452\n",
      "Validation loss: 3.4706086226269206 RMSE: 1.8629569\n",
      "100 0 0.6580424308776855\n",
      "Validation loss: 2.212220689891714 RMSE: 1.4873536\n",
      "101 21 0.851568341255188\n",
      "Validation loss: 2.250398935469906 RMSE: 1.5001329\n",
      "Validation loss: 2.4047789742461347 RMSE: 1.550735\n",
      "103 13 0.5860810279846191\n",
      "Validation loss: 2.6022534075036514 RMSE: 1.6131501\n",
      "Validation loss: 1.9253945202954048 RMSE: 1.3875859\n",
      "105 5 0.5691227316856384\n",
      "Validation loss: 3.2067098396014324 RMSE: 1.7907288\n",
      "106 26 0.6284667253494263\n",
      "Validation loss: 1.9091040581728504 RMSE: 1.3817033\n",
      "Validation loss: 1.936481756446636 RMSE: 1.3915753\n",
      "108 18 0.32955360412597656\n",
      "Validation loss: 1.8356835272459857 RMSE: 1.354874\n",
      "Validation loss: 2.8624192613416013 RMSE: 1.6918685\n",
      "110 10 0.5551863312721252\n",
      "Validation loss: 1.921669493734309 RMSE: 1.386243\n",
      "Validation loss: 2.190693175898189 RMSE: 1.4800991\n",
      "112 2 0.27044400572776794\n",
      "Validation loss: 1.8471574445741366 RMSE: 1.3591015\n",
      "113 23 0.6501147747039795\n",
      "Validation loss: 1.9149245519553666 RMSE: 1.3838079\n",
      "Validation loss: 2.2483987428445733 RMSE: 1.4994663\n",
      "115 15 0.8867672681808472\n",
      "Validation loss: 2.3082125777691864 RMSE: 1.5192803\n",
      "Validation loss: 1.8805234033977036 RMSE: 1.3713218\n",
      "117 7 0.9496855735778809\n",
      "Validation loss: 1.6980882851423416 RMSE: 1.3031071\n",
      "118 28 0.7033469080924988\n",
      "Validation loss: 2.233108484639531 RMSE: 1.4943589\n",
      "Validation loss: 1.6982685011045067 RMSE: 1.3031763\n",
      "120 20 0.46836596727371216\n",
      "Validation loss: 1.713663994738486 RMSE: 1.3090699\n",
      "Validation loss: 1.9091293832897085 RMSE: 1.3817126\n",
      "122 12 0.3875004053115845\n",
      "Validation loss: 2.1864873972614256 RMSE: 1.4786775\n",
      "Validation loss: 1.9372624253804704 RMSE: 1.3918558\n",
      "124 4 1.0471887588500977\n",
      "Validation loss: 2.9293713801729995 RMSE: 1.7115407\n",
      "125 25 0.43815526366233826\n",
      "Validation loss: 2.466761156521012 RMSE: 1.5705925\n",
      "Validation loss: 2.1036861618008236 RMSE: 1.450409\n",
      "127 17 1.3371070623397827\n",
      "Validation loss: 3.7169849724896187 RMSE: 1.9279485\n",
      "Validation loss: 3.427887237177486 RMSE: 1.8514555\n",
      "129 9 0.384037584066391\n",
      "Validation loss: 2.449243662631617 RMSE: 1.5650059\n",
      "Validation loss: 2.1668442941344943 RMSE: 1.4720205\n",
      "131 1 0.5074283480644226\n",
      "Validation loss: 2.137744720003246 RMSE: 1.4621029\n",
      "132 22 0.23447944223880768\n",
      "Validation loss: 2.980694386811383 RMSE: 1.7264687\n",
      "Validation loss: 1.561303263216947 RMSE: 1.2495213\n",
      "134 14 0.6548177003860474\n",
      "Validation loss: 2.4578165712609756 RMSE: 1.5677425\n",
      "Validation loss: 3.3979086622727657 RMSE: 1.8433417\n",
      "136 6 0.4285001754760742\n",
      "Validation loss: 3.3673265644934327 RMSE: 1.8350277\n",
      "137 27 0.3123983144760132\n",
      "Validation loss: 4.155251306770122 RMSE: 2.0384433\n",
      "Validation loss: 1.700166893216361 RMSE: 1.3039045\n",
      "139 19 1.1794884204864502\n",
      "Validation loss: 1.6735736827934737 RMSE: 1.2936667\n",
      "Validation loss: 3.5200581845983994 RMSE: 1.8761818\n",
      "141 11 0.7783536314964294\n",
      "Validation loss: 2.438884300468242 RMSE: 1.5616927\n",
      "Validation loss: 1.870921619170535 RMSE: 1.3678164\n",
      "143 3 0.2494259476661682\n",
      "Validation loss: 2.019854178470848 RMSE: 1.4212158\n",
      "144 24 0.744172215461731\n",
      "Validation loss: 2.6480823090646117 RMSE: 1.6272929\n",
      "Validation loss: 2.3395644036014525 RMSE: 1.5295635\n",
      "146 16 1.271708369255066\n",
      "Validation loss: 1.5676310262848845 RMSE: 1.2520506\n",
      "Validation loss: 2.1776931918827835 RMSE: 1.4757009\n",
      "148 8 0.3708876073360443\n",
      "Validation loss: 2.038762698131325 RMSE: 1.4278524\n",
      "Validation loss: 2.2448302644543943 RMSE: 1.4982758\n",
      "150 0 0.6880297064781189\n",
      "Validation loss: 1.4892296643383736 RMSE: 1.2203399\n",
      "151 21 0.4286198616027832\n",
      "Validation loss: 2.5133996410707455 RMSE: 1.5853705\n",
      "Validation loss: 2.509720650394406 RMSE: 1.5842098\n",
      "153 13 0.4706672728061676\n",
      "Validation loss: 2.074532466652119 RMSE: 1.4403237\n",
      "Validation loss: 1.8474709955991897 RMSE: 1.359217\n",
      "155 5 0.4862750172615051\n",
      "Validation loss: 2.5459999679464156 RMSE: 1.5956191\n",
      "156 26 0.6704494953155518\n",
      "Validation loss: 1.811358529909522 RMSE: 1.3458673\n",
      "Validation loss: 2.2332505757829786 RMSE: 1.4944063\n",
      "158 18 0.3824620842933655\n",
      "Validation loss: 1.8079290126277283 RMSE: 1.3445926\n",
      "Validation loss: 3.142534262311142 RMSE: 1.7727194\n",
      "160 10 0.6033687591552734\n",
      "Validation loss: 2.1595820853140504 RMSE: 1.4695517\n",
      "Validation loss: 1.6785209875191207 RMSE: 1.2955774\n",
      "162 2 0.5763581991195679\n",
      "Validation loss: 2.3600913718738386 RMSE: 1.5362588\n",
      "163 23 0.38342949748039246\n",
      "Validation loss: 2.2505352054022056 RMSE: 1.5001785\n",
      "Validation loss: 2.8398673154611505 RMSE: 1.6851904\n",
      "165 15 0.2915620505809784\n",
      "Validation loss: 2.189564319838465 RMSE: 1.4797177\n",
      "Validation loss: 2.0159027555347544 RMSE: 1.419825\n",
      "167 7 0.32349124550819397\n",
      "Validation loss: 1.9582529975249705 RMSE: 1.3993759\n",
      "168 28 1.9197584390640259\n",
      "Validation loss: 2.516227627222517 RMSE: 1.5862622\n",
      "Validation loss: 2.556241496474342 RMSE: 1.598825\n",
      "170 20 0.3224095106124878\n",
      "Validation loss: 2.0271195320956474 RMSE: 1.4237695\n",
      "Validation loss: 2.3097564414539167 RMSE: 1.5197883\n",
      "172 12 0.8229004740715027\n",
      "Validation loss: 1.6397248622590461 RMSE: 1.2805175\n",
      "Validation loss: 2.042553295076421 RMSE: 1.4291793\n",
      "174 4 0.48309358954429626\n",
      "Validation loss: 2.2534918510808355 RMSE: 1.5011635\n",
      "175 25 0.3799055814743042\n",
      "Validation loss: 1.7959306725358541 RMSE: 1.3401234\n",
      "Validation loss: 1.8885023994783385 RMSE: 1.3742279\n",
      "177 17 0.3104678988456726\n",
      "Validation loss: 1.9960486899434993 RMSE: 1.4128158\n",
      "Validation loss: 2.1464786213056177 RMSE: 1.4650866\n",
      "179 9 0.7630314230918884\n",
      "Validation loss: 1.7973352290887747 RMSE: 1.3406475\n",
      "Validation loss: 2.2118896256505916 RMSE: 1.4872422\n",
      "181 1 0.2974308133125305\n",
      "Validation loss: 1.80606736757059 RMSE: 1.3439\n",
      "182 22 0.7158632278442383\n",
      "Validation loss: 2.081122870993825 RMSE: 1.4426098\n",
      "Validation loss: 3.197448342247347 RMSE: 1.788141\n",
      "184 14 0.3525022268295288\n",
      "Validation loss: 2.109063044058538 RMSE: 1.4522613\n",
      "Validation loss: 2.1060593724250793 RMSE: 1.4512268\n",
      "186 6 0.5280334949493408\n",
      "Validation loss: 1.8298271504123653 RMSE: 1.352711\n",
      "187 27 0.28415367007255554\n",
      "Validation loss: 2.0728640872820288 RMSE: 1.4397445\n",
      "Validation loss: 2.5416008139078596 RMSE: 1.5942398\n",
      "189 19 0.31928086280822754\n",
      "Validation loss: 2.172394946613143 RMSE: 1.4739046\n",
      "Validation loss: 1.7742164483112572 RMSE: 1.3319972\n",
      "191 11 0.39343345165252686\n",
      "Validation loss: 1.9749169286373442 RMSE: 1.4053173\n",
      "Validation loss: 1.5706102695085307 RMSE: 1.2532399\n",
      "193 3 0.6739643216133118\n",
      "Validation loss: 2.051648065052201 RMSE: 1.4323575\n",
      "194 24 0.5010974407196045\n",
      "Validation loss: 1.7810427235291066 RMSE: 1.334557\n",
      "Validation loss: 1.469214149280987 RMSE: 1.2121115\n",
      "196 16 0.2715814709663391\n",
      "Validation loss: 2.2686178198957867 RMSE: 1.5061932\n",
      "Validation loss: 2.689803849279353 RMSE: 1.6400621\n",
      "198 8 0.9106476902961731\n",
      "Validation loss: 2.698125130307358 RMSE: 1.6425972\n",
      "Validation loss: 2.4491304676089665 RMSE: 1.5649699\n",
      "Loaded trained model with success.\n",
      "Test loss: 1.8086467612106187 Test RMSE: 1.3448594\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:0', 'task_name': 'esol', 'model_type': 'gin', 'model': {'num_layer': 5, 'emb_dim': 300, 'feat_dim': 300, 'drop_ratio': 0.3, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 780, 'task': 'regression', 'data_path': 'data/esol/esol.csv', 'target': ['measured log solubility in mols per litre']}}\n",
      "Running on: cuda:0\n",
      "1127\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/1127\n",
      "Generating scaffold 1000/1127\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 17.151592254638672\n",
      "Validation loss: 2.7364855825373557 RMSE: 1.6542326\n",
      "1 21 3.675234079360962\n",
      "Validation loss: 4.492775379028996 RMSE: 2.119617\n",
      "Validation loss: 6.350762130939855 RMSE: 2.5200717\n",
      "3 13 2.0940780639648438\n",
      "Validation loss: 6.413239673175643 RMSE: 2.5324376\n",
      "Validation loss: 5.193860805140132 RMSE: 2.2790043\n",
      "5 5 1.3337352275848389\n",
      "Validation loss: 6.275390338053746 RMSE: 2.505073\n",
      "6 26 1.500174880027771\n",
      "Validation loss: 3.22416568224409 RMSE: 1.7955962\n",
      "Validation loss: 6.767274286894672 RMSE: 2.6013985\n",
      "8 18 1.3184462785720825\n",
      "Validation loss: 6.498242749577075 RMSE: 2.549165\n",
      "Validation loss: 6.632062101786116 RMSE: 2.575279\n",
      "10 10 0.8578850626945496\n",
      "Validation loss: 7.662762962611375 RMSE: 2.7681696\n",
      "Validation loss: 4.061784292744324 RMSE: 2.015387\n",
      "12 2 0.8994168639183044\n",
      "Validation loss: 5.4528636425997306 RMSE: 2.3351367\n",
      "13 23 1.6768975257873535\n",
      "Validation loss: 8.568351429120629 RMSE: 2.9271748\n",
      "Validation loss: 5.101630561119687 RMSE: 2.258679\n",
      "15 15 1.4168058633804321\n",
      "Validation loss: 3.0351688798549956 RMSE: 1.7421736\n",
      "Validation loss: 4.434237056073889 RMSE: 2.1057627\n",
      "17 7 0.7304533123970032\n",
      "Validation loss: 9.155109886574534 RMSE: 3.025741\n",
      "18 28 4.484230995178223\n",
      "Validation loss: 5.895823529336305 RMSE: 2.4281318\n",
      "Validation loss: 5.6738081780155145 RMSE: 2.3819757\n",
      "20 20 0.6355273723602295\n",
      "Validation loss: 2.0875383267360452 RMSE: 1.4448316\n",
      "Validation loss: 3.8093498639300862 RMSE: 1.9517556\n",
      "22 12 0.7048773169517517\n",
      "Validation loss: 4.14289801310649 RMSE: 2.0354111\n",
      "Validation loss: 6.077819309403411 RMSE: 2.4653232\n",
      "24 4 0.9512601494789124\n",
      "Validation loss: 5.910551488926981 RMSE: 2.4311626\n",
      "25 25 0.7216740846633911\n",
      "Validation loss: 3.9264042989342616 RMSE: 1.9815156\n",
      "Validation loss: 6.114751659663377 RMSE: 2.4728022\n",
      "27 17 0.9657043218612671\n",
      "Validation loss: 2.8880876186674675 RMSE: 1.6994375\n",
      "Validation loss: 4.164407763860922 RMSE: 2.040688\n",
      "29 9 0.5281250476837158\n",
      "Validation loss: 3.079834045562069 RMSE: 1.7549456\n",
      "Validation loss: 3.5381886494898165 RMSE: 1.8810073\n",
      "31 1 0.8826203942298889\n",
      "Validation loss: 3.004581172909357 RMSE: 1.7333728\n",
      "32 22 0.5636681914329529\n",
      "Validation loss: 4.944438689577896 RMSE: 2.2236094\n",
      "Validation loss: 2.9607546519389194 RMSE: 1.7206844\n",
      "34 14 0.7081009149551392\n",
      "Validation loss: 4.656897401387712 RMSE: 2.1579845\n",
      "Validation loss: 3.771039614635231 RMSE: 1.9419165\n",
      "36 6 1.1322622299194336\n",
      "Validation loss: 4.205147515356013 RMSE: 2.0506456\n",
      "37 27 1.2068791389465332\n",
      "Validation loss: 4.574764563974026 RMSE: 2.1388698\n",
      "Validation loss: 1.9902398449129763 RMSE: 1.4107587\n",
      "39 19 1.469701886177063\n",
      "Validation loss: 5.517837186830234 RMSE: 2.3490078\n",
      "Validation loss: 5.533432795938137 RMSE: 2.352325\n",
      "41 11 0.6198468804359436\n",
      "Validation loss: 3.573854564565473 RMSE: 1.8904642\n",
      "Validation loss: 4.593533266962102 RMSE: 2.143253\n",
      "43 3 0.5321832895278931\n",
      "Validation loss: 9.368233224986929 RMSE: 3.060757\n",
      "44 24 0.5382149815559387\n",
      "Validation loss: 2.4756254748960513 RMSE: 1.573412\n",
      "Validation loss: 4.334134760156142 RMSE: 2.0818586\n",
      "46 16 1.0778131484985352\n",
      "Validation loss: 4.995647109715285 RMSE: 2.2350943\n",
      "Validation loss: 4.054095027721034 RMSE: 2.0134783\n",
      "48 8 0.7304608821868896\n",
      "Validation loss: 4.326980652007381 RMSE: 2.0801396\n",
      "Validation loss: 2.6966546731712544 RMSE: 1.6421493\n",
      "50 0 0.42150282859802246\n",
      "Validation loss: 3.151979075068921 RMSE: 1.7753814\n",
      "51 21 0.8474127054214478\n",
      "Validation loss: 5.814485963466948 RMSE: 2.4113245\n",
      "Validation loss: 2.0268679530219695 RMSE: 1.4236811\n",
      "53 13 0.8481626510620117\n",
      "Validation loss: 5.251112131945855 RMSE: 2.2915304\n",
      "Validation loss: 4.198477559385046 RMSE: 2.0490186\n",
      "55 5 0.9681227207183838\n",
      "Validation loss: 3.2086017195102388 RMSE: 1.791257\n",
      "56 26 0.25424525141716003\n",
      "Validation loss: 4.080077643943044 RMSE: 2.01992\n",
      "Validation loss: 2.382636722210234 RMSE: 1.5435792\n",
      "58 18 0.6241647005081177\n",
      "Validation loss: 3.926325839177697 RMSE: 1.9814959\n",
      "Validation loss: 2.7554072458132177 RMSE: 1.6599419\n",
      "60 10 0.7400394082069397\n",
      "Validation loss: 5.124318776932438 RMSE: 2.263696\n",
      "Validation loss: 4.2739773471798514 RMSE: 2.06736\n",
      "62 2 0.4241316020488739\n",
      "Validation loss: 3.477017246516405 RMSE: 1.8646762\n",
      "63 23 1.0535666942596436\n",
      "Validation loss: 3.600183351904945 RMSE: 1.8974149\n",
      "Validation loss: 3.7902422499867665 RMSE: 1.9468545\n",
      "65 15 0.33223313093185425\n",
      "Validation loss: 3.109712474114072 RMSE: 1.7634376\n",
      "Validation loss: 2.118333151910157 RMSE: 1.4554496\n",
      "67 7 0.44194528460502625\n",
      "Validation loss: 4.888092332181677 RMSE: 2.2109032\n",
      "68 28 2.744030475616455\n",
      "Validation loss: 3.5771000659571284 RMSE: 1.8913223\n",
      "Validation loss: 2.7377299133655244 RMSE: 1.6546086\n",
      "70 20 1.1563055515289307\n",
      "Validation loss: 2.3165134318106997 RMSE: 1.5220097\n",
      "Validation loss: 4.879514428366602 RMSE: 2.2089624\n",
      "72 12 0.6713591814041138\n",
      "Validation loss: 3.4772342939292433 RMSE: 1.8647343\n",
      "Validation loss: 2.260084731388936 RMSE: 1.5033578\n",
      "74 4 1.2461150884628296\n",
      "Validation loss: 4.727580045176818 RMSE: 2.1743\n",
      "75 25 0.5170499086380005\n",
      "Validation loss: 3.5204936993860567 RMSE: 1.876298\n",
      "Validation loss: 3.4205280194240335 RMSE: 1.8494669\n",
      "77 17 1.0399022102355957\n",
      "Validation loss: 2.020969920453772 RMSE: 1.4216082\n",
      "Validation loss: 3.641080138957606 RMSE: 1.9081614\n",
      "79 9 0.8244091868400574\n",
      "Validation loss: 3.1504364382904186 RMSE: 1.7749469\n",
      "Validation loss: 4.2856010588924445 RMSE: 2.0701694\n",
      "81 1 0.6345154047012329\n",
      "Validation loss: 5.1792175031341285 RMSE: 2.2757895\n",
      "82 22 0.319391667842865\n",
      "Validation loss: 6.105688124631358 RMSE: 2.470969\n",
      "Validation loss: 4.0070474021202696 RMSE: 2.0017612\n",
      "84 14 0.32003727555274963\n",
      "Validation loss: 4.57419827976058 RMSE: 2.1387374\n",
      "Validation loss: 1.9759277938741497 RMSE: 1.405677\n",
      "86 6 0.5147292017936707\n",
      "Validation loss: 2.9108600173376304 RMSE: 1.7061243\n",
      "87 27 0.5452378988265991\n",
      "Validation loss: 2.958352076268829 RMSE: 1.719986\n",
      "Validation loss: 2.2126444921029353 RMSE: 1.487496\n",
      "89 19 0.8761911988258362\n",
      "Validation loss: 4.163673514813448 RMSE: 2.040508\n",
      "Validation loss: 2.7438687982812393 RMSE: 1.6564628\n",
      "91 11 0.808224618434906\n",
      "Validation loss: 5.348475532194154 RMSE: 2.3126771\n",
      "Validation loss: 2.972779263437322 RMSE: 1.724175\n",
      "93 3 0.5620481371879578\n",
      "Validation loss: 7.403989859386883 RMSE: 2.7210271\n",
      "94 24 0.637148916721344\n",
      "Validation loss: 2.5040786878197596 RMSE: 1.5824281\n",
      "Validation loss: 2.867493420575572 RMSE: 1.6933674\n",
      "96 16 0.7634813189506531\n",
      "Validation loss: 5.385155878235809 RMSE: 2.3205938\n",
      "Validation loss: 3.9791521245399406 RMSE: 1.9947811\n",
      "98 8 0.7844833135604858\n",
      "Validation loss: 3.8708814055518768 RMSE: 1.9674555\n",
      "Validation loss: 6.006818923275028 RMSE: 2.4508812\n",
      "100 0 0.2782573103904724\n",
      "Validation loss: 2.788596495062904 RMSE: 1.6699091\n",
      "101 21 0.81553053855896\n",
      "Validation loss: 2.9582528329528537 RMSE: 1.7199571\n",
      "Validation loss: 3.128229101147272 RMSE: 1.76868\n",
      "103 13 0.356884241104126\n",
      "Validation loss: 2.246102501860762 RMSE: 1.4987003\n",
      "Validation loss: 2.890310696795978 RMSE: 1.7000915\n",
      "105 5 0.5528066158294678\n",
      "Validation loss: 6.461473751912075 RMSE: 2.5419428\n",
      "106 26 0.41619327664375305\n",
      "Validation loss: 2.4965913633329677 RMSE: 1.5800606\n",
      "Validation loss: 4.088862611129221 RMSE: 2.0220935\n",
      "108 18 0.6615805625915527\n",
      "Validation loss: 2.5604489123926752 RMSE: 1.6001402\n",
      "Validation loss: 4.185365670550186 RMSE: 2.0458167\n",
      "110 10 0.8951476812362671\n",
      "Validation loss: 2.7626072199998704 RMSE: 1.6621093\n",
      "Validation loss: 3.7263061135216096 RMSE: 1.9303643\n",
      "112 2 0.6523463726043701\n",
      "Validation loss: 3.7970957967032373 RMSE: 1.9486138\n",
      "113 23 0.3228965699672699\n",
      "Validation loss: 3.944056578442059 RMSE: 1.9859649\n",
      "Validation loss: 2.5176774738109216 RMSE: 1.586719\n",
      "115 15 1.0223544836044312\n",
      "Validation loss: 3.0628998174076587 RMSE: 1.7501143\n",
      "Validation loss: 2.666949510574341 RMSE: 1.6330798\n",
      "117 7 0.5375285744667053\n",
      "Validation loss: 4.093635791170914 RMSE: 2.0232737\n",
      "118 28 0.8090494871139526\n",
      "Validation loss: 2.3273922996183414 RMSE: 1.5255793\n",
      "Validation loss: 2.135619444129741 RMSE: 1.4613758\n",
      "120 20 0.3167917728424072\n",
      "Validation loss: 3.462747392401231 RMSE: 1.8608459\n",
      "Validation loss: 3.0748039452375564 RMSE: 1.7535119\n",
      "122 12 0.543658971786499\n",
      "Validation loss: 2.472599795434327 RMSE: 1.5724503\n",
      "Validation loss: 2.727608786220044 RMSE: 1.6515473\n",
      "124 4 1.3882535696029663\n",
      "Validation loss: 1.9512621786742084 RMSE: 1.3968759\n",
      "125 25 0.36221450567245483\n",
      "Validation loss: 2.1041460079429424 RMSE: 1.4505675\n",
      "Validation loss: 2.2591805204880977 RMSE: 1.5030571\n",
      "127 17 0.807199239730835\n",
      "Validation loss: 4.73057161179264 RMSE: 2.1749876\n",
      "Validation loss: 2.7366822061285507 RMSE: 1.6542921\n",
      "129 9 0.927039384841919\n",
      "Validation loss: 2.3312047245228187 RMSE: 1.5268284\n",
      "Validation loss: 4.217591787861512 RMSE: 2.0536776\n",
      "131 1 0.44073817133903503\n",
      "Validation loss: 3.6319971654267436 RMSE: 1.9057798\n",
      "132 22 0.5455828309059143\n",
      "Validation loss: 2.4422215402653786 RMSE: 1.562761\n",
      "Validation loss: 2.9143061099854193 RMSE: 1.7071339\n",
      "134 14 0.7788516283035278\n",
      "Validation loss: 2.286387785346107 RMSE: 1.5120807\n",
      "Validation loss: 2.0621081495707014 RMSE: 1.4360042\n",
      "136 6 0.6134408712387085\n",
      "Validation loss: 1.9889545609465742 RMSE: 1.410303\n",
      "137 27 0.4331844449043274\n",
      "Validation loss: 5.0590110373708 RMSE: 2.2492244\n",
      "Validation loss: 3.2762528921650573 RMSE: 1.8100423\n",
      "139 19 0.6447293758392334\n",
      "Validation loss: 2.1568707544191748 RMSE: 1.4686289\n",
      "Validation loss: 3.6717834599250185 RMSE: 1.9161898\n",
      "141 11 0.49158069491386414\n",
      "Validation loss: 2.1321139019147486 RMSE: 1.460176\n",
      "Validation loss: 2.745318070977135 RMSE: 1.6569\n",
      "143 3 0.5882783532142639\n",
      "Validation loss: 2.689440452947026 RMSE: 1.6399513\n",
      "144 24 0.1734430491924286\n",
      "Validation loss: 3.0053989180421405 RMSE: 1.7336086\n",
      "Validation loss: 3.1376223690741885 RMSE: 1.7713336\n",
      "146 16 0.3138270676136017\n",
      "Validation loss: 2.757297697320449 RMSE: 1.6605113\n",
      "Validation loss: 2.645566279909252 RMSE: 1.6265197\n",
      "148 8 0.7230282425880432\n",
      "Validation loss: 2.4173598943558416 RMSE: 1.5547861\n",
      "Validation loss: 4.106572682878612 RMSE: 2.026468\n",
      "150 0 0.45780932903289795\n",
      "Validation loss: 2.0173655917159223 RMSE: 1.42034\n",
      "151 21 1.816454291343689\n",
      "Validation loss: 3.1902720084232565 RMSE: 1.7861333\n",
      "Validation loss: 1.9725581017215694 RMSE: 1.4044778\n",
      "153 13 0.5254207253456116\n",
      "Validation loss: 6.312996986692986 RMSE: 2.5125678\n",
      "Validation loss: 2.8550089777043435 RMSE: 1.6896771\n",
      "155 5 0.4666702449321747\n",
      "Validation loss: 2.5244852272810134 RMSE: 1.5888628\n",
      "156 26 0.3866472840309143\n",
      "Validation loss: 3.1210196925475535 RMSE: 1.7666407\n",
      "Validation loss: 4.02188493931188 RMSE: 2.0054638\n",
      "158 18 0.22308608889579773\n",
      "Validation loss: 4.872404900272335 RMSE: 2.2073524\n",
      "Validation loss: 2.9707769799021495 RMSE: 1.7235941\n",
      "160 10 0.9419118165969849\n",
      "Validation loss: 2.0061775118903777 RMSE: 1.4163959\n",
      "Validation loss: 2.058774556733866 RMSE: 1.4348431\n",
      "162 2 1.1135952472686768\n",
      "Validation loss: 1.9716967966704242 RMSE: 1.4041712\n",
      "163 23 0.6310576796531677\n",
      "Validation loss: 2.3015789932909265 RMSE: 1.5170956\n",
      "Validation loss: 2.976742441675304 RMSE: 1.7253238\n",
      "165 15 0.2763797342777252\n",
      "Validation loss: 2.032126237860823 RMSE: 1.4255266\n",
      "Validation loss: 2.5581575948580175 RMSE: 1.599424\n",
      "167 7 0.3700748682022095\n",
      "Validation loss: 1.9534745490656489 RMSE: 1.3976675\n",
      "168 28 0.17314225435256958\n",
      "Validation loss: 2.955876910580998 RMSE: 1.7192664\n",
      "Validation loss: 2.2821302772623246 RMSE: 1.510672\n",
      "170 20 0.5280086398124695\n",
      "Validation loss: 2.721717330206812 RMSE: 1.6497627\n",
      "Validation loss: 2.5792883012147074 RMSE: 1.6060164\n",
      "172 12 0.697001039981842\n",
      "Validation loss: 2.1280923448832687 RMSE: 1.4587983\n",
      "Validation loss: 2.3421270024459973 RMSE: 1.5304009\n",
      "174 4 0.44750601053237915\n",
      "Validation loss: 2.7648011097865823 RMSE: 1.6627692\n",
      "175 25 0.5833204984664917\n",
      "Validation loss: 1.9301277620602497 RMSE: 1.3892903\n",
      "Validation loss: 2.08690599635639 RMSE: 1.4446129\n",
      "177 17 0.4945371448993683\n",
      "Validation loss: 1.8151723441824448 RMSE: 1.3472834\n",
      "Validation loss: 2.4226033033522882 RMSE: 1.5564713\n",
      "179 9 0.2642335593700409\n",
      "Validation loss: 1.8811408597811135 RMSE: 1.3715469\n",
      "Validation loss: 1.826920568415549 RMSE: 1.3516363\n",
      "181 1 0.45087969303131104\n",
      "Validation loss: 2.2552835498235924 RMSE: 1.5017601\n",
      "182 22 0.2615180015563965\n",
      "Validation loss: 2.193060936126034 RMSE: 1.4808986\n",
      "Validation loss: 2.1357296019528818 RMSE: 1.4614135\n",
      "184 14 0.7920554280281067\n",
      "Validation loss: 4.520305975348548 RMSE: 2.126101\n",
      "Validation loss: 2.27614176167851 RMSE: 1.5086887\n",
      "186 6 0.33199208974838257\n",
      "Validation loss: 2.5626926063436324 RMSE: 1.6008413\n",
      "187 27 0.5456141829490662\n",
      "Validation loss: 2.341724497027102 RMSE: 1.5302694\n",
      "Validation loss: 1.88858756976845 RMSE: 1.374259\n",
      "189 19 0.4521069824695587\n",
      "Validation loss: 2.1016944340899983 RMSE: 1.4497222\n",
      "Validation loss: 2.9996114937605056 RMSE: 1.7319386\n",
      "191 11 0.49065911769866943\n",
      "Validation loss: 2.140497973534913 RMSE: 1.4630439\n",
      "Validation loss: 2.0878516773206997 RMSE: 1.44494\n",
      "193 3 0.23256827890872955\n",
      "Validation loss: 1.9563238262075238 RMSE: 1.3986864\n",
      "194 24 0.48224547505378723\n",
      "Validation loss: 2.4035106127241015 RMSE: 1.5503259\n",
      "Validation loss: 2.567748837766394 RMSE: 1.6024197\n",
      "196 16 0.9882380366325378\n",
      "Validation loss: 1.8618283208492583 RMSE: 1.3644882\n",
      "Validation loss: 2.1648505145469596 RMSE: 1.471343\n",
      "198 8 0.7134212851524353\n",
      "Validation loss: 2.1083149730631736 RMSE: 1.4520037\n",
      "Validation loss: 4.818475398342167 RMSE: 2.1951027\n",
      "Loaded trained model with success.\n",
      "Test loss: 2.228804662164334 Test RMSE: 1.4929183\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:0', 'task_name': 'esol', 'model_type': 'gin', 'model': {'num_layer': 5, 'emb_dim': 300, 'feat_dim': 300, 'drop_ratio': 0.3, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 781, 'task': 'regression', 'data_path': 'data/esol/esol.csv', 'target': ['measured log solubility in mols per litre']}}\n",
      "Running on: cuda:0\n",
      "1127\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/1127\n",
      "Generating scaffold 1000/1127\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 14.923595428466797\n",
      "Validation loss: 5.895604251760297 RMSE: 2.4280865\n",
      "1 21 2.1992762088775635\n",
      "Validation loss: 9.731670282583321 RMSE: 3.1195626\n",
      "Validation loss: 3.7479782642516413 RMSE: 1.9359696\n",
      "3 13 1.6491941213607788\n",
      "Validation loss: 4.322783824616828 RMSE: 2.0791304\n",
      "Validation loss: 8.841299040127645 RMSE: 2.9734323\n",
      "5 5 2.0026659965515137\n",
      "Validation loss: 7.011122332210034 RMSE: 2.6478524\n",
      "6 26 0.9330570697784424\n",
      "Validation loss: 3.8233989529905066 RMSE: 1.9553514\n",
      "Validation loss: 6.855330944061279 RMSE: 2.6182685\n",
      "8 18 1.266379714012146\n",
      "Validation loss: 8.313454252428713 RMSE: 2.883306\n",
      "Validation loss: 8.19095157100036 RMSE: 2.8619838\n",
      "10 10 2.269440174102783\n",
      "Validation loss: 3.3718135082616216 RMSE: 1.83625\n",
      "Validation loss: 6.885179025936971 RMSE: 2.6239626\n",
      "12 2 1.9313610792160034\n",
      "Validation loss: 3.6975424142010445 RMSE: 1.9228995\n",
      "13 23 1.7738102674484253\n",
      "Validation loss: 9.15439695594585 RMSE: 3.0256233\n",
      "Validation loss: 3.6012142232034057 RMSE: 1.8976865\n",
      "15 15 0.9720425605773926\n",
      "Validation loss: 4.263055522884943 RMSE: 2.0647168\n",
      "Validation loss: 2.8254392294757134 RMSE: 1.6809043\n",
      "17 7 1.0340083837509155\n",
      "Validation loss: 2.698087508699535 RMSE: 1.6425858\n",
      "18 28 1.0794451236724854\n",
      "Validation loss: 6.960283346935711 RMSE: 2.6382349\n",
      "Validation loss: 4.750589961499239 RMSE: 2.179585\n",
      "20 20 0.9986356496810913\n",
      "Validation loss: 8.108910117529135 RMSE: 2.847615\n",
      "Validation loss: 4.2917402908865325 RMSE: 2.0716515\n",
      "22 12 1.184832215309143\n",
      "Validation loss: 3.760774243194445 RMSE: 1.9392717\n",
      "Validation loss: 2.2241934358546165 RMSE: 1.4913731\n",
      "24 4 0.8505599498748779\n",
      "Validation loss: 2.679088276044457 RMSE: 1.6367922\n",
      "25 25 0.4945223033428192\n",
      "Validation loss: 2.6606728967312163 RMSE: 1.6311569\n",
      "Validation loss: 2.542156027481619 RMSE: 1.5944139\n",
      "27 17 0.4732072651386261\n",
      "Validation loss: 6.547687998915141 RMSE: 2.558845\n",
      "Validation loss: 3.9650655256963407 RMSE: 1.9912472\n",
      "29 9 0.7459845542907715\n",
      "Validation loss: 3.7667082600888953 RMSE: 1.9408009\n",
      "Validation loss: 3.12175620340668 RMSE: 1.7668492\n",
      "31 1 0.7049764394760132\n",
      "Validation loss: 4.762938113339179 RMSE: 2.1824157\n",
      "32 22 1.1773220300674438\n",
      "Validation loss: 4.825580741451905 RMSE: 2.1967204\n",
      "Validation loss: 8.349715173771951 RMSE: 2.8895874\n",
      "34 14 1.2716543674468994\n",
      "Validation loss: 4.804781972834494 RMSE: 2.1919813\n",
      "Validation loss: 2.2536001479731196 RMSE: 1.5011995\n",
      "36 6 0.8085522651672363\n",
      "Validation loss: 2.96097897217337 RMSE: 1.7207495\n",
      "37 27 0.6768048405647278\n",
      "Validation loss: 5.090224293480932 RMSE: 2.2561526\n",
      "Validation loss: 3.5552582867377627 RMSE: 1.8855393\n",
      "39 19 1.2652204036712646\n",
      "Validation loss: 7.70214800286082 RMSE: 2.7752743\n",
      "Validation loss: 4.626532904869687 RMSE: 2.1509378\n",
      "41 11 0.54729163646698\n",
      "Validation loss: 2.9674771865912244 RMSE: 1.7226366\n",
      "Validation loss: 2.4513976806032973 RMSE: 1.5656941\n",
      "43 3 0.6105074882507324\n",
      "Validation loss: 3.8870633386932645 RMSE: 1.9715636\n",
      "44 24 0.7837329506874084\n",
      "Validation loss: 2.5009459647457155 RMSE: 1.581438\n",
      "Validation loss: 2.31461303845971 RMSE: 1.5213853\n",
      "46 16 1.1388098001480103\n",
      "Validation loss: 4.431720556411068 RMSE: 2.1051652\n",
      "Validation loss: 2.205291800794348 RMSE: 1.4850224\n",
      "48 8 0.7612935304641724\n",
      "Validation loss: 3.2648978423228305 RMSE: 1.806903\n",
      "Validation loss: 3.3795849876066226 RMSE: 1.8383648\n",
      "50 0 0.45595937967300415\n",
      "Validation loss: 3.6341865590188354 RMSE: 1.9063542\n",
      "51 21 0.41020870208740234\n",
      "Validation loss: 4.236508774546396 RMSE: 2.0582783\n",
      "Validation loss: 3.060743011204542 RMSE: 1.749498\n",
      "53 13 0.48406144976615906\n",
      "Validation loss: 2.4998681355366665 RMSE: 1.5810971\n",
      "Validation loss: 2.985731000393893 RMSE: 1.7279269\n",
      "55 5 0.5164598226547241\n",
      "Validation loss: 2.190625421768796 RMSE: 1.4800762\n",
      "56 26 0.5601961612701416\n",
      "Validation loss: 3.062696874669168 RMSE: 1.7500563\n",
      "Validation loss: 2.123927021448591 RMSE: 1.4573699\n",
      "58 18 0.2886436879634857\n",
      "Validation loss: 3.459082067540262 RMSE: 1.8598607\n",
      "Validation loss: 4.14549773562271 RMSE: 2.0360496\n",
      "60 10 1.6567000150680542\n",
      "Validation loss: 3.6193673420796353 RMSE: 1.9024634\n",
      "Validation loss: 2.7985028904096216 RMSE: 1.6728727\n",
      "62 2 0.8095740079879761\n",
      "Validation loss: 2.523018177631682 RMSE: 1.5884013\n",
      "63 23 0.42790669202804565\n",
      "Validation loss: 1.875003907005344 RMSE: 1.3693078\n",
      "Validation loss: 3.4390677034327415 RMSE: 1.8544723\n",
      "65 15 0.8102706670761108\n",
      "Validation loss: 2.1812679050243005 RMSE: 1.4769117\n",
      "Validation loss: 3.2140500060224957 RMSE: 1.7927773\n",
      "67 7 0.6982859373092651\n",
      "Validation loss: 2.6869385938728807 RMSE: 1.6391884\n",
      "68 28 2.402167558670044\n",
      "Validation loss: 4.308155102012432 RMSE: 2.0756094\n",
      "Validation loss: 2.295141353016406 RMSE: 1.5149724\n",
      "70 20 0.7660648226737976\n",
      "Validation loss: 3.357273747435713 RMSE: 1.8322865\n",
      "Validation loss: 2.2442088116586736 RMSE: 1.4980683\n",
      "72 12 0.9664948582649231\n",
      "Validation loss: 2.702356836437124 RMSE: 1.6438847\n",
      "Validation loss: 3.423482358983133 RMSE: 1.8502656\n",
      "74 4 0.7960048913955688\n",
      "Validation loss: 2.162637020634339 RMSE: 1.4705907\n",
      "75 25 0.6395102143287659\n",
      "Validation loss: 2.684955122196569 RMSE: 1.6385832\n",
      "Validation loss: 2.616236927235021 RMSE: 1.6174786\n",
      "77 17 0.8208293914794922\n",
      "Validation loss: 4.077533865396956 RMSE: 2.0192904\n",
      "Validation loss: 6.340272646034713 RMSE: 2.5179896\n",
      "79 9 0.41768738627433777\n",
      "Validation loss: 2.192971995446534 RMSE: 1.4808687\n",
      "Validation loss: 2.1097773935942525 RMSE: 1.4525073\n",
      "81 1 0.5057516694068909\n",
      "Validation loss: 3.2745587551488287 RMSE: 1.8095742\n",
      "82 22 0.28751340508461\n",
      "Validation loss: 2.342130810813566 RMSE: 1.5304022\n",
      "Validation loss: 2.763821525911314 RMSE: 1.6624744\n",
      "84 14 0.37081247568130493\n",
      "Validation loss: 2.6919962178289363 RMSE: 1.6407304\n",
      "Validation loss: 1.844947113400012 RMSE: 1.3582883\n",
      "86 6 0.9830073118209839\n",
      "Validation loss: 1.87563188624593 RMSE: 1.3695371\n",
      "87 27 0.4690486788749695\n",
      "Validation loss: 3.057941586570402 RMSE: 1.748697\n",
      "Validation loss: 3.630266506060035 RMSE: 1.9053259\n",
      "89 19 0.4129553437232971\n",
      "Validation loss: 2.203561875672467 RMSE: 1.4844398\n",
      "Validation loss: 3.102025975168279 RMSE: 1.7612569\n",
      "91 11 0.8026958107948303\n",
      "Validation loss: 3.925080130585527 RMSE: 1.9811814\n",
      "Validation loss: 4.302470920360194 RMSE: 2.07424\n",
      "93 3 0.49109888076782227\n",
      "Validation loss: 4.329539564858496 RMSE: 2.0807545\n",
      "94 24 0.48187899589538574\n",
      "Validation loss: 3.2055714573480385 RMSE: 1.790411\n",
      "Validation loss: 2.073041782969922 RMSE: 1.4398061\n",
      "96 16 0.8867158889770508\n",
      "Validation loss: 1.9270396338099927 RMSE: 1.3881785\n",
      "Validation loss: 3.1351213391903228 RMSE: 1.7706275\n",
      "98 8 0.6336798071861267\n",
      "Validation loss: 2.5358800128497907 RMSE: 1.5924447\n",
      "Validation loss: 2.1817219552740585 RMSE: 1.4770653\n",
      "100 0 0.5750458836555481\n",
      "Validation loss: 2.004740556784436 RMSE: 1.4158885\n",
      "101 21 0.6345598101615906\n",
      "Validation loss: 2.5972805191985273 RMSE: 1.6116081\n",
      "Validation loss: 1.9096157846197617 RMSE: 1.3818884\n",
      "103 13 0.49497586488723755\n",
      "Validation loss: 3.0265062361691903 RMSE: 1.7396857\n",
      "Validation loss: 2.2273324822957536 RMSE: 1.492425\n",
      "105 5 0.6900286078453064\n",
      "Validation loss: 3.587620802685223 RMSE: 1.8941015\n",
      "106 26 0.46097609400749207\n",
      "Validation loss: 2.098118332634985 RMSE: 1.4484882\n",
      "Validation loss: 4.0632458872499715 RMSE: 2.0157495\n",
      "108 18 0.4740646481513977\n",
      "Validation loss: 1.9551606705758424 RMSE: 1.3982706\n",
      "Validation loss: 3.235438936579544 RMSE: 1.7987326\n",
      "110 10 0.483783096075058\n",
      "Validation loss: 2.1544845209712475 RMSE: 1.4678162\n",
      "Validation loss: 4.444512852525289 RMSE: 2.1082013\n",
      "112 2 0.49940550327301025\n",
      "Validation loss: 2.0800657293437856 RMSE: 1.4422432\n",
      "113 23 0.3951337933540344\n",
      "Validation loss: 3.3628818545721275 RMSE: 1.8338162\n",
      "Validation loss: 2.1835111685558757 RMSE: 1.4776708\n",
      "115 15 0.38035523891448975\n",
      "Validation loss: 2.396096556587557 RMSE: 1.547933\n",
      "Validation loss: 2.8232524563780927 RMSE: 1.6802536\n",
      "117 7 0.6922343969345093\n",
      "Validation loss: 2.4694576537714594 RMSE: 1.5714508\n",
      "118 28 0.1476631611585617\n",
      "Validation loss: 1.9372634961541775 RMSE: 1.3918562\n",
      "Validation loss: 2.5406239496923124 RMSE: 1.5939336\n",
      "120 20 0.4587170481681824\n",
      "Validation loss: 4.8336790262070375 RMSE: 2.1985629\n",
      "Validation loss: 2.1557896232183 RMSE: 1.4682606\n",
      "122 12 0.6815627813339233\n",
      "Validation loss: 5.752788708273289 RMSE: 2.398497\n",
      "Validation loss: 1.8970813941111606 RMSE: 1.3773458\n",
      "124 4 0.7370912432670593\n",
      "Validation loss: 2.2097044328672695 RMSE: 1.4865075\n",
      "125 25 0.7611839175224304\n",
      "Validation loss: 2.347251252790468 RMSE: 1.5320742\n",
      "Validation loss: 2.75661648691228 RMSE: 1.6603061\n",
      "127 17 1.0595340728759766\n",
      "Validation loss: 4.411311103179392 RMSE: 2.1003122\n",
      "Validation loss: 2.814475616522595 RMSE: 1.6776398\n",
      "129 9 0.34559550881385803\n",
      "Validation loss: 2.0961234580099055 RMSE: 1.4477996\n",
      "Validation loss: 2.4366056539316094 RMSE: 1.560963\n",
      "131 1 0.3758506774902344\n",
      "Validation loss: 2.091778788946371 RMSE: 1.4462982\n",
      "132 22 0.45179176330566406\n",
      "Validation loss: 2.57352898395167 RMSE: 1.6042223\n",
      "Validation loss: 1.9198724316284719 RMSE: 1.3855946\n",
      "134 14 0.29974642395973206\n",
      "Validation loss: 2.211944099021169 RMSE: 1.4872606\n",
      "Validation loss: 2.2644448934403143 RMSE: 1.5048072\n",
      "136 6 0.5570686459541321\n",
      "Validation loss: 1.9148913801243874 RMSE: 1.383796\n",
      "137 27 0.4447647035121918\n",
      "Validation loss: 2.100005564436448 RMSE: 1.4491396\n",
      "Validation loss: 2.042474463977645 RMSE: 1.4291517\n",
      "139 19 0.6381922364234924\n",
      "Validation loss: 2.4265050613774664 RMSE: 1.5577244\n",
      "Validation loss: 2.0743860765896014 RMSE: 1.4402729\n",
      "141 11 0.6072494983673096\n",
      "Validation loss: 2.1465224628954864 RMSE: 1.4651015\n",
      "Validation loss: 3.090798709244855 RMSE: 1.7580669\n",
      "143 3 0.35537371039390564\n",
      "Validation loss: 2.809527175616374 RMSE: 1.6761642\n",
      "144 24 0.27445217967033386\n",
      "Validation loss: 2.8702146141929963 RMSE: 1.6941707\n",
      "Validation loss: 2.3417343333759137 RMSE: 1.5302726\n",
      "146 16 0.5651473999023438\n",
      "Validation loss: 2.5065599194670147 RMSE: 1.5832119\n",
      "Validation loss: 3.1500147633847937 RMSE: 1.7748281\n",
      "148 8 0.33564719557762146\n",
      "Validation loss: 2.042933675040186 RMSE: 1.4293122\n",
      "Validation loss: 2.0166272515744232 RMSE: 1.42008\n",
      "150 0 0.6740236282348633\n",
      "Validation loss: 2.02673014497335 RMSE: 1.4236327\n",
      "151 21 0.46110111474990845\n",
      "Validation loss: 1.8741386358716847 RMSE: 1.3689917\n",
      "Validation loss: 1.8750630121315475 RMSE: 1.3693293\n",
      "153 13 0.8135769963264465\n",
      "Validation loss: 2.0284093069819225 RMSE: 1.4242224\n",
      "Validation loss: 2.659125739494256 RMSE: 1.6306826\n",
      "155 5 0.822009265422821\n",
      "Validation loss: 1.926129414444476 RMSE: 1.3878506\n",
      "156 26 0.603059709072113\n",
      "Validation loss: 2.205571993262367 RMSE: 1.4851168\n",
      "Validation loss: 1.9400915181742304 RMSE: 1.3928717\n",
      "158 18 0.5825859308242798\n",
      "Validation loss: 2.2049489306137624 RMSE: 1.484907\n",
      "Validation loss: 1.835200416303314 RMSE: 1.3546957\n",
      "160 10 0.6356269717216492\n",
      "Validation loss: 2.4466728594450826 RMSE: 1.5641844\n",
      "Validation loss: 2.300077160902783 RMSE: 1.5166005\n",
      "162 2 0.5872445106506348\n",
      "Validation loss: 2.3319586441580173 RMSE: 1.5270752\n",
      "163 23 0.5437384247779846\n",
      "Validation loss: 1.7905174019062413 RMSE: 1.3381022\n",
      "Validation loss: 3.6403519453200617 RMSE: 1.9079705\n",
      "165 15 0.7326778173446655\n",
      "Validation loss: 2.0271055930483657 RMSE: 1.4237646\n",
      "Validation loss: 2.172443482728131 RMSE: 1.4739212\n",
      "167 7 0.4938809871673584\n",
      "Validation loss: 1.7501477256285405 RMSE: 1.3229315\n",
      "168 28 0.6350860595703125\n",
      "Validation loss: 2.3757674609665322 RMSE: 1.5413525\n",
      "Validation loss: 2.108825186712552 RMSE: 1.4521796\n",
      "170 20 0.7448691725730896\n",
      "Validation loss: 2.1225487721704805 RMSE: 1.456897\n",
      "Validation loss: 2.935213088989258 RMSE: 1.7132463\n",
      "172 12 0.5419155359268188\n",
      "Validation loss: 2.267372449942395 RMSE: 1.5057797\n",
      "Validation loss: 2.197136617339818 RMSE: 1.4822742\n",
      "174 4 0.38870900869369507\n",
      "Validation loss: 2.271480300785166 RMSE: 1.5071431\n",
      "175 25 0.258353590965271\n",
      "Validation loss: 2.1709352489066336 RMSE: 1.4734094\n",
      "Validation loss: 2.6170630286225176 RMSE: 1.617734\n",
      "177 17 0.7657027840614319\n",
      "Validation loss: 2.247028597688253 RMSE: 1.4990093\n",
      "Validation loss: 3.646211649464295 RMSE: 1.9095056\n",
      "179 9 0.47833478450775146\n",
      "Validation loss: 1.8991732597351074 RMSE: 1.378105\n",
      "Validation loss: 2.335704397311253 RMSE: 1.5283011\n",
      "181 1 0.5126603245735168\n",
      "Validation loss: 3.158975843834666 RMSE: 1.7773509\n",
      "182 22 0.26909130811691284\n",
      "Validation loss: 2.233154345402675 RMSE: 1.4943742\n",
      "Validation loss: 2.5444850795036924 RMSE: 1.5951443\n",
      "184 14 0.5142623782157898\n",
      "Validation loss: 2.356121403981099 RMSE: 1.5349662\n",
      "Validation loss: 2.2662146758716717 RMSE: 1.5053953\n",
      "186 6 0.43231478333473206\n",
      "Validation loss: 1.9800907299581882 RMSE: 1.407157\n",
      "187 27 0.45030173659324646\n",
      "Validation loss: 2.1479474485447976 RMSE: 1.4655879\n",
      "Validation loss: 3.636589917461429 RMSE: 1.9069844\n",
      "189 19 0.5639572739601135\n",
      "Validation loss: 2.241270978893854 RMSE: 1.4970875\n",
      "Validation loss: 2.2529606966845757 RMSE: 1.5009867\n",
      "191 11 0.6505802273750305\n",
      "Validation loss: 2.1225577803839624 RMSE: 1.4568999\n",
      "Validation loss: 1.7779189685804655 RMSE: 1.3333862\n",
      "193 3 0.36244988441467285\n",
      "Validation loss: 2.4614435423791936 RMSE: 1.5688989\n",
      "194 24 0.5976866483688354\n",
      "Validation loss: 2.032461744494143 RMSE: 1.4256443\n",
      "Validation loss: 2.1858480905009583 RMSE: 1.4784614\n",
      "196 16 0.530631959438324\n",
      "Validation loss: 2.6961771197023645 RMSE: 1.642004\n",
      "Validation loss: 1.7265184748489244 RMSE: 1.3139706\n",
      "198 8 0.3785437047481537\n",
      "Validation loss: 3.5796756448998917 RMSE: 1.892003\n",
      "Validation loss: 1.7475330787422383 RMSE: 1.3219429\n",
      "Loaded trained model with success.\n",
      "Test loss: 2.0038506393938995 Test RMSE: 1.4155743\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:0', 'task_name': 'lipo', 'model_type': 'gin', 'model': {'num_layer': 5, 'emb_dim': 300, 'feat_dim': 300, 'drop_ratio': 0.3, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 777, 'task': 'regression', 'data_path': 'data/lipophilicity/Lipophilicity.csv', 'target': ['exp']}}\n",
      "Running on: cuda:0\n",
      "4199\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/4199\n",
      "Generating scaffold 1000/4199\n",
      "Generating scaffold 2000/4199\n",
      "Generating scaffold 3000/4199\n",
      "Generating scaffold 4000/4199\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 7.479947090148926\n",
      "0 50 1.0853040218353271\n",
      "0 100 1.2554837465286255\n",
      "Validation loss: 2.0846349579947336 RMSE: 1.4438266\n",
      "1 45 1.2640902996063232\n",
      "1 95 0.9652350544929504\n",
      "Validation loss: 1.9491338979630244 RMSE: 1.3961139\n",
      "2 40 0.6670188307762146\n",
      "2 90 0.7845529317855835\n",
      "Validation loss: 1.2107603913261777 RMSE: 1.1003456\n",
      "3 35 0.7358889579772949\n",
      "3 85 0.6830469965934753\n",
      "Validation loss: 1.8051112697238014 RMSE: 1.3435442\n",
      "4 30 0.5210204720497131\n",
      "4 80 0.6526806354522705\n",
      "Validation loss: 1.7225781009310768 RMSE: 1.3124702\n",
      "5 25 0.7734288573265076\n",
      "5 75 0.5395979285240173\n",
      "Validation loss: 1.1773181699571156 RMSE: 1.085043\n",
      "6 20 0.6309284567832947\n",
      "6 70 0.722492516040802\n",
      "Validation loss: 2.008841982341948 RMSE: 1.4173362\n",
      "7 15 0.7237166166305542\n",
      "7 65 0.5044452548027039\n",
      "Validation loss: 0.914560238804136 RMSE: 0.9563264\n",
      "8 10 0.8005412220954895\n",
      "8 60 1.1810755729675293\n",
      "Validation loss: 0.8610016510600136 RMSE: 0.92790174\n",
      "9 5 0.8679682016372681\n",
      "9 55 0.5175833702087402\n",
      "Validation loss: 0.9355670917601813 RMSE: 0.9672472\n",
      "10 0 0.7833722233772278\n",
      "10 50 0.6120203733444214\n",
      "10 100 0.5532697439193726\n",
      "Validation loss: 1.0883565448579333 RMSE: 1.0432433\n",
      "11 45 0.661088228225708\n",
      "11 95 0.7106204032897949\n",
      "Validation loss: 1.0403443949563163 RMSE: 1.0199727\n",
      "12 40 0.4914335608482361\n",
      "12 90 0.6825113296508789\n",
      "Validation loss: 1.6975930974597022 RMSE: 1.3029172\n",
      "13 35 0.5974394083023071\n",
      "13 85 0.8139326572418213\n",
      "Validation loss: 1.1890905334835962 RMSE: 1.0904542\n",
      "14 30 0.5390323996543884\n",
      "14 80 0.8531131744384766\n",
      "Validation loss: 0.78330500125885 RMSE: 0.8850452\n",
      "15 25 0.558762788772583\n",
      "15 75 0.47917401790618896\n",
      "Validation loss: 0.9836786837804885 RMSE: 0.9918058\n",
      "16 20 0.6775956749916077\n",
      "16 70 0.6211686730384827\n",
      "Validation loss: 1.0525585237003507 RMSE: 1.0259428\n",
      "17 15 1.0757689476013184\n",
      "17 65 0.5336238741874695\n",
      "Validation loss: 0.7402949103287289 RMSE: 0.8604039\n",
      "18 10 1.0068399906158447\n",
      "18 60 0.454414963722229\n",
      "Validation loss: 0.723982884770348 RMSE: 0.8508718\n",
      "19 5 0.7489471435546875\n",
      "19 55 0.3278823792934418\n",
      "Validation loss: 0.7430273842243921 RMSE: 0.8619904\n",
      "20 0 0.4276096820831299\n",
      "20 50 0.5819869637489319\n",
      "20 100 0.7645687460899353\n",
      "Validation loss: 0.8618053549811954 RMSE: 0.9283347\n",
      "21 45 1.1430689096450806\n",
      "21 95 0.35225045680999756\n",
      "Validation loss: 0.9571077049842902 RMSE: 0.9783188\n",
      "22 40 0.673452615737915\n",
      "22 90 0.3395571708679199\n",
      "Validation loss: 0.9622472297577631 RMSE: 0.9809421\n",
      "23 35 0.9455159902572632\n",
      "23 85 0.31528350710868835\n",
      "Validation loss: 0.686740855943589 RMSE: 0.8286983\n",
      "24 30 0.5493341088294983\n",
      "24 80 0.3622082769870758\n",
      "Validation loss: 0.682603143794196 RMSE: 0.82619804\n",
      "25 25 0.4109838008880615\n",
      "25 75 0.753907322883606\n",
      "Validation loss: 0.9230506298087892 RMSE: 0.9607552\n",
      "26 20 0.4672173857688904\n",
      "26 70 0.5863122940063477\n",
      "Validation loss: 0.6601931799025763 RMSE: 0.8125227\n",
      "27 15 0.6254028081893921\n",
      "27 65 0.5994378328323364\n",
      "Validation loss: 0.9362116421972002 RMSE: 0.96758026\n",
      "28 10 0.4653519093990326\n",
      "28 60 0.5606153607368469\n",
      "Validation loss: 0.6950365753400893 RMSE: 0.8336885\n",
      "29 5 0.6628073453903198\n",
      "29 55 0.758345365524292\n",
      "Validation loss: 0.7207188038598924 RMSE: 0.8489516\n",
      "30 0 0.5926387906074524\n",
      "30 50 0.8012385964393616\n",
      "30 100 0.43632569909095764\n",
      "Validation loss: 0.6573388261454446 RMSE: 0.8107643\n",
      "31 45 0.7878280282020569\n",
      "31 95 0.5562417507171631\n",
      "Validation loss: 0.7250821204412551 RMSE: 0.8515175\n",
      "32 40 0.38792574405670166\n",
      "32 90 0.4525100290775299\n",
      "Validation loss: 0.6424834433056059 RMSE: 0.80155057\n",
      "33 35 0.44913920760154724\n",
      "33 85 0.6756938099861145\n",
      "Validation loss: 1.0209529876708985 RMSE: 1.0104222\n",
      "34 30 0.2827233076095581\n",
      "34 80 0.3268241584300995\n",
      "Validation loss: 0.92004278529258 RMSE: 0.9591886\n",
      "35 25 0.36955636739730835\n",
      "35 75 0.5647616982460022\n",
      "Validation loss: 0.6293221919309525 RMSE: 0.7932983\n",
      "36 20 0.28003445267677307\n",
      "36 70 0.48833367228507996\n",
      "Validation loss: 0.7481481052580334 RMSE: 0.86495554\n",
      "37 15 0.7319119572639465\n",
      "37 65 0.5327162742614746\n",
      "Validation loss: 0.954032454036531 RMSE: 0.97674584\n",
      "38 10 0.8213147521018982\n",
      "38 60 0.6865412592887878\n",
      "Validation loss: 0.7352436270032611 RMSE: 0.8574634\n",
      "39 5 0.3496285080909729\n",
      "39 55 0.7326276302337646\n",
      "Validation loss: 0.69075589236759 RMSE: 0.8311173\n",
      "40 0 0.491383820772171\n",
      "40 50 0.3451117277145386\n",
      "40 100 0.4132094383239746\n",
      "Validation loss: 0.7225032692863828 RMSE: 0.85000193\n",
      "41 45 0.4338146150112152\n",
      "41 95 0.5832370519638062\n",
      "Validation loss: 0.8249366148596718 RMSE: 0.9082602\n",
      "42 40 0.3425394892692566\n",
      "42 90 0.29163870215415955\n",
      "Validation loss: 0.7270978042057582 RMSE: 0.8527003\n",
      "43 35 0.46004214882850647\n",
      "43 85 0.6040545701980591\n",
      "Validation loss: 0.7480770565214612 RMSE: 0.8649145\n",
      "44 30 0.5202484130859375\n",
      "44 80 0.6067988872528076\n",
      "Validation loss: 0.713138595081511 RMSE: 0.84447527\n",
      "45 25 0.3580622375011444\n",
      "45 75 0.5427542924880981\n",
      "Validation loss: 0.7134887627192906 RMSE: 0.84468263\n",
      "46 20 0.3230903446674347\n",
      "46 70 0.3563025891780853\n",
      "Validation loss: 0.6205009040378389 RMSE: 0.78771883\n",
      "47 15 0.5933774709701538\n",
      "47 65 0.303141325712204\n",
      "Validation loss: 1.1799771604083833 RMSE: 1.0862676\n",
      "48 10 0.452384352684021\n",
      "48 60 0.3963925540447235\n",
      "Validation loss: 0.7795967567534674 RMSE: 0.88294774\n",
      "49 5 0.5891159176826477\n",
      "49 55 0.3146589696407318\n",
      "Validation loss: 0.7709914105279105 RMSE: 0.8780612\n",
      "50 0 0.6624018549919128\n",
      "50 50 0.43460771441459656\n",
      "50 100 0.32994380593299866\n",
      "Validation loss: 0.7102749858583722 RMSE: 0.84277815\n",
      "51 45 0.7820755839347839\n",
      "51 95 0.7236652374267578\n",
      "Validation loss: 0.734782141730899 RMSE: 0.8571943\n",
      "52 40 0.38970398902893066\n",
      "52 90 0.3449644148349762\n",
      "Validation loss: 0.6770071103459313 RMSE: 0.82280445\n",
      "53 35 0.4602311849594116\n",
      "53 85 0.4972110986709595\n",
      "Validation loss: 0.658498163308416 RMSE: 0.811479\n",
      "54 30 0.3251572847366333\n",
      "54 80 0.2414926439523697\n",
      "Validation loss: 0.8487310122875941 RMSE: 0.921266\n",
      "55 25 0.3262040615081787\n",
      "55 75 0.5696039199829102\n",
      "Validation loss: 0.7476992327542532 RMSE: 0.864696\n",
      "56 20 0.34984278678894043\n",
      "56 70 0.3802109956741333\n",
      "Validation loss: 0.6971488157908122 RMSE: 0.8349544\n",
      "57 15 0.3322899043560028\n",
      "57 65 0.3886137008666992\n",
      "Validation loss: 0.645477751323155 RMSE: 0.80341625\n",
      "58 10 0.5520221590995789\n",
      "58 60 0.3916586935520172\n",
      "Validation loss: 0.6407572973342169 RMSE: 0.80047315\n",
      "59 5 0.4517034888267517\n",
      "59 55 0.35895171761512756\n",
      "Validation loss: 0.7796827202751523 RMSE: 0.88299644\n",
      "60 0 0.3624301254749298\n",
      "60 50 0.5087670087814331\n",
      "60 100 0.31063932180404663\n",
      "Validation loss: 0.6832640863600231 RMSE: 0.8265979\n",
      "61 45 0.31208163499832153\n",
      "61 95 0.39977341890335083\n",
      "Validation loss: 0.6698924246288481 RMSE: 0.8184695\n",
      "62 40 0.7097846865653992\n",
      "62 90 0.4456205666065216\n",
      "Validation loss: 0.6889082011722383 RMSE: 0.83000493\n",
      "63 35 0.34193742275238037\n",
      "63 85 0.3119880259037018\n",
      "Validation loss: 0.8120589858009701 RMSE: 0.90114313\n",
      "64 30 0.583744466304779\n",
      "64 80 0.190931499004364\n",
      "Validation loss: 0.6493216060456776 RMSE: 0.80580497\n",
      "65 25 0.3236444890499115\n",
      "65 75 0.15344545245170593\n",
      "Validation loss: 0.7409590346472604 RMSE: 0.8607898\n",
      "66 20 0.8125798106193542\n",
      "66 70 0.6301040053367615\n",
      "Validation loss: 0.6830146460306077 RMSE: 0.826447\n",
      "67 15 0.4204266667366028\n",
      "67 65 0.25775766372680664\n",
      "Validation loss: 1.187827281724839 RMSE: 1.089875\n",
      "68 10 0.44321632385253906\n",
      "68 60 0.4101027548313141\n",
      "Validation loss: 0.7618465654906772 RMSE: 0.87283826\n",
      "69 5 0.35071682929992676\n",
      "69 55 0.45298656821250916\n",
      "Validation loss: 0.8506646763710749 RMSE: 0.9223148\n",
      "70 0 0.3941058814525604\n",
      "70 50 0.21525932848453522\n",
      "70 100 0.6566535830497742\n",
      "Validation loss: 0.7972687948317755 RMSE: 0.8928991\n",
      "71 45 0.28793099522590637\n",
      "71 95 0.326368123292923\n",
      "Validation loss: 0.8149838999623344 RMSE: 0.90276456\n",
      "72 40 0.425442636013031\n",
      "72 90 0.5406965613365173\n",
      "Validation loss: 0.9718222436450776 RMSE: 0.98581046\n",
      "73 35 0.3780297040939331\n",
      "73 85 0.3237907588481903\n",
      "Validation loss: 0.621251320271265 RMSE: 0.788195\n",
      "74 30 0.2833801507949829\n",
      "74 80 0.27136552333831787\n",
      "Validation loss: 0.6081794669230779 RMSE: 0.7798586\n",
      "75 25 0.3183823227882385\n",
      "75 75 0.655131995677948\n",
      "Validation loss: 0.8034195372036526 RMSE: 0.89633673\n",
      "76 20 0.24104933440685272\n",
      "76 70 0.372398316860199\n",
      "Validation loss: 1.0609702110290526 RMSE: 1.0300342\n",
      "77 15 0.3604747951030731\n",
      "77 65 0.4658316969871521\n",
      "Validation loss: 0.6172809510003953 RMSE: 0.7856723\n",
      "78 10 0.5567218065261841\n",
      "78 60 0.25101232528686523\n",
      "Validation loss: 0.6999232865515209 RMSE: 0.8366142\n",
      "79 5 0.4292043447494507\n",
      "79 55 0.26703307032585144\n",
      "Validation loss: 0.7252403787204198 RMSE: 0.8516104\n",
      "80 0 0.2686237692832947\n",
      "80 50 0.2499643713235855\n",
      "80 100 0.3273165225982666\n",
      "Validation loss: 0.6223158620652698 RMSE: 0.78887\n",
      "81 45 0.2370067983865738\n",
      "81 95 0.2144954651594162\n",
      "Validation loss: 0.7327831217930431 RMSE: 0.85602754\n",
      "82 40 0.3097984492778778\n",
      "82 90 0.2769324481487274\n",
      "Validation loss: 0.8601144021465665 RMSE: 0.92742354\n",
      "83 35 0.3373081684112549\n",
      "83 85 0.20237234234809875\n",
      "Validation loss: 0.6337382055464245 RMSE: 0.79607683\n",
      "84 30 0.3579115569591522\n",
      "84 80 0.2579880356788635\n",
      "Validation loss: 0.6979946919849941 RMSE: 0.8354608\n",
      "85 25 0.3257978856563568\n",
      "85 75 0.4410945177078247\n",
      "Validation loss: 0.6462676911127 RMSE: 0.80390775\n",
      "86 20 0.3690741956233978\n",
      "86 70 0.2576903998851776\n",
      "Validation loss: 0.73930907476516 RMSE: 0.8598308\n",
      "87 15 0.22694164514541626\n",
      "87 65 0.3761488199234009\n",
      "Validation loss: 0.6608411289396741 RMSE: 0.8129214\n",
      "88 10 0.20906856656074524\n",
      "88 60 0.296059787273407\n",
      "Validation loss: 0.7039767654169173 RMSE: 0.83903325\n",
      "89 5 0.23918047547340393\n",
      "89 55 0.31642603874206543\n",
      "Validation loss: 0.7678498898233687 RMSE: 0.8762704\n",
      "90 0 0.30878955125808716\n",
      "90 50 0.4433384835720062\n",
      "90 100 0.3139907717704773\n",
      "Validation loss: 0.634503934496925 RMSE: 0.79655755\n",
      "91 45 0.3477638363838196\n",
      "91 95 0.3536039888858795\n",
      "Validation loss: 0.8203541599568867 RMSE: 0.90573406\n",
      "92 40 0.2845551669597626\n",
      "92 90 0.29107823967933655\n",
      "Validation loss: 0.7575554595107123 RMSE: 0.8703766\n",
      "93 35 0.29734665155410767\n",
      "93 85 0.25505000352859497\n",
      "Validation loss: 0.7032772921380542 RMSE: 0.8386163\n",
      "94 30 0.37852004170417786\n",
      "94 80 0.5534470677375793\n",
      "Validation loss: 0.7231777826944987 RMSE: 0.8503986\n",
      "95 25 0.23992674052715302\n",
      "95 75 0.2712981700897217\n",
      "Validation loss: 0.8429747541745504 RMSE: 0.9181366\n",
      "96 20 0.34130600094795227\n",
      "96 70 0.12620916962623596\n",
      "Validation loss: 0.8358321984608968 RMSE: 0.91423863\n",
      "97 15 0.26312488317489624\n",
      "97 65 0.19369535148143768\n",
      "Validation loss: 0.6494994651703607 RMSE: 0.80591524\n",
      "98 10 0.3405895531177521\n",
      "98 60 0.17787714302539825\n",
      "Validation loss: 0.6385384746960231 RMSE: 0.79908603\n",
      "99 5 0.46321815252304077\n",
      "99 55 0.35593077540397644\n",
      "Validation loss: 0.8918129920959472 RMSE: 0.9443585\n",
      "100 0 0.20610317587852478\n",
      "100 50 0.3124619126319885\n",
      "100 100 0.2208065241575241\n",
      "Validation loss: 0.7570254155567714 RMSE: 0.87007207\n",
      "101 45 0.3165033459663391\n",
      "101 95 0.33214426040649414\n",
      "Validation loss: 0.6284556218555996 RMSE: 0.7927519\n",
      "102 40 0.3225730061531067\n",
      "102 90 0.25901833176612854\n",
      "Validation loss: 0.8760497615450904 RMSE: 0.9359753\n",
      "103 35 0.24647536873817444\n",
      "103 85 0.35937121510505676\n",
      "Validation loss: 0.9985939911433629 RMSE: 0.9992967\n",
      "104 30 0.13172100484371185\n",
      "104 80 0.2811824679374695\n",
      "Validation loss: 0.5654097778456552 RMSE: 0.7519373\n",
      "105 25 0.17974063754081726\n",
      "105 75 0.25485652685165405\n",
      "Validation loss: 0.6596229473749796 RMSE: 0.81217176\n",
      "106 20 0.3356802463531494\n",
      "106 70 0.293291300535202\n",
      "Validation loss: 0.6302175246533893 RMSE: 0.7938624\n",
      "107 15 0.4061228930950165\n",
      "107 65 0.2576073408126831\n",
      "Validation loss: 0.702256190776825 RMSE: 0.8380073\n",
      "108 10 0.1742904782295227\n",
      "108 60 0.33964237570762634\n",
      "Validation loss: 0.7126223200843448 RMSE: 0.8441696\n",
      "109 5 0.33549872040748596\n",
      "109 55 0.25904563069343567\n",
      "Validation loss: 0.6533681199664161 RMSE: 0.8083119\n",
      "110 0 0.26373377442359924\n",
      "110 50 0.412777841091156\n",
      "110 100 0.2648317217826843\n",
      "Validation loss: 0.6380164594877333 RMSE: 0.79875934\n",
      "111 45 0.17938949167728424\n",
      "111 95 0.19694101810455322\n",
      "Validation loss: 0.6524685223897299 RMSE: 0.80775523\n",
      "112 40 0.19855733215808868\n",
      "112 90 0.23721012473106384\n",
      "Validation loss: 0.9023622214794159 RMSE: 0.9499275\n",
      "113 35 0.23244188725948334\n",
      "113 85 0.4580202102661133\n",
      "Validation loss: 0.8196353143169767 RMSE: 0.9053371\n",
      "114 30 0.23531979322433472\n",
      "114 80 0.30405861139297485\n",
      "Validation loss: 0.8501939489727929 RMSE: 0.9220596\n",
      "115 25 0.19518055021762848\n",
      "115 75 0.2594553530216217\n",
      "Validation loss: 0.7256914933522542 RMSE: 0.85187525\n",
      "116 20 0.3369930386543274\n",
      "116 70 0.45892682671546936\n",
      "Validation loss: 0.7255226810773213 RMSE: 0.8517762\n",
      "117 15 0.33065685629844666\n",
      "117 65 0.2380179762840271\n",
      "Validation loss: 0.9355047484238942 RMSE: 0.96721494\n",
      "118 10 0.18050682544708252\n",
      "118 60 0.2716750502586365\n",
      "Validation loss: 0.712921560945965 RMSE: 0.8443468\n",
      "119 5 0.1168462336063385\n",
      "119 55 0.2508929967880249\n",
      "Validation loss: 0.8426460152580625 RMSE: 0.91795754\n",
      "120 0 0.18017277121543884\n",
      "120 50 0.27109333872795105\n",
      "120 100 0.21146303415298462\n",
      "Validation loss: 0.8474713904517037 RMSE: 0.9205821\n",
      "121 45 0.17078007757663727\n",
      "121 95 0.2965606451034546\n",
      "Validation loss: 0.7268093273753211 RMSE: 0.8525311\n",
      "122 40 0.36081191897392273\n",
      "122 90 0.15336567163467407\n",
      "Validation loss: 0.758754232951573 RMSE: 0.87106496\n",
      "123 35 0.12041568756103516\n",
      "123 85 0.16048020124435425\n",
      "Validation loss: 0.6345096803137235 RMSE: 0.7965612\n",
      "124 30 0.3430074453353882\n",
      "124 80 0.46835240721702576\n",
      "Validation loss: 0.7937754443713597 RMSE: 0.8909408\n",
      "125 25 0.15146982669830322\n",
      "125 75 0.18771016597747803\n",
      "Validation loss: 0.6185784476143973 RMSE: 0.7864976\n",
      "126 20 0.21391363441944122\n",
      "126 70 0.22885125875473022\n",
      "Validation loss: 0.7083925190426055 RMSE: 0.8416606\n",
      "127 15 0.1670733243227005\n",
      "127 65 0.37569117546081543\n",
      "Validation loss: 0.7421315772192819 RMSE: 0.86147064\n",
      "128 10 0.25302910804748535\n",
      "128 60 0.20751924812793732\n",
      "Validation loss: 0.817672982670012 RMSE: 0.9042527\n",
      "129 5 0.1928536295890808\n",
      "129 55 0.40833160281181335\n",
      "Validation loss: 0.7709158602214995 RMSE: 0.87801814\n",
      "130 0 0.20367594063282013\n",
      "130 50 0.36037713289260864\n",
      "130 100 0.1889801323413849\n",
      "Validation loss: 0.7663867411159334 RMSE: 0.8754352\n",
      "131 45 0.3517206907272339\n",
      "131 95 0.2376335859298706\n",
      "Validation loss: 0.7047708102634975 RMSE: 0.83950627\n",
      "132 40 0.24445989727973938\n",
      "132 90 0.22639620304107666\n",
      "Validation loss: 0.7458939779372442 RMSE: 0.8636515\n",
      "133 35 0.19962002336978912\n",
      "133 85 0.2751677334308624\n",
      "Validation loss: 0.7329514253707159 RMSE: 0.85612583\n",
      "134 30 0.2095206379890442\n",
      "134 80 0.2601665258407593\n",
      "Validation loss: 0.7480380041258675 RMSE: 0.8648919\n",
      "135 25 0.28577613830566406\n",
      "135 75 0.32234299182891846\n",
      "Validation loss: 0.6874559226490202 RMSE: 0.82912964\n",
      "136 20 0.16234317421913147\n",
      "136 70 0.27307915687561035\n",
      "Validation loss: 0.6483370656058902 RMSE: 0.8051938\n",
      "137 15 0.18335016071796417\n",
      "137 65 0.3070085346698761\n",
      "Validation loss: 0.6322904779797509 RMSE: 0.79516685\n",
      "138 10 0.27985483407974243\n",
      "138 60 0.23369728028774261\n",
      "Validation loss: 0.8968857714108058 RMSE: 0.94704056\n",
      "139 5 0.2102198749780655\n",
      "139 55 0.3164865970611572\n",
      "Validation loss: 0.796558286746343 RMSE: 0.8925012\n",
      "140 0 0.2324373722076416\n",
      "140 50 0.4120889902114868\n",
      "140 100 0.2263352870941162\n",
      "Validation loss: 0.9189776102701823 RMSE: 0.9586332\n",
      "141 45 0.2941165268421173\n",
      "141 95 0.2820214033126831\n",
      "Validation loss: 0.6642213401340303 RMSE: 0.81499773\n",
      "142 40 0.22721484303474426\n",
      "142 90 0.16342009603977203\n",
      "Validation loss: 0.6660584364618574 RMSE: 0.816124\n",
      "143 35 0.22186486423015594\n",
      "143 85 0.22582471370697021\n",
      "Validation loss: 0.955828542936416 RMSE: 0.9776648\n",
      "144 30 0.2565939426422119\n",
      "144 80 0.20298022031784058\n",
      "Validation loss: 0.8545634088062105 RMSE: 0.924426\n",
      "145 25 0.2530032992362976\n",
      "145 75 0.1679682582616806\n",
      "Validation loss: 0.7009027051074165 RMSE: 0.83719933\n",
      "146 20 0.222768634557724\n",
      "146 70 0.2744048535823822\n",
      "Validation loss: 0.6642363131046295 RMSE: 0.8150069\n",
      "147 15 0.17040389776229858\n",
      "147 65 0.25389260053634644\n",
      "Validation loss: 0.7865965985116504 RMSE: 0.8869028\n",
      "148 10 0.25344014167785645\n",
      "148 60 0.21758797764778137\n",
      "Validation loss: 0.769171340692611 RMSE: 0.8770241\n",
      "149 5 0.21637728810310364\n",
      "149 55 0.31204840540885925\n",
      "Validation loss: 0.7941148689814976 RMSE: 0.8911312\n",
      "150 0 0.2639324367046356\n",
      "150 50 0.23484788835048676\n",
      "150 100 0.3491886258125305\n",
      "Validation loss: 0.7974778544335138 RMSE: 0.89301616\n",
      "151 45 0.1522742360830307\n",
      "151 95 0.18453900516033173\n",
      "Validation loss: 0.7104078122547695 RMSE: 0.84285694\n",
      "152 40 0.29174208641052246\n",
      "152 90 0.4867597818374634\n",
      "Validation loss: 0.7649415680340358 RMSE: 0.8746094\n",
      "153 35 0.15058884024620056\n",
      "153 85 0.26570314168930054\n",
      "Validation loss: 0.7425988356272379 RMSE: 0.8617417\n",
      "154 30 0.3162636458873749\n",
      "154 80 0.1664404273033142\n",
      "Validation loss: 0.8517582461947486 RMSE: 0.92290753\n",
      "155 25 0.18033085763454437\n",
      "155 75 0.2798503339290619\n",
      "Validation loss: 0.8399405419826508 RMSE: 0.9164827\n",
      "156 20 0.18913692235946655\n",
      "156 70 0.23283416032791138\n",
      "Validation loss: 0.8491622587754613 RMSE: 0.92149997\n",
      "157 15 0.2254866659641266\n",
      "157 65 0.20598715543746948\n",
      "Validation loss: 0.8300367718651182 RMSE: 0.9110635\n",
      "158 10 0.21406522393226624\n",
      "158 60 0.3478657603263855\n",
      "Validation loss: 1.0062920661199661 RMSE: 1.0031412\n",
      "159 5 0.22935281693935394\n",
      "159 55 0.11035171896219254\n",
      "Validation loss: 0.7515352691922869 RMSE: 0.86691135\n",
      "160 0 0.13493168354034424\n",
      "160 50 0.20356552302837372\n",
      "160 100 0.26585665345191956\n",
      "Validation loss: 0.7134016105106898 RMSE: 0.8446311\n",
      "161 45 0.12375545501708984\n",
      "161 95 0.21574008464813232\n",
      "Validation loss: 0.7193096115475609 RMSE: 0.8481212\n",
      "162 40 0.12352381646633148\n",
      "162 90 0.18020139634609222\n",
      "Validation loss: 0.7358345304216657 RMSE: 0.85780793\n",
      "163 35 0.24581488966941833\n",
      "163 85 0.20661582052707672\n",
      "Validation loss: 0.7146054737269878 RMSE: 0.8453434\n",
      "164 30 0.19082018733024597\n",
      "164 80 0.28861334919929504\n",
      "Validation loss: 0.8155466738201324 RMSE: 0.90307623\n",
      "165 25 0.2633727788925171\n",
      "165 75 0.19192539155483246\n",
      "Validation loss: 0.6519965597561428 RMSE: 0.80746305\n",
      "166 20 0.12134438753128052\n",
      "166 70 0.18290087580680847\n",
      "Validation loss: 0.7591602030254546 RMSE: 0.871298\n",
      "167 15 0.1729258894920349\n",
      "167 65 0.18287545442581177\n",
      "Validation loss: 0.7343234925043015 RMSE: 0.8569268\n",
      "168 10 0.20184998214244843\n",
      "168 60 0.10370037704706192\n",
      "Validation loss: 0.6599015275637309 RMSE: 0.81234324\n",
      "169 5 0.13606150448322296\n",
      "169 55 0.13908225297927856\n",
      "Validation loss: 0.6845562321799142 RMSE: 0.82737917\n",
      "170 0 0.11581346392631531\n",
      "170 50 0.17031003534793854\n",
      "170 100 0.2074209451675415\n",
      "Validation loss: 0.6909532819475447 RMSE: 0.831236\n",
      "171 45 0.15192633867263794\n",
      "171 95 0.20335300266742706\n",
      "Validation loss: 0.7694737797691709 RMSE: 0.87719655\n",
      "172 40 0.1743626743555069\n",
      "172 90 0.19150876998901367\n",
      "Validation loss: 0.6221028279690516 RMSE: 0.788735\n",
      "173 35 0.14123064279556274\n",
      "173 85 0.2572948634624481\n",
      "Validation loss: 0.8451976793152945 RMSE: 0.9193463\n",
      "174 30 0.17566248774528503\n",
      "174 80 0.29130345582962036\n",
      "Validation loss: 0.791159040587289 RMSE: 0.88947123\n",
      "175 25 0.2510927617549896\n",
      "175 75 0.4914543032646179\n",
      "Validation loss: 0.66319433649381 RMSE: 0.8143675\n",
      "176 20 0.23585662245750427\n",
      "176 70 0.20540478825569153\n",
      "Validation loss: 0.6249209054878779 RMSE: 0.7905194\n",
      "177 15 0.18920408189296722\n",
      "177 65 0.1692420095205307\n",
      "Validation loss: 0.6346853006453741 RMSE: 0.7966714\n",
      "178 10 0.18406176567077637\n",
      "178 60 0.15765202045440674\n",
      "Validation loss: 0.5790583772318704 RMSE: 0.76095885\n",
      "179 5 0.25846603512763977\n",
      "179 55 0.1924021691083908\n",
      "Validation loss: 0.7623965625252043 RMSE: 0.8731532\n",
      "180 0 0.21769893169403076\n",
      "180 50 0.20679210126399994\n",
      "180 100 0.22998730838298798\n",
      "Validation loss: 0.666442532766433 RMSE: 0.8163593\n",
      "181 45 0.18089686334133148\n",
      "181 95 0.23584133386611938\n",
      "Validation loss: 0.6147282019967124 RMSE: 0.78404605\n",
      "182 40 0.14322291314601898\n",
      "182 90 0.14181600511074066\n",
      "Validation loss: 0.7344170593080066 RMSE: 0.8569814\n",
      "183 35 0.1317422240972519\n",
      "183 85 0.22465819120407104\n",
      "Validation loss: 0.861185880502065 RMSE: 0.928001\n",
      "184 30 0.26706546545028687\n",
      "184 80 0.11678850650787354\n",
      "Validation loss: 0.7715984668050494 RMSE: 0.87840676\n",
      "185 25 0.2600269019603729\n",
      "185 75 0.182667076587677\n",
      "Validation loss: 0.7177311278524853 RMSE: 0.84719014\n",
      "186 20 0.11688738316297531\n",
      "186 70 0.2004822939634323\n",
      "Validation loss: 0.8295382499694824 RMSE: 0.9107899\n",
      "187 15 0.19569334387779236\n",
      "187 65 0.16775846481323242\n",
      "Validation loss: 0.7125496039787929 RMSE: 0.8441265\n",
      "188 10 0.17435193061828613\n",
      "188 60 0.2211211621761322\n",
      "Validation loss: 0.7015436643645877 RMSE: 0.837582\n",
      "189 5 0.14653465151786804\n",
      "189 55 0.1752791702747345\n",
      "Validation loss: 0.660907194727943 RMSE: 0.812962\n",
      "190 0 0.1622808873653412\n",
      "190 50 0.2573883831501007\n",
      "190 100 0.24664215743541718\n",
      "Validation loss: 0.6506277064482371 RMSE: 0.80661494\n",
      "191 45 0.16770720481872559\n",
      "191 95 0.23956012725830078\n",
      "Validation loss: 0.8396663665771484 RMSE: 0.9163331\n",
      "192 40 0.2518453299999237\n",
      "192 90 0.15804381668567657\n",
      "Validation loss: 0.7285004326275417 RMSE: 0.8535224\n",
      "193 35 0.16919469833374023\n",
      "193 85 0.07870976626873016\n",
      "Validation loss: 0.718853311311631 RMSE: 0.8478522\n",
      "194 30 0.13075178861618042\n",
      "194 80 0.06590517610311508\n",
      "Validation loss: 0.7924181234268916 RMSE: 0.8901787\n",
      "195 25 0.19875568151474\n",
      "195 75 0.14329460263252258\n",
      "Validation loss: 0.7299860823722113 RMSE: 0.85439223\n",
      "196 20 0.18216465413570404\n",
      "196 70 0.26447027921676636\n",
      "Validation loss: 0.6937285383542379 RMSE: 0.8329037\n",
      "197 15 0.258985310792923\n",
      "197 65 0.14865289628505707\n",
      "Validation loss: 0.688646003745851 RMSE: 0.829847\n",
      "198 10 0.2272707223892212\n",
      "198 60 0.0795016810297966\n",
      "Validation loss: 0.6752769430478414 RMSE: 0.82175237\n",
      "199 5 0.1208338513970375\n",
      "199 55 0.152812659740448\n",
      "Validation loss: 0.7015209430739993 RMSE: 0.83756846\n",
      "Loaded trained model with success.\n",
      "Test loss: 0.604955480212257 Test RMSE: 0.7777889\n",
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:0', 'task_name': 'lipo', 'model_type': 'gin', 'model': {'num_layer': 5, 'emb_dim': 300, 'feat_dim': 300, 'drop_ratio': 0.3, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 778, 'task': 'regression', 'data_path': 'data/lipophilicity/Lipophilicity.csv', 'target': ['exp']}}\n",
      "Running on: cuda:0\n",
      "4199\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/4199\n",
      "Generating scaffold 1000/4199\n",
      "Generating scaffold 2000/4199\n",
      "Generating scaffold 3000/4199\n",
      "Generating scaffold 4000/4199\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 7.696639060974121\n",
      "0 50 2.101868152618408\n",
      "0 100 1.1567734479904175\n",
      "Validation loss: 2.3252889542352584 RMSE: 1.5248898\n",
      "1 45 0.9797375798225403\n",
      "1 95 0.9872274398803711\n",
      "Validation loss: 2.16029136407943 RMSE: 1.469793\n",
      "2 40 0.8828537464141846\n",
      "2 90 1.0013887882232666\n",
      "Validation loss: 1.9319495746067592 RMSE: 1.3899459\n",
      "3 35 1.1653870344161987\n",
      "3 85 1.0954835414886475\n",
      "Validation loss: 1.582962505590348 RMSE: 1.2581583\n",
      "4 30 0.7436355948448181\n",
      "4 80 1.1348705291748047\n",
      "Validation loss: 1.0905927388440995 RMSE: 1.0443144\n",
      "5 25 0.626083254814148\n",
      "5 75 0.5603001713752747\n",
      "Validation loss: 1.999525976322946 RMSE: 1.4140459\n",
      "6 20 0.968799352645874\n",
      "6 70 0.587486207485199\n",
      "Validation loss: 1.0399481305054257 RMSE: 1.0197785\n",
      "7 15 0.7335625290870667\n",
      "7 65 0.8851189613342285\n",
      "Validation loss: 0.9812902918883732 RMSE: 0.99060094\n",
      "8 10 0.8426111936569214\n",
      "8 60 0.5876376628875732\n",
      "Validation loss: 1.7295056025187174 RMSE: 1.3151066\n",
      "9 5 1.2725896835327148\n",
      "9 55 0.672860860824585\n",
      "Validation loss: 0.919473139444987 RMSE: 0.95889163\n",
      "10 0 1.0430196523666382\n",
      "10 50 0.5189236998558044\n",
      "10 100 0.4530860185623169\n",
      "Validation loss: 1.0347759817327773 RMSE: 1.0172393\n",
      "11 45 0.8347018957138062\n",
      "11 95 0.30408042669296265\n",
      "Validation loss: 1.3842015243711925 RMSE: 1.176521\n",
      "12 40 0.810733437538147\n",
      "12 90 1.011696457862854\n",
      "Validation loss: 1.214769660858881 RMSE: 1.1021659\n",
      "13 35 0.7549213171005249\n",
      "13 85 0.8864408135414124\n",
      "Validation loss: 1.3361312368086407 RMSE: 1.1559114\n",
      "14 30 0.649172306060791\n",
      "14 80 0.32755085825920105\n",
      "Validation loss: 1.0080049162819271 RMSE: 1.0039945\n",
      "15 25 0.7408119440078735\n",
      "15 75 1.040749192237854\n",
      "Validation loss: 0.7790626426537831 RMSE: 0.88264525\n",
      "16 20 0.7380382418632507\n",
      "16 70 0.6920436024665833\n",
      "Validation loss: 1.0794860612778436 RMSE: 1.0389832\n",
      "17 15 0.6497246026992798\n",
      "17 65 0.39066579937934875\n",
      "Validation loss: 0.9513399731545221 RMSE: 0.9753666\n",
      "18 10 0.4648187458515167\n",
      "18 60 0.5832098126411438\n",
      "Validation loss: 1.0471311228615896 RMSE: 1.0232943\n",
      "19 5 0.5938106775283813\n",
      "19 55 0.6769754886627197\n",
      "Validation loss: 0.8906612603437333 RMSE: 0.9437486\n",
      "20 0 0.5237558484077454\n",
      "20 50 0.5215433835983276\n",
      "20 100 0.3868338167667389\n",
      "Validation loss: 1.0807944933573406 RMSE: 1.0396127\n",
      "21 45 0.4377847909927368\n",
      "21 95 1.0793390274047852\n",
      "Validation loss: 0.9803363141559419 RMSE: 0.99011934\n",
      "22 40 0.6424059867858887\n",
      "22 90 0.6480292081832886\n",
      "Validation loss: 0.924797850279581 RMSE: 0.9616641\n",
      "23 35 0.5232254862785339\n",
      "23 85 0.530854344367981\n",
      "Validation loss: 1.0019603564625694 RMSE: 1.0009797\n",
      "24 30 0.9498262405395508\n",
      "24 80 0.4424417018890381\n",
      "Validation loss: 0.956335619517735 RMSE: 0.97792417\n",
      "25 25 0.4462651312351227\n",
      "25 75 0.5019600987434387\n",
      "Validation loss: 1.0010745956784204 RMSE: 1.0005372\n",
      "26 20 0.785681426525116\n",
      "26 70 0.5317408442497253\n",
      "Validation loss: 0.9055639868690855 RMSE: 0.9516113\n",
      "27 15 0.5330389142036438\n",
      "27 65 0.6957195997238159\n",
      "Validation loss: 0.839555621714819 RMSE: 0.9162727\n",
      "28 10 0.8765316605567932\n",
      "28 60 0.6695131063461304\n",
      "Validation loss: 0.7724533228647141 RMSE: 0.87889326\n",
      "29 5 0.5649533271789551\n",
      "29 55 0.9829801321029663\n",
      "Validation loss: 0.7816181364513579 RMSE: 0.8840917\n",
      "30 0 0.4218306243419647\n",
      "30 50 0.29472988843917847\n",
      "30 100 0.39772114157676697\n",
      "Validation loss: 1.055286163375491 RMSE: 1.0272713\n",
      "31 45 0.408829003572464\n",
      "31 95 0.43493375182151794\n",
      "Validation loss: 0.7620088651066734 RMSE: 0.87293124\n",
      "32 40 0.4325302243232727\n",
      "32 90 0.601691722869873\n",
      "Validation loss: 0.7751676502681913 RMSE: 0.88043606\n",
      "33 35 0.7585209012031555\n",
      "33 85 0.8799166679382324\n",
      "Validation loss: 0.9204476424625941 RMSE: 0.95939964\n",
      "34 30 0.38442444801330566\n",
      "34 80 0.8775705695152283\n",
      "Validation loss: 0.8189230402310689 RMSE: 0.9049437\n",
      "35 25 0.6841831207275391\n",
      "35 75 0.4424670934677124\n",
      "Validation loss: 0.7250195026397706 RMSE: 0.85148084\n",
      "36 20 0.5628423094749451\n",
      "36 70 1.0439761877059937\n",
      "Validation loss: 0.8066154062038376 RMSE: 0.8981177\n",
      "37 15 0.43899208307266235\n",
      "37 65 0.46667513251304626\n",
      "Validation loss: 0.8453483059292748 RMSE: 0.9194282\n",
      "38 10 0.41988885402679443\n",
      "38 60 0.405630499124527\n",
      "Validation loss: 0.9798832598186674 RMSE: 0.9898906\n",
      "39 5 0.43444159626960754\n",
      "39 55 0.4321068227291107\n",
      "Validation loss: 0.8092730272383917 RMSE: 0.89959604\n",
      "40 0 0.3964995741844177\n",
      "40 50 0.3876197934150696\n",
      "40 100 0.24336017668247223\n",
      "Validation loss: 0.9393958534513202 RMSE: 0.96922433\n",
      "41 45 0.36477968096733093\n",
      "41 95 0.47989869117736816\n",
      "Validation loss: 0.8244506501016162 RMSE: 0.90799266\n",
      "42 40 0.43635690212249756\n",
      "42 90 0.4226272702217102\n",
      "Validation loss: 0.8899931663558597 RMSE: 0.9433945\n",
      "43 35 0.4942196011543274\n",
      "43 85 0.4418073296546936\n",
      "Validation loss: 1.0191570826939174 RMSE: 1.0095332\n",
      "44 30 0.4835583567619324\n",
      "44 80 0.30638375878334045\n",
      "Validation loss: 0.7969499247414725 RMSE: 0.89272046\n",
      "45 25 0.2291547656059265\n",
      "45 75 0.41558024287223816\n",
      "Validation loss: 0.7676709680330186 RMSE: 0.8761684\n",
      "46 20 0.6311607360839844\n",
      "46 70 0.8092350363731384\n",
      "Validation loss: 0.9121623260634286 RMSE: 0.95507187\n",
      "47 15 0.48332932591438293\n",
      "47 65 0.47483450174331665\n",
      "Validation loss: 0.8002673580532982 RMSE: 0.89457667\n",
      "48 10 0.5742704272270203\n",
      "48 60 0.296968936920166\n",
      "Validation loss: 0.9325263749985467 RMSE: 0.96567404\n",
      "49 5 0.5509586334228516\n",
      "49 55 0.3698291480541229\n",
      "Validation loss: 0.8589504900432768 RMSE: 0.9267958\n",
      "50 0 0.31613436341285706\n",
      "50 50 0.3715287446975708\n",
      "50 100 0.584426760673523\n",
      "Validation loss: 1.0068655309222994 RMSE: 1.0034269\n",
      "51 45 0.2890092730522156\n",
      "51 95 0.41459333896636963\n",
      "Validation loss: 0.7373310418356033 RMSE: 0.85867983\n",
      "52 40 0.4530649185180664\n",
      "52 90 0.35805192589759827\n",
      "Validation loss: 0.7476775566736857 RMSE: 0.8646835\n",
      "53 35 0.23279233276844025\n",
      "53 85 0.46807238459587097\n",
      "Validation loss: 0.8543453716096424 RMSE: 0.92430806\n",
      "54 30 0.6438179612159729\n",
      "54 80 0.33868691325187683\n",
      "Validation loss: 0.727640205195972 RMSE: 0.85301834\n",
      "55 25 0.33835944533348083\n",
      "55 75 0.36855196952819824\n",
      "Validation loss: 0.9849773350216093 RMSE: 0.9924603\n",
      "56 20 0.27761632204055786\n",
      "56 70 0.394796222448349\n",
      "Validation loss: 0.9941603056022099 RMSE: 0.99707586\n",
      "57 15 0.5572413206100464\n",
      "57 65 0.7921246886253357\n",
      "Validation loss: 0.8525161527452014 RMSE: 0.923318\n",
      "58 10 0.270534485578537\n",
      "58 60 0.4822300672531128\n",
      "Validation loss: 0.7704653149559384 RMSE: 0.87776154\n",
      "59 5 0.43262121081352234\n",
      "59 55 0.4454265832901001\n",
      "Validation loss: 0.8681316327481042 RMSE: 0.9317358\n",
      "60 0 0.4323095381259918\n",
      "60 50 0.6002544164657593\n",
      "60 100 0.433222234249115\n",
      "Validation loss: 0.7124392107838676 RMSE: 0.84406114\n",
      "61 45 0.5566495060920715\n",
      "61 95 0.3984423875808716\n",
      "Validation loss: 0.8429581131253924 RMSE: 0.91812754\n",
      "62 40 0.36407163739204407\n",
      "62 90 0.286523699760437\n",
      "Validation loss: 0.9808526953061422 RMSE: 0.99038005\n",
      "63 35 0.47556546330451965\n",
      "63 85 0.4001295864582062\n",
      "Validation loss: 0.8475900053977966 RMSE: 0.9206465\n",
      "64 30 0.5885140895843506\n",
      "64 80 0.4301964044570923\n",
      "Validation loss: 1.243429472332909 RMSE: 1.1150917\n",
      "65 25 0.3772571086883545\n",
      "65 75 0.4178787171840668\n",
      "Validation loss: 0.8203821332681747 RMSE: 0.9057495\n",
      "66 20 0.23668286204338074\n",
      "66 70 0.5406747460365295\n",
      "Validation loss: 1.0389991396949405 RMSE: 1.0193131\n",
      "67 15 0.4602283537387848\n",
      "67 65 0.6106858849525452\n",
      "Validation loss: 0.9170560462134225 RMSE: 0.95763046\n",
      "68 10 0.3087306320667267\n",
      "68 60 0.34484952688217163\n",
      "Validation loss: 0.8909363848822457 RMSE: 0.94389427\n",
      "69 5 0.6063246726989746\n",
      "69 55 0.38478267192840576\n",
      "Validation loss: 0.8443410737173898 RMSE: 0.91888034\n",
      "70 0 0.3852291405200958\n",
      "70 50 0.23001374304294586\n",
      "70 100 0.45409873127937317\n",
      "Validation loss: 0.6785656384059361 RMSE: 0.823751\n",
      "71 45 0.6523513197898865\n",
      "71 95 0.5109744668006897\n",
      "Validation loss: 0.7650842859631493 RMSE: 0.87469095\n",
      "72 40 0.2618967294692993\n",
      "72 90 0.2251080572605133\n",
      "Validation loss: 1.0060825756617955 RMSE: 1.0030366\n",
      "73 35 0.15863408148288727\n",
      "73 85 0.31858208775520325\n",
      "Validation loss: 1.0538829042797997 RMSE: 1.026588\n",
      "74 30 0.3702757954597473\n",
      "74 80 0.4031597375869751\n",
      "Validation loss: 0.9000049715950376 RMSE: 0.9486859\n",
      "75 25 0.5371130704879761\n",
      "75 75 0.32746320962905884\n",
      "Validation loss: 0.8749349594116211 RMSE: 0.93537956\n",
      "76 20 0.25897398591041565\n",
      "76 70 0.40774789452552795\n",
      "Validation loss: 0.7996210944084894 RMSE: 0.8942154\n",
      "77 15 0.21446849405765533\n",
      "77 65 0.5369205474853516\n",
      "Validation loss: 0.8251156931831723 RMSE: 0.9083588\n",
      "78 10 0.2911859154701233\n",
      "78 60 0.2993296980857849\n",
      "Validation loss: 1.1939906199773154 RMSE: 1.0926988\n",
      "79 5 0.31409308314323425\n",
      "79 55 0.36705198884010315\n",
      "Validation loss: 0.8335906357992263 RMSE: 0.91301185\n",
      "80 0 0.3607674539089203\n",
      "80 50 0.1734047532081604\n",
      "80 100 0.2814149558544159\n",
      "Validation loss: 0.7219132781028748 RMSE: 0.8496548\n",
      "81 45 0.23370720446109772\n",
      "81 95 0.3394385576248169\n",
      "Validation loss: 0.9788372993469239 RMSE: 0.98936206\n",
      "82 40 0.311964213848114\n",
      "82 90 0.26421740651130676\n",
      "Validation loss: 0.9152700480960664 RMSE: 0.95669746\n",
      "83 35 0.29571443796157837\n",
      "83 85 0.3443896472454071\n",
      "Validation loss: 0.7803389889853342 RMSE: 0.883368\n",
      "84 30 0.3798125684261322\n",
      "84 80 0.18204365670681\n",
      "Validation loss: 0.734412910257067 RMSE: 0.85697895\n",
      "85 25 0.2535232901573181\n",
      "85 75 0.24162237346172333\n",
      "Validation loss: 0.815028106598627 RMSE: 0.90278906\n",
      "86 20 0.3893575370311737\n",
      "86 70 0.18879379332065582\n",
      "Validation loss: 0.8147604025545574 RMSE: 0.90264076\n",
      "87 15 0.23757360875606537\n",
      "87 65 0.2174755483865738\n",
      "Validation loss: 0.6917181994233813 RMSE: 0.83169603\n",
      "88 10 0.2647997736930847\n",
      "88 60 0.18647581338882446\n",
      "Validation loss: 0.750987030352865 RMSE: 0.8665951\n",
      "89 5 0.712846577167511\n",
      "89 55 0.3749651610851288\n",
      "Validation loss: 0.7712424936748686 RMSE: 0.87820417\n",
      "90 0 0.3671622574329376\n",
      "90 50 0.4594660699367523\n",
      "90 100 0.3224674463272095\n",
      "Validation loss: 1.0258492838768731 RMSE: 1.0128422\n",
      "91 45 0.5415397882461548\n",
      "91 95 0.24843110144138336\n",
      "Validation loss: 0.7896949768066406 RMSE: 0.88864785\n",
      "92 40 0.24522459506988525\n",
      "92 90 0.3496667146682739\n",
      "Validation loss: 0.7526909345672244 RMSE: 0.8675776\n",
      "93 35 0.525612473487854\n",
      "93 85 0.22349733114242554\n",
      "Validation loss: 0.8167229544548761 RMSE: 0.90372723\n",
      "94 30 0.2702246904373169\n",
      "94 80 0.37062177062034607\n",
      "Validation loss: 0.8034629021372114 RMSE: 0.8963609\n",
      "95 25 0.1895149052143097\n",
      "95 75 0.2759126126766205\n",
      "Validation loss: 0.969776581582569 RMSE: 0.9847723\n",
      "96 20 0.36385294795036316\n",
      "96 70 0.5244536399841309\n",
      "Validation loss: 0.6595014157749358 RMSE: 0.81209695\n",
      "97 15 0.3834686875343323\n",
      "97 65 0.27033334970474243\n",
      "Validation loss: 0.8161821473212469 RMSE: 0.903428\n",
      "98 10 0.41753366589546204\n",
      "98 60 0.22774548828601837\n",
      "Validation loss: 0.8810611270722889 RMSE: 0.9386486\n",
      "99 5 0.2007634937763214\n",
      "99 55 0.2683669328689575\n",
      "Validation loss: 0.8720723858901432 RMSE: 0.93384814\n",
      "100 0 0.2737911641597748\n",
      "100 50 0.20001350343227386\n",
      "100 100 0.2560414671897888\n",
      "Validation loss: 0.8971098635877882 RMSE: 0.9471588\n",
      "101 45 0.23215138912200928\n",
      "101 95 0.33449968695640564\n",
      "Validation loss: 0.8735410389446077 RMSE: 0.9346342\n",
      "102 40 0.2531987130641937\n",
      "102 90 0.37716642022132874\n",
      "Validation loss: 0.8299374966394334 RMSE: 0.911009\n",
      "103 35 0.20196881890296936\n",
      "103 85 0.2898164391517639\n",
      "Validation loss: 0.8101447894459679 RMSE: 0.90008044\n",
      "104 30 0.3335481882095337\n",
      "104 80 0.37847915291786194\n",
      "Validation loss: 0.8799155462355841 RMSE: 0.9380381\n",
      "105 25 0.23125925660133362\n",
      "105 75 0.2748808264732361\n",
      "Validation loss: 1.1922111670176188 RMSE: 1.0918843\n",
      "106 20 0.2887762188911438\n",
      "106 70 0.2574492394924164\n",
      "Validation loss: 1.226875390892937 RMSE: 1.1076442\n",
      "107 15 0.2356027066707611\n",
      "107 65 0.4705181419849396\n",
      "Validation loss: 1.044316798164731 RMSE: 1.0219182\n",
      "108 10 0.18824900686740875\n",
      "108 60 0.2892262935638428\n",
      "Validation loss: 0.7704849305607023 RMSE: 0.8777727\n",
      "109 5 0.34616541862487793\n",
      "109 55 0.32666337490081787\n",
      "Validation loss: 0.9129265626271565 RMSE: 0.95547193\n",
      "110 0 0.2783501446247101\n",
      "110 50 0.30687403678894043\n",
      "110 100 0.2614343762397766\n",
      "Validation loss: 0.9762115837562652 RMSE: 0.9880342\n",
      "111 45 0.22498348355293274\n",
      "111 95 0.37288033962249756\n",
      "Validation loss: 0.7453454891840617 RMSE: 0.86333394\n",
      "112 40 0.24274508655071259\n",
      "112 90 0.42803269624710083\n",
      "Validation loss: 0.9440700443018051 RMSE: 0.97163266\n",
      "113 35 0.25092482566833496\n",
      "113 85 0.3787739872932434\n",
      "Validation loss: 0.7889966973236628 RMSE: 0.8882549\n",
      "114 30 0.30169352889060974\n",
      "114 80 0.3982270658016205\n",
      "Validation loss: 0.8966550330320994 RMSE: 0.94691867\n",
      "115 25 0.3963894546031952\n",
      "115 75 0.2517101466655731\n",
      "Validation loss: 0.8132760317552657 RMSE: 0.90181816\n",
      "116 20 0.2547357380390167\n",
      "116 70 0.2901208698749542\n",
      "Validation loss: 0.9891534260341099 RMSE: 0.9945619\n",
      "117 15 0.34869301319122314\n",
      "117 65 0.3007815480232239\n",
      "Validation loss: 0.7974754833039783 RMSE: 0.8930148\n",
      "118 10 0.3550393283367157\n",
      "118 60 0.2785285711288452\n",
      "Validation loss: 0.9337637151990618 RMSE: 0.96631455\n",
      "119 5 0.2184402197599411\n",
      "119 55 0.20694614946842194\n",
      "Validation loss: 0.8331290551594326 RMSE: 0.91275907\n",
      "120 0 0.25811055302619934\n",
      "120 50 0.21609658002853394\n",
      "120 100 0.24726150929927826\n",
      "Validation loss: 0.9032339947564262 RMSE: 0.9503862\n",
      "121 45 0.24536025524139404\n",
      "121 95 0.24202121794223785\n",
      "Validation loss: 0.7345616493906294 RMSE: 0.8570658\n",
      "122 40 0.29165711998939514\n",
      "122 90 0.19286201894283295\n",
      "Validation loss: 0.6742179149673099 RMSE: 0.82110775\n",
      "123 35 0.18842317163944244\n",
      "123 85 0.1601029634475708\n",
      "Validation loss: 0.829159190541222 RMSE: 0.9105818\n",
      "124 30 0.23405484855175018\n",
      "124 80 0.313884973526001\n",
      "Validation loss: 0.7585991155533564 RMSE: 0.870976\n",
      "125 25 0.27564898133277893\n",
      "125 75 0.29173579812049866\n",
      "Validation loss: 0.7915897443180993 RMSE: 0.8897132\n",
      "126 20 0.17048820853233337\n",
      "126 70 0.3291681408882141\n",
      "Validation loss: 0.8281268006279355 RMSE: 0.9100148\n",
      "127 15 0.24002879858016968\n",
      "127 65 0.1723676323890686\n",
      "Validation loss: 0.929767504192534 RMSE: 0.96424454\n",
      "128 10 0.1541350781917572\n",
      "128 60 0.17764614522457123\n",
      "Validation loss: 0.8194483927318028 RMSE: 0.9052339\n",
      "129 5 0.18652498722076416\n",
      "129 55 0.1706552356481552\n",
      "Validation loss: 0.9857107684725807 RMSE: 0.9928297\n",
      "130 0 0.2159493863582611\n",
      "130 50 0.20340245962142944\n",
      "130 100 0.23110303282737732\n",
      "Validation loss: 0.8330557017099289 RMSE: 0.91271883\n",
      "131 45 0.36015334725379944\n",
      "131 95 0.23801451921463013\n",
      "Validation loss: 0.7461848179499309 RMSE: 0.86381984\n",
      "132 40 0.16383612155914307\n",
      "132 90 0.25039610266685486\n",
      "Validation loss: 0.8979881882667542 RMSE: 0.94762236\n",
      "133 35 0.21831391751766205\n"
     ]
    }
   ],
   "source": [
    "datasets = ['FreeSolv', 'ESOL', 'Lipo', 'qm7', \"bace\",  \"bbbp\",  'tox21', 'clintox', 'sider',]\n",
    "seeds = [777, 778, 779, 780, 781]\n",
    "\n",
    "for dataset in datasets:\n",
    "    for seed in seeds:\n",
    "            if dataset == 'FreeSolv':\n",
    "            # FreeSolv 데이터셋에 대한 특정 옵션을 적용\n",
    "                !python finetuneReconOrigin.py \\\n",
    "                --task_name {dataset} \\\n",
    "                --seed {seed} \\\n",
    "                --dropout 0.5 \\\n",
    "                --num_layer 3 \\\n",
    "                --emb_dim 64 \\\n",
    "                --feat_dim 64 \\\n",
    "                --alpha 0.1\n",
    "                \n",
    "            else:\n",
    "                !python finetuneReconOrigin.py \\\n",
    "                --task_name {dataset} \\\n",
    "                --seed {seed} \\\n",
    "                --alpha 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ebf688b-6297-46a2-a89f-a81302e280c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 1, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:0', 'task_name': 'freesolv', 'model_type': 'gin', 'model': {'num_layer': 5, 'emb_dim': 300, 'feat_dim': 300, 'drop_ratio': 0.3, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 740, 'task': 'regression', 'data_path': 'data/freesolv/freesolv.csv', 'target': ['expt']}}\n",
      "Running on: cuda:0\n",
      "641\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/641\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 12.416099548339844\n",
      "Validation loss: 36.89025688171387 RMSE: 6.073735\n",
      "Loaded trained model with success.\n",
      "Test loss: 14.601755259587215 Test RMSE: 3.8212245\n"
     ]
    }
   ],
   "source": [
    "!python finetuneReconOrigin.py \\\n",
    "--task_name freesolv \\\n",
    "--seed 740 \\\n",
    "--alpha 0.1 \\\n",
    "--epochs 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6764b07a-c44b-44d3-ab6b-161100c0d53b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 200, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:0', 'task_name': 'esol', 'model_type': 'gin', 'model': {'num_layer': 5, 'emb_dim': 300, 'feat_dim': 300, 'drop_ratio': 0.3, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 751, 'task': 'regression', 'data_path': 'data/esol/esol.csv', 'target': ['measured log solubility in mols per litre']}}\n",
      "Running on: cuda:0\n",
      "1127\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/1127\n",
      "Generating scaffold 1000/1127\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "tensor([[ 0.0806, -0.0410, -0.0872,  ..., -0.0789, -0.0887, -0.1107],\n",
      "        [ 0.0313,  0.0164,  0.1077,  ..., -0.0785,  0.1171,  0.1046],\n",
      "        [-0.0002, -0.0291, -0.0762,  ...,  0.0110,  0.0630,  0.0511],\n",
      "        ...,\n",
      "        [ 0.0313,  0.0164,  0.1077,  ..., -0.0785,  0.1171,  0.1046],\n",
      "        [-0.0175,  0.1190, -0.0096,  ...,  0.0821,  0.0621,  0.0488],\n",
      "        [ 0.0313,  0.0164,  0.1077,  ..., -0.0785,  0.1171,  0.1046]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "0 0 8.955981254577637\n",
      "tensor([[ 0.0312,  0.0165,  0.1078,  ..., -0.0786,  0.1172,  0.1047],\n",
      "        [ 0.0312,  0.0165,  0.1078,  ..., -0.0786,  0.1172,  0.1047],\n",
      "        [ 0.0312,  0.0165,  0.1078,  ..., -0.0786,  0.1172,  0.1047],\n",
      "        ...,\n",
      "        [ 0.0312,  0.0165,  0.1078,  ..., -0.0786,  0.1172,  0.1047],\n",
      "        [ 0.0312,  0.0165,  0.1078,  ..., -0.0786,  0.1172,  0.1047],\n",
      "        [ 0.0312,  0.0165,  0.1078,  ..., -0.0786,  0.1172,  0.1047]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0004, -0.0289, -0.0764,  ...,  0.0108,  0.0629,  0.0509],\n",
      "        [-0.0004, -0.0289, -0.0764,  ...,  0.0108,  0.0629,  0.0509],\n",
      "        [-0.0004, -0.0289, -0.0764,  ...,  0.0108,  0.0629,  0.0509],\n",
      "        ...,\n",
      "        [ 0.0311,  0.0166,  0.1079,  ..., -0.0787,  0.1173,  0.1048],\n",
      "        [ 0.0311,  0.0166,  0.1079,  ..., -0.0787,  0.1173,  0.1048],\n",
      "        [ 0.0311,  0.0166,  0.1079,  ..., -0.0787,  0.1173,  0.1048]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0004, -0.0288, -0.0764,  ...,  0.0108,  0.0629,  0.0509],\n",
      "        [-0.0004, -0.0288, -0.0764,  ...,  0.0108,  0.0629,  0.0509],\n",
      "        [ 0.0311,  0.0167,  0.1079,  ..., -0.0788,  0.1174,  0.1048],\n",
      "        ...,\n",
      "        [ 0.0187, -0.0968, -0.0305,  ...,  0.0381, -0.0655, -0.0279],\n",
      "        [ 0.0311,  0.0167,  0.1079,  ..., -0.0788,  0.1174,  0.1048],\n",
      "        [ 0.0311,  0.0167,  0.1079,  ..., -0.0788,  0.1174,  0.1048]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0310,  0.0168,  0.1079,  ..., -0.0789,  0.1174,  0.1049],\n",
      "        [-0.0003, -0.0287, -0.0764,  ...,  0.0107,  0.0629,  0.0509],\n",
      "        [-0.0003, -0.0287, -0.0764,  ...,  0.0107,  0.0629,  0.0509],\n",
      "        ...,\n",
      "        [ 0.0310,  0.0168,  0.1079,  ..., -0.0789,  0.1174,  0.1049],\n",
      "        [ 0.0310,  0.0168,  0.1079,  ..., -0.0789,  0.1174,  0.1049],\n",
      "        [ 0.0310,  0.0168,  0.1079,  ..., -0.0789,  0.1174,  0.1049]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0309,  0.0168,  0.1080,  ..., -0.0789,  0.1175,  0.1050],\n",
      "        [ 0.0309,  0.0168,  0.1080,  ..., -0.0789,  0.1175,  0.1050],\n",
      "        [-0.0961, -0.0527,  0.0569,  ...,  0.0218, -0.0656,  0.0982],\n",
      "        ...,\n",
      "        [ 0.0309,  0.0168,  0.1080,  ..., -0.0789,  0.1175,  0.1050],\n",
      "        [-0.0961, -0.0527,  0.0569,  ...,  0.0218, -0.0656,  0.0982],\n",
      "        [ 0.0065, -0.1107,  0.0619,  ...,  0.0173,  0.0921, -0.0094]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0065, -0.1107,  0.0619,  ...,  0.0174,  0.0921, -0.0095],\n",
      "        [ 0.0065, -0.1107,  0.0619,  ...,  0.0174,  0.0921, -0.0095],\n",
      "        [ 0.0309,  0.0169,  0.1080,  ..., -0.0790,  0.1176,  0.1050],\n",
      "        ...,\n",
      "        [-0.0961, -0.0528,  0.0569,  ...,  0.0219, -0.0655,  0.0981],\n",
      "        [ 0.0309,  0.0169,  0.1080,  ..., -0.0790,  0.1176,  0.1050],\n",
      "        [ 0.0065, -0.1107,  0.0619,  ...,  0.0174,  0.0921, -0.0095]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0002, -0.0285, -0.0763,  ...,  0.0108,  0.0630,  0.0507],\n",
      "        [-0.0002, -0.0285, -0.0763,  ...,  0.0108,  0.0630,  0.0507],\n",
      "        [ 0.0309,  0.0169,  0.1081,  ..., -0.0791,  0.1177,  0.1051],\n",
      "        ...,\n",
      "        [ 0.0309,  0.0169,  0.1081,  ..., -0.0791,  0.1177,  0.1051],\n",
      "        [ 0.0309,  0.0169,  0.1081,  ..., -0.0791,  0.1177,  0.1051],\n",
      "        [-0.0123,  0.1087, -0.0120,  ...,  0.0790,  0.0517, -0.0861]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0961, -0.0528,  0.0569,  ...,  0.0220, -0.0653,  0.0979],\n",
      "        [-0.0961, -0.0528,  0.0569,  ...,  0.0220, -0.0653,  0.0979],\n",
      "        [ 0.0804, -0.0412, -0.0873,  ..., -0.0789, -0.0891, -0.1105],\n",
      "        ...,\n",
      "        [ 0.0309,  0.0170,  0.1081,  ..., -0.0791,  0.1178,  0.1052],\n",
      "        [ 0.0309,  0.0170,  0.1081,  ..., -0.0791,  0.1178,  0.1052],\n",
      "        [-0.0961, -0.0528,  0.0569,  ...,  0.0220, -0.0653,  0.0979]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0002, -0.0284, -0.0764,  ...,  0.0108,  0.0631,  0.0506],\n",
      "        [-0.0002, -0.0284, -0.0764,  ...,  0.0108,  0.0631,  0.0506],\n",
      "        [ 0.0309,  0.0170,  0.1082,  ..., -0.0792,  0.1179,  0.1052],\n",
      "        ...,\n",
      "        [ 0.0309,  0.0170,  0.1082,  ..., -0.0792,  0.1179,  0.1052],\n",
      "        [ 0.0309,  0.0170,  0.1082,  ..., -0.0792,  0.1179,  0.1052],\n",
      "        [ 0.0309,  0.0170,  0.1082,  ..., -0.0792,  0.1179,  0.1052]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0310,  0.0170,  0.1083,  ..., -0.0792,  0.1179,  0.1053],\n",
      "        [-0.0002, -0.0284, -0.0765,  ...,  0.0108,  0.0631,  0.0505],\n",
      "        [-0.0002, -0.0284, -0.0765,  ...,  0.0108,  0.0631,  0.0505],\n",
      "        ...,\n",
      "        [ 0.0310,  0.0170,  0.1083,  ..., -0.0792,  0.1179,  0.1053],\n",
      "        [ 0.0310,  0.0170,  0.1083,  ..., -0.0792,  0.1179,  0.1053],\n",
      "        [ 0.0310,  0.0170,  0.1083,  ..., -0.0792,  0.1179,  0.1053]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0310,  0.0171,  0.1083,  ..., -0.0793,  0.1179,  0.1054],\n",
      "        [ 0.0070, -0.1110,  0.0617,  ...,  0.0178,  0.0925, -0.0099],\n",
      "        [ 0.0310,  0.0171,  0.1083,  ..., -0.0793,  0.1179,  0.1054],\n",
      "        ...,\n",
      "        [ 0.0310,  0.0171,  0.1083,  ..., -0.0793,  0.1179,  0.1054],\n",
      "        [ 0.0310,  0.0171,  0.1083,  ..., -0.0793,  0.1179,  0.1054],\n",
      "        [-0.0961, -0.0529,  0.0568,  ...,  0.0223, -0.0651,  0.0977]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0309,  0.0171,  0.1084,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [ 0.0309,  0.0171,  0.1084,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [-0.0961, -0.0529,  0.0568,  ...,  0.0223, -0.0650,  0.0976],\n",
      "        ...,\n",
      "        [ 0.0309,  0.0171,  0.1084,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [ 0.0309,  0.0171,  0.1084,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [ 0.0309,  0.0171,  0.1084,  ..., -0.0794,  0.1179,  0.1054]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0961, -0.0529,  0.0568,  ...,  0.0224, -0.0649,  0.0975],\n",
      "        [-0.0003, -0.0284, -0.0766,  ...,  0.0107,  0.0633,  0.0504],\n",
      "        [-0.0003, -0.0284, -0.0766,  ...,  0.0107,  0.0633,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0309,  0.0171,  0.1085,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [ 0.0071, -0.1112,  0.0617,  ...,  0.0179,  0.0926, -0.0100],\n",
      "        [ 0.0309,  0.0171,  0.1085,  ..., -0.0794,  0.1179,  0.1054]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0309,  0.0171,  0.1085,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [ 0.0309,  0.0171,  0.1085,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [ 0.0309,  0.0171,  0.1085,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        ...,\n",
      "        [ 0.0071, -0.1112,  0.0616,  ...,  0.0180,  0.0927, -0.0100],\n",
      "        [ 0.0309,  0.0171,  0.1085,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [ 0.0309,  0.0171,  0.1085,  ..., -0.0794,  0.1179,  0.1054]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0309,  0.0171,  0.1086,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [ 0.0309,  0.0171,  0.1086,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [ 0.0309,  0.0171,  0.1086,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        ...,\n",
      "        [ 0.0309,  0.0171,  0.1086,  ..., -0.0794,  0.1179,  0.1054],\n",
      "        [-0.0962, -0.0529,  0.0568,  ...,  0.0225, -0.0647,  0.0974],\n",
      "        [-0.0127,  0.1084, -0.0120,  ...,  0.0788,  0.0512, -0.0857]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0309,  0.0171,  0.1087,  ..., -0.0795,  0.1179,  0.1054],\n",
      "        [-0.0002, -0.0284, -0.0766,  ...,  0.0107,  0.0632,  0.0503],\n",
      "        [-0.0002, -0.0284, -0.0766,  ...,  0.0107,  0.0632,  0.0503],\n",
      "        ...,\n",
      "        [ 0.0309,  0.0171,  0.1087,  ..., -0.0795,  0.1179,  0.1054],\n",
      "        [ 0.0309,  0.0171,  0.1087,  ..., -0.0795,  0.1179,  0.1054],\n",
      "        [ 0.0309,  0.0171,  0.1087,  ..., -0.0795,  0.1179,  0.1054]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 3.0917e-02,  1.7134e-02,  1.0878e-01,  ..., -7.9471e-02,\n",
      "          1.1795e-01,  1.0532e-01],\n",
      "        [-1.1599e-04, -2.8463e-02, -7.6550e-02,  ...,  1.0742e-02,\n",
      "          6.3165e-02,  5.0226e-02],\n",
      "        [-1.1599e-04, -2.8463e-02, -7.6550e-02,  ...,  1.0742e-02,\n",
      "          6.3165e-02,  5.0226e-02],\n",
      "        ...,\n",
      "        [ 3.0917e-02,  1.7134e-02,  1.0878e-01,  ..., -7.9471e-02,\n",
      "          1.1795e-01,  1.0532e-01],\n",
      "        [ 3.0917e-02,  1.7134e-02,  1.0878e-01,  ..., -7.9471e-02,\n",
      "          1.1795e-01,  1.0532e-01],\n",
      "        [ 3.0917e-02,  1.7134e-02,  1.0878e-01,  ..., -7.9471e-02,\n",
      "          1.1795e-01,  1.0532e-01]], device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0309,  0.0172,  0.1088,  ..., -0.0795,  0.1180,  0.1053],\n",
      "        [ 0.0309,  0.0172,  0.1088,  ..., -0.0795,  0.1180,  0.1053],\n",
      "        [ 0.0309,  0.0172,  0.1088,  ..., -0.0795,  0.1180,  0.1053],\n",
      "        ...,\n",
      "        [ 0.0074, -0.1113,  0.0618,  ...,  0.0180,  0.0929, -0.0100],\n",
      "        [ 0.0309,  0.0172,  0.1088,  ..., -0.0795,  0.1180,  0.1053],\n",
      "        [-0.0964, -0.0527,  0.0568,  ...,  0.0224, -0.0647,  0.0975]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-9.6424e-02, -5.2667e-02,  5.6764e-02,  ...,  2.2343e-02,\n",
      "         -6.4659e-02,  9.7490e-02],\n",
      "        [ 3.0807e-02,  1.7243e-02,  1.0880e-01,  ..., -7.9564e-02,\n",
      "          1.1803e-01,  1.0522e-01],\n",
      "        [-1.7022e-05, -2.8596e-02, -7.6470e-02,  ...,  1.0681e-02,\n",
      "          6.3049e-02,  5.0292e-02],\n",
      "        ...,\n",
      "        [ 3.0807e-02,  1.7243e-02,  1.0880e-01,  ..., -7.9564e-02,\n",
      "          1.1803e-01,  1.0522e-01],\n",
      "        [-1.7267e-02,  1.1858e-01, -1.0125e-02,  ...,  8.2536e-02,\n",
      "          6.1876e-02,  4.8307e-02],\n",
      "        [ 3.0807e-02,  1.7243e-02,  1.0880e-01,  ..., -7.9564e-02,\n",
      "          1.1803e-01,  1.0522e-01]], device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 3.0753e-02,  1.7299e-02,  1.0883e-01,  ..., -7.9583e-02,\n",
      "          1.1807e-01,  1.0516e-01],\n",
      "        [ 3.0753e-02,  1.7299e-02,  1.0883e-01,  ..., -7.9583e-02,\n",
      "          1.1807e-01,  1.0516e-01],\n",
      "        [ 4.1295e-05, -2.8627e-02, -7.6411e-02,  ...,  1.0639e-02,\n",
      "          6.3006e-02,  5.0348e-02],\n",
      "        ...,\n",
      "        [ 3.0753e-02,  1.7299e-02,  1.0883e-01,  ..., -7.9583e-02,\n",
      "          1.1807e-01,  1.0516e-01],\n",
      "        [-1.2426e-02,  1.0806e-01, -1.2240e-02,  ...,  7.9107e-02,\n",
      "          5.1456e-02, -8.5975e-02],\n",
      "        [ 3.0753e-02,  1.7299e-02,  1.0883e-01,  ..., -7.9583e-02,\n",
      "          1.1807e-01,  1.0516e-01]], device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0307,  0.0173,  0.1089,  ..., -0.0796,  0.1181,  0.1051],\n",
      "        [ 0.0307,  0.0173,  0.1089,  ..., -0.0796,  0.1181,  0.1051],\n",
      "        [ 0.0307,  0.0173,  0.1089,  ..., -0.0796,  0.1181,  0.1051],\n",
      "        ...,\n",
      "        [ 0.0076, -0.1114,  0.0619,  ...,  0.0181,  0.0930, -0.0100],\n",
      "        [ 0.0307,  0.0173,  0.1089,  ..., -0.0796,  0.1181,  0.1051],\n",
      "        [ 0.0307,  0.0173,  0.1089,  ..., -0.0796,  0.1181,  0.1051]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0307,  0.0174,  0.1089,  ..., -0.0796,  0.1181,  0.1051],\n",
      "        [ 0.0808, -0.0411, -0.0881,  ..., -0.0790, -0.0893, -0.1108],\n",
      "        [ 0.0076, -0.1113,  0.0619,  ...,  0.0181,  0.0930, -0.0100],\n",
      "        ...,\n",
      "        [ 0.0307,  0.0174,  0.1089,  ..., -0.0796,  0.1181,  0.1051],\n",
      "        [-0.0966, -0.0525,  0.0567,  ...,  0.0222, -0.0646,  0.0975],\n",
      "        [-0.0966, -0.0525,  0.0567,  ...,  0.0222, -0.0646,  0.0975]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0001, -0.0288, -0.0764,  ...,  0.0106,  0.0630,  0.0504],\n",
      "        [ 0.0898,  0.0321,  0.0691,  ...,  0.0426, -0.0070,  0.0773],\n",
      "        [ 0.0001, -0.0288, -0.0764,  ...,  0.0106,  0.0630,  0.0504],\n",
      "        ...,\n",
      "        [-0.0967, -0.0524,  0.0567,  ...,  0.0222, -0.0646,  0.0975],\n",
      "        [ 0.0307,  0.0174,  0.1089,  ..., -0.0797,  0.1182,  0.1051],\n",
      "        [-0.0967, -0.0524,  0.0567,  ...,  0.0222, -0.0646,  0.0975]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0968, -0.0523,  0.0567,  ...,  0.0222, -0.0646,  0.0974],\n",
      "        [ 0.0001, -0.0288, -0.0764,  ...,  0.0106,  0.0630,  0.0504],\n",
      "        [ 0.0001, -0.0288, -0.0764,  ...,  0.0106,  0.0630,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0307,  0.0174,  0.1090,  ..., -0.0797,  0.1182,  0.1052],\n",
      "        [-0.0968, -0.0523,  0.0567,  ...,  0.0222, -0.0646,  0.0974],\n",
      "        [ 0.0307,  0.0174,  0.1090,  ..., -0.0797,  0.1182,  0.1052]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0306,  0.0174,  0.1090,  ..., -0.0798,  0.1182,  0.1052],\n",
      "        [ 0.0001, -0.0289, -0.0764,  ...,  0.0106,  0.0630,  0.0504],\n",
      "        [ 0.0001, -0.0289, -0.0764,  ...,  0.0106,  0.0630,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0306,  0.0174,  0.1090,  ..., -0.0798,  0.1182,  0.1052],\n",
      "        [-0.0968, -0.0523,  0.0566,  ...,  0.0222, -0.0646,  0.0974],\n",
      "        [-0.0122,  0.1078, -0.0123,  ...,  0.0793,  0.0515, -0.0860]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0122,  0.1078, -0.0123,  ...,  0.0793,  0.0516, -0.0860],\n",
      "        [ 0.0306,  0.0174,  0.1091,  ..., -0.0798,  0.1182,  0.1053],\n",
      "        [ 0.0306,  0.0174,  0.1091,  ..., -0.0798,  0.1182,  0.1053],\n",
      "        ...,\n",
      "        [ 0.0306,  0.0174,  0.1091,  ..., -0.0798,  0.1182,  0.1053],\n",
      "        [ 0.0306,  0.0174,  0.1091,  ..., -0.0798,  0.1182,  0.1053],\n",
      "        [ 0.0306,  0.0174,  0.1091,  ..., -0.0798,  0.1182,  0.1053]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0306,  0.0174,  0.1091,  ..., -0.0799,  0.1182,  0.1053],\n",
      "        [-0.0121,  0.1078, -0.0122,  ...,  0.0793,  0.0516, -0.0860],\n",
      "        [ 0.0306,  0.0174,  0.1091,  ..., -0.0799,  0.1182,  0.1053],\n",
      "        ...,\n",
      "        [ 0.0306,  0.0174,  0.1091,  ..., -0.0799,  0.1182,  0.1053],\n",
      "        [ 0.0306,  0.0174,  0.1091,  ..., -0.0799,  0.1182,  0.1053],\n",
      "        [-0.0170,  0.1189, -0.0102,  ...,  0.0827,  0.0622,  0.0487]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0121,  0.1077, -0.0122,  ...,  0.0793,  0.0516, -0.0860],\n",
      "        [ 0.0305,  0.0174,  0.1092,  ..., -0.0799,  0.1182,  0.1053],\n",
      "        [ 0.0305,  0.0174,  0.1092,  ..., -0.0799,  0.1182,  0.1053],\n",
      "        ...,\n",
      "        [ 0.0305,  0.0174,  0.1092,  ..., -0.0799,  0.1182,  0.1053],\n",
      "        [ 0.0001, -0.0291, -0.0765,  ...,  0.0107,  0.0630,  0.0504],\n",
      "        [ 0.0001, -0.0291, -0.0765,  ...,  0.0107,  0.0630,  0.0504]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Validation loss: 2.2496231530619935 RMSE: 1.4998744\n",
      "tensor([[ 0.0305,  0.0174,  0.1092,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [-0.0970, -0.0521,  0.0565,  ...,  0.0224, -0.0646,  0.0973],\n",
      "        [ 0.0001, -0.0292, -0.0766,  ...,  0.0107,  0.0630,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0305,  0.0174,  0.1092,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [ 0.0305,  0.0174,  0.1092,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [ 0.0305,  0.0174,  0.1092,  ..., -0.0800,  0.1182,  0.1054]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0002, -0.0292, -0.0766,  ...,  0.0108,  0.0630,  0.0504],\n",
      "        [ 0.0305,  0.0174,  0.1092,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [ 0.0305,  0.0174,  0.1092,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        ...,\n",
      "        [ 0.0305,  0.0174,  0.1092,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [-0.0971, -0.0521,  0.0565,  ...,  0.0224, -0.0646,  0.0972],\n",
      "        [ 0.0078, -0.1111,  0.0621,  ...,  0.0182,  0.0933, -0.0101]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0971, -0.0521,  0.0565,  ...,  0.0224, -0.0647,  0.0972],\n",
      "        [ 0.0002, -0.0292, -0.0766,  ...,  0.0108,  0.0630,  0.0504],\n",
      "        [ 0.0002, -0.0292, -0.0766,  ...,  0.0108,  0.0630,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0305,  0.0174,  0.1093,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [ 0.0305,  0.0174,  0.1093,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [ 0.0078, -0.1111,  0.0621,  ...,  0.0182,  0.0934, -0.0101]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0304,  0.0174,  0.1093,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [ 0.0304,  0.0174,  0.1093,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [ 0.0002, -0.0293, -0.0766,  ...,  0.0108,  0.0630,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0304,  0.0174,  0.1093,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [ 0.0304,  0.0174,  0.1093,  ..., -0.0800,  0.1182,  0.1054],\n",
      "        [ 0.0304,  0.0174,  0.1093,  ..., -0.0800,  0.1182,  0.1054]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [-0.0120,  0.1077, -0.0121,  ...,  0.0793,  0.0515, -0.0860],\n",
      "        ...,\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [ 0.0809, -0.0410, -0.0884,  ..., -0.0791, -0.0894, -0.1111],\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [ 0.0003, -0.0293, -0.0766,  ...,  0.0108,  0.0631,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [-0.0120,  0.1076, -0.0121,  ...,  0.0793,  0.0515, -0.0860]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        ...,\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055],\n",
      "        [ 0.0304,  0.0174,  0.1094,  ..., -0.0800,  0.1182,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0304,  0.0174,  0.1095,  ..., -0.0801,  0.1182,  0.1055],\n",
      "        [ 0.0304,  0.0174,  0.1095,  ..., -0.0801,  0.1182,  0.1055],\n",
      "        [ 0.0304,  0.0174,  0.1095,  ..., -0.0801,  0.1182,  0.1055],\n",
      "        ...,\n",
      "        [-0.0974, -0.0521,  0.0564,  ...,  0.0225, -0.0648,  0.0970],\n",
      "        [ 0.0233,  0.0582, -0.1101,  ...,  0.0704,  0.0737,  0.0701],\n",
      "        [-0.0974, -0.0521,  0.0564,  ...,  0.0225, -0.0648,  0.0970]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0303,  0.0174,  0.1095,  ..., -0.0801,  0.1182,  0.1055],\n",
      "        [ 0.0303,  0.0174,  0.1095,  ..., -0.0801,  0.1182,  0.1055],\n",
      "        [ 0.0003, -0.0294, -0.0767,  ...,  0.0108,  0.0631,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0303,  0.0174,  0.1095,  ..., -0.0801,  0.1182,  0.1055],\n",
      "        [ 0.0303,  0.0174,  0.1095,  ..., -0.0801,  0.1182,  0.1055],\n",
      "        [-0.0974, -0.0522,  0.0564,  ...,  0.0225, -0.0649,  0.0970]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0003, -0.0295, -0.0767,  ...,  0.0108,  0.0631,  0.0503],\n",
      "        [ 0.0003, -0.0295, -0.0767,  ...,  0.0108,  0.0631,  0.0503],\n",
      "        [ 0.0303,  0.0174,  0.1096,  ..., -0.0801,  0.1182,  0.1055],\n",
      "        ...,\n",
      "        [ 0.0303,  0.0174,  0.1096,  ..., -0.0801,  0.1182,  0.1055],\n",
      "        [ 0.0303,  0.0174,  0.1096,  ..., -0.0801,  0.1182,  0.1055],\n",
      "        [ 0.0303,  0.0174,  0.1096,  ..., -0.0801,  0.1182,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0303,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1055],\n",
      "        [ 0.0003, -0.0295, -0.0767,  ...,  0.0108,  0.0631,  0.0503],\n",
      "        [ 0.0003, -0.0295, -0.0767,  ...,  0.0108,  0.0631,  0.0503],\n",
      "        ...,\n",
      "        [-0.0167,  0.1191, -0.0102,  ...,  0.0827,  0.0622,  0.0491],\n",
      "        [-0.0167,  0.1191, -0.0102,  ...,  0.0827,  0.0622,  0.0491],\n",
      "        [ 0.0303,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0302,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1055],\n",
      "        [ 0.0004, -0.0296, -0.0768,  ...,  0.0108,  0.0631,  0.0503],\n",
      "        [ 0.0004, -0.0296, -0.0768,  ...,  0.0108,  0.0631,  0.0503],\n",
      "        ...,\n",
      "        [ 0.0302,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1055],\n",
      "        [ 0.0302,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1055],\n",
      "        [ 0.0302,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0079, -0.1110,  0.0623,  ...,  0.0182,  0.0935, -0.0101],\n",
      "        [-0.0976, -0.0523,  0.0563,  ...,  0.0225, -0.0650,  0.0970],\n",
      "        [ 0.0302,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0302,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0302,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0302,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0302,  0.0174,  0.1097,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0302,  0.0174,  0.1097,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0302,  0.0174,  0.1097,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        ...,\n",
      "        [-0.0977, -0.0523,  0.0563,  ...,  0.0225, -0.0649,  0.0970],\n",
      "        [ 0.0302,  0.0174,  0.1097,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0302,  0.0174,  0.1097,  ..., -0.0802,  0.1182,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [-0.0977, -0.0524,  0.0563,  ...,  0.0225, -0.0649,  0.0969],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0802,  0.1182,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0118,  0.1075, -0.0120,  ...,  0.0793,  0.0513, -0.0859],\n",
      "        [-0.0118,  0.1075, -0.0120,  ...,  0.0793,  0.0513, -0.0859],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0803,  0.1182,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0803,  0.1182,  0.1056],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0803,  0.1182,  0.1056],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0803,  0.1182,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0003, -0.0297, -0.0768,  ...,  0.0108,  0.0631,  0.0504],\n",
      "        [ 0.0003, -0.0297, -0.0768,  ...,  0.0108,  0.0631,  0.0504],\n",
      "        [ 0.0003, -0.0297, -0.0768,  ...,  0.0108,  0.0631,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0082, -0.1109,  0.0626,  ...,  0.0178,  0.0937, -0.0099],\n",
      "        [ 0.0301,  0.0174,  0.1096,  ..., -0.0803,  0.1181,  0.1056],\n",
      "        [ 0.0813, -0.0407, -0.0882,  ..., -0.0792, -0.0893, -0.1111]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0300,  0.0174,  0.1096,  ..., -0.0804,  0.1181,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0804,  0.1181,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0804,  0.1181,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0804,  0.1181,  0.1056],\n",
      "        [ 0.0082, -0.1109,  0.0626,  ...,  0.0178,  0.0938, -0.0099],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0804,  0.1181,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0002, -0.0298, -0.0769,  ...,  0.0107,  0.0631,  0.0504],\n",
      "        [ 0.0002, -0.0298, -0.0769,  ...,  0.0107,  0.0631,  0.0504],\n",
      "        [ 0.0002, -0.0298, -0.0769,  ...,  0.0107,  0.0631,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0804,  0.1181,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0804,  0.1181,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0804,  0.1181,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0300,  0.0174,  0.1096,  ..., -0.0805,  0.1181,  0.1055],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0805,  0.1181,  0.1055],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0805,  0.1181,  0.1055],\n",
      "        ...,\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0805,  0.1181,  0.1055],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0805,  0.1181,  0.1055],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0805,  0.1181,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0174,  0.1097,  ..., -0.0805,  0.1181,  0.1055],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0805,  0.1181,  0.1055],\n",
      "        [ 0.0001, -0.0298, -0.0769,  ...,  0.0107,  0.0631,  0.0504],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0805,  0.1181,  0.1055],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0805,  0.1181,  0.1055],\n",
      "        [ 0.0900,  0.0317,  0.0689,  ...,  0.0427, -0.0067,  0.0771]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "1 21 1.6086289882659912\n",
      "tensor([[ 0.0299,  0.0174,  0.1097,  ..., -0.0806,  0.1181,  0.1055],\n",
      "        [ 0.0083, -0.1108,  0.0627,  ...,  0.0176,  0.0939, -0.0098],\n",
      "        [ 0.0083, -0.1108,  0.0627,  ...,  0.0176,  0.0939, -0.0098],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0806,  0.1181,  0.1055],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0806,  0.1181,  0.1055],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0806,  0.1181,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0174,  0.1097,  ..., -0.0806,  0.1181,  0.1056],\n",
      "        [-0.0983, -0.0523,  0.0559,  ...,  0.0224, -0.0645,  0.0968],\n",
      "        [-0.0983, -0.0523,  0.0559,  ...,  0.0224, -0.0645,  0.0968],\n",
      "        ...,\n",
      "        [ 0.0083, -0.1107,  0.0627,  ...,  0.0175,  0.0939, -0.0097],\n",
      "        [-0.0983, -0.0523,  0.0559,  ...,  0.0224, -0.0645,  0.0968],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0806,  0.1181,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0174,  0.1097,  ..., -0.0807,  0.1182,  0.1056],\n",
      "        [ 0.0002, -0.0300, -0.0769,  ...,  0.0107,  0.0631,  0.0505],\n",
      "        [ 0.0002, -0.0300, -0.0769,  ...,  0.0107,  0.0631,  0.0505],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0807,  0.1182,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0807,  0.1182,  0.1056],\n",
      "        [ 0.0901,  0.0316,  0.0689,  ...,  0.0427, -0.0068,  0.0770]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0174,  0.1097,  ..., -0.0807,  0.1182,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0807,  0.1182,  0.1056],\n",
      "        [ 0.0003, -0.0300, -0.0769,  ...,  0.0106,  0.0631,  0.0506],\n",
      "        ...,\n",
      "        [ 0.0901,  0.0316,  0.0689,  ...,  0.0427, -0.0069,  0.0771],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0807,  0.1182,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0807,  0.1182,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0174,  0.1097,  ..., -0.0808,  0.1182,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0808,  0.1182,  0.1056],\n",
      "        [-0.0116,  0.1073, -0.0120,  ...,  0.0794,  0.0513, -0.0860],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0808,  0.1182,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0808,  0.1182,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0808,  0.1182,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0986, -0.0523,  0.0560,  ...,  0.0223, -0.0644,  0.0966],\n",
      "        [ 0.0299,  0.0175,  0.1097,  ..., -0.0808,  0.1182,  0.1056],\n",
      "        [ 0.0299,  0.0175,  0.1097,  ..., -0.0808,  0.1182,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0175,  0.1097,  ..., -0.0808,  0.1182,  0.1056],\n",
      "        [ 0.0299,  0.0175,  0.1097,  ..., -0.0808,  0.1182,  0.1056],\n",
      "        [ 0.0299,  0.0175,  0.1097,  ..., -0.0808,  0.1182,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0082, -0.1106,  0.0628,  ...,  0.0174,  0.0939, -0.0098],\n",
      "        [-0.0987, -0.0523,  0.0560,  ...,  0.0223, -0.0644,  0.0966],\n",
      "        [ 0.0004, -0.0301, -0.0769,  ...,  0.0105,  0.0631,  0.0508],\n",
      "        ...,\n",
      "        [ 0.0004, -0.0301, -0.0769,  ...,  0.0105,  0.0631,  0.0508],\n",
      "        [ 0.0004, -0.0301, -0.0769,  ...,  0.0105,  0.0631,  0.0508],\n",
      "        [ 0.0004, -0.0301, -0.0769,  ...,  0.0105,  0.0631,  0.0508]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Validation loss: 17.010034324848547 RMSE: 4.1243224\n",
      "tensor([[ 0.0004, -0.0302, -0.0769,  ...,  0.0105,  0.0631,  0.0508],\n",
      "        [-0.0115,  0.1073, -0.0120,  ...,  0.0794,  0.0512, -0.0860],\n",
      "        [ 0.0299,  0.0175,  0.1096,  ..., -0.0809,  0.1182,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0082, -0.1106,  0.0628,  ...,  0.0174,  0.0939, -0.0097],\n",
      "        [ 0.0299,  0.0175,  0.1096,  ..., -0.0809,  0.1182,  0.1056],\n",
      "        [ 0.0299,  0.0175,  0.1096,  ..., -0.0809,  0.1182,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0175,  0.1096,  ..., -0.0809,  0.1181,  0.1056],\n",
      "        [ 0.0004, -0.0302, -0.0769,  ...,  0.0105,  0.0631,  0.0508],\n",
      "        [ 0.0004, -0.0302, -0.0769,  ...,  0.0105,  0.0631,  0.0508],\n",
      "        ...,\n",
      "        [-0.0989, -0.0523,  0.0560,  ...,  0.0224, -0.0644,  0.0966],\n",
      "        [ 0.0299,  0.0175,  0.1096,  ..., -0.0809,  0.1181,  0.1056],\n",
      "        [ 0.0299,  0.0175,  0.1096,  ..., -0.0809,  0.1181,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0175,  0.1096,  ..., -0.0810,  0.1181,  0.1056],\n",
      "        [ 0.0004, -0.0302, -0.0769,  ...,  0.0105,  0.0631,  0.0508],\n",
      "        [ 0.0004, -0.0302, -0.0769,  ...,  0.0105,  0.0631,  0.0508],\n",
      "        ...,\n",
      "        [-0.0989, -0.0523,  0.0560,  ...,  0.0225, -0.0644,  0.0966],\n",
      "        [ 0.0299,  0.0175,  0.1096,  ..., -0.0810,  0.1181,  0.1056],\n",
      "        [-0.0115,  0.1073, -0.0119,  ...,  0.0795,  0.0512, -0.0860]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0174,  0.1096,  ..., -0.0810,  0.1181,  0.1056],\n",
      "        [ 0.0004, -0.0303, -0.0770,  ...,  0.0104,  0.0630,  0.0508],\n",
      "        [ 0.0004, -0.0303, -0.0770,  ...,  0.0104,  0.0630,  0.0508],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0810,  0.1181,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0810,  0.1181,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0810,  0.1181,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056],\n",
      "        [-0.0115,  0.1073, -0.0118,  ...,  0.0795,  0.0512, -0.0859],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1096,  ..., -0.0811,  0.1180,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0300,  0.0174,  0.1096,  ..., -0.0811,  0.1179,  0.1056],\n",
      "        [-0.0993, -0.0524,  0.0558,  ...,  0.0228, -0.0645,  0.0966],\n",
      "        [ 0.0004, -0.0304, -0.0772,  ...,  0.0104,  0.0629,  0.0507],\n",
      "        ...,\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0811,  0.1179,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0811,  0.1179,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0811,  0.1179,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1179,  0.1056],\n",
      "        [-0.0115,  0.1073, -0.0117,  ...,  0.0795,  0.0512, -0.0859],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1179,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1179,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1179,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1179,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0994, -0.0524,  0.0558,  ...,  0.0229, -0.0646,  0.0966],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1179,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1179,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1179,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1179,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1179,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1178,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1178,  0.1056],\n",
      "        [ 0.0003, -0.0305, -0.0773,  ...,  0.0104,  0.0628,  0.0506],\n",
      "        ...,\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1178,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1178,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0812,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0995, -0.0524,  0.0558,  ...,  0.0229, -0.0646,  0.0966],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        ...,\n",
      "        [-0.0995, -0.0524,  0.0558,  ...,  0.0229, -0.0646,  0.0966],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0300,  0.0174,  0.1096,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0299,  0.0174,  0.1097,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0004, -0.0305, -0.0774,  ...,  0.0104,  0.0628,  0.0506],\n",
      "        [ 0.0004, -0.0305, -0.0774,  ...,  0.0104,  0.0628,  0.0506],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1097,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0165,  0.1185, -0.0105,  ...,  0.0828,  0.0629,  0.0487],\n",
      "        [ 0.0299,  0.0174,  0.1098,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0004, -0.0305, -0.0774,  ...,  0.0104,  0.0627,  0.0506],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0174,  0.1098,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1098,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1098,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0115,  0.1073, -0.0117,  ...,  0.0795,  0.0511, -0.0859],\n",
      "        [ 0.0299,  0.0174,  0.1098,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [-0.0997, -0.0524,  0.0558,  ...,  0.0230, -0.0647,  0.0966],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0174,  0.1098,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1098,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0299,  0.0174,  0.1098,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0298,  0.0175,  0.1099,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        ...,\n",
      "        [-0.0998, -0.0524,  0.0558,  ...,  0.0230, -0.0647,  0.0966],\n",
      "        [ 0.0234,  0.0589, -0.1091,  ...,  0.0714,  0.0753,  0.0714],\n",
      "        [ 0.0808, -0.0412, -0.0879,  ..., -0.0785, -0.0889, -0.1103]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0298,  0.0175,  0.1100,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1100,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0004, -0.0305, -0.0774,  ...,  0.0103,  0.0626,  0.0507],\n",
      "        ...,\n",
      "        [ 0.0298,  0.0175,  0.1100,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1100,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1100,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0298,  0.0175,  0.1100,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1100,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1100,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        ...,\n",
      "        [-0.0998, -0.0524,  0.0558,  ...,  0.0230, -0.0647,  0.0966],\n",
      "        [ 0.0298,  0.0175,  0.1100,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1100,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0005, -0.0305, -0.0774,  ...,  0.0103,  0.0626,  0.0507],\n",
      "        [ 0.0005, -0.0305, -0.0774,  ...,  0.0103,  0.0626,  0.0507],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0005, -0.0306, -0.0774,  ...,  0.0102,  0.0626,  0.0507],\n",
      "        [ 0.0005, -0.0306, -0.0774,  ...,  0.0102,  0.0626,  0.0507],\n",
      "        [ 0.0005, -0.0306, -0.0774,  ...,  0.0102,  0.0626,  0.0507],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.1000, -0.0523,  0.0557,  ...,  0.0230, -0.0646,  0.0964],\n",
      "        [ 0.0005, -0.0306, -0.0775,  ...,  0.0103,  0.0625,  0.0507],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0806, -0.0413, -0.0879,  ..., -0.0785, -0.0888, -0.1102],\n",
      "        [-0.1000, -0.0523,  0.0557,  ...,  0.0230, -0.0646,  0.0964],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0005, -0.0305, -0.0775,  ...,  0.0103,  0.0625,  0.0507],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0005, -0.0305, -0.0776,  ...,  0.0103,  0.0625,  0.0507],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0813,  0.1178,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [ 0.0005, -0.0305, -0.0776,  ...,  0.0103,  0.0625,  0.0507],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [-0.1001, -0.0524,  0.0557,  ...,  0.0231, -0.0646,  0.0963],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0112,  0.1071, -0.0116,  ...,  0.0795,  0.0511, -0.0860],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [ 0.0087, -0.1107,  0.0631,  ...,  0.0173,  0.0944, -0.0097],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [-0.1001, -0.0524,  0.0557,  ...,  0.0232, -0.0646,  0.0963],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.1001, -0.0524,  0.0556,  ...,  0.0232, -0.0645,  0.0963],\n",
      "        [-0.1001, -0.0524,  0.0556,  ...,  0.0232, -0.0645,  0.0963],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1102,  ..., -0.0812,  0.1177,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [ 0.0005, -0.0306, -0.0777,  ...,  0.0103,  0.0624,  0.0507],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [-0.1002, -0.0524,  0.0556,  ...,  0.0232, -0.0645,  0.0962],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1177,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.1002, -0.0524,  0.0556,  ...,  0.0233, -0.0645,  0.0962],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1177,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1177,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1176,  0.1056],\n",
      "        [ 0.0005, -0.0305, -0.0778,  ...,  0.0104,  0.0624,  0.0506],\n",
      "        [ 0.0089, -0.1107,  0.0631,  ...,  0.0172,  0.0945, -0.0097],\n",
      "        ...,\n",
      "        [-0.1002, -0.0524,  0.0556,  ...,  0.0233, -0.0645,  0.0962],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1176,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Validation loss: 3.044654910543324 RMSE: 1.744894\n",
      "tensor([[ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1176,  0.1056],\n",
      "        ...,\n",
      "        [-0.1002, -0.0523,  0.0556,  ...,  0.0233, -0.0645,  0.0962],\n",
      "        [ 0.0297,  0.0176,  0.1101,  ..., -0.0812,  0.1176,  0.1056],\n",
      "        [ 0.0089, -0.1107,  0.0632,  ...,  0.0171,  0.0946, -0.0098]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0005, -0.0306, -0.0779,  ...,  0.0104,  0.0623,  0.0507],\n",
      "        [ 0.0005, -0.0306, -0.0779,  ...,  0.0104,  0.0623,  0.0507],\n",
      "        [ 0.0297,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0005, -0.0306, -0.0779,  ...,  0.0105,  0.0622,  0.0507],\n",
      "        [ 0.0005, -0.0306, -0.0779,  ...,  0.0105,  0.0622,  0.0507],\n",
      "        [ 0.0005, -0.0306, -0.0779,  ...,  0.0105,  0.0622,  0.0507],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0297,  0.0175,  0.1100,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0175,  0.1100,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0175,  0.1100,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0175,  0.1100,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        [-0.1002, -0.0523,  0.0556,  ...,  0.0233, -0.0645,  0.0962],\n",
      "        [ 0.0297,  0.0175,  0.1100,  ..., -0.0811,  0.1176,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0297,  0.0175,  0.1099,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        [-0.1002, -0.0523,  0.0556,  ...,  0.0233, -0.0645,  0.0962],\n",
      "        [ 0.0005, -0.0306, -0.0779,  ...,  0.0105,  0.0621,  0.0507],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0175,  0.1099,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0175,  0.1099,  ..., -0.0811,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0175,  0.1099,  ..., -0.0811,  0.1176,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0113,  0.1071, -0.0114,  ...,  0.0795,  0.0511, -0.0859],\n",
      "        [ 0.0297,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [ 0.0005, -0.0306, -0.0779,  ...,  0.0105,  0.0621,  0.0507],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0297,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [ 0.0297,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [ 0.0005, -0.0306, -0.0779,  ...,  0.0105,  0.0621,  0.0507],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [-0.1003, -0.0524,  0.0556,  ...,  0.0234, -0.0646,  0.0962],\n",
      "        [ 0.0297,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0909,  0.0324,  0.0702,  ...,  0.0420, -0.0075,  0.0781],\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0298,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [-0.1003, -0.0525,  0.0556,  ...,  0.0234, -0.0645,  0.0961],\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        ...,\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056],\n",
      "        [ 0.0298,  0.0175,  0.1099,  ..., -0.0810,  0.1176,  0.1056]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0298,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1055],\n",
      "        ...,\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0811,  0.1176,  0.1055],\n",
      "        [-0.1004, -0.0525,  0.0556,  ...,  0.0234, -0.0645,  0.0961]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.1004, -0.0525,  0.0556,  ...,  0.0234, -0.0645,  0.0961],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1176,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1176,  0.1055],\n",
      "        ...,\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1176,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1176,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1176,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0005, -0.0305, -0.0779,  ...,  0.0106,  0.0620,  0.0506],\n",
      "        ...,\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        ...,\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [-0.1004, -0.0525,  0.0556,  ...,  0.0234, -0.0643,  0.0961],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        ...,\n",
      "        [ 0.0088, -0.1103,  0.0630,  ...,  0.0169,  0.0950, -0.0102],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "3 13 1.791797399520874\n",
      "tensor([[ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0239,  0.0583, -0.1088,  ...,  0.0713,  0.0755,  0.0716],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        ...,\n",
      "        [ 0.0089, -0.1103,  0.0630,  ...,  0.0169,  0.0951, -0.0102],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1055]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0910,  0.0325,  0.0701,  ...,  0.0420, -0.0072,  0.0780],\n",
      "        [ 0.0005, -0.0306, -0.0780,  ...,  0.0108,  0.0618,  0.0505],\n",
      "        [ 0.0005, -0.0306, -0.0780,  ...,  0.0108,  0.0618,  0.0505],\n",
      "        ...,\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1054],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1054],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1175,  0.1054]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0111,  0.1072, -0.0113,  ...,  0.0795,  0.0512, -0.0859],\n",
      "        [-0.1005, -0.0524,  0.0556,  ...,  0.0235, -0.0642,  0.0960],\n",
      "        [ 0.0298,  0.0176,  0.1101,  ..., -0.0810,  0.1174,  0.1054],\n",
      "        ...,\n",
      "        [ 0.0298,  0.0176,  0.1101,  ..., -0.0810,  0.1174,  0.1054],\n",
      "        [ 0.0298,  0.0176,  0.1101,  ..., -0.0810,  0.1174,  0.1054],\n",
      "        [ 0.0298,  0.0176,  0.1101,  ..., -0.0810,  0.1174,  0.1054]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1174,  0.1054],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1174,  0.1054],\n",
      "        [ 0.0005, -0.0306, -0.0780,  ...,  0.0108,  0.0617,  0.0505],\n",
      "        ...,\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1174,  0.1054],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1174,  0.1054],\n",
      "        [ 0.0298,  0.0176,  0.1100,  ..., -0.0810,  0.1174,  0.1054]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n"
     ]
    }
   ],
   "source": [
    "seeds = [range(777,782)]\n",
    "for seed in seeds:\n",
    "    !python finetuneRecon.py \\\n",
    "    --task_name bbbp \\\n",
    "    --splitting scaffold \\\n",
    "    --seed {seed}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ce4ecd8-eaee-4c19-8a8d-195f56a1af16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\n",
      "{'batch_size': 32, 'epochs': 1, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_gin', 'log_every_n_steps': 50, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:0', 'task_name': 'freesolv', 'model_type': 'gin', 'model': {'num_layer': 5, 'emb_dim': 300, 'feat_dim': 300, 'drop_ratio': 0.3, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold', 'seed': 750, 'task': 'regression', 'data_path': 'data/freesolv/freesolv.csv', 'target': ['expt']}}\n",
      "Running on: cuda:0\n",
      "641\n",
      "About to generate scaffolds\n",
      "Generating scaffold 0/641\n",
      "About to sort in scaffold sets\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 43.755775451660156\n",
      "Validation loss: 49.42946243286133 RMSE: 7.030609\n",
      "Loaded trained model with success.\n",
      "Test loss: 25.113933739295373 Test RMSE: 5.01138\n"
     ]
    }
   ],
   "source": [
    "!python finetuneReconOrigin.py \\\n",
    "--task_name freesolv \\\n",
    "--splitting scaffold \\\n",
    "--seed 750 \\\n",
    "--random_masking 1 \\\n",
    "--epochs 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7db26113-9d29-4cf4-a5bb-8127937e6404",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "N = 5  # 노드의 수\n",
    "x = torch.randint(0, 10, (N, 2))  # 랜덤 인덱스 정보\n",
    "\n",
    "# 임베딩 레이어 생성\n",
    "embedding_dim = 4\n",
    "num_embeddings = 10  # 임베딩을 위한 최대 인덱스 수\n",
    "emb_layer1 = torch.nn.Embedding(num_embeddings=num_embeddings, embedding_dim=embedding_dim)\n",
    "emb_layer2 = torch.nn.Embedding(num_embeddings=num_embeddings, embedding_dim=embedding_dim)\n",
    "\n",
    "# 임베딩 적용\n",
    "feature_0_mask = emb_layer1(x[:,0].long())\n",
    "feature_1_mask = emb_layer2(x[:,1].long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42df157f-7f6b-49db-ba7b-c5b120ad86ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_0_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "802695d2-7b38-4607-81d1-75ac8c742aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_node = [0, 3]\n",
    "mask_node = torch.tensor(mask_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9a1f770-d251-4d0a-b9d5-6de9b1bb6fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.4629, -1.1094, -2.0033,  0.6418],\n",
      "        [ 0.6016,  0.6937, -1.5951, -0.5150],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.8669,  1.0128,  0.4207,  0.1260]], grad_fn=<IndexPutBackward0>)\n"
     ]
    }
   ],
   "source": [
    "feature_0_mask[mask_node] = torch.zeros(embedding_dim)\n",
    "feature_1_mask[mask_node] = torch.zeros(embedding_dim)\n",
    "\n",
    "print(feature_0_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503a1c54-4873-4d00-bbc8-c26b4c8b966b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
